<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 研究探索了在真实世界环境中人们对机器人错误的社交反应，发现即使在非社交机器人重复和群体交互中，参与者仍会表达丰富但"嘈杂"的社交信号


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人经常出错，但人们对这些错误的社交反应在实验室外环境中的表现尚不清楚。先前研究表明社交信号在受限交互中对错误管理可靠有用，但不确定这在真实世界、非社交机器人、重复和群体交互以及连续或传播错误的情况下是否成立。

Method: 构建了一个咖啡机器人并进行公共现场部署（N=49），观察参与者在真实世界环境中对机器人错误和其他刺激的社交反应，特别关注群体交互场景。

Result: 参与者对错误和其他刺激持续表达多样化的社交信号，尤其在群体交互中更为明显。研究发现真实世界中的社交信号丰富（参与者自愿提供交互信息）但"嘈杂"。

Conclusion: 讨论了在真实世界人机交互中使用社交信号的教训、益处和挑战，表明即使在非社交机器人场景中，社交信号仍然是理解用户反应的重要信息来源。

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [2] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: 提出一种可微分神经图形模型，结合神经基础模型与基于物理的可微分渲染，实现零样本场景重建和机器人抓取，无需额外3D数据或测试时采样。


<details>
  <summary>Details</summary>
Motivation: 当前最先进模型依赖大量训练数据和测试时采样来构建黑盒场景表示，这限制了机器人在新环境中的操作效率。需要一种更数据高效、可解释且泛化能力强的自主系统。

Method: 开发可微分神经图形模型，结合神经基础模型与物理可微分渲染。通过约束优化问题估计物理一致的场景参数（网格、光照条件、材质属性、6D位姿），仅需单张RGBD图像和边界框。

Result: 在标准模型无关少样本基准测试中优于现有算法；在零样本抓取任务中验证了场景重建的准确性；实现了无需大量数据集或测试时采样的物理一致场景重建。

Conclusion: 该方法为实现更数据高效、可解释且泛化能力强的机器人自主系统提供了途径，能够在全新环境中进行零样本、物理一致的场景重建和抓取。

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [3] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: 提出神经符号特征表示和软一阶逻辑奖励函数，提升自动驾驶强化学习的场景理解和安全决策能力


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的深度强化学习方法存在两个问题：很少将高层场景上下文整合到特征表示中，以及依赖僵化的固定奖励函数。这限制了自动驾驶系统对复杂交通环境的理解能力和安全决策水平。

Method: 提出神经符号特征表示管道，包含语义、空间和形状信息，以及场景中动态实体的空间增强特征，特别关注安全关键的道路使用者。同时提出软一阶逻辑（SFOL）奖励函数，通过符号推理模块平衡人类价值观。从分割图中提取语义和空间谓词，并应用于语言规则以获得奖励权重。

Result: 在CARLA仿真环境中的定量实验表明，与基线表示和奖励公式相比，所提出的神经符号表示和SFOL奖励函数在不同交通密度和遮挡水平下提高了策略鲁棒性和安全相关性能指标。

Conclusion: 将整体表示和软推理整合到强化学习中，可以支持自动驾驶中更具上下文感知和价值观对齐的决策制定，为更安全、更智能的自动驾驶系统提供了有前景的方向。

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


### [4] [A Framework for Combining Optimization-Based and Analytic Inverse Kinematics](https://arxiv.org/abs/2602.05092)
*Thomas Cohn,Lihan Tang,Alexandre Amice,Russ Tedrake*

Main category: cs.RO

TL;DR: 提出一种新的优化逆运动学公式，使用解析IK解作为变量变换，使优化器更容易求解，在避障、抓取选择和人形机器人稳定性等挑战性IK问题上获得更高成功率


<details>
  <summary>Details</summary>
Motivation: 解析方法和优化方法在解决逆运动学问题上各有优缺点，但开发统一方法具有挑战性。优化方法面临的主要挑战是关节角度与末端执行器姿态之间的复杂非线性关系，特别是在处理避障等非凸约束时，优化IK算法可能失败率较高

Method: 提出新的优化IK公式，使用解析IK解作为变量变换，将复杂的非线性关系转化为更容易优化的形式。该方法在三种流行的约束非线性优化求解器上进行了测试，代表了三种不同的优化范式

Result: 广泛的实验比较表明，新公式在避障、抓取选择和人形机器人稳定性等各种挑战性IK问题上，比旧公式和基线方法获得更高的成功率

Conclusion: 通过使用解析IK解作为变量变换的新优化IK公式，能够有效结合解析方法和优化方法的优势，显著提高复杂约束条件下逆运动学问题的求解成功率

Abstract: Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.

</details>


### [5] [PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation](https://arxiv.org/abs/2602.05156)
*Dong Ho Kang,Aaron Kim,Mingyo Seo,Kazuto Yokoyama,Tetsuya Narita,Luis Sentis*

Main category: cs.RO

TL;DR: PLATO Hand是一种灵巧机器人手，采用混合指尖设计，将刚性指甲嵌入柔性指腹中，通过结构化接触几何实现多种交互模式，提升操作精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人手在精确操作任务中面临接触行为控制困难，特别是在边缘敏感操作（如纸张分离、卡片拾取、橙子剥皮）中需要同时保持局部压痕和抑制全局弯曲。

Method: 提出混合指尖设计：刚性指甲嵌入柔性指腹中；开发基于应变能的弯曲-压痕模型指导设计；通过引导接触实现局部压痕保持和全局弯曲抑制；构建力-运动透明机制。

Result: 实验结果显示：改进的夹持稳定性、增强的力可观测性、成功执行边缘敏感操作任务（纸张分离、卡片拾取、橙子剥皮）。

Conclusion: 结构化接触几何与力-运动透明机制的结合为精确操作提供了基于物理原理的实现方法，展示了混合指尖设计在灵巧操作中的优势。

Abstract: We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

</details>


### [6] [Informative Path Planning with Guaranteed Estimation Uncertainty](https://arxiv.org/abs/2602.05198)
*Kalvik Jakkala,Saurav Agarwal,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: 该论文提出了一种具有保证估计不确定性的信息路径规划方法，通过最短路径确保高斯过程后验方差在监测区域内低于用户指定阈值。


<details>
  <summary>Details</summary>
Motivation: 环境监测机器人需要在距离和能量约束下重建空间场。传统的割草式调查提供几何覆盖保证但会过度采样可预测区域，而信息路径规划方法虽然利用空间相关性减少过度采样，但通常无法保证重建质量。

Method: 提出三阶段方法：(1) 从先验信息学习高斯过程模型；(2) 将学习的GP核转换为每个候选传感位置的二进制覆盖图，指示哪些位置的不确定性可以降低到指定目标以下；(3) 规划接近最短的路径，其组合覆盖满足全局不确定性约束。针对异构现象，采用非平稳核捕捉空间变化的关联结构，并适应具有障碍物的非凸环境。

Result: 在真实地形数据上的实验表明，该规划器使用比现有基线更少的传感位置和更短的旅行距离就能达到不确定性目标。通过测深测绘自主水面和水下车辆的现场实验证明了现实世界的可行性。

Conclusion: 该方法成功桥接了传统几何覆盖和信息路径规划方法，提供了具有保证估计不确定性的路径规划解决方案，在环境监测应用中具有实际可行性和效率优势。

Abstract: Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.
  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.

</details>


### [7] [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233)
*Wenbo Wang,Fangyun Wei,QiXiu Li,Xi Chen,Yaobo Liang,Chang Xu,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: 提出MobileManiBench基准测试，用于移动机器人操作的仿真验证，包含大规模数据集和多样化场景，加速VLA模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型依赖静态桌面场景的遥操作数据集，缺乏移动机器人操作的验证框架，需要可控、可扩展的仿真环境来评估模型性能。

Method: 基于NVIDIA Isaac Sim构建仿真优先框架，使用强化学习自动生成多样化操作轨迹，包含丰富的标注数据（语言指令、多视角RGB-D分割图像、同步状态和动作）。

Result: 创建了包含2个移动平台、2个同步摄像头、630个物体、20个类别、5种技能、100个任务、100个场景、30万条轨迹的大规模基准测试。

Conclusion: MobileManiBench为机器人本体、感知模态和策略架构提供了可控研究平台，加速了数据效率和泛化能力研究，为复杂仿真环境中的VLA模型评估提供了标准。

Abstract: Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.

</details>


### [8] [Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265)
*Kalvik Jakkala,Jason O'Kane*

Main category: cs.RO

TL;DR: 该论文提出了一种用于水下管道自主巡检的极简传感策略，仅使用IMU、压力传感器和两个声纳，无需复杂传感器阵列即可实现机器人在管道中的自主居中导航。


<details>
  <summary>Details</summary>
Motivation: 水下管道巡检面临几何空间受限、水体浑浊、定位线索稀缺等挑战，传统方法需要复杂的传感器阵列或外部跟踪系统，成本高且不实用。

Method: 采用IMU、压力传感器、向下单波束声纳和360度旋转声纳的极简传感器组合；提出从单波束声纳强度数据中提取距离估计的计算高效方法；建立闭式几何模型利用两个声纳距离估计管道中心；设计自适应置信度加权PD控制器保持居中。

Result: 在直径46厘米的淹没管道中使用BlueROV2重型ROV进行实验，实现了稳定的居中控制和完整的管道穿越，即使在环境水流和结构变形条件下也能成功工作。

Conclusion: 研究表明，通过轻量级、计算高效的传感和处理架构，可以在受限环境中实现可靠的水下管道导航和检查，推进了自主水下检查的实际应用性。

Abstract: Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.

</details>


### [9] [Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions](https://arxiv.org/abs/2602.05273)
*Hengxuan Xu,Fengbo Lan,Zhixin Zhao,Shengjie Wang,Mengqiao Liu,Jieqian Sun,Yu Cheng,Tao Zhang*

Main category: cs.RO

TL;DR: AIDE是一个双流框架，通过交互式探索与视觉语言推理结合，解决机器人在模糊指令下识别任务相关物体的挑战，实现零样本可供性分析和实时任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的方法在模糊人类指令下（如"我渴了"需要识别杯子或饮料）存在推理效率低和缺乏环境交互的问题，阻碍了实时任务规划与执行。

Method: 提出AIDE双流框架：多阶段推理（MSI）作为决策流进行交互式探索和视觉语言推理，加速决策（ADM）作为执行流，实现零样本可供性分析和模糊指令解释。

Result: 在仿真和真实环境中的大量实验显示，AIDE任务规划成功率超过80%，闭环连续执行准确率超过95%（10Hz频率），在多样化开放世界场景中优于现有VLM方法。

Conclusion: AIDE通过集成交互式探索与视觉语言推理，有效解决了模糊指令下的物体识别和任务执行问题，为机器人在陌生环境中的实时决策和执行提供了可行方案。

Abstract: Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for "I'm thirsty") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\% and more than 95\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

</details>


### [10] [Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310)
*Jipeng Kong,Xinzhe Liu,Yuhang Lin,Jinrui Han,Sören Schwertfeger,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: PAiD提出了一种渐进式架构，将人形机器人足球技能学习分解为三个阶段：通过人体运动跟踪获取运动技能、轻量级感知-动作集成实现位置泛化、物理感知的仿真到现实迁移，解决了现有方法中模块间不稳定或训练目标冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 足球对人形机器人提出了重大挑战，需要紧密集成的感知-动作能力。现有方法存在模块化管道中的模块间不稳定性问题，或端到端框架中的训练目标冲突问题。

Method: 提出感知-动作集成决策（PAiD）架构，采用分阶段方法：1）通过人体运动跟踪获取运动技能；2）轻量级感知-动作集成实现位置泛化；3）物理感知的仿真到现实迁移。

Result: 在Unitree G1机器人上实验显示，能够实现高保真度的人形踢球动作，在静态或滚动球、不同位置和干扰等多样化条件下表现稳健，在室内和室外场景中保持一致的执行效果。

Conclusion: 这种分而治之的策略推进了稳健的人形机器人足球能力，并为复杂具身技能获取提供了一个可扩展的框架。

Abstract: Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.

</details>


### [11] [RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325)
*Jiacheng Fan,Zhiyue Zhao,Yiqian Zhang,Chao Chen,Peide Wang,Hengdi Zhang,Zhengxue Cheng*

Main category: cs.RO

TL;DR: 提出Real-Sim-Real数据管道，通过人类演示生成机器人可执行训练数据，无需直接遥操作，用于灵巧操作任务


<details>
  <summary>Details</summary>
Motivation: 获取大规模高质量机器人演示数据是扩展视觉-语言-动作模型在灵巧操作中应用的关键瓶颈，需要替代遥操作的成本效益方案

Method: 建立标准化数据采集室收集多模态人类演示，提出触觉感知重定向方法将人手状态映射到机器人灵巧手状态，在Isaac Sim中渲染生成训练数据

Result: 重定向的灵巧手轨迹在10个不同物体操作任务中达到84%成功率；基于生成数据训练的VLA策略在三个代表性任务中达到80%平均成功率

Conclusion: 通过Real-Sim-Real数据管道可以从人类演示中高效"绘制"机器人训练数据，为复杂灵巧操作提供可扩展、成本效益高的遥操作替代方案

Abstract: Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently "painted" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.

</details>


### [12] [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441)
*Dean Fortier,Timothy Adamson,Tess Hellebrekers,Teresa LaScala,Kofi Ennin,Michael Murray,Andrey Kolobov,Galen Mullins*

Main category: cs.RO

TL;DR: BusyBox是一个用于系统评估VLA模型物理操作泛化能力的基准测试平台，包含6个可互换模块，挑战现有VLA模型在未见物理配置下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型在视觉和语言空间泛化方面取得进展，但物理操作泛化能力（affordance generalization）——即操纵具有熟悉物理特征的新物体的能力——是VLA需要掌握的关键元技能，目前缺乏系统评估方法。

Method: 设计了BusyBox物理基准测试平台，包含开关、滑块、电线、按钮、显示屏和旋钮6个模块，这些模块可互换和旋转以创建多种视觉外观不同但操作功能相同的变体。平台易于在机器人实验室构建，并发布了3D打印CAD文件、材料清单和演示数据集。

Result: 实验表明，即使对于强大的开源VLA模型如π₀.₅和GR00T-N1.6，在BusyBox变体间的泛化仍然极具挑战性，突显了物理操作泛化问题的难度。

Conclusion: BusyBox为研究社区提供了一个标准化、可复现的物理操作泛化评估平台，通过开源硬件设计和数据集，促进VLA模型在物理操作泛化能力方面的研究进展。

Abstract: Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.
  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.

</details>


### [13] [Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456)
*Maksym Figat,Ryan M. Mackey,Michel D. Ingham*

Main category: cs.RO

TL;DR: RSTM2方法通过本体驱动的层次化随机时间Petri网，将高层任务目标转化为可执行的形式化规范，支持复杂机器人系统的架构权衡、资源分配和不确定性下的性能分析。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键和任务关键机器人应用中高层目标与可执行形式化规范之间的鸿沟，为复杂多机器人系统（如NASA CADRE任务）提供系统工程技术支持。

Method: 提出RSTM2方法，采用本体驱动的层次化方法，使用带资源的随机时间Petri网，支持任务、系统和子系统级别的蒙特卡洛仿真。

Result: 通过假设案例研究展示了RSTM2方法如何支持架构权衡、资源分配和不确定性下的性能分析，本体概念还支持可解释的AI助手，实现完全自主的规范合成。

Conclusion: 该方法特别适用于复杂的多机器人系统，代表了未来去中心化、资源感知和自适应自主系统的发展方向。

Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

</details>


### [14] [TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation](https://arxiv.org/abs/2602.05468)
*Pranav Ponnivalavan,Satoshi Funabashi,Alexander Schmitz,Tetsuya Ogata,Shigeki Sugano*

Main category: cs.RO

TL;DR: 论文提出TaSA框架，通过两阶段深度预测学习解决机器人灵巧操作中的自接触感知问题，显著提升插入任务的完成率。


<details>
  <summary>Details</summary>
Motivation: 人类能够进行复杂的多指操作，但机器人灵巧操作面临自接触与外部接触难以区分的挑战。现有方法大多回避自接触问题，限制了实际应用。受人类通过感觉衰减机制区分自接触信号的启发，需要开发能够区分自接触与外部接触的机器人感知框架。

Method: 提出TaSA两阶段深度预测学习框架：第一阶段学习自接触动力学，建模机器人自身动作如何产生触觉反馈；第二阶段将学习到的模型融入运动学习阶段，强调操作过程中的物体接触信号。

Result: 在需要精细触觉分辨的插入任务（铅笔芯插入自动铅笔、硬币插入插槽、回形针固定在纸上）中，使用TaSA训练的策略在所有任务上都显著优于基线方法，成功率高得多。

Conclusion: 基于感觉衰减的结构化触觉感知对于机器人灵巧操作至关重要，TaSA框架通过明确学习自接触动力学并强调物体接触信号，有效解决了自接触与外部接触的区分问题。

Abstract: Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.

</details>


### [15] [DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter](https://arxiv.org/abs/2602.05513)
*Xukun Li,Yu Sun,Lei Zhang,Bosheng Huang,Yibo Peng,Yuan Meng,Haojun Jiang,Shaoxuan Xie,Guacai Yao,Alois Knoll,Zhenshan Bing,Xinlong Wang,Zhenguo Sun*

Main category: cs.RO

TL;DR: DECO是一个基于DiT的策略框架，通过解耦多模态条件处理实现灵巧操作，并附带包含50小时数据的DECO-50双手机器人数据集


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态条件（图像、动作、本体感知、触觉信号）在机器人灵巧操作中的有效整合问题，提出解耦多模态条件的策略框架

Method: 使用DiT-based策略，通过联合自注意力处理图像和动作token，自适应层归一化注入本体感知状态，交叉注意力注入触觉信号，并采用轻量级LoRA适配器进行高效微调

Result: 开发了DECO-50数据集，包含4个场景、28个子任务，超过50小时数据、约500万帧和8000个成功轨迹的双手机器人灵巧操作数据

Conclusion: DECO框架通过解耦多模态条件处理，结合高效微调机制和大规模数据集，为机器人灵巧操作提供了有效的解决方案

Abstract: Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.

</details>


### [16] [Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments](https://arxiv.org/abs/2602.05516)
*Runxiao Liu,Pengda Mao,Xiangli Le,Shuang Gu,Yapeng Chen,Quan Quan*

Main category: cs.RO

TL;DR: 提出基于虚拟管道理论和耗散系统理论的多无人机协同运输控制框架，适用于受限环境中的电缆悬挂负载运输


<details>
  <summary>Details</summary>
Motivation: 解决多无人机在受限环境中协同运输电缆悬挂负载的挑战，需要高效协调、避障和系统稳定性

Method: 结合虚拟管道理论和耗散系统理论，实现低计算开销的张力分配和协调运输，根据障碍物布局动态调整无人机配置

Result: 通过大量仿真验证了方法的有效性，展示了大规模多无人机系统的可扩展性，并在室外场景中实验验证了实际可行性和鲁棒性

Conclusion: 提出的控制框架能够实现高效、稳定、鲁棒的多无人机协同运输，适用于复杂受限环境，具有实际应用价值

Abstract: This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.

</details>


### [17] [VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator](https://arxiv.org/abs/2602.05552)
*Bessie Dominguez-Dager,Sergio Suescun-Ferrandiz,Felix Escalona,Francisco Gomez-Donoso,Miguel Cazorla*

Main category: cs.RO

TL;DR: VLN-Pilot是一个新颖框架，利用大型视觉语言模型作为室内无人机导航的"飞行员"，通过自然语言指令和视觉感知实现自主导航，无需GPS或传统路径规划方法。


<details>
  <summary>Details</summary>
Motivation: 传统室内无人机导航依赖基于规则或几何的路径规划方法，需要大量任务特定工程。该研究旨在利用VLLM的多模态推理能力，实现更自然、语义感知的无人机控制，减少操作员工作量并提高安全性。

Method: VLN-Pilot框架将大型视觉语言模型作为核心"飞行员"，模型能够解释自由形式的自然语言指令，并将其与视觉观察相结合，规划并执行无人机轨迹。框架集成了语言驱动的语义理解和视觉感知，支持空间关系推理、障碍物避让和对意外事件的动态响应。

Result: 在定制的逼真室内仿真基准测试中，VLLM驱动的智能体在复杂指令跟随任务上取得了高成功率，包括具有多个语义目标的长期导航。结果表明语言引导的自主智能体有望替代远程无人机飞行员。

Conclusion: VLN-Pilot展示了VLLM在室内无人机导航中的潜力，能够显著减少操作员工作量，同时提高安全性和任务灵活性，为检查、搜救、设施监控等任务提供了可扩展、人性化的室内无人机控制方案。

Abstract: This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

</details>


### [18] [TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards](https://arxiv.org/abs/2602.05596)
*Hokyun Lee,Woo-Jeong Baek,Junhyeok Cha,Jaeheung Park*

Main category: cs.RO

TL;DR: TOLEBI是一个用于双足机器人步态的故障容忍学习框架，通过模拟关节锁定、电源故障和外部干扰来学习容错步态策略，并包含在线关节状态监测模块实现实时故障分类。


<details>
  <summary>Details</summary>
Motivation: 随着学习算法在机器人应用中的广泛使用，双足机器人步态的强化学习研究已成为人形机器人领域的核心课题。虽然已有研究在步态任务中取得了高成功率，但很少有方法能够处理步态过程中可能发生的硬件故障。在实际环境中，环境干扰或突发硬件故障可能导致严重后果。

Method: 提出TOLEBI框架，在模拟中注入关节锁定、电源故障和外部干扰来学习容错步态策略。除了通过模拟到现实的迁移将学习策略转移到真实机器人外，还集成了在线关节状态模块，该模块能够根据实际观测在运行时对关节状态进行分类。

Result: 在人形机器人TOCABI上进行的真实世界和模拟验证实验证明了该方法的适用性。据作者所知，这是第一个基于学习的双足机器人步态容错框架。

Conclusion: 该论文提供了第一个基于学习的双足机器人步态容错框架，促进了该领域高效学习方法的发展。

Abstract: With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.

</details>


### [19] [HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments](https://arxiv.org/abs/2602.05608)
*Yufei Zhu,Shih-Min Yang,Martin Magnusson,Allan Wang*

Main category: cs.RO

TL;DR: HiCrowd：分层框架结合强化学习和模型预测控制，利用行人运动作为引导，解决机器人在密集人群中的冻结问题，提高导航效率和安全


<details>
  <summary>Details</summary>
Motivation: 解决机器人在密集人群中的"冻结机器人问题"——机器人因找不到安全运动路径而被困在人群中。现有方法通常将行人视为动态障碍物，而HiCrowd提出利用行人运动作为引导的新思路

Method: 分层框架：高层RL策略生成跟随点，使机器人与合适的行人群体对齐；低层MPC安全跟踪引导点，进行短期规划。结合长期人群感知决策和安全的短期执行

Result: 在真实世界数据集和合成人群数据集上评估，相比反应式和基于学习的基线方法，在离线（回放记录轨迹）和在线（模拟中行人反应）设置下，导航效率和安全性能更优，减少冻结行为

Conclusion: 将人类运动作为引导而非仅视为动态障碍物，为机器人在人群中的安全高效导航提供了有力原则。HiCrowd框架通过分层设计成功解决了冻结机器人问题

Abstract: Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.

</details>


### [20] [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](https://arxiv.org/abs/2602.05683)
*Chuwei Wang,Eduardo Sebastián,Amanda Prorok,Anastasia Bizyaeva*

Main category: cs.RO

TL;DR: 提出了一种简约的神经形态控制框架，用于视觉引导导航和跟踪，通过动态神经元群体将视觉目标兴奋直接转化为自我中心运动命令，利用动态分叉机制解决目标对称性导致的决策困难。


<details>
  <summary>Details</summary>
Motivation: 机器人导航历史上难以协调反应式传感器控制与基于模型规划器的决定性能力。当目标间缺乏主导选项导致决策困难时，这种二元性变得尤为关键，挑战反应式系统在不依赖计算密集型规划器的情况下打破对称性。

Method: 提出简约神经形态控制框架：将机载相机图像像素编码为动态神经元群体的输入，直接将视觉目标兴奋转化为自我中心运动命令。采用动态分叉机制，延迟决策直到环境几何诱导的临界点出现。受动物认知和意见动态的机制模型启发，该控制器具有最小计算负担、少量可解释参数，并能无缝集成到特定应用的图像处理流程中。

Result: 在仿真环境和实验四旋翼平台上验证了该方法的有效性，实现了实时自主性。

Conclusion: 该神经形态控制框架成功弥合了反应式控制与基于模型规划之间的差距，为视觉引导导航和跟踪提供了一种计算高效、参数可解释的解决方案，能够有效处理目标对称性导致的决策困难。

Abstract: Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

</details>


### [21] [Scalable and General Whole-Body Control for Cross-Humanoid Locomotion](https://arxiv.org/abs/2602.05791)
*Yufei Xue,YunFeng Lin,Wentao Dong,Yang Tang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Minghuan Liu,Weinan Zhang*

Main category: cs.RO

TL;DR: 提出XHugWBC框架，实现单策略跨多种人形机器人设计的通用控制，无需针对特定机器人训练


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的人形机器人全身控制器大多需要针对特定机器人进行训练，缺乏通用性。研究跨具身人形控制问题，探索如何通过一次性训练实现单策略在多种人形机器人设计上的鲁棒泛化。

Method: 提出XHugWBC框架，包含三个关键技术：(1)物理一致的形态随机化；(2)跨多样人形机器人的语义对齐观测和动作空间；(3)建模形态和动力学特性的有效策略架构。该框架不绑定特定机器人，而是在训练中内化广泛的形态和动力学特征分布。

Result: 在12个模拟人形机器人和7个真实机器人上的实验证明，该通用控制器具有强大的泛化能力和鲁棒性。通过从多样随机化具身中学习运动先验，策略获得了支持零样本迁移到未见机器人的强结构偏置。

Conclusion: XHugWBC框架成功实现了跨具身人形控制，通过一次性训练即可获得适用于多种人形机器人设计的通用控制器，为机器人控制领域提供了新的通用化解决方案。

Abstract: Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.

</details>


### [22] [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855)
*Dennis Bank,Joost Cordes,Thomas Seel,Simon F. G. Ehlers*

Main category: cs.RO

TL;DR: 提出基于学习的框架，使用机器人中心高度图表示，通过混合编码器-解码器结构融合多模态传感器数据，提升地形感知精度和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化、以人为中心的环境中部署人形机器人需要可靠的地形感知。传统系统通常依赖手动设计的单传感器管道，存在局限性。

Method: 采用基于学习的框架，使用机器人中心高度图表示。提出混合编码器-解码器结构，结合CNN进行空间特征提取和GRU核心确保时序一致性。融合Intel RealSense深度相机、LIVOX MID-360 LiDAR（通过高效球面投影处理）和板载IMU的多模态数据。

Result: 多模态融合比纯深度配置提高7.2%的重建精度，比纯LiDAR配置提高9.9%。集成3.2秒时序上下文减少了建图漂移。

Conclusion: 提出的学习型多模态融合框架显著提升了人形机器人在非结构化环境中的地形感知能力，优于传统单传感器方法。

Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.

</details>


### [23] [Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools](https://arxiv.org/abs/2602.05895)
*Qi Li,Karsten Berns*

Main category: cs.RO

TL;DR: 本文提出了一种残差强化学习方法，用于城市环境中垃圾容器回收任务的容器提升阶段，通过结合名义笛卡尔控制器和学习的残差策略，在液压装载起重机上实现精确轨迹跟踪和摆动抑制。


<details>
  <summary>Details</summary>
Motivation: 城市环境中垃圾容器回收任务的容器提升阶段存在严格几何公差要求，相对于起重机整体规模，卸料单元挂钩与容器环之间的间隙很小，需要精确的轨迹跟踪和摆动抑制。传统控制器难以处理未建模动力学和参数变化。

Method: 提出残差强化学习方法：1) 名义控制器使用导纳控制进行轨迹跟踪和摆锤感知的摆动阻尼，然后通过阻尼最小二乘逆运动学生成关节速度命令；2) 在Isaac Lab中使用PPO训练的残差策略补偿未建模动力学和参数变化；3) 采用随机化初始化和领域随机化（载荷属性、执行器增益、被动关节参数）增强泛化能力。

Result: 仿真结果表明，与单独使用名义控制器相比，该方法提高了跟踪精度、减少了振荡、并获得了更高的提升成功率。

Conclusion: 残差强化学习方法能够有效补偿未建模动力学和参数变化，提高液压装载起重机在垃圾容器回收任务中的精度和鲁棒性，无需从头开始的端到端学习。

Abstract: This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.

</details>


### [24] [From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits](https://arxiv.org/abs/2602.05922)
*Aziz Mohamed Mili,Louis Catar,Paul Gérard,Ilyass Tabiai,David St-Onge*

Main category: cs.RO

TL;DR: 开发了一个端到端的开源工具链，可将台式冲击测试转化为可部署的无人机安全控制器，通过数据驱动模型计算速度限制，确保无人机在室内接近人类时满足安全约束。


<details>
  <summary>Details</summary>
Motivation: 室内微型飞行器（MAV）越来越多地用于需要接近人员的任务，但从业者缺乏基于实测冲击风险来调整运动限制的实用方法。需要建立从冲击测试到运行时限制的实用桥梁。

Method: 1. 设计紧凑可复制的冲击测试装置和协议，捕获不同无人机类别和接触表面的力-时间曲线；2. 开发数据驱动模型，将冲击前速度映射到冲量和接触持续时间，直接计算目标力限制下的速度边界；3. 发布脚本和ROS2节点，在线执行这些边界并记录合规性，支持特定设施策略。

Result: 在多个商用现成四旋翼无人机和代表性室内资产上验证了工作流程，证明推导出的控制器在满足安全利益相关者指定的力约束的同时，保持了任务吞吐量。

Conclusion: 提供了一个实用的桥梁，将实测冲击转化为运行时限制，包含可共享的数据集、代码和可重复的过程，团队可以采用这些来认证室内MAV在人类附近的操作。

Abstract: Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.

</details>


### [25] [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001)
*Carolina Higuera,Sergio Arnaud,Byron Boots,Mustafa Mukadam,Francois Robert Hogan,Franziska Meier*

Main category: cs.RO

TL;DR: VT-WM通过结合视觉和触觉感知，构建多任务视觉-触觉世界模型，在接触丰富的机器人操作任务中提升物理保真度和规划性能


<details>
  <summary>Details</summary>
Motivation: 纯视觉模型在遮挡或模糊接触状态下容易出现物体消失、瞬移或违反物理规律等失败模式，需要结合触觉感知来更好地理解机器人-物体接触交互

Method: 开发多任务视觉-触觉世界模型(VT-WM)，通过触觉推理捕捉接触物理特性，在接触丰富的操作任务上进行训练

Result: VT-WM在自回归推演中物体持久性提升33%，运动规律符合度提升29%；零样本真实机器人实验中成功率提升高达35%；能够有效适应新任务

Conclusion: 结合视觉和触觉感知的世界模型显著提升了接触丰富任务中的物理保真度和规划性能，展示了触觉推理在机器人操作中的重要性

Abstract: We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.

</details>


### [26] [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038)
*Xiaopan Zhang,Zejin Wang,Zhixu Li,Jianpeng Yao,Jiachen Li*

Main category: cs.RO

TL;DR: 论文提出CommCP框架，用于解决多智能体多任务具身问答问题，通过基于LLM的去中心化通信和保形预测校准消息，提高任务成功率和探索效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人需要协作完成人类自然语言指令，但现有研究缺乏对多异构机器人协作中信息收集过程的系统研究。传统具身问答主要关注单智能体，而多智能体协作需要有效的通信协调以避免冗余。

Method: 提出CommCP框架：基于大语言模型的去中心化通信框架，使用保形预测校准生成的消息，减少接收者分心并提高通信可靠性。框架针对多智能体多任务具身问答问题设计。

Result: 实验结果表明，CommCP在提出的MM-EQA基准测试中显著提高了任务成功率和探索效率，优于基线方法。基准包含多样化的照片级真实家庭场景和具身问题。

Conclusion: CommCP框架有效解决了多智能体协作中的通信协调问题，通过消息校准提高了多机器人系统的协作效率，为具身智能的协作任务提供了新的解决方案。

Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.

</details>
