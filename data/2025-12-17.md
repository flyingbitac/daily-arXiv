<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration](https://arxiv.org/abs/2512.13903)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 提出一个预测-精炼框架，结合人机观察运动来精炼预训练预测器的初始预测，实现实时、真实且交互感知的人体运动预测


<details>
  <summary>Details</summary>
Motivation: 现有随机人体运动预测方法要么生成不真实的运动，要么只关注准确性而忽略机器人运动对人类行为的影响，需要改进预测质量同时保持实时性

Method: 提出预测-精炼框架：使用预训练的最先进预测器生成初始预测，然后通过Flow Matching结构的精炼模块，结合人类和机器人观察运动来精炼预测

Result: 在HRC桌面拆卸数据集上的实验表明，该方法显著提高了预测准确性，同时保持了人体运动的不确定性和多模态特性，且总推理时间在时间预算内

Conclusion: 该框架实现了实时、真实且交互感知的人体运动预测，提高了人机协作的安全性和有效性，具有实际应用价值

Abstract: Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.

</details>


### [2] [Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline](https://arxiv.org/abs/2512.13974)
*Hossein Naderi,Alireza Shojaei,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: 提出基于机器人和AI的多层框架，将机器人自主导航的视觉信息与建筑工地安全规则连接，自动生成安全检查报告


<details>
  <summary>Details</summary>
Motivation: 建筑安全检查仍主要依赖人工，自动化方法需要特定数据集且维护困难，机器人现场检查依赖人工遥操作和报告，劳动强度大

Method: 多层框架包含机器人模块和AI模块：机器人侧使用SLAM和自主导航实现可重复覆盖；AI侧包括VLM生成场景描述、基于OSHA和现场政策的检索组件、基于规则的VLM安全评估、以及生成报告的LLM层

Result: 在模拟三种常见危险场景的实验室环境中验证，相比最先进的闭源模型，显示出高召回率和有竞争力的精确度

Conclusion: 提供了一个透明、可泛化的管道，通过暴露各层中间产物并保持人在回路中，超越了黑盒模型，为未来扩展到建筑及其他领域的任务奠定了基础

Abstract: Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.

</details>


### [3] [Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair](https://arxiv.org/abs/2512.13981)
*Hossein Naderi,Alireza Shojaei,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: 研究机器人任务表现和情感表达如何动态影响人类信任，发现成功提升信任，失败导致信任骤降，道歉表达能部分修复信任，修复效果受年龄和任务类型影响。


<details>
  <summary>Details</summary>
Motivation: 尽管AEC行业机器人和人机协作取得进展，但信任通常被视为静态因素，缺乏对协作过程中信任动态变化的指导。本研究旨在探究机器人任务表现和情感表达如何塑造人类信任随时间的变化。

Method: 设计受控组内研究，包含两个建筑启发任务：物料递送（物理协助）和信息收集（感知协助）。使用14项HRI信任感知量表加重新委托选择重复测量信任（每个任务四次）。机器人产生两种多模态表达：成功后的"高兴"显示和失败后的"悲伤"显示（道歉并请求第二次机会）。在实验室环境中使用四足平台对30名参与者进行研究。

Result: 机器人成功可靠地增加信任，失败导致信任骤降，基于道歉的表达部分恢复信任（物料递送恢复44%；信息收集恢复38%）。项目级分析显示恢复的信任主要由互动和沟通因素驱动，能力部分恢复，自主性方面变化最小。年龄组和先前态度调节信任动态：年轻参与者变化更大但更短暂，25岁左右参与者修复最持久，年长参与者动态最保守。

Conclusion: 这项工作为未来根据任务需求和用户特征调整修复策略奠定了基础，以支持建筑工地机器人的安全、高效采用。研究揭示了信任动态变化的模式，为设计适应性人机交互系统提供了实证依据。

Abstract: Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a "glad" display with a brief confirmation after success, and a "sad" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.

</details>


### [4] [CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth](https://arxiv.org/abs/2512.14001)
*Zhuo Zhang,Yonghui Liu,Meijie Zhang,Feiyang Tan,Yikang Ding*

Main category: cs.RO

TL;DR: CLAIM是一种新颖的相机-LiDAR标定方法，利用单目深度模型的潜力，通过粗到细搜索优化变换参数，无需复杂的数据处理或特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有的相机-LiDAR标定方法通常需要复杂的数据处理、特征提取和特征匹配步骤，限制了方法的简单性和场景适应性。本文旨在开发一种更简单、适应性更强的标定方法。

Method: CLAIM采用粗到细搜索策略，通过最小化基于块状皮尔逊相关的结构损失和基于互信息的纹理损失来寻找最优变换。这两种损失函数作为相机-LiDAR对齐的良好度量，无需复杂的数据处理步骤。

Result: 在KITTI、Waymo和MIAS-LCEC公开数据集上的实验验证表明，CLAIM相比现有最先进方法具有优越性能。

Conclusion: CLAIM是一种简单且适应性强的相机-LiDAR标定方法，利用单目深度模型的潜力，通过创新的损失函数和优化策略实现了优异的标定性能。

Abstract: In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.

</details>


### [5] [Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model](https://arxiv.org/abs/2512.14031)
*Zhaofeng Hu,Hongrui Yu,Vaidhyanathan Chandramouli,Ci-Jyun Liang*

Main category: cs.RO

TL;DR: 本研究评估了两种建筑机器人技能学习方法：视觉-语言-动作模型和强化学习方法，通过三阶段实验对比了它们在任务性能、泛化能力和实际部署工作量方面的表现。


<details>
  <summary>Details</summary>
Motivation: 为了理解不同方法在建筑自动化中的适用性，研究需要评估视觉-语言-动作模型和强化学习方法在任务性能、泛化能力以及实际部署工作量方面的差异，为建筑机器人技能学习提供实践指导。

Method: 研究开发了两种遥操作界面收集演示数据，进行了三阶段评估：1) 比较MLP策略和DQN模仿模型确定更强的RL基线；2) 在两种场景下训练三种VLA模型并进行比较；3) 将选定的RL基线与VLA模型在计算效率、样本效率和多阶段面板安装任务上进行基准测试。

Result: VLA模型表现出强大的泛化和少样本能力，在拾取阶段达到60%和100%成功率；DQN虽然可以变得稳健但需要在调优时添加额外噪声，增加了工作量。VLA在任务变更时减少了编程工作量，能用最少数据实现有用性能，而DQN在可接受足够调优工作量时提供了可行的基线。

Conclusion: VLA模型在建筑机器人技能学习中具有实际优势，特别是对于需要快速适应新任务和减少编程工作量的场景；而DQN在可接受额外调优工作量的情况下仍是一个可行的选择。两种方法各有适用场景，为建筑自动化提供了不同的技术路径。

Abstract: This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.

</details>


### [6] [E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms](https://arxiv.org/abs/2512.14046)
*Boyang Li,Zhongpeng Jin,Shuai Zhao,Jiahui Liao,Tian Liu,Han Liu,Yuanhai Zhang,Kai Huang*

Main category: cs.RO

TL;DR: E-Navi是一个环境自适应无人机导航系统，通过动态调整CPU任务执行来适应环境变化，基于可用计算资源优化映射分辨率和执行频率。


<details>
  <summary>Details</summary>
Motivation: 现有无人机导航系统采用固定执行配置，不考虑环境动态变化和可用计算资源，导致飞行策略僵化、计算过度，影响飞行性能甚至导致失败。需要自适应系统来动态调整工作负载。

Method: 重新设计无人机导航系统的感知-规划流程，通过动态调整映射分辨率和执行频率来适应环境变化，基于定量环境复杂度评估，并支持在不同计算能力的硬件平台上灵活部署。

Result: 硬件在环和真实世界实验表明，E-Navi在各种硬件平台上显著优于基线方法，实现高达53.9%的导航任务工作负载减少，高达63.8%的飞行时间节省，并提供更稳定的速度控制。

Conclusion: E-Navi系统通过环境自适应机制有效解决了无人机导航中的计算资源优化问题，在动态环境中实现了更好的性能和效率。

Abstract: The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.

</details>


### [7] [Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation](https://arxiv.org/abs/2512.14054)
*Humaira Tasnim,Ashik E Rasul,Bruce Jo,Hyung-Jin Yoon*

Main category: cs.RO

TL;DR: 提出一种用于无人机自主着陆的尺度自适应双专家感知框架，通过两个专门处理不同尺度目标的YOLOv8模型和几何门控机制，显著提升了在极端尺度变化下的直升机停机坪检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模型检测器（如YOLOv8）在无人机着陆过程中面临极端尺度变化的挑战：在高空时停机坪目标小且分辨率低，在接近地面时目标占据大部分视野。单模型难以在整个下降过程中保持鲁棒性，需要一种能适应尺度变化的检测方案。

Method: 提出尺度自适应双专家感知框架：1）训练两个YOLOv8专家模型，分别专门处理远距离（小目标）和近距离（大目标）的直升机停机坪检测；2）在推理时两个专家并行运行；3）引入几何门控机制，根据无人机视角选择预测最一致的专家输出；4）在CARLA和NASA GUAM集成的闭环着陆环境中进行评估。

Result: 与单检测器基线相比，该框架在准直稳定性、着陆精度和整体鲁棒性方面均有显著提升。双专家系统能够有效防止单检测器系统在宽高度范围内操作时常见的性能退化问题。

Conclusion: 通过引入针对着陆问题量身定制的尺度感知专家路由策略，这项工作推进了自主下降的弹性视觉感知，并为未来多专家无人机框架奠定了基础。该方法特别适用于GPS受限或视觉退化条件下的可靠着陆。

Abstract: Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.

</details>


### [8] [Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning](https://arxiv.org/abs/2512.14057)
*Amir M. Soufi Enayati,Homayoun Honari,Homayoun Najjaran*

Main category: cs.RO

TL;DR: CRAFT提出了一种仅从状态和奖励序列推断任务表示的方法，解耦了任务推断与策略优化，在机器人操作任务中实现了更快的适应和更好的泛化。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习方法在未见任务上泛化能力差，而现有的上下文自适应元强化学习方法虽然通过任务表示进行条件化，但严重依赖完整动作信息，导致任务推断与特定策略紧密耦合。

Method: CRAFT是一种信念模型，仅从状态和奖励序列推断任务表示，使用带有旋转位置编码的Transformer编码器-解码器架构，通过摊销变分推断进行可扩展的信念更新。

Result: 在MetaWorld ML-10机器人操作基准测试中，CRAFT相比上下文自适应元强化学习基线实现了更快的适应速度、更好的泛化能力和更有效的探索。

Conclusion: 无动作推断方法为机器人控制中的可扩展强化学习提供了有前景的基础，通过解耦任务推断与策略优化支持模块化训练。

Abstract: Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.

</details>


### [9] [Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field](https://arxiv.org/abs/2512.14111)
*Chenzui Li,Yiming Chen,Xi Wu,Tao Teng,Sylvain Calinon,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: 提出配置空间人体工学场(CSEF)，一种连续可微的人体关节空间场，用于实时人体工学感知的运动规划，在工业人机协作中提高成功率和降低肌肉负荷。


<details>
  <summary>Details</summary>
Motivation: 工业人机协作需要无碰撞、响应迅速且符合人体工学的运动规划，以减少操作者疲劳和肌肉骨骼风险。现有方法在实时性和人体工学优化方面存在不足。

Method: 提出配置空间人体工学场(CSEF)，在人体关节空间构建连续可微的场，量化人体工学质量并提供梯度。开发高效算法从现有指标构建CSEF，包含关节权重和任务条件，并将其集成到基于梯度的规划器中，兼容阻抗控制机器人。

Result: 在2自由度基准测试中，CSEF规划比任务空间人体工学规划器具有更高的成功率、更低的人体工学成本和更快的计算速度。硬件实验显示在单臂引导、协作钻孔和双臂协同搬运任务中，能更快降低人体工学成本、更接近优化关节目标、降低肌肉激活水平。协作钻孔任务平均人体工学评分降低10.31%，双臂协同搬运任务降低5.60%。

Conclusion: CSEF方法为工业人机协作提供了有效的实时人体工学感知运动规划，显著降低操作者肌肉负荷，具有实际部署价值。

Abstract: Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.

</details>


### [10] [SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry](https://arxiv.org/abs/2512.14189)
*Johannes A. Gaus,Daniel Häufle,Woo-Jeong Baek*

Main category: cs.RO

TL;DR: SUPER框架通过灵敏度传播不确定性，实现VIO实时风险评估，可提前50帧预测轨迹退化，并启动停止或重定位策略。


<details>
  <summary>Details</summary>
Motivation: 现有VO/VIO/SLAM系统虽然精度高，但缺乏运行时风险评估能力。需要一种通用、可解释的框架来实时评估VIO中的风险。

Method: 提出SUPER框架，利用高斯-牛顿正规矩阵的Schur补块传播不确定性，通过灵敏度反映不确定性对风险发生的影响。基于残差大小、几何条件和短期时间趋势估计风险，无需地面真值知识。

Result: 能够可靠预测50帧前的轨迹退化，比基线提升20%；启动停止或重定位策略的召回率达89.1%；框架后端无关，实时运行，额外CPU成本低于0.2%；提供一致的不确定性估计，适用于长时程建图。

Conclusion: SUPER是一个通用、可解释的实时风险评估框架，通过灵敏度传播不确定性，有效预测VIO性能退化并触发应对策略，具有实际应用价值。

Abstract: While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.

</details>


### [11] [Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments](https://arxiv.org/abs/2512.14206)
*Mayank Sewlia,Christos K. Verginis,Dimos V. Dimarogonas*

Main category: cs.RO

TL;DR: 提出一个多速率规划控制框架，用于移动多机械臂系统在复杂环境中协同搬运物体，满足时空任务规范


<details>
  <summary>Details</summary>
Motivation: 解决移动多机械臂系统在障碍物密集、约束严格环境下的协同操作问题，需要同时满足连续机器人动力学和离散几何约束

Method: 采用多速率规划控制框架：离线生成满足STL规范的对象轨迹和无碰撞基座足迹，在线进行约束逆运动学和连续时间反馈控制

Result: 通过三台Franka Emika Panda移动机械臂协同刚性抓取物体的高保真物理仿真验证了方法的有效性

Conclusion: 提出的框架能够实现多机械臂的协调重构，同时跟踪期望物体运动，适用于复杂环境下的协同操作任务

Abstract: We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.

</details>


### [12] [CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics](https://arxiv.org/abs/2512.14270)
*Zixin Tang,Yiming Chen,Quentin Rouxel,Dianxi Li,Shuang Wu,Fei Chen*

Main category: cs.RO

TL;DR: CaFe-TeleVision是一个粗到细的远程操作系统，通过沉浸式情境可视化提升人机工程学，在双手机器人上验证了其在挑战性任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前远程操作系统在效率和人体工程学方面存在局限，特别是在挑战性场景中。需要开发更高效、更符合人体工程学的远程操作系统。

Method: 提出粗到细控制机制解决工作空间差异问题，结合沉浸式情境可视化技术降低多视图处理的认知负荷，系统基于人形协作机器人构建。

Result: 在24名参与者的用户研究中，系统显著提升了人体工程学，降低了任务负荷，提高了用户接受度。在6个挑战性任务中，成功率提升达28.89%，完成时间加速26.81%。

Conclusion: CaFe-TeleVision通过粗到细控制和沉浸式情境可视化，有效提升了远程操作的人体工程学和性能，为远程控制提供了更优解决方案。

Abstract: Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/

</details>


### [13] [ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning](https://arxiv.org/abs/2512.14331)
*Rishabh Dev Yadav,Avirup Das,Hongyu Song,Samuel Kaski,Wei Pan*

Main category: cs.RO

TL;DR: 提出一个实时适应机器人非线性动力学的框架，通过离线学习潜在表示支持在线贝叶斯更新，包含变化点检测机制处理动态变化，在理论和实验中验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人需要在不断变化的动态环境下运行，包括渐变漂移、瞬态波动和突变偏移，需要实时适应能力，既要对短期变化鲁棒，又要对持久变化响应。

Method: 将表示学习与在线适应解耦：离线学习潜在表示，支持在线闭式贝叶斯更新；引入变化点感知机制，通过数据似然推断潜在变量指示连续性或变化；连续性时积累证据优化预测，检测到变化时调整过去信息实现快速重新学习。

Result: 理论证明框架的自适应遗憾仅随时间对数增长，与变化次数线性相关，性能接近知道变化时间的神谕；在倒立摆仿真和真实四旋翼飞行实验中（包括摆动载荷和飞行中掉落），相比基线方法显示出更好的预测精度、更快恢复和更准确的闭环跟踪。

Conclusion: 该框架能够有效处理机器人系统中的动态变化，保持校准的不确定性，支持对瞬态、渐变或结构变化的概率推理，在理论和实际应用中均表现出色。

Abstract: Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.

</details>


### [14] [Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments](https://arxiv.org/abs/2512.14340)
*Aleksi Karhunen,Teemu Hakala,Väinö Karjalainen,Eija Honkavaara*

Main category: cs.RO

TL;DR: 该研究开发了基于轻量级激光雷达的自主飞行四旋翼无人机系统，在森林冠层下进行严格测试，优化后系统在中等密度和茂密森林中分别达到12/15和15/15的成功率，并提出了标准化测试框架。


<details>
  <summary>Details</summary>
Motivation: 虽然无人机在森林应用中日益普及，但冠层下自主导航仍是重大挑战。现有研究缺乏严谨的实验设计和报告标准，很少报告测试森林密度、难度、多次飞行和成功率，需要开发可靠的自主飞行系统并建立标准化测试方法。

Method: 基于开源算法实现轻量级激光雷达自主飞行四旋翼无人机，使用IPC路径规划器和LTA-OM SLAM算法。进行93次飞行测试，包括33次初步测试后优化系统，再进行60次测试。在不同森林密度（中等密度和茂密森林）和飞行速度（1m/s和2m/s）条件下评估性能。

Result: 优化后系统性能显著提升：在1m/s飞行速度下，中等密度森林成功率12/15（80%），茂密森林成功率15/15（100%）；在2m/s飞行速度下，中等密度森林成功率12/15（80%），茂密森林成功率5/15（33%）。系统可靠性和任务完成时间均有改善。

Conclusion: 成功开发了森林冠层下自主飞行无人机系统，证明了开源算法在实际森林环境中的可行性。提出了标准化测试设置和评估标准，为自主冠层下无人机系统的性能比较提供了统一框架，有助于提高可重复性、指导系统改进并加速森林机器人领域发展。

Abstract: The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.

</details>


### [15] [CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection](https://arxiv.org/abs/2512.14355)
*Jörg Gamerdinger,Sven Teufel,Georg Volk,Oliver Bringmann*

Main category: cs.RO

TL;DR: 提出了一种基于样条估计的实时车道集体感知方法，通过车车通信扩展局部感知能力，将感知范围提升200%


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要全面环境感知以确保安全，但受限于传感器范围、遮挡和弯道等因素，难以实现完整的车道检测。在缺乏精确定位或高精地图的情况下，车辆必须依赖自身感知的道路信息，因此需要通过车车通信扩展局部感知能力

Method: 采用基于样条的方法估计未检测到的道路路段，提出实时可行的车道集体感知融合算法，利用车辆间通信共享道路信息

Result: 实现了实时处理能力，在各种情况和道路类型下评估了融合算法，感知范围扩展了最高200%

Conclusion: 通过车车通信实现车道集体感知是可行的策略，能够显著扩展自动驾驶车辆的感知范围，提高在复杂环境下的安全性

Abstract: Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.

</details>


### [16] [A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems](https://arxiv.org/abs/2512.14367)
*Georg Volk,Jörg Gamerdinger,Alexander von Bernuth,Oliver Bringmann*

Main category: cs.RO

TL;DR: 提出一种新的自动驾驶物体感知安全评估指标，综合考虑物体速度、方向、距离、尺寸和潜在碰撞损害等因素，生成单一可解释的安全评分。


<details>
  <summary>Details</summary>
Motivation: 现有物体感知评估指标未能充分考虑不同物体对安全的重要性差异。物体的速度、方向、距离、尺寸以及潜在碰撞损害等因素都会影响其安全重要性，需要综合考虑这些参数进行更准确的安全评估。

Method: 提出一种新的安全评估指标，整合多个安全相关参数（速度、方向、距离、尺寸、潜在碰撞损害），将这些参数统一到一个单一、易于解释的安全评估分数中。

Result: 使用真实世界和虚拟数据集对新指标进行评估，并与现有最先进的评估指标进行比较验证。

Conclusion: 新提出的安全评估指标能够综合考虑多种安全相关因素，为自动驾驶物体感知提供更全面、更准确的安全评估，有助于提升自动驾驶系统的安全性。

Abstract: Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.

</details>


### [17] [Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids](https://arxiv.org/abs/2512.14411)
*Mohammed Ayman Habib,Aldo Petruzzelli*

Main category: cs.RO

TL;DR: Omnia提出基于合成数据的流水线，加速军事化人形机器人的训练、验证和部署准备，通过将第一人称空间观测转换为可扩展的任务特定合成数据集。


<details>
  <summary>Details</summary>
Motivation: 传统实地试验成本高、风险大、耗时长，限制了人形机器人的开发迭代速度。需要一种方法能够快速生成多样化场景数据，支持复杂对抗环境中的鲁棒性开发。

Method: 将第一人称空间观测（来自POV录像、智能眼镜、AR头显、空间浏览工作流）转换为可扩展的任务特定合成数据集，生成大量高保真模拟场景，结合自动标注和模型训练。

Result: 能够快速为新作战环境和威胁条件调整数据集，支持人形机器人基线性能和高级子系统（多模态感知、反检测生存能力、CBRNE相关侦察行为）的开发。

Conclusion: 该合成数据驱动流水线通过早期暴露于广泛场景多样性，实现了更快的开发周期和复杂对抗环境中人形机器人系统鲁棒性的提升。

Abstract: Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.

</details>


### [18] [Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations](https://arxiv.org/abs/2512.14428)
*Aaron Kurda,Simon Steuernagel,Lukas Jung,Marcus Baum*

Main category: cs.RO

TL;DR: Odyssey是一个专注于GNSS信号缺失环境（如隧道、停车场）的激光雷达惯性里程计数据集，首次采用基于环形激光陀螺的导航级惯性导航系统提供高精度地面真值。


<details>
  <summary>Details</summary>
Motivation: 现有LIO/SLAM数据集在GNSS信号缺失环境下存在局限性：GNSS信号在遮挡环境中不可靠，而现有数据集使用的MEMS或FOG惯性测量单元无法支持长时间GNSS缺失环境的研究。

Method: 开发了Odyssey数据集，专注于GNSS信号缺失环境（隧道、停车场等）以及其他代表性场景（走走停停交通、颠簸道路、开阔场地）。采用基于环形激光陀螺的导航级惯性导航系统提供地面真值，具有优异的偏置稳定性。

Result: Odyssey成为首个公开可用的基于环形激光陀螺惯性导航系统的数据集，支持长时间GNSS缺失环境的精确研究。数据集包含所有轨迹的三次重复以支持位置识别任务，并提供精确的大地坐标以集成外部地图数据。

Conclusion: Odyssey填补了现有LIO数据集在GNSS缺失环境研究方面的空白，通过高质量的环形激光陀螺惯性导航系统地面真值，为LIO/SLAM系统在挑战性环境中的开发和评估提供了重要资源。

Abstract: The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .

</details>


### [19] [Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination](https://arxiv.org/abs/2512.14434)
*Quan Yuan,Daqian Cao,Weibang Bai*

Main category: cs.RO

TL;DR: 本文提出了一种新型3-(PP(2-(UPS)))冗余并联机构，并研究了其关键几何参数对工作空间体积、形状、边界完整性和定向能力的影响，定义了评价机构定向性能的指标，为参数优化提供参考。


<details>
  <summary>Details</summary>
Motivation: 冗余并联机器人通常用于需要良好精度、高负载能力和大工作空间的场景，但其基本机器人配置和几何参数优化仍然具有挑战性。本文旨在解决这一问题。

Method: 提出新型3-(PP(2-(UPS)))冗余并联机构，分析关键几何参数对工作空间特性的影响，定义扭转能力指数TI_1和倾斜能力指数TI_2来评估机构定向性能，并通过数值仿真验证分析。

Result: 通过数值仿真研究验证了分析结果，为3-(PP(2-(UPS)))及其他类似冗余并联机构的参数优化提供了合理且必要的参考依据。

Conclusion: 本文提出的新型冗余并联机构具有良好的通用性，通过分析关键几何参数对工作空间的影响并定义定向性能评价指标，为冗余并联机构的参数优化提供了有效方法。

Abstract: Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.

</details>


### [20] [EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models](https://arxiv.org/abs/2512.14666)
*Zechen Bai,Chen Gao,Mike Zheng Shou*

Main category: cs.RO

TL;DR: EVOLVE-VLA是一个测试时训练框架，让视觉-语言-动作模型能够通过环境交互持续自适应，无需大量任务特定演示，解决了传统监督微调方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型依赖于监督微调，需要大量演示、死记硬背轨迹、无法适应部署条件变化。需要实现真正的自适应具身智能，让智能体像人类一样通过实践持续改进。

Method: 通过学习的进度估计器提供密集反馈，采用累积进度估计机制平滑噪声点估计，以及渐进式视野扩展策略实现逐步策略演化，从而驯服噪声信号。

Result: 在长视野任务上提升8.6%，在单样本学习上提升22.0%，实现跨任务泛化——在未见任务上达到20.8%成功率（纯监督微调为0%），展现出错误恢复和新策略等涌现能力。

Conclusion: EVOLVE-VLA代表了视觉-语言-动作模型从静态模仿向持续自我改进的关键一步，为实现真正学习和自适应的具身智能奠定了基础。

Abstract: Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.

</details>


### [21] [CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation](https://arxiv.org/abs/2512.14689)
*Sirui Chen,Zi-ang Cao,Zhengyi Luo,Fernando Castañeda,Chenran Li,Tingwu Wang,Ye Yuan,Linxi "Jim" Fan,C. Karen Liu,Yuke Zhu*

Main category: cs.RO

TL;DR: CHIP是一个即插即用的自适应柔顺控制模块，使人形机器人能够在保持动态参考运动敏捷跟踪的同时，实现可控的末端执行器刚度，从而执行需要不同柔顺度的强力操作任务。


<details>
  <summary>Details</summary>
Motivation: 虽然人形机器人在敏捷运动技能（如后空翻、跑步、爬行）方面取得了进展，但在执行强力操作任务（如移动物体、擦拭、推车）方面仍然面临挑战。现有方法难以在保持动态运动跟踪的同时实现可控的末端执行器柔顺度。

Method: 提出了自适应柔顺人形控制通过视觉扰动（CHIP），这是一个即插即用的模块，能够在保持动态参考运动敏捷跟踪的同时实现可控的末端执行器刚度。该方法易于实现，既不需要数据增强，也不需要额外的奖励调整。

Result: 使用CHIP训练的通用运动跟踪控制器能够执行多种需要不同末端执行器柔顺度的强力操作任务，包括多机器人协作、擦拭、箱子递送和开门等任务。

Conclusion: CHIP模块有效解决了人形机器人在强力操作任务中的柔顺控制问题，为机器人执行多样化操作任务提供了灵活且易于实现的解决方案。

Abstract: Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.

</details>
