{"id": "2512.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13903", "abs": "https://arxiv.org/abs/2512.13903", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration", "comment": null, "summary": "Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9884\u6d4b-\u7cbe\u70bc\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u673a\u89c2\u5bdf\u8fd0\u52a8\u6765\u7cbe\u70bc\u9884\u8bad\u7ec3\u9884\u6d4b\u5668\u7684\u521d\u59cb\u9884\u6d4b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u771f\u5b9e\u4e14\u4ea4\u4e92\u611f\u77e5\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b", "motivation": "\u73b0\u6709\u968f\u673a\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\u8981\u4e48\u751f\u6210\u4e0d\u771f\u5b9e\u7684\u8fd0\u52a8\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u51c6\u786e\u6027\u800c\u5ffd\u7565\u673a\u5668\u4eba\u8fd0\u52a8\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u9700\u8981\u6539\u8fdb\u9884\u6d4b\u8d28\u91cf\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027", "method": "\u63d0\u51fa\u9884\u6d4b-\u7cbe\u70bc\u6846\u67b6\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u9884\u6d4b\u5668\u751f\u6210\u521d\u59cb\u9884\u6d4b\uff0c\u7136\u540e\u901a\u8fc7Flow Matching\u7ed3\u6784\u7684\u7cbe\u70bc\u6a21\u5757\uff0c\u7ed3\u5408\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u89c2\u5bdf\u8fd0\u52a8\u6765\u7cbe\u70bc\u9884\u6d4b", "result": "\u5728HRC\u684c\u9762\u62c6\u5378\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4eba\u4f53\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6a21\u6001\u7279\u6027\uff0c\u4e14\u603b\u63a8\u7406\u65f6\u95f4\u5728\u65f6\u95f4\u9884\u7b97\u5185", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u771f\u5b9e\u4e14\u4ea4\u4e92\u611f\u77e5\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.13974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13974", "abs": "https://arxiv.org/abs/2512.13974", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline", "comment": null, "summary": "Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u4eba\u548cAI\u7684\u591a\u5c42\u6846\u67b6\uff0c\u5c06\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u7684\u89c6\u89c9\u4fe1\u606f\u4e0e\u5efa\u7b51\u5de5\u5730\u5b89\u5168\u89c4\u5219\u8fde\u63a5\uff0c\u81ea\u52a8\u751f\u6210\u5b89\u5168\u68c0\u67e5\u62a5\u544a", "motivation": "\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u9700\u8981\u7279\u5b9a\u6570\u636e\u96c6\u4e14\u7ef4\u62a4\u56f0\u96be\uff0c\u673a\u5668\u4eba\u73b0\u573a\u68c0\u67e5\u4f9d\u8d56\u4eba\u5de5\u9065\u64cd\u4f5c\u548c\u62a5\u544a\uff0c\u52b3\u52a8\u5f3a\u5ea6\u5927", "method": "\u591a\u5c42\u6846\u67b6\u5305\u542b\u673a\u5668\u4eba\u6a21\u5757\u548cAI\u6a21\u5757\uff1a\u673a\u5668\u4eba\u4fa7\u4f7f\u7528SLAM\u548c\u81ea\u4e3b\u5bfc\u822a\u5b9e\u73b0\u53ef\u91cd\u590d\u8986\u76d6\uff1bAI\u4fa7\u5305\u62ecVLM\u751f\u6210\u573a\u666f\u63cf\u8ff0\u3001\u57fa\u4e8eOSHA\u548c\u73b0\u573a\u653f\u7b56\u7684\u68c0\u7d22\u7ec4\u4ef6\u3001\u57fa\u4e8e\u89c4\u5219\u7684VLM\u5b89\u5168\u8bc4\u4f30\u3001\u4ee5\u53ca\u751f\u6210\u62a5\u544a\u7684LLM\u5c42", "result": "\u5728\u6a21\u62df\u4e09\u79cd\u5e38\u89c1\u5371\u9669\u573a\u666f\u7684\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u6a21\u578b\uff0c\u663e\u793a\u51fa\u9ad8\u53ec\u56de\u7387\u548c\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u786e\u5ea6", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6cdb\u5316\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u66b4\u9732\u5404\u5c42\u4e2d\u95f4\u4ea7\u7269\u5e76\u4fdd\u6301\u4eba\u5728\u56de\u8def\u4e2d\uff0c\u8d85\u8d8a\u4e86\u9ed1\u76d2\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u6269\u5c55\u5230\u5efa\u7b51\u53ca\u5176\u4ed6\u9886\u57df\u7684\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2512.13981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13981", "abs": "https://arxiv.org/abs/2512.13981", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair", "comment": null, "summary": "Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a \"glad\" display with a brief confirmation after success, and a \"sad\" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u4efb\u52a1\u8868\u73b0\u548c\u60c5\u611f\u8868\u8fbe\u5982\u4f55\u52a8\u6001\u5f71\u54cd\u4eba\u7c7b\u4fe1\u4efb\uff0c\u53d1\u73b0\u6210\u529f\u63d0\u5347\u4fe1\u4efb\uff0c\u5931\u8d25\u5bfc\u81f4\u4fe1\u4efb\u9aa4\u964d\uff0c\u9053\u6b49\u8868\u8fbe\u80fd\u90e8\u5206\u4fee\u590d\u4fe1\u4efb\uff0c\u4fee\u590d\u6548\u679c\u53d7\u5e74\u9f84\u548c\u4efb\u52a1\u7c7b\u578b\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1AEC\u884c\u4e1a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4fe1\u4efb\u901a\u5e38\u88ab\u89c6\u4e3a\u9759\u6001\u56e0\u7d20\uff0c\u7f3a\u4e4f\u5bf9\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u4fe1\u4efb\u52a8\u6001\u53d8\u5316\u7684\u6307\u5bfc\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u673a\u5668\u4eba\u4efb\u52a1\u8868\u73b0\u548c\u60c5\u611f\u8868\u8fbe\u5982\u4f55\u5851\u9020\u4eba\u7c7b\u4fe1\u4efb\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "method": "\u8bbe\u8ba1\u53d7\u63a7\u7ec4\u5185\u7814\u7a76\uff0c\u5305\u542b\u4e24\u4e2a\u5efa\u7b51\u542f\u53d1\u4efb\u52a1\uff1a\u7269\u6599\u9012\u9001\uff08\u7269\u7406\u534f\u52a9\uff09\u548c\u4fe1\u606f\u6536\u96c6\uff08\u611f\u77e5\u534f\u52a9\uff09\u3002\u4f7f\u752814\u9879HRI\u4fe1\u4efb\u611f\u77e5\u91cf\u8868\u52a0\u91cd\u65b0\u59d4\u6258\u9009\u62e9\u91cd\u590d\u6d4b\u91cf\u4fe1\u4efb\uff08\u6bcf\u4e2a\u4efb\u52a1\u56db\u6b21\uff09\u3002\u673a\u5668\u4eba\u4ea7\u751f\u4e24\u79cd\u591a\u6a21\u6001\u8868\u8fbe\uff1a\u6210\u529f\u540e\u7684\"\u9ad8\u5174\"\u663e\u793a\u548c\u5931\u8d25\u540e\u7684\"\u60b2\u4f24\"\u663e\u793a\uff08\u9053\u6b49\u5e76\u8bf7\u6c42\u7b2c\u4e8c\u6b21\u673a\u4f1a\uff09\u3002\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u4f7f\u7528\u56db\u8db3\u5e73\u53f0\u5bf930\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u673a\u5668\u4eba\u6210\u529f\u53ef\u9760\u5730\u589e\u52a0\u4fe1\u4efb\uff0c\u5931\u8d25\u5bfc\u81f4\u4fe1\u4efb\u9aa4\u964d\uff0c\u57fa\u4e8e\u9053\u6b49\u7684\u8868\u8fbe\u90e8\u5206\u6062\u590d\u4fe1\u4efb\uff08\u7269\u6599\u9012\u9001\u6062\u590d44%\uff1b\u4fe1\u606f\u6536\u96c6\u6062\u590d38%\uff09\u3002\u9879\u76ee\u7ea7\u5206\u6790\u663e\u793a\u6062\u590d\u7684\u4fe1\u4efb\u4e3b\u8981\u7531\u4e92\u52a8\u548c\u6c9f\u901a\u56e0\u7d20\u9a71\u52a8\uff0c\u80fd\u529b\u90e8\u5206\u6062\u590d\uff0c\u81ea\u4e3b\u6027\u65b9\u9762\u53d8\u5316\u6700\u5c0f\u3002\u5e74\u9f84\u7ec4\u548c\u5148\u524d\u6001\u5ea6\u8c03\u8282\u4fe1\u4efb\u52a8\u6001\uff1a\u5e74\u8f7b\u53c2\u4e0e\u8005\u53d8\u5316\u66f4\u5927\u4f46\u66f4\u77ed\u6682\uff0c25\u5c81\u5de6\u53f3\u53c2\u4e0e\u8005\u4fee\u590d\u6700\u6301\u4e45\uff0c\u5e74\u957f\u53c2\u4e0e\u8005\u52a8\u6001\u6700\u4fdd\u5b88\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u6839\u636e\u4efb\u52a1\u9700\u6c42\u548c\u7528\u6237\u7279\u5f81\u8c03\u6574\u4fee\u590d\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ee5\u652f\u6301\u5efa\u7b51\u5de5\u5730\u673a\u5668\u4eba\u7684\u5b89\u5168\u3001\u9ad8\u6548\u91c7\u7528\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4fe1\u4efb\u52a8\u6001\u53d8\u5316\u7684\u6a21\u5f0f\uff0c\u4e3a\u8bbe\u8ba1\u9002\u5e94\u6027\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2512.14001", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14001", "abs": "https://arxiv.org/abs/2512.14001", "authors": ["Zhuo Zhang", "Yonghui Liu", "Meijie Zhang", "Feiyang Tan", "Yikang Ding"], "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth", "comment": "Accepted by IROS 2025", "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.", "AI": {"tldr": "CLAIM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u673a-LiDAR\u6807\u5b9a\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u641c\u7d22\u4f18\u5316\u53d8\u6362\u53c2\u6570\uff0c\u65e0\u9700\u590d\u6742\u7684\u6570\u636e\u5904\u7406\u6216\u7279\u5f81\u5339\u914d\u3002", "motivation": "\u73b0\u6709\u7684\u76f8\u673a-LiDAR\u6807\u5b9a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u5339\u914d\u6b65\u9aa4\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u7b80\u5355\u6027\u548c\u573a\u666f\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u6807\u5b9a\u65b9\u6cd5\u3002", "method": "CLAIM\u91c7\u7528\u7c97\u5230\u7ec6\u641c\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u57fa\u4e8e\u5757\u72b6\u76ae\u5c14\u900a\u76f8\u5173\u7684\u7ed3\u6784\u635f\u5931\u548c\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u7eb9\u7406\u635f\u5931\u6765\u5bfb\u627e\u6700\u4f18\u53d8\u6362\u3002\u8fd9\u4e24\u79cd\u635f\u5931\u51fd\u6570\u4f5c\u4e3a\u76f8\u673a-LiDAR\u5bf9\u9f50\u7684\u826f\u597d\u5ea6\u91cf\uff0c\u65e0\u9700\u590d\u6742\u7684\u6570\u636e\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5728KITTI\u3001Waymo\u548cMIAS-LCEC\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cCLAIM\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CLAIM\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u76f8\u673a-LiDAR\u6807\u5b9a\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6807\u5b9a\u6027\u80fd\u3002"}}
{"id": "2512.14031", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14031", "abs": "https://arxiv.org/abs/2512.14031", "authors": ["Zhaofeng Hu", "Hongrui Yu", "Vaidhyanathan Chandramouli", "Ci-Jyun Liang"], "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model", "comment": null, "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u5efa\u7b51\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u65b9\u6cd5\uff1a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5b9e\u9a8c\u5bf9\u6bd4\u4e86\u5b83\u4eec\u5728\u4efb\u52a1\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u5de5\u4f5c\u91cf\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u4e0d\u540c\u65b9\u6cd5\u5728\u5efa\u7b51\u81ea\u52a8\u5316\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7814\u7a76\u9700\u8981\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4efb\u52a1\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u5de5\u4f5c\u91cf\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u4e3a\u5efa\u7b51\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e24\u79cd\u9065\u64cd\u4f5c\u754c\u9762\u6536\u96c6\u6f14\u793a\u6570\u636e\uff0c\u8fdb\u884c\u4e86\u4e09\u9636\u6bb5\u8bc4\u4f30\uff1a1) \u6bd4\u8f83MLP\u7b56\u7565\u548cDQN\u6a21\u4eff\u6a21\u578b\u786e\u5b9a\u66f4\u5f3a\u7684RL\u57fa\u7ebf\uff1b2) \u5728\u4e24\u79cd\u573a\u666f\u4e0b\u8bad\u7ec3\u4e09\u79cdVLA\u6a21\u578b\u5e76\u8fdb\u884c\u6bd4\u8f83\uff1b3) \u5c06\u9009\u5b9a\u7684RL\u57fa\u7ebf\u4e0eVLA\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6837\u672c\u6548\u7387\u548c\u591a\u9636\u6bb5\u9762\u677f\u5b89\u88c5\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "VLA\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u548c\u5c11\u6837\u672c\u80fd\u529b\uff0c\u5728\u62fe\u53d6\u9636\u6bb5\u8fbe\u523060%\u548c100%\u6210\u529f\u7387\uff1bDQN\u867d\u7136\u53ef\u4ee5\u53d8\u5f97\u7a33\u5065\u4f46\u9700\u8981\u5728\u8c03\u4f18\u65f6\u6dfb\u52a0\u989d\u5916\u566a\u58f0\uff0c\u589e\u52a0\u4e86\u5de5\u4f5c\u91cf\u3002VLA\u5728\u4efb\u52a1\u53d8\u66f4\u65f6\u51cf\u5c11\u4e86\u7f16\u7a0b\u5de5\u4f5c\u91cf\uff0c\u80fd\u7528\u6700\u5c11\u6570\u636e\u5b9e\u73b0\u6709\u7528\u6027\u80fd\uff0c\u800cDQN\u5728\u53ef\u63a5\u53d7\u8db3\u591f\u8c03\u4f18\u5de5\u4f5c\u91cf\u65f6\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u57fa\u7ebf\u3002", "conclusion": "VLA\u6a21\u578b\u5728\u5efa\u7b51\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u5177\u6709\u5b9e\u9645\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u51cf\u5c11\u7f16\u7a0b\u5de5\u4f5c\u91cf\u7684\u573a\u666f\uff1b\u800cDQN\u5728\u53ef\u63a5\u53d7\u989d\u5916\u8c03\u4f18\u5de5\u4f5c\u91cf\u7684\u60c5\u51b5\u4e0b\u4ecd\u662f\u4e00\u4e2a\u53ef\u884c\u7684\u9009\u62e9\u3002\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u9002\u7528\u573a\u666f\uff0c\u4e3a\u5efa\u7b51\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.14046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14046", "abs": "https://arxiv.org/abs/2512.14046", "authors": ["Boyang Li", "Zhongpeng Jin", "Shuai Zhao", "Jiahui Liao", "Tian Liu", "Han Liu", "Yuanhai Zhang", "Kai Huang"], "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms", "comment": null, "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.", "AI": {"tldr": "E-Navi\u662f\u4e00\u4e2a\u73af\u5883\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574CPU\u4efb\u52a1\u6267\u884c\u6765\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u57fa\u4e8e\u53ef\u7528\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u6620\u5c04\u5206\u8fa8\u7387\u548c\u6267\u884c\u9891\u7387\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\u91c7\u7528\u56fa\u5b9a\u6267\u884c\u914d\u7f6e\uff0c\u4e0d\u8003\u8651\u73af\u5883\u52a8\u6001\u53d8\u5316\u548c\u53ef\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u98de\u884c\u7b56\u7565\u50f5\u5316\u3001\u8ba1\u7b97\u8fc7\u5ea6\uff0c\u5f71\u54cd\u98de\u884c\u6027\u80fd\u751a\u81f3\u5bfc\u81f4\u5931\u8d25\u3002\u9700\u8981\u81ea\u9002\u5e94\u7cfb\u7edf\u6765\u52a8\u6001\u8c03\u6574\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\u7684\u611f\u77e5-\u89c4\u5212\u6d41\u7a0b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6620\u5c04\u5206\u8fa8\u7387\u548c\u6267\u884c\u9891\u7387\u6765\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u57fa\u4e8e\u5b9a\u91cf\u73af\u5883\u590d\u6742\u5ea6\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u5728\u4e0d\u540c\u8ba1\u7b97\u80fd\u529b\u7684\u786c\u4ef6\u5e73\u53f0\u4e0a\u7075\u6d3b\u90e8\u7f72\u3002", "result": "\u786c\u4ef6\u5728\u73af\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cE-Navi\u5728\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8fbe53.9%\u7684\u5bfc\u822a\u4efb\u52a1\u5de5\u4f5c\u8d1f\u8f7d\u51cf\u5c11\uff0c\u9ad8\u8fbe63.8%\u7684\u98de\u884c\u65f6\u95f4\u8282\u7701\uff0c\u5e76\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u901f\u5ea6\u63a7\u5236\u3002", "conclusion": "E-Navi\u7cfb\u7edf\u901a\u8fc7\u73af\u5883\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u95ee\u9898\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2512.14054", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14054", "abs": "https://arxiv.org/abs/2512.14054", "authors": ["Humaira Tasnim", "Ashik E Rasul", "Bruce Jo", "Hyung-Jin Yoon"], "title": "Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation", "comment": null, "summary": "Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u81ea\u4e3b\u7740\u9646\u7684\u5c3a\u5ea6\u81ea\u9002\u5e94\u53cc\u4e13\u5bb6\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e13\u95e8\u5904\u7406\u4e0d\u540c\u5c3a\u5ea6\u76ee\u6807\u7684YOLOv8\u6a21\u578b\u548c\u51e0\u4f55\u95e8\u63a7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u4e0b\u7684\u76f4\u5347\u673a\u505c\u673a\u576a\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u6a21\u578b\u68c0\u6d4b\u5668\uff08\u5982YOLOv8\uff09\u5728\u65e0\u4eba\u673a\u7740\u9646\u8fc7\u7a0b\u4e2d\u9762\u4e34\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u7684\u6311\u6218\uff1a\u5728\u9ad8\u7a7a\u65f6\u505c\u673a\u576a\u76ee\u6807\u5c0f\u4e14\u5206\u8fa8\u7387\u4f4e\uff0c\u5728\u63a5\u8fd1\u5730\u9762\u65f6\u76ee\u6807\u5360\u636e\u5927\u90e8\u5206\u89c6\u91ce\u3002\u5355\u6a21\u578b\u96be\u4ee5\u5728\u6574\u4e2a\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u5c3a\u5ea6\u53d8\u5316\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5c3a\u5ea6\u81ea\u9002\u5e94\u53cc\u4e13\u5bb6\u611f\u77e5\u6846\u67b6\uff1a1\uff09\u8bad\u7ec3\u4e24\u4e2aYOLOv8\u4e13\u5bb6\u6a21\u578b\uff0c\u5206\u522b\u4e13\u95e8\u5904\u7406\u8fdc\u8ddd\u79bb\uff08\u5c0f\u76ee\u6807\uff09\u548c\u8fd1\u8ddd\u79bb\uff08\u5927\u76ee\u6807\uff09\u7684\u76f4\u5347\u673a\u505c\u673a\u576a\u68c0\u6d4b\uff1b2\uff09\u5728\u63a8\u7406\u65f6\u4e24\u4e2a\u4e13\u5bb6\u5e76\u884c\u8fd0\u884c\uff1b3\uff09\u5f15\u5165\u51e0\u4f55\u95e8\u63a7\u673a\u5236\uff0c\u6839\u636e\u65e0\u4eba\u673a\u89c6\u89d2\u9009\u62e9\u9884\u6d4b\u6700\u4e00\u81f4\u7684\u4e13\u5bb6\u8f93\u51fa\uff1b4\uff09\u5728CARLA\u548cNASA GUAM\u96c6\u6210\u7684\u95ed\u73af\u7740\u9646\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0e\u5355\u68c0\u6d4b\u5668\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u76f4\u7a33\u5b9a\u6027\u3001\u7740\u9646\u7cbe\u5ea6\u548c\u6574\u4f53\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u53cc\u4e13\u5bb6\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u9632\u6b62\u5355\u68c0\u6d4b\u5668\u7cfb\u7edf\u5728\u5bbd\u9ad8\u5ea6\u8303\u56f4\u5185\u64cd\u4f5c\u65f6\u5e38\u89c1\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u9488\u5bf9\u7740\u9646\u95ee\u9898\u91cf\u8eab\u5b9a\u5236\u7684\u5c3a\u5ea6\u611f\u77e5\u4e13\u5bb6\u8def\u7531\u7b56\u7565\uff0c\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u81ea\u4e3b\u4e0b\u964d\u7684\u5f39\u6027\u89c6\u89c9\u611f\u77e5\uff0c\u5e76\u4e3a\u672a\u6765\u591a\u4e13\u5bb6\u65e0\u4eba\u673a\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8eGPS\u53d7\u9650\u6216\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u7740\u9646\u3002"}}
{"id": "2512.14057", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14057", "abs": "https://arxiv.org/abs/2512.14057", "authors": ["Amir M. Soufi Enayati", "Homayoun Honari", "Homayoun Najjaran"], "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.", "AI": {"tldr": "CRAFT\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4ece\u72b6\u6001\u548c\u5956\u52b1\u5e8f\u5217\u63a8\u65ad\u4efb\u52a1\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u89e3\u8026\u4e86\u4efb\u52a1\u63a8\u65ad\u4e0e\u7b56\u7565\u4f18\u5316\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u9002\u5e94\u548c\u66f4\u597d\u7684\u6cdb\u5316\u3002", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u4efb\u52a1\u8868\u793a\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u5b8c\u6574\u52a8\u4f5c\u4fe1\u606f\uff0c\u5bfc\u81f4\u4efb\u52a1\u63a8\u65ad\u4e0e\u7279\u5b9a\u7b56\u7565\u7d27\u5bc6\u8026\u5408\u3002", "method": "CRAFT\u662f\u4e00\u79cd\u4fe1\u5ff5\u6a21\u578b\uff0c\u4ec5\u4ece\u72b6\u6001\u548c\u5956\u52b1\u5e8f\u5217\u63a8\u65ad\u4efb\u52a1\u8868\u793a\uff0c\u4f7f\u7528\u5e26\u6709\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u644a\u9500\u53d8\u5206\u63a8\u65ad\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u4fe1\u5ff5\u66f4\u65b0\u3002", "result": "\u5728MetaWorld ML-10\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRAFT\u76f8\u6bd4\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5143\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u9002\u5e94\u901f\u5ea6\u3001\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u6709\u6548\u7684\u63a2\u7d22\u3002", "conclusion": "\u65e0\u52a8\u4f5c\u63a8\u65ad\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u63a8\u65ad\u4e0e\u7b56\u7565\u4f18\u5316\u652f\u6301\u6a21\u5757\u5316\u8bad\u7ec3\u3002"}}
{"id": "2512.14111", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14111", "abs": "https://arxiv.org/abs/2512.14111", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Tao Teng", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field", "comment": "10 pages, 9 figures", "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u914d\u7f6e\u7a7a\u95f4\u4eba\u4f53\u5de5\u5b66\u573a(CSEF)\uff0c\u4e00\u79cd\u8fde\u7eed\u53ef\u5fae\u7684\u4eba\u4f53\u5173\u8282\u7a7a\u95f4\u573a\uff0c\u7528\u4e8e\u5b9e\u65f6\u4eba\u4f53\u5de5\u5b66\u611f\u77e5\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5728\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u4e2d\u63d0\u9ad8\u6210\u529f\u7387\u548c\u964d\u4f4e\u808c\u8089\u8d1f\u8377\u3002", "motivation": "\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u9700\u8981\u65e0\u78b0\u649e\u3001\u54cd\u5e94\u8fc5\u901f\u4e14\u7b26\u5408\u4eba\u4f53\u5de5\u5b66\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u51cf\u5c11\u64cd\u4f5c\u8005\u75b2\u52b3\u548c\u808c\u8089\u9aa8\u9abc\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u4eba\u4f53\u5de5\u5b66\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u914d\u7f6e\u7a7a\u95f4\u4eba\u4f53\u5de5\u5b66\u573a(CSEF)\uff0c\u5728\u4eba\u4f53\u5173\u8282\u7a7a\u95f4\u6784\u5efa\u8fde\u7eed\u53ef\u5fae\u7684\u573a\uff0c\u91cf\u5316\u4eba\u4f53\u5de5\u5b66\u8d28\u91cf\u5e76\u63d0\u4f9b\u68af\u5ea6\u3002\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u4ece\u73b0\u6709\u6307\u6807\u6784\u5efaCSEF\uff0c\u5305\u542b\u5173\u8282\u6743\u91cd\u548c\u4efb\u52a1\u6761\u4ef6\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u57fa\u4e8e\u68af\u5ea6\u7684\u89c4\u5212\u5668\u4e2d\uff0c\u517c\u5bb9\u963b\u6297\u63a7\u5236\u673a\u5668\u4eba\u3002", "result": "\u57282\u81ea\u7531\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCSEF\u89c4\u5212\u6bd4\u4efb\u52a1\u7a7a\u95f4\u4eba\u4f53\u5de5\u5b66\u89c4\u5212\u5668\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u4f4e\u7684\u4eba\u4f53\u5de5\u5b66\u6210\u672c\u548c\u66f4\u5feb\u7684\u8ba1\u7b97\u901f\u5ea6\u3002\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\u5728\u5355\u81c2\u5f15\u5bfc\u3001\u534f\u4f5c\u94bb\u5b54\u548c\u53cc\u81c2\u534f\u540c\u642c\u8fd0\u4efb\u52a1\u4e2d\uff0c\u80fd\u66f4\u5feb\u964d\u4f4e\u4eba\u4f53\u5de5\u5b66\u6210\u672c\u3001\u66f4\u63a5\u8fd1\u4f18\u5316\u5173\u8282\u76ee\u6807\u3001\u964d\u4f4e\u808c\u8089\u6fc0\u6d3b\u6c34\u5e73\u3002\u534f\u4f5c\u94bb\u5b54\u4efb\u52a1\u5e73\u5747\u4eba\u4f53\u5de5\u5b66\u8bc4\u5206\u964d\u4f4e10.31%\uff0c\u53cc\u81c2\u534f\u540c\u642c\u8fd0\u4efb\u52a1\u964d\u4f4e5.60%\u3002", "conclusion": "CSEF\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u4eba\u4f53\u5de5\u5b66\u611f\u77e5\u8fd0\u52a8\u89c4\u5212\uff0c\u663e\u8457\u964d\u4f4e\u64cd\u4f5c\u8005\u808c\u8089\u8d1f\u8377\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2512.14189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14189", "abs": "https://arxiv.org/abs/2512.14189", "authors": ["Johannes A. Gaus", "Daniel H\u00e4ufle", "Woo-Jeong Baek"], "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry", "comment": null, "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.", "AI": {"tldr": "SUPER\u6846\u67b6\u901a\u8fc7\u7075\u654f\u5ea6\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0VIO\u5b9e\u65f6\u98ce\u9669\u8bc4\u4f30\uff0c\u53ef\u63d0\u524d50\u5e27\u9884\u6d4b\u8f68\u8ff9\u9000\u5316\uff0c\u5e76\u542f\u52a8\u505c\u6b62\u6216\u91cd\u5b9a\u4f4d\u7b56\u7565\u3002", "motivation": "\u73b0\u6709VO/VIO/SLAM\u7cfb\u7edf\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u8fd0\u884c\u65f6\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u6765\u5b9e\u65f6\u8bc4\u4f30VIO\u4e2d\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faSUPER\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u65af-\u725b\u987f\u6b63\u89c4\u77e9\u9635\u7684Schur\u8865\u5757\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u7075\u654f\u5ea6\u53cd\u6620\u4e0d\u786e\u5b9a\u6027\u5bf9\u98ce\u9669\u53d1\u751f\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u6b8b\u5dee\u5927\u5c0f\u3001\u51e0\u4f55\u6761\u4ef6\u548c\u77ed\u671f\u65f6\u95f4\u8d8b\u52bf\u4f30\u8ba1\u98ce\u9669\uff0c\u65e0\u9700\u5730\u9762\u771f\u503c\u77e5\u8bc6\u3002", "result": "\u80fd\u591f\u53ef\u9760\u9884\u6d4b50\u5e27\u524d\u7684\u8f68\u8ff9\u9000\u5316\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534720%\uff1b\u542f\u52a8\u505c\u6b62\u6216\u91cd\u5b9a\u4f4d\u7b56\u7565\u7684\u53ec\u56de\u7387\u8fbe89.1%\uff1b\u6846\u67b6\u540e\u7aef\u65e0\u5173\uff0c\u5b9e\u65f6\u8fd0\u884c\uff0c\u989d\u5916CPU\u6210\u672c\u4f4e\u4e8e0.2%\uff1b\u63d0\u4f9b\u4e00\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u957f\u65f6\u7a0b\u5efa\u56fe\u3002", "conclusion": "SUPER\u662f\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u5b9e\u65f6\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u654f\u5ea6\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u6709\u6548\u9884\u6d4bVIO\u6027\u80fd\u9000\u5316\u5e76\u89e6\u53d1\u5e94\u5bf9\u7b56\u7565\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.14206", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14206", "abs": "https://arxiv.org/abs/2512.14206", "authors": ["Mayank Sewlia", "Christos K. Verginis", "Dimos V. Dimarogonas"], "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments", "comment": null, "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u901f\u7387\u89c4\u5212\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u534f\u540c\u642c\u8fd0\u7269\u4f53\uff0c\u6ee1\u8db3\u65f6\u7a7a\u4efb\u52a1\u89c4\u8303", "motivation": "\u89e3\u51b3\u79fb\u52a8\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u5728\u969c\u788d\u7269\u5bc6\u96c6\u3001\u7ea6\u675f\u4e25\u683c\u73af\u5883\u4e0b\u7684\u534f\u540c\u64cd\u4f5c\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u8fde\u7eed\u673a\u5668\u4eba\u52a8\u529b\u5b66\u548c\u79bb\u6563\u51e0\u4f55\u7ea6\u675f", "method": "\u91c7\u7528\u591a\u901f\u7387\u89c4\u5212\u63a7\u5236\u6846\u67b6\uff1a\u79bb\u7ebf\u751f\u6210\u6ee1\u8db3STL\u89c4\u8303\u7684\u5bf9\u8c61\u8f68\u8ff9\u548c\u65e0\u78b0\u649e\u57fa\u5ea7\u8db3\u8ff9\uff0c\u5728\u7ebf\u8fdb\u884c\u7ea6\u675f\u9006\u8fd0\u52a8\u5b66\u548c\u8fde\u7eed\u65f6\u95f4\u53cd\u9988\u63a7\u5236", "result": "\u901a\u8fc7\u4e09\u53f0Franka Emika Panda\u79fb\u52a8\u673a\u68b0\u81c2\u534f\u540c\u521a\u6027\u6293\u53d6\u7269\u4f53\u7684\u9ad8\u4fdd\u771f\u7269\u7406\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u591a\u673a\u68b0\u81c2\u7684\u534f\u8c03\u91cd\u6784\uff0c\u540c\u65f6\u8ddf\u8e2a\u671f\u671b\u7269\u4f53\u8fd0\u52a8\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u534f\u540c\u64cd\u4f5c\u4efb\u52a1"}}
{"id": "2512.14270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14270", "abs": "https://arxiv.org/abs/2512.14270", "authors": ["Zixin Tang", "Yiming Chen", "Quentin Rouxel", "Dianxi Li", "Shuang Wu", "Fei Chen"], "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics", "comment": null, "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/", "AI": {"tldr": "CaFe-TeleVision\u662f\u4e00\u4e2a\u7c97\u5230\u7ec6\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u6c89\u6d78\u5f0f\u60c5\u5883\u53ef\u89c6\u5316\u63d0\u5347\u4eba\u673a\u5de5\u7a0b\u5b66\uff0c\u5728\u53cc\u624b\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u5728\u6548\u7387\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u7b26\u5408\u4eba\u4f53\u5de5\u7a0b\u5b66\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u63a7\u5236\u673a\u5236\u89e3\u51b3\u5de5\u4f5c\u7a7a\u95f4\u5dee\u5f02\u95ee\u9898\uff0c\u7ed3\u5408\u6c89\u6d78\u5f0f\u60c5\u5883\u53ef\u89c6\u5316\u6280\u672f\u964d\u4f4e\u591a\u89c6\u56fe\u5904\u7406\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u7cfb\u7edf\u57fa\u4e8e\u4eba\u5f62\u534f\u4f5c\u673a\u5668\u4eba\u6784\u5efa\u3002", "result": "\u572824\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u5de5\u7a0b\u5b66\uff0c\u964d\u4f4e\u4e86\u4efb\u52a1\u8d1f\u8377\uff0c\u63d0\u9ad8\u4e86\u7528\u6237\u63a5\u53d7\u5ea6\u3002\u57286\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u5347\u8fbe28.89%\uff0c\u5b8c\u6210\u65f6\u95f4\u52a0\u901f26.81%\u3002", "conclusion": "CaFe-TeleVision\u901a\u8fc7\u7c97\u5230\u7ec6\u63a7\u5236\u548c\u6c89\u6d78\u5f0f\u60c5\u5883\u53ef\u89c6\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u548c\u6027\u80fd\uff0c\u4e3a\u8fdc\u7a0b\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.14331", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14331", "abs": "https://arxiv.org/abs/2512.14331", "authors": ["Rishabh Dev Yadav", "Avirup Das", "Hongyu Song", "Samuel Kaski", "Wei Pan"], "title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning", "comment": null, "summary": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b9e\u65f6\u9002\u5e94\u673a\u5668\u4eba\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u6f5c\u5728\u8868\u793a\u652f\u6301\u5728\u7ebf\u8d1d\u53f6\u65af\u66f4\u65b0\uff0c\u5305\u542b\u53d8\u5316\u70b9\u68c0\u6d4b\u673a\u5236\u5904\u7406\u52a8\u6001\u53d8\u5316\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u9700\u8981\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u52a8\u6001\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u5305\u62ec\u6e10\u53d8\u6f02\u79fb\u3001\u77ac\u6001\u6ce2\u52a8\u548c\u7a81\u53d8\u504f\u79fb\uff0c\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u65e2\u8981\u5bf9\u77ed\u671f\u53d8\u5316\u9c81\u68d2\uff0c\u53c8\u8981\u5bf9\u6301\u4e45\u53d8\u5316\u54cd\u5e94\u3002", "method": "\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u5728\u7ebf\u9002\u5e94\u89e3\u8026\uff1a\u79bb\u7ebf\u5b66\u4e60\u6f5c\u5728\u8868\u793a\uff0c\u652f\u6301\u5728\u7ebf\u95ed\u5f0f\u8d1d\u53f6\u65af\u66f4\u65b0\uff1b\u5f15\u5165\u53d8\u5316\u70b9\u611f\u77e5\u673a\u5236\uff0c\u901a\u8fc7\u6570\u636e\u4f3c\u7136\u63a8\u65ad\u6f5c\u5728\u53d8\u91cf\u6307\u793a\u8fde\u7eed\u6027\u6216\u53d8\u5316\uff1b\u8fde\u7eed\u6027\u65f6\u79ef\u7d2f\u8bc1\u636e\u4f18\u5316\u9884\u6d4b\uff0c\u68c0\u6d4b\u5230\u53d8\u5316\u65f6\u8c03\u6574\u8fc7\u53bb\u4fe1\u606f\u5b9e\u73b0\u5feb\u901f\u91cd\u65b0\u5b66\u4e60\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u6846\u67b6\u7684\u81ea\u9002\u5e94\u9057\u61be\u4ec5\u968f\u65f6\u95f4\u5bf9\u6570\u589e\u957f\uff0c\u4e0e\u53d8\u5316\u6b21\u6570\u7ebf\u6027\u76f8\u5173\uff0c\u6027\u80fd\u63a5\u8fd1\u77e5\u9053\u53d8\u5316\u65f6\u95f4\u7684\u795e\u8c15\uff1b\u5728\u5012\u7acb\u6446\u4eff\u771f\u548c\u771f\u5b9e\u56db\u65cb\u7ffc\u98de\u884c\u5b9e\u9a8c\u4e2d\uff08\u5305\u62ec\u6446\u52a8\u8f7d\u8377\u548c\u98de\u884c\u4e2d\u6389\u843d\uff09\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u793a\u51fa\u66f4\u597d\u7684\u9884\u6d4b\u7cbe\u5ea6\u3001\u66f4\u5feb\u6062\u590d\u548c\u66f4\u51c6\u786e\u7684\u95ed\u73af\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u53d8\u5316\uff0c\u4fdd\u6301\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u5bf9\u77ac\u6001\u3001\u6e10\u53d8\u6216\u7ed3\u6784\u53d8\u5316\u7684\u6982\u7387\u63a8\u7406\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.14340", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14340", "abs": "https://arxiv.org/abs/2512.14340", "authors": ["Aleksi Karhunen", "Teemu Hakala", "V\u00e4in\u00f6 Karjalainen", "Eija Honkavaara"], "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u6fc0\u5149\u96f7\u8fbe\u7684\u81ea\u4e3b\u98de\u884c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u5728\u68ee\u6797\u51a0\u5c42\u4e0b\u8fdb\u884c\u4e25\u683c\u6d4b\u8bd5\uff0c\u4f18\u5316\u540e\u7cfb\u7edf\u5728\u4e2d\u7b49\u5bc6\u5ea6\u548c\u8302\u5bc6\u68ee\u6797\u4e2d\u5206\u522b\u8fbe\u523012/15\u548c15/15\u7684\u6210\u529f\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6\u3002", "motivation": "\u867d\u7136\u65e0\u4eba\u673a\u5728\u68ee\u6797\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u51a0\u5c42\u4e0b\u81ea\u4e3b\u5bfc\u822a\u4ecd\u662f\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e25\u8c28\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u62a5\u544a\u6807\u51c6\uff0c\u5f88\u5c11\u62a5\u544a\u6d4b\u8bd5\u68ee\u6797\u5bc6\u5ea6\u3001\u96be\u5ea6\u3001\u591a\u6b21\u98de\u884c\u548c\u6210\u529f\u7387\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\u5e76\u5efa\u7acb\u6807\u51c6\u5316\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90\u7b97\u6cd5\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6fc0\u5149\u96f7\u8fbe\u81ea\u4e3b\u98de\u884c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u4f7f\u7528IPC\u8def\u5f84\u89c4\u5212\u5668\u548cLTA-OM SLAM\u7b97\u6cd5\u3002\u8fdb\u884c93\u6b21\u98de\u884c\u6d4b\u8bd5\uff0c\u5305\u62ec33\u6b21\u521d\u6b65\u6d4b\u8bd5\u540e\u4f18\u5316\u7cfb\u7edf\uff0c\u518d\u8fdb\u884c60\u6b21\u6d4b\u8bd5\u3002\u5728\u4e0d\u540c\u68ee\u6797\u5bc6\u5ea6\uff08\u4e2d\u7b49\u5bc6\u5ea6\u548c\u8302\u5bc6\u68ee\u6797\uff09\u548c\u98de\u884c\u901f\u5ea6\uff081m/s\u548c2m/s\uff09\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u4f18\u5316\u540e\u7cfb\u7edf\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1a\u57281m/s\u98de\u884c\u901f\u5ea6\u4e0b\uff0c\u4e2d\u7b49\u5bc6\u5ea6\u68ee\u6797\u6210\u529f\u738712/15\uff0880%\uff09\uff0c\u8302\u5bc6\u68ee\u6797\u6210\u529f\u738715/15\uff08100%\uff09\uff1b\u57282m/s\u98de\u884c\u901f\u5ea6\u4e0b\uff0c\u4e2d\u7b49\u5bc6\u5ea6\u68ee\u6797\u6210\u529f\u738712/15\uff0880%\uff09\uff0c\u8302\u5bc6\u68ee\u6797\u6210\u529f\u73875/15\uff0833%\uff09\u3002\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u5747\u6709\u6539\u5584\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u68ee\u6797\u51a0\u5c42\u4e0b\u81ea\u4e3b\u98de\u884c\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5f00\u6e90\u7b97\u6cd5\u5728\u5b9e\u9645\u68ee\u6797\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u8bbe\u7f6e\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u81ea\u4e3b\u51a0\u5c42\u4e0b\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u6027\u80fd\u6bd4\u8f83\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3001\u6307\u5bfc\u7cfb\u7edf\u6539\u8fdb\u5e76\u52a0\u901f\u68ee\u6797\u673a\u5668\u4eba\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2512.14355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14355", "abs": "https://arxiv.org/abs/2512.14355", "authors": ["J\u00f6rg Gamerdinger", "Sven Teufel", "Georg Volk", "Oliver Bringmann"], "title": "CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection", "comment": "Accepted at IEEE IV 2023", "summary": "Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6837\u6761\u4f30\u8ba1\u7684\u5b9e\u65f6\u8f66\u9053\u96c6\u4f53\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f66\u8f66\u901a\u4fe1\u6269\u5c55\u5c40\u90e8\u611f\u77e5\u80fd\u529b\uff0c\u5c06\u611f\u77e5\u8303\u56f4\u63d0\u5347200%", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u5168\u9762\u73af\u5883\u611f\u77e5\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u4f46\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u8303\u56f4\u3001\u906e\u6321\u548c\u5f2f\u9053\u7b49\u56e0\u7d20\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b8c\u6574\u7684\u8f66\u9053\u68c0\u6d4b\u3002\u5728\u7f3a\u4e4f\u7cbe\u786e\u5b9a\u4f4d\u6216\u9ad8\u7cbe\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\uff0c\u8f66\u8f86\u5fc5\u987b\u4f9d\u8d56\u81ea\u8eab\u611f\u77e5\u7684\u9053\u8def\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u8f66\u8f66\u901a\u4fe1\u6269\u5c55\u5c40\u90e8\u611f\u77e5\u80fd\u529b", "method": "\u91c7\u7528\u57fa\u4e8e\u6837\u6761\u7684\u65b9\u6cd5\u4f30\u8ba1\u672a\u68c0\u6d4b\u5230\u7684\u9053\u8def\u8def\u6bb5\uff0c\u63d0\u51fa\u5b9e\u65f6\u53ef\u884c\u7684\u8f66\u9053\u96c6\u4f53\u611f\u77e5\u878d\u5408\u7b97\u6cd5\uff0c\u5229\u7528\u8f66\u8f86\u95f4\u901a\u4fe1\u5171\u4eab\u9053\u8def\u4fe1\u606f", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u5728\u5404\u79cd\u60c5\u51b5\u548c\u9053\u8def\u7c7b\u578b\u4e0b\u8bc4\u4f30\u4e86\u878d\u5408\u7b97\u6cd5\uff0c\u611f\u77e5\u8303\u56f4\u6269\u5c55\u4e86\u6700\u9ad8200%", "conclusion": "\u901a\u8fc7\u8f66\u8f66\u901a\u4fe1\u5b9e\u73b0\u8f66\u9053\u96c6\u4f53\u611f\u77e5\u662f\u53ef\u884c\u7684\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u6269\u5c55\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u611f\u77e5\u8303\u56f4\uff0c\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027"}}
{"id": "2512.14367", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14367", "abs": "https://arxiv.org/abs/2512.14367", "authors": ["Georg Volk", "J\u00f6rg Gamerdinger", "Alexander von Bernuth", "Oliver Bringmann"], "title": "A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems", "comment": "Accepted at IEEE ITSC 2020", "summary": "Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u9a7e\u9a76\u7269\u4f53\u611f\u77e5\u5b89\u5168\u8bc4\u4f30\u6307\u6807\uff0c\u7efc\u5408\u8003\u8651\u7269\u4f53\u901f\u5ea6\u3001\u65b9\u5411\u3001\u8ddd\u79bb\u3001\u5c3a\u5bf8\u548c\u6f5c\u5728\u78b0\u649e\u635f\u5bb3\u7b49\u56e0\u7d20\uff0c\u751f\u6210\u5355\u4e00\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u7269\u4f53\u611f\u77e5\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u5145\u5206\u8003\u8651\u4e0d\u540c\u7269\u4f53\u5bf9\u5b89\u5168\u7684\u91cd\u8981\u6027\u5dee\u5f02\u3002\u7269\u4f53\u7684\u901f\u5ea6\u3001\u65b9\u5411\u3001\u8ddd\u79bb\u3001\u5c3a\u5bf8\u4ee5\u53ca\u6f5c\u5728\u78b0\u649e\u635f\u5bb3\u7b49\u56e0\u7d20\u90fd\u4f1a\u5f71\u54cd\u5176\u5b89\u5168\u91cd\u8981\u6027\uff0c\u9700\u8981\u7efc\u5408\u8003\u8651\u8fd9\u4e9b\u53c2\u6570\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u5b89\u5168\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u8bc4\u4f30\u6307\u6807\uff0c\u6574\u5408\u591a\u4e2a\u5b89\u5168\u76f8\u5173\u53c2\u6570\uff08\u901f\u5ea6\u3001\u65b9\u5411\u3001\u8ddd\u79bb\u3001\u5c3a\u5bf8\u3001\u6f5c\u5728\u78b0\u649e\u635f\u5bb3\uff09\uff0c\u5c06\u8fd9\u4e9b\u53c2\u6570\u7edf\u4e00\u5230\u4e00\u4e2a\u5355\u4e00\u3001\u6613\u4e8e\u89e3\u91ca\u7684\u5b89\u5168\u8bc4\u4f30\u5206\u6570\u4e2d\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u548c\u865a\u62df\u6570\u636e\u96c6\u5bf9\u65b0\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u6bd4\u8f83\u9a8c\u8bc1\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u5b89\u5168\u8bc4\u4f30\u6307\u6807\u80fd\u591f\u7efc\u5408\u8003\u8651\u591a\u79cd\u5b89\u5168\u76f8\u5173\u56e0\u7d20\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7269\u4f53\u611f\u77e5\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u66f4\u51c6\u786e\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2512.14411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14411", "abs": "https://arxiv.org/abs/2512.14411", "authors": ["Mohammed Ayman Habib", "Aldo Petruzzelli"], "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids", "comment": "6 pages; xTech Humanoid white paper submission", "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.", "AI": {"tldr": "Omnia\u63d0\u51fa\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u6d41\u6c34\u7ebf\uff0c\u52a0\u901f\u519b\u4e8b\u5316\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u90e8\u7f72\u51c6\u5907\uff0c\u901a\u8fc7\u5c06\u7b2c\u4e00\u4eba\u79f0\u7a7a\u95f4\u89c2\u6d4b\u8f6c\u6362\u4e3a\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u5b9e\u5730\u8bd5\u9a8c\u6210\u672c\u9ad8\u3001\u98ce\u9669\u5927\u3001\u8017\u65f6\u957f\uff0c\u9650\u5236\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5f00\u53d1\u8fed\u4ee3\u901f\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u751f\u6210\u591a\u6837\u5316\u573a\u666f\u6570\u636e\uff0c\u652f\u6301\u590d\u6742\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u5f00\u53d1\u3002", "method": "\u5c06\u7b2c\u4e00\u4eba\u79f0\u7a7a\u95f4\u89c2\u6d4b\uff08\u6765\u81eaPOV\u5f55\u50cf\u3001\u667a\u80fd\u773c\u955c\u3001AR\u5934\u663e\u3001\u7a7a\u95f4\u6d4f\u89c8\u5de5\u4f5c\u6d41\uff09\u8f6c\u6362\u4e3a\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u6570\u636e\u96c6\uff0c\u751f\u6210\u5927\u91cf\u9ad8\u4fdd\u771f\u6a21\u62df\u573a\u666f\uff0c\u7ed3\u5408\u81ea\u52a8\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u80fd\u591f\u5feb\u901f\u4e3a\u65b0\u4f5c\u6218\u73af\u5883\u548c\u5a01\u80c1\u6761\u4ef6\u8c03\u6574\u6570\u636e\u96c6\uff0c\u652f\u6301\u4eba\u5f62\u673a\u5668\u4eba\u57fa\u7ebf\u6027\u80fd\u548c\u9ad8\u7ea7\u5b50\u7cfb\u7edf\uff08\u591a\u6a21\u6001\u611f\u77e5\u3001\u53cd\u68c0\u6d4b\u751f\u5b58\u80fd\u529b\u3001CBRNE\u76f8\u5173\u4fa6\u5bdf\u884c\u4e3a\uff09\u7684\u5f00\u53d1\u3002", "conclusion": "\u8be5\u5408\u6210\u6570\u636e\u9a71\u52a8\u6d41\u6c34\u7ebf\u901a\u8fc7\u65e9\u671f\u66b4\u9732\u4e8e\u5e7f\u6cdb\u573a\u666f\u591a\u6837\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u5f00\u53d1\u5468\u671f\u548c\u590d\u6742\u5bf9\u6297\u73af\u5883\u4e2d\u4eba\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u63d0\u5347\u3002"}}
{"id": "2512.14428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14428", "abs": "https://arxiv.org/abs/2512.14428", "authors": ["Aaron Kurda", "Simon Steuernagel", "Lukas Jung", "Marcus Baum"], "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations", "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)", "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .", "AI": {"tldr": "Odyssey\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8eGNSS\u4fe1\u53f7\u7f3a\u5931\u73af\u5883\uff08\u5982\u96a7\u9053\u3001\u505c\u8f66\u573a\uff09\u7684\u6fc0\u5149\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\u6570\u636e\u96c6\uff0c\u9996\u6b21\u91c7\u7528\u57fa\u4e8e\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u7684\u5bfc\u822a\u7ea7\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u503c\u3002", "motivation": "\u73b0\u6709LIO/SLAM\u6570\u636e\u96c6\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u73af\u5883\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff1aGNSS\u4fe1\u53f7\u5728\u906e\u6321\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u4f7f\u7528\u7684MEMS\u6216FOG\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u65e0\u6cd5\u652f\u6301\u957f\u65f6\u95f4GNSS\u7f3a\u5931\u73af\u5883\u7684\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86Odyssey\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8eGNSS\u4fe1\u53f7\u7f3a\u5931\u73af\u5883\uff08\u96a7\u9053\u3001\u505c\u8f66\u573a\u7b49\uff09\u4ee5\u53ca\u5176\u4ed6\u4ee3\u8868\u6027\u573a\u666f\uff08\u8d70\u8d70\u505c\u505c\u4ea4\u901a\u3001\u98a0\u7c38\u9053\u8def\u3001\u5f00\u9614\u573a\u5730\uff09\u3002\u91c7\u7528\u57fa\u4e8e\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u7684\u5bfc\u822a\u7ea7\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u5730\u9762\u771f\u503c\uff0c\u5177\u6709\u4f18\u5f02\u7684\u504f\u7f6e\u7a33\u5b9a\u6027\u3002", "result": "Odyssey\u6210\u4e3a\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u57fa\u4e8e\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u957f\u65f6\u95f4GNSS\u7f3a\u5931\u73af\u5883\u7684\u7cbe\u786e\u7814\u7a76\u3002\u6570\u636e\u96c6\u5305\u542b\u6240\u6709\u8f68\u8ff9\u7684\u4e09\u6b21\u91cd\u590d\u4ee5\u652f\u6301\u4f4d\u7f6e\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u7cbe\u786e\u7684\u5927\u5730\u5750\u6807\u4ee5\u96c6\u6210\u5916\u90e8\u5730\u56fe\u6570\u636e\u3002", "conclusion": "Odyssey\u586b\u8865\u4e86\u73b0\u6709LIO\u6570\u636e\u96c6\u5728GNSS\u7f3a\u5931\u73af\u5883\u7814\u7a76\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u7684\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u5730\u9762\u771f\u503c\uff0c\u4e3aLIO/SLAM\u7cfb\u7edf\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.14434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.14434", "abs": "https://arxiv.org/abs/2512.14434", "authors": ["Quan Yuan", "Daqian Cao", "Weibang Bai"], "title": "Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination", "comment": "7 pages, 5 figures", "summary": "Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b3-(PP(2-(UPS)))\u5197\u4f59\u5e76\u8054\u673a\u6784\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5173\u952e\u51e0\u4f55\u53c2\u6570\u5bf9\u5de5\u4f5c\u7a7a\u95f4\u4f53\u79ef\u3001\u5f62\u72b6\u3001\u8fb9\u754c\u5b8c\u6574\u6027\u548c\u5b9a\u5411\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5b9a\u4e49\u4e86\u8bc4\u4ef7\u673a\u6784\u5b9a\u5411\u6027\u80fd\u7684\u6307\u6807\uff0c\u4e3a\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u5197\u4f59\u5e76\u8054\u673a\u5668\u4eba\u901a\u5e38\u7528\u4e8e\u9700\u8981\u826f\u597d\u7cbe\u5ea6\u3001\u9ad8\u8d1f\u8f7d\u80fd\u529b\u548c\u5927\u5de5\u4f5c\u7a7a\u95f4\u7684\u573a\u666f\uff0c\u4f46\u5176\u57fa\u672c\u673a\u5668\u4eba\u914d\u7f6e\u548c\u51e0\u4f55\u53c2\u6570\u4f18\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u578b3-(PP(2-(UPS)))\u5197\u4f59\u5e76\u8054\u673a\u6784\uff0c\u5206\u6790\u5173\u952e\u51e0\u4f55\u53c2\u6570\u5bf9\u5de5\u4f5c\u7a7a\u95f4\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5b9a\u4e49\u626d\u8f6c\u80fd\u529b\u6307\u6570TI_1\u548c\u503e\u659c\u80fd\u529b\u6307\u6570TI_2\u6765\u8bc4\u4f30\u673a\u6784\u5b9a\u5411\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u7814\u7a76\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\uff0c\u4e3a3-(PP(2-(UPS)))\u53ca\u5176\u4ed6\u7c7b\u4f3c\u5197\u4f59\u5e76\u8054\u673a\u6784\u7684\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u5408\u7406\u4e14\u5fc5\u8981\u7684\u53c2\u8003\u4f9d\u636e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u578b\u5197\u4f59\u5e76\u8054\u673a\u6784\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u901a\u8fc7\u5206\u6790\u5173\u952e\u51e0\u4f55\u53c2\u6570\u5bf9\u5de5\u4f5c\u7a7a\u95f4\u7684\u5f71\u54cd\u5e76\u5b9a\u4e49\u5b9a\u5411\u6027\u80fd\u8bc4\u4ef7\u6307\u6807\uff0c\u4e3a\u5197\u4f59\u5e76\u8054\u673a\u6784\u7684\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2512.14666", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14666", "abs": "https://arxiv.org/abs/2512.14666", "authors": ["Zechen Bai", "Chen Gao", "Mike Zheng Shou"], "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models", "comment": "15 pages", "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.", "AI": {"tldr": "EVOLVE-VLA\u662f\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6846\u67b6\uff0c\u8ba9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u6301\u7eed\u81ea\u9002\u5e94\uff0c\u65e0\u9700\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u6f14\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u9700\u8981\u5927\u91cf\u6f14\u793a\u3001\u6b7b\u8bb0\u786c\u80cc\u8f68\u8ff9\u3001\u65e0\u6cd5\u9002\u5e94\u90e8\u7f72\u6761\u4ef6\u53d8\u5316\u3002\u9700\u8981\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u9002\u5e94\u5177\u8eab\u667a\u80fd\uff0c\u8ba9\u667a\u80fd\u4f53\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u5b9e\u8df5\u6301\u7eed\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u7684\u8fdb\u5ea6\u4f30\u8ba1\u5668\u63d0\u4f9b\u5bc6\u96c6\u53cd\u9988\uff0c\u91c7\u7528\u7d2f\u79ef\u8fdb\u5ea6\u4f30\u8ba1\u673a\u5236\u5e73\u6ed1\u566a\u58f0\u70b9\u4f30\u8ba1\uff0c\u4ee5\u53ca\u6e10\u8fdb\u5f0f\u89c6\u91ce\u6269\u5c55\u7b56\u7565\u5b9e\u73b0\u9010\u6b65\u7b56\u7565\u6f14\u5316\uff0c\u4ece\u800c\u9a6f\u670d\u566a\u58f0\u4fe1\u53f7\u3002", "result": "\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u63d0\u53478.6%\uff0c\u5728\u5355\u6837\u672c\u5b66\u4e60\u4e0a\u63d0\u534722.0%\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316\u2014\u2014\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8fbe\u523020.8%\u6210\u529f\u7387\uff08\u7eaf\u76d1\u7763\u5fae\u8c03\u4e3a0%\uff09\uff0c\u5c55\u73b0\u51fa\u9519\u8bef\u6062\u590d\u548c\u65b0\u7b56\u7565\u7b49\u6d8c\u73b0\u80fd\u529b\u3002", "conclusion": "EVOLVE-VLA\u4ee3\u8868\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4ece\u9759\u6001\u6a21\u4eff\u5411\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u7684\u5173\u952e\u4e00\u6b65\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7684\u5177\u8eab\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.14689", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14689", "abs": "https://arxiv.org/abs/2512.14689", "authors": ["Sirui Chen", "Zi-ang Cao", "Zhengyi Luo", "Fernando Casta\u00f1eda", "Chenran Li", "Tingwu Wang", "Ye Yuan", "Linxi \"Jim\" Fan", "C. Karen Liu", "Yuke Zhu"], "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation", "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/", "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.", "AI": {"tldr": "CHIP\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u81ea\u9002\u5e94\u67d4\u987a\u63a7\u5236\u6a21\u5757\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u4fdd\u6301\u52a8\u6001\u53c2\u8003\u8fd0\u52a8\u654f\u6377\u8ddf\u8e2a\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u672b\u7aef\u6267\u884c\u5668\u521a\u5ea6\uff0c\u4ece\u800c\u6267\u884c\u9700\u8981\u4e0d\u540c\u67d4\u987a\u5ea6\u7684\u5f3a\u529b\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u867d\u7136\u4eba\u5f62\u673a\u5668\u4eba\u5728\u654f\u6377\u8fd0\u52a8\u6280\u80fd\uff08\u5982\u540e\u7a7a\u7ffb\u3001\u8dd1\u6b65\u3001\u722c\u884c\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u6267\u884c\u5f3a\u529b\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u79fb\u52a8\u7269\u4f53\u3001\u64e6\u62ed\u3001\u63a8\u8f66\uff09\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u63a7\u7684\u672b\u7aef\u6267\u884c\u5668\u67d4\u987a\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u67d4\u987a\u4eba\u5f62\u63a7\u5236\u901a\u8fc7\u89c6\u89c9\u6270\u52a8\uff08CHIP\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u52a8\u6001\u53c2\u8003\u8fd0\u52a8\u654f\u6377\u8ddf\u8e2a\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u63a7\u7684\u672b\u7aef\u6267\u884c\u5668\u521a\u5ea6\u3002\u8be5\u65b9\u6cd5\u6613\u4e8e\u5b9e\u73b0\uff0c\u65e2\u4e0d\u9700\u8981\u6570\u636e\u589e\u5f3a\uff0c\u4e5f\u4e0d\u9700\u8981\u989d\u5916\u7684\u5956\u52b1\u8c03\u6574\u3002", "result": "\u4f7f\u7528CHIP\u8bad\u7ec3\u7684\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u63a7\u5236\u5668\u80fd\u591f\u6267\u884c\u591a\u79cd\u9700\u8981\u4e0d\u540c\u672b\u7aef\u6267\u884c\u5668\u67d4\u987a\u5ea6\u7684\u5f3a\u529b\u64cd\u4f5c\u4efb\u52a1\uff0c\u5305\u62ec\u591a\u673a\u5668\u4eba\u534f\u4f5c\u3001\u64e6\u62ed\u3001\u7bb1\u5b50\u9012\u9001\u548c\u5f00\u95e8\u7b49\u4efb\u52a1\u3002", "conclusion": "CHIP\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5f3a\u529b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u67d4\u987a\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u6267\u884c\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
