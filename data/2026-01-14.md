<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Contact-aware Path Planning for Autonomous Neuroendovascular Navigation](https://arxiv.org/abs/2601.07945)
*Aabha Tamhankar,Ron Alterovitz,Ajit S. Puri,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 提出一种确定性、时间高效的接触感知路径规划器，用于神经血管导航，通过利用血管的术前和术中图像信息，智能预测和利用与解剖结构的相互作用来导航预弯曲被动工具。


<details>
  <summary>Details</summary>
Motivation: 神经血管导航需要精确的路径规划，特别是在处理复杂的血管解剖结构时。现有的方法可能计算效率低或准确性不足。本文旨在开发一种能够快速计算可行路径的算法，同时保持高精度，以支持神经血管介入手术。

Method: 提出一种确定性、时间高效的接触感知路径规划算法。该算法利用血管的术前和术中图像信息，通过智能预测和利用与解剖结构的相互作用来导航预弯曲被动工具。推导了运动学模型，并采用基于采样的规划器进行树扩展，利用简化的运动原语。

Result: 在多样化和具有代表性的血管解剖结构中，该算法在最坏情况下在22.8秒内达到100%的收敛率，跟踪误差小于0.64毫米（亚毫米级）。在代表约94%患者的解剖模型中，该算法被证明是有效的。

Conclusion: 该研究提出了一种高效的接触感知路径规划器，能够在神经血管导航中快速计算可行路径，同时保持高精度。该算法在多种解剖结构中表现出优异的性能，具有临床应用的潜力。

Abstract: We propose a deterministic and time-efficient contact-aware path planner for neurovascular navigation. The algorithm leverages information from pre- and intra-operative images of the vessels to navigate pre-bent passive tools, by intelligently predicting and exploiting interactions with the anatomy. A kinematic model is derived and employed by the sampling-based planner for tree expansion that utilizes simplified motion primitives. This approach enables fast computation of the feasible path, with negligible loss in accuracy, as demonstrated in diverse and representative anatomies of the vessels. In these anatomical demonstrators, the algorithm shows a 100% convergence rate within 22.8s in the worst case, with sub-millimeter tracking errors (less than 0.64 mm), and is found effective on anatomical phantoms representative of around 94% of patients.

</details>


### [2] [Fiducial Exoskeletons: Image-Centric Robot State Estimation](https://arxiv.org/abs/2601.08034)
*Cameron Smith,Basile Van Hoorick,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: Fiducial Exoskeletons：一种基于图像的3D机器人状态估计方法，通过单张RGB图像估计机器人各连杆的6D位姿，替代了传统繁琐的手眼标定流程。


<details>
  <summary>Details</summary>
Motivation: 传统机器人状态估计方法依赖高精度执行器和耗时的手眼标定流程，而现代基于学习的机器人控制越来越多地使用RGB观测在低成本硬件上训练和部署。需要一种更简单、更鲁棒的机器人状态估计方法。

Method: 1. 将机器人状态估计重新定义为从单张RGB图像估计每个连杆的6D位姿；2. 引入"基准外骨骼"：在每个连杆上安装带有基准标记的3D打印支架，已知标记-连杆几何关系；3. 通过轻量级全局优化恢复关节状态，确保与观测到的连杆位姿保持运动学一致性。

Result: 在低成本机械臂上验证，基准外骨骼显著简化了设置流程，同时提高了标定精度、状态准确性和下游3D控制性能。即使机器人断电也能实现鲁棒的状态估计。

Conclusion: 基准外骨骼提供了一种简单、鲁棒的图像式机器人状态估计方法，实现了从单张图像获取相机-机器人外参、每连杆SE(3)位姿和关节角度状态，促进了算法-硬件协同设计。

Abstract: We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.
  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.
  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.

</details>


### [3] [Efficient Incremental SLAM via Information-Guided and Selective Optimization](https://arxiv.org/abs/2601.08110)
*Reza Arablouei*

Main category: cs.RO

TL;DR: 提出了一种高效的增量SLAM后端优化方法，通过信息引导门控和选择性部分优化，在保持全批量优化精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统增量SLAM方法在计算效率和全局一致性之间存在权衡。全批量优化虽然准确但计算成本高，而常规增量方法可能牺牲精度或导致不必要的重新线性化。需要一种既能保持全局一致性又能高效处理动态数据流的方法。

Method: 结合两种互补技术：1) 信息引导门控(IGG)：基于信息矩阵对数行列式的信息理论准则量化新测量的贡献，仅在观察到显著信息增益时触发全局优化；2) 选择性部分优化(SPO)：执行多轮高斯牛顿更新，但每轮仅优化受新测量影响最大的变量子集，动态调整活跃集直至收敛。

Result: 在基准SLAM数据集上的实验表明，该方法始终匹配批量求解器的估计精度，同时相比传统增量方法实现显著计算节省。理论分析证明该方法保持了完整高斯牛顿的收敛保证。

Conclusion: 该方法在精度和效率之间提供了原则性的平衡，是动态数据丰富环境中实时操作的稳健可扩展解决方案，保留了所有测量以保持全局一致性，同时将计算集中在收益最大的图部分。

Abstract: We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.

</details>


### [4] [A Pin-Array Structure for Gripping and Shape Recognition of Convex and Concave Terrain Profiles](https://arxiv.org/abs/2601.08143)
*Takuya Kato,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发了一种用于移动机器人的夹持器，能够抓握和识别极端环境中的地形形状，通过销钉阵列结构实现自适应抓握和地形测量。


<details>
  <summary>Details</summary>
Motivation: 多肢攀爬机器人在粗糙地形（如悬崖和洞穴墙壁）上很有效，但在未知自然环境中可能因误抓表面或失去可抓握点而摔倒或卡住。需要一种能够自适应抓握不规则地形并准确测量地形形状的夹持器。

Method: 开发了一种采用销钉阵列结构的夹持器，能够抓握凸面和凹面地形，并同时测量地形形状。通过原型机评估了其抓握和地形识别性能。

Result: 销钉阵列设计不仅适用于自适应抓握不规则地形，还能很好地用于3D地形映射。原型机验证了该夹持器的抓握和地形识别性能。

Conclusion: 提出的销钉阵列夹持器能够有效解决极端环境中移动机器人的自适应抓握和地形识别问题，为机器人导航和操作提供了重要工具。

Abstract: This paper presents a gripper capable of grasping and recognizing terrain shapes for mobile robots in extreme environments. Multi-limbed climbing robots with grippers are effective on rough terrains, such as cliffs and cave walls. However, such robots may fall over by misgrasping the surface or getting stuck owing to the loss of graspable points in unknown natural environments. To overcome these issues, we need a gripper capable of adaptive grasping to irregular terrains, not only for grasping but also for measuring the shape of the terrain surface accurately. We developed a gripper that can grasp both convex and concave terrains and simultaneously measure the terrain shape by introducing a pin-array structure. We demonstrated the mechanism of the gripper and evaluated its grasping and terrain recognition performance using a prototype. Moreover, the proposed pin-array design works well for 3D terrain mapping as well as adaptive grasping for irregular terrains.

</details>


### [5] [Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching](https://arxiv.org/abs/2601.08161)
*Jing Tao,Banglei Guan,Yang Shang,Shunkun Liang,Qifeng Yu*

Main category: cs.RO

TL;DR: 提出一种用于大规模飞行导航的鲁棒高精度定位方法，通过多层角点筛选和自适应模板匹配解决复杂背景干扰和传统滑动窗口匹配计算效率低的问题


<details>
  <summary>Details</summary>
Motivation: 解决大规模飞行导航中复杂背景干扰导致的定位失败问题，以及传统滑动窗口匹配技术计算效率低下的局限性

Method: 采用三层框架：1) 通过光照均衡和结构信息提取降维；2) 粗到精候选点选择策略减少滑动窗口计算成本；3) 为候选点生成自适应模板，通过改进的模板匹配和相关系数极值拟合实现亚像素精度

Result: 实验结果表明该方法在复杂大规模环境中能有效提取和定位对角标记，适合导航任务中的视场测量

Conclusion: 该方法为大规模飞行导航提供了一种鲁棒、高精度的定位解决方案，特别适用于复杂背景环境下的标记定位任务

Abstract: This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.

</details>


### [6] [FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models](https://arxiv.org/abs/2601.08246)
*Yifan Han,Pengfei Yi,Junyan Li,Hanqing Wang,Gaojing Zhang,Qi Peng Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出了一种数据高效的灵巧抓取框架，利用预训练扩散模型中的语义先验，从人类视频演示中提取抓取能力，实现跨手部硬件的泛化，无需为每种手型收集大量抓取数据。


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取合成面临高维度和运动多样性的挑战，现有方法依赖硬件特定的大规模抓取数据集，限制了新灵巧手设计的可扩展性。需要一种数据高效的方法来绕过机器人抓取数据收集。

Method: 1) 从原始人类视频演示中提取时间对齐的细粒度抓取能力；2) 将抓取能力与深度图像的3D场景几何融合，推断语义接地点；3) 通过运动学感知的重定向模块将能力表示映射到不同的灵巧手，无需为每种手型重新训练。

Result: 系统产生稳定、功能适当的多接触抓取，在常见物体和工具上保持可靠成功，并在未见过的物体实例、姿态变化和多种手部实现上表现出强大的泛化能力。

Conclusion: 这项工作展示了利用人类演示和预训练生成模型实现可扩展、硬件无关的灵巧操作路径，证明了单一深度模态结合基础模型语义足以实现高性能抓取合成。

Abstract: Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.

</details>


### [7] [ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation](https://arxiv.org/abs/2601.08325)
*Zhenyang Liu,Yongchong Gu,Yikai Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: ActiveVLA是一个结合主动感知能力的视觉-语言-动作框架，通过粗到细的两阶段方法（关键区域定位和主动感知优化）实现高精度细粒度机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法大多依赖静态腕部摄像头，缺乏主动感知能力，无法在任务执行过程中自适应选择最佳视角或分辨率，限制了长时程任务和细粒度操作场景的性能。

Method: 采用粗到细的两阶段范式：1）关键区域定位：将3D输入投影到多视角2D投影，识别关键3D区域，支持动态空间感知；2）主动感知优化：基于定位的关键区域，使用主动视角选择策略选择最佳视角，最大化模态相关性和多样性，最小化遮挡，并对关键区域进行3D放大以提高分辨率。

Result: 在三个仿真基准测试中，ActiveVLA实现了精确的3D操作，并优于最先进的基线方法。同时能够无缝迁移到真实世界场景，使机器人能够在复杂环境中学习高精度任务。

Conclusion: ActiveVLA通过赋予机器人主动感知能力，解决了现有视觉-语言-动作方法在细粒度操作中的局限性，为高精度机器人操作提供了有效解决方案。

Abstract: Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.

</details>


### [8] [Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments](https://arxiv.org/abs/2601.08405)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.RO

TL;DR: 提出了一种基于微调CodeT5模型与AirSim无人机模拟器的自然语言控制方法，通过ChatGPT生成的大规模数据集训练，实现自然语言到可执行无人机代码的自动翻译。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，人机交互迎来了前所未有的机遇。传统无人机操作需要专业技能，本文旨在通过自然语言命令降低无人机操作门槛，让用户能够更便捷地控制无人机状态。

Method: 1. 集成微调的CodeT5模型与基于Unreal Engine的AirSim无人机模拟器；2. 使用ChatGPT生成的大规模（自然语言，程序代码）命令-执行对数据集，结合开发者编写的无人机代码作为训练数据；3. 在AirSim中构建视觉逼真的动态环境模拟复杂场景；4. 实现自然语言到可执行无人机代码的自动翻译。

Result: 实验结果表明，所提方法在模拟环境中展现出优越的任务执行效率和命令理解能力。系统能够有效处理多任务操作，显著降低了无人机操作的技术门槛。

Conclusion: 该方法成功实现了通过自然语言控制模拟无人机，未来计划以模块化方式扩展模型功能，增强对复杂场景的适应性，推动无人机技术在现实环境中的应用。

Abstract: Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.

</details>


### [9] [Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech](https://arxiv.org/abs/2601.08422)
*Taerim Yoon,Dongho Kang,Jin Cheng,Fatemeh Zargarbashi,Yijiang Huang,Minsung Ahn,Stelian Coros,Sungjoon Choi*

Main category: cs.RO

TL;DR: 提出一个人机交互框架，让四足机器人通过少量人类演示数据学习理解社交线索和导航行为，支持手势和语音多模态输入，在真实世界敏捷导航场景中达到97.15%成功率。


<details>
  <summary>Details</summary>
Motivation: 让四足机器人能够理解人类社交线索并产生适当行为，但传统物理引导学习需要大量人类数据，给用户带来沉重负担，需要更高效的数据利用方法。

Method: 提出人机交互框架，使用基于物理的仿真重建交互场景并聚合数据以减少分布偏移；采用渐进式目标提示策略，在训练中自适应提供适当命令和导航目标；支持手势和语音多模态自然输入。

Result: 在六个真实世界敏捷导航场景（包括跳跃障碍和避障）中评估，方法在几乎所有试验中都成功，总演示数据少于1小时的情况下达到97.15%任务成功率。

Conclusion: 该方法能够以数据高效的方式让机器人学习导航行为，实现人类输入与机器人行为之间的强对齐，在真实世界复杂导航任务中表现出色。

Abstract: In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.

</details>


### [10] [Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?](https://arxiv.org/abs/2601.08434)
*Long Zhang,Yuchen Xia*

Main category: cs.RO

TL;DR: 本文提出了一种语义与策略双驱动的混合决策框架，将大型多模态模型用于语义理解和认知表示，结合深度强化学习进行实时策略优化，以解决自动驾驶在开放世界场景中的持续学习和联合决策问题。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型虽然能解决模块化自动驾驶设计在开放世界场景中的局限性，但仅依赖LMMs提升具身智能驾驶能力会受到限制，缺乏联合决策能力。需要结合持续学习和联合决策来推进自动驾驶向具身智能驾驶发展。

Method: 提出语义与策略双驱动的混合决策框架，融合LMMs进行语义理解和认知表示，结合深度强化学习进行实时策略优化。通过案例研究验证了该框架在车道变换规划任务中的性能优势。

Result: 通过实验案例研究验证了该框架在完成车道变换规划任务中的性能优越性，展示了其在具身智能驾驶中的有效性。

Conclusion: 该语义与策略双驱动的混合决策框架能够有效解决自动驾驶中的持续学习和联合决策问题，为具身智能驾驶提供了新的研究方向，并识别了多个未来研究方向以指导后续工作。

Abstract: The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.

</details>


### [11] [Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees](https://arxiv.org/abs/2601.08454)
*Alessandro Adami,Sebastian Zudaire,Ruggero Carli,Pietro Falco*

Main category: cs.RO

TL;DR: 提出一种基于行为树的Real2Sim框架，通过视觉语言模型推理生成任务特定的物理交互行为树，自主获取仿真所需物理参数，无需预定义任务模板或专家设计的探索流程。


<details>
  <summary>Details</summary>
Motivation: 传统Real2Sim流水线依赖手动测量或固定的预编程探索流程，限制了其适应不同任务和用户意图的能力。需要一种能够根据具体仿真目标自主获取所需物理参数的方法。

Method: 使用视觉语言模型进行多模态推理，识别相关物体、推断所需物理参数，并生成由基本机器人动作组成的行为树。在扭矩控制的Franka Emika Panda机械臂上执行这些行为，通过顺应性、接触丰富的交互进行参数估计。

Result: 在真实机械臂上的实验结果表明，该方法能够在多种场景下（包括遮挡物体和不完整先验模型）准确估计物体质量、表面高度和摩擦相关参数。

Conclusion: 所提出的方法实现了可解释、意图驱动且自主的Real2Sim流水线，将高级推理与物理基础的机器人交互相结合，为构建准确的环境仿真模型提供了新途径。

Abstract: Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.

</details>


### [12] [AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding](https://arxiv.org/abs/2601.08485)
*Chong Zhang,Victor Klemm,Fan Yang,Marco Hutter*

Main category: cs.RO

TL;DR: AME-2是一个统一的强化学习框架，通过注意力机制的地图编码器和基于学习的映射管道，实现了四足和双足机器人在复杂地形上的敏捷和泛化运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：端到端传感器运动模型泛化能力和可解释性有限；针对泛化运动的方法通常敏捷性不足且难以处理视觉遮挡。需要一种能够同时实现敏捷性和泛化性，并能处理遮挡和稀疏立足点的统一框架。

Method: 提出AME-2框架，包含：1）注意力机制的地图编码器，提取局部和全局地图特征，聚焦关键区域，生成可解释的嵌入表示；2）基于学习的映射管道，将深度观测快速转换为带有不确定性的局部高程图，并与里程计融合，对噪声和遮挡具有鲁棒性；3）与并行仿真集成，支持在线映射训练，促进仿真到现实的迁移。

Result: 在四足和双足机器人上验证了AME-2框架，控制器在仿真和真实世界实验中展现出强大的敏捷性和对未见地形的泛化能力。

Conclusion: AME-2通过注意力机制的地图编码器和鲁棒的映射管道，成功实现了感知与控制的紧密集成，为机器人在复杂地形上的敏捷和泛化运动提供了有效的解决方案。

Abstract: Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.

</details>


### [13] [AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization](https://arxiv.org/abs/2601.08491)
*Mohamed Afouene Melki,Mohammad Shehab,Mohamed-Slim Alouini*

Main category: cs.RO

TL;DR: 本文提出了一种可持续的水下物联网(IoUT)解决方案，通过自主水下航行器(AUV)同时实现信息上行和声学能量传输，采用深度强化学习算法优化年龄信息(AoI)和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统水下物联网设备依赖电池供电，存在寿命限制和废弃后的环境危害问题，需要可持续的解决方案来支持长期水下监测和操作。

Method: 提出通过AUV同时进行信息上行和声学能量传输的可持续方案，采用年龄信息(AoI)和Jain公平性指数作为评估指标，开发了两种深度强化学习算法：高性能的频分双工(FDD)方案和中等性能的时分双工(TDD)方案。

Result: 提出的FDD和TDD解决方案相比基线方法显著降低了平均AoI，提高了能量收集效率和数据收集公平性。

Conclusion: 该研究为水下物联网设备提供了可持续的解决方案，通过AUV同时进行信息传输和能量补给，有望实现设备的无限期运行，并通过DRL算法优化了系统性能。

Abstract: Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain's fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.

</details>


### [14] [Simplifying ROS2 controllers with a modular architecture for robot-agnostic reference generation](https://arxiv.org/abs/2601.08514)
*Davide Risi,Vincenzo Petrone,Antonio Langella,Lorenzo Pagliara,Enrico Ferrentino,Pasquale Chiacchio*

Main category: cs.RO

TL;DR: 提出ROS2模块化架构，通过Reference Generator组件分离参考轨迹获取与控制器逻辑，减少代码重复，提高控制器复用性


<details>
  <summary>Details</summary>
Motivation: 解决ROS2控制器中参考轨迹处理代码重复的问题，提高控制器在不同机器人平台上的复用性

Method: 设计Reference Generator组件，分离参考轨迹获取、验证和插值逻辑；实现关节空间和笛卡尔空间两种参考生成器；开发PD重力补偿、笛卡尔位姿和导纳控制器

Result: 在所有测试场景中可靠跟踪参考轨迹；显著减少控制器中的重复代码；控制器实现专注于控制算法本身

Conclusion: 提出的模块化架构有效分离了参考轨迹处理与控制逻辑，提高了ROS2控制系统的可重用性和可维护性

Abstract: This paper introduces a novel modular architecture for ROS2 that decouples the logic required to acquire, validate, and interpolate references from the control laws that track them. The design includes a dedicated component, named Reference Generator, that receives references, in the form of either single points or trajectories, from external nodes (e.g., planners), and writes single-point references at the controller's sampling period via the existing ros2_control chaining mechanism to downstream controllers. This separation removes duplicated reference-handling code from controllers and improves reusability across robot platforms. We implement two reference generators: one for handling joint-space references and one for Cartesian references, along with a set of new controllers (PD with gravity compensation, Cartesian pose, and admittance controllers) and validate the approach on simulated and real Universal Robots and Franka Emika manipulators. Results show that (i) references are tracked reliably in all tested scenarios, (ii) reference generators reduce duplicated reference-handling code across chained controllers to favor the construction and reuse of complex controller pipelines, and (iii) controller implementations remain focused only on control laws.

</details>


### [15] [Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps](https://arxiv.org/abs/2601.08520)
*Krzysztof Zielinski,Dominik Belter*

Main category: cs.RO

TL;DR: 提出了一种基于关键帧的NDT地图构建系统，利用RGB-D传感器数据更新局部NDT地图，通过姿态图存储局部地图实现回环检测后的全局地图校正，并与现有方法进行对比。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地利用RGB-D相机的特性和不确定性模型，提高近距离物体的表示精度，并实现回环检测后的全局地图校正。

Method: 使用RGB-D传感器数据更新局部NDT地图，将NDT单元存储在2D视图相关结构中，局部地图存储在姿态图中，提出局部地图合并和过滤程序以获得全局环境地图。

Result: 该方法能够更精确地表示靠近相机原点的物体，实现了回环检测后的全局地图校正，并与Octomap和NDT-OM进行了对比。

Conclusion: 提出的基于关键帧的NDT地图构建系统有效利用了RGB-D相机特性，提高了地图精度，并通过姿态图实现了全局地图的优化校正。

Abstract: In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.

</details>


### [16] [QP-Based Control of an Underactuated Aerial Manipulator under Constraints](https://arxiv.org/abs/2601.08523)
*Nesserine Laribi,Mohammed Rida Mokhtari,Abdelaziz Benallegue,Abdelhafid El-Hadri,Mehdi Benallegue*

Main category: cs.RO

TL;DR: 提出了一种针对欠驱动空中机械臂的约束感知控制框架，能够在考虑安全性和可行性约束的同时实现精确的末端执行器轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 针对欠驱动空中机械臂的控制问题，需要在实现精确末端执行器轨迹跟踪的同时，显式地考虑安全性、可行性约束以及系统鲁棒性需求。

Method: 将控制问题表述为二次规划问题，计算满足欠驱动、执行器限制和系统约束的动态一致广义加速度；在扭矩层面引入基于被动性的积分作用以增强鲁棒性。

Result: 通过高保真物理仿真验证了方法的有效性，在包含参数扰动、粘性关节摩擦和实际传感状态估计的情况下，实现了精确跟踪、平滑控制输入和可靠的约束满足。

Conclusion: 提出的约束感知控制框架能够为欠驱动空中机械臂在实际操作条件下提供准确、鲁棒且安全的控制性能。

Abstract: This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.

</details>


### [17] [VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory](https://arxiv.org/abs/2601.08665)
*Shaoan Wang,Yuanfei Luo,Xingyu Chen,Aocheng Luo,Dongyue Li,Chang Liu,Sheng Chen,Yangang Zhang,Junzhi Yu*

Main category: cs.RO

TL;DR: VLingNav是一个基于语言驱动认知的视觉语言动作模型，通过自适应思维链机制和视觉辅助语言记忆模块，在具身导航任务中实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型大多采用从观察到动作的被动映射方式，缺乏显式推理能力和持久记忆，难以处理复杂、长视野的导航任务。需要一种能够结合人类认知双过程理论的方法，使智能体能够在快速直觉执行和慢速深思规划之间灵活切换。

Method: 1. 引入自适应思维链机制，动态触发显式推理；2. 开发视觉辅助语言记忆模块，构建跨模态语义记忆；3. 构建Nav-AdaCoT-2.9M数据集，包含推理标注；4. 采用在线专家引导的强化学习阶段，超越纯模仿学习。

Result: VLingNav在广泛的具身导航基准测试中实现了最先进的性能。值得注意的是，该模型能够以零样本方式迁移到真实世界机器人平台，执行各种导航任务，并展现出强大的跨领域和跨任务泛化能力。

Conclusion: VLingNav通过语言驱动认知方法成功解决了VLA模型在具身导航中的推理和记忆限制问题，实现了在复杂长视野导航任务中的优异表现，并展示了向真实世界机器人平台迁移的潜力。

Abstract: VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.

</details>


### [18] [Real-Time Localization Framework for Autonomous Basketball Robots](https://arxiv.org/abs/2601.08713)
*Naren Medarametla,Sreejon Mondal*

Main category: cs.RO

TL;DR: 提出了一种融合经典技术与学习方法的混合定位算法，仅利用球场地面视觉数据实现篮球场上的自主定位


<details>
  <summary>Details</summary>
Motivation: 定位是自主机器人的基本能力，在Robocon 2025竞赛中，准确可靠的定位对于提高射击精度、避免与其他机器人碰撞以及高效导航比赛场地至关重要

Method: 提出混合定位算法，集成经典技术与基于学习的方法，仅依赖球场地面视觉数据进行自定位

Result: 未在摘要中明确说明具体结果

Conclusion: 该混合定位方法能够实现篮球场上的自主定位，满足Robocon竞赛需求

Abstract: Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.

</details>


### [19] [Older Adults' Preferences for Feedback Cadence from an Exercise Coach Robot](https://arxiv.org/abs/2601.08819)
*Roshni Kaushik,Reid Simmons*

Main category: cs.RO

TL;DR: 研究探索老年人对机器人运动教练不同节奏反馈的感知，发现改变一种模态的节奏会影响对该模态及另一种模态的感知


<details>
  <summary>Details</summary>
Motivation: 人们以不同方式回应反馈和指导，机器人需要个性化互动并利用言语和非言语沟通线索。特别关注老年人对机器人运动教练反馈节奏的响应方式

Method: 通过在线研究，让老年人观看机器人以不同节奏提供言语和非言语反馈的视频，评估他们对不同节奏反馈的感知

Result: 改变一种模态的节奏会影响对该模态及另一种模态的感知，这表明模态间存在相互影响

Conclusion: 研究结果可用于优化机器人教练在老年人运动训练中的反馈频率设计，实现更有效的个性化互动

Abstract: People can respond to feedback and guidance in different ways, and it is important for robots to personalize their interactions and utilize verbal and nonverbal communication cues. We aim to understand how older adults respond to different cadences of verbal and nonverbal feedback of a robot exercise coach. We conducted an online study of older adults, where participants evaluated videos of the robot giving feedback at different cadences for each modality. The results indicate that changing the cadence of one modality affects the perception of both it and the other modality. We can use the results from this study to better design the frequency of the robot coach's feedback during an exercise session with this population.

</details>
