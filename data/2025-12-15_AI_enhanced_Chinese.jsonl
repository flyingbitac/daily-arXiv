{"id": "2512.11047", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11047", "abs": "https://arxiv.org/abs/2512.11047", "authors": ["Haoran Jiang", "Jin Chen", "Qingwen Bu", "Li Chen", "Modi Shi", "Yanjie Zhang", "Delong Li", "Chuanzhe Suo", "Chuang Wang", "Zhihui Peng", "Hongyang Li"], "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control", "comment": null, "summary": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.", "AI": {"tldr": "\u63d0\u51faWholeBodyVLA\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u5b66\u4e60\u4ece\u4f4e\u6210\u672c\u65e0\u52a8\u4f5c\u89c6\u9891\u83b7\u53d6\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u77e5\u8bc6\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u8fd0\u52a8\u64cd\u4f5c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u5927\u7a7a\u95f4\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u64cd\u4f5c\u611f\u77e5\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u5de5\u4f5c\u7a7a\u95f4\uff1b\u96be\u4ee5\u83b7\u53d6\u8fd0\u52a8\u64cd\u4f5c\u77e5\u8bc6\uff08\u7f3a\u4e4f\u9065\u64cd\u4f5c\u6570\u636e\uff09\uff1b\u73b0\u6709RL\u63a7\u5236\u5668\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u53ef\u9760\u6267\u884c\u8fd0\u52a8\u547d\u4ee4\u3002", "method": "1. \u63d0\u51fa\u7edf\u4e00\u6f5c\u5728\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\u4ece\u4f4e\u6210\u672c\u65e0\u52a8\u4f5c\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5b66\u4e60\uff1b2. \u8bbe\u8ba1\u9ad8\u6548\u7684\u4eba\u7c7b\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u6765\u6269\u5c55\u6570\u636e\u96c6\uff1b3. \u63d0\u51fa\u4e13\u95e8\u7684\u8fd0\u52a8\u64cd\u4f5c\u5bfc\u5411RL\u7b56\u7565\uff0c\u7528\u4e8e\u7cbe\u786e\u7a33\u5b9a\u7684\u6838\u5fc3\u8fd0\u52a8\u64cd\u4f5c\u52a8\u4f5c\uff1b4. \u6574\u5408\u4e3aWholeBodyVLA\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5728AgiBot X2\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5148\u524d\u57fa\u7ebf\u63d0\u534721.3%\uff1b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u7684\u4efb\u52a1\u8303\u56f4\uff1b\u5b9e\u73b0\u4e86\u5927\u7a7a\u95f4\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u4f5c\u3002", "conclusion": "WholeBodyVLA\u662f\u9996\u4e2a\u5b9e\u73b0\u5927\u7a7a\u95f4\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u4f5c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u65e0\u52a8\u4f5c\u89c6\u9891\u5b66\u4e60\u77e5\u8bc6\u548c\u4e13\u95e8\u7684\u8fd0\u52a8\u64cd\u4f5cRL\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.11080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11080", "abs": "https://arxiv.org/abs/2512.11080", "authors": ["Cedric-Pascal Sommer", "Robert J. Wood", "Justin Werfel"], "title": "Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations", "comment": "34 pages, 10 figures, 2 tables, supplementary videos: https://youtu.be/Hcefy53PY0M, https://youtu.be/nFF9k91hsfU, https://youtu.be/EulPLskNIZQ", "summary": "General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u5de5\u5177\u7cfb\u7edf\uff0c\u8ba9\u6807\u51c6\u53cc\u6307\u5939\u722a\u80fd\u591f\u6267\u884c\u6309\u538b\u3001\u6469\u64e6\u3001\u522e\u64e6\u7b49\u975e\u6293\u53d6\u64cd\u4f5c\uff0c\u6269\u5c55\u4e86\u7b80\u5355\u673a\u68b0\u624b\u7684\u591a\u529f\u80fd\u6027\u3002", "motivation": "\u867d\u7136\u5e73\u884c\u5939\u722a\u7b49\u7b80\u5355\u673a\u68b0\u624b\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5e76\u4e0d\u64c5\u957f\u6267\u884c\u6309\u538b\u3001\u6469\u64e6\u3001\u522e\u64e6\u7b49\u975e\u6293\u53d6\u64cd\u4f5c\u3002\u4eba\u7c7b\u4f1a\u6839\u636e\u4efb\u52a1\u9700\u6c42\u4f7f\u7528\u4e0d\u540c\u8eab\u4f53\u90e8\u4f4d\u6216\u5de5\u5177\uff0c\u800c\u73b0\u6709\u673a\u68b0\u624b\u7f3a\u4e4f\u8fd9\u79cd\u9002\u5e94\u6027\u3002", "method": "\u9996\u5148\u63d0\u51fa\u4e86\u975e\u9a71\u52a8\u672b\u7aef\u6267\u884c\u5668\u5173\u952e\u5c5e\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u7136\u540e\u57fa\u4e8e\u8be5\u5206\u7c7b\u6cd5\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u5de5\u5177\u7cfb\u7edf\uff0c\u53ef\u4ee5\u8ba9\u6807\u51c6\u53cc\u6307\u5939\u722a\u901a\u8fc7\u66f4\u6362\u5de5\u5177\u6765\u6267\u884c\u5404\u79cd\u975e\u6293\u53d6\u64cd\u4f5c\u3002", "result": "\u5728\u822a\u7a7a\u822a\u5929\u548c\u5bb6\u5ead\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u8be5\u5de5\u5177\u7cfb\u7edf\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u5b83\u80fd\u591f\u6709\u6548\u6267\u884c\u4e00\u7cfb\u5217\u975e\u6293\u53d6\u548c\u6293\u53d6\u64cd\u4f5c\uff0c\u6269\u5c55\u4e86\u7b80\u5355\u673a\u68b0\u624b\u7684\u591a\u529f\u80fd\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5757\u5316\u5de5\u5177\u7cfb\u7edf\uff0c\u7b80\u5355\u7684\u53cc\u6307\u5939\u722a\u53ef\u4ee5\u6269\u5c55\u5176\u529f\u80fd\u8303\u56f4\uff0c\u6267\u884c\u5404\u79cd\u975e\u6293\u53d6\u64cd\u4f5c\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11125", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11125", "abs": "https://arxiv.org/abs/2512.11125", "authors": ["Benedictus C. G. Cinun", "Tua A. Tamba", "Immanuel R. Santjoko", "Xiaofeng Wang", "Michael A. Gunarso", "Bin Hu"], "title": "Design and Experimental Validation of Closed-Form CBF-Based Safe Control for Stewart Platform Under Multiple Constraints", "comment": "9 pages", "summary": "This letter presents a closed-form solution of Control Barrier Function (CBF) framework for enforcing safety constraints on a Stewart robotic platform. The proposed method simultaneously handles multiple position and velocity constraints through an explicit closed-form control law, eliminating the need to solve a Quadratic Program (QP) at every control step and enabling efficient real-time implementation. This letter derives necessary and sufficient conditions under which the closed-form expression remains non-singular, thereby ensuring well-posedness of the CBF solution to multi-constraint problem. The controller is validated in both simulation and hardware experiments on a custom-built Stewart platform prototype, demonstrating safetyguaranteed performance that is comparable to the QP-based formulation, while reducing computation time by more than an order of magnitude. The results confirm that the proposed approach provides a reliable and computationally lightweight framework for real-time safe control of parallel robotic systems. The experimental videos are available on the project website. (https://nail-uh.github.io/StewartPlatformSafeControl.github.io/)", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eStewart\u673a\u5668\u4eba\u5e73\u53f0\u7684CBF\u95ed\u5f0f\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u63a7\u5236\u5f8b\u540c\u65f6\u5904\u7406\u591a\u4e2a\u4f4d\u7f6e\u548c\u901f\u5ea6\u7ea6\u675f\uff0c\u65e0\u9700\u5728\u6bcf\u4e2a\u63a7\u5236\u6b65\u9aa4\u6c42\u89e3\u4e8c\u6b21\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b9e\u65f6\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u7684CBF\u65b9\u6cd5\u5728\u5b9e\u65f6\u63a7\u5236\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5728\u5e76\u884c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3001\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86CBF\u6846\u67b6\u7684\u95ed\u5f0f\u89e3\u65b9\u6cd5\uff0c\u63a8\u5bfc\u4e86\u591a\u7ea6\u675f\u95ee\u9898\u7684\u663e\u5f0f\u63a7\u5236\u5f8b\uff0c\u5e76\u5efa\u7acb\u4e86\u786e\u4fdd\u89e3\u975e\u5947\u5f02\u7684\u5145\u8981\u6761\u4ef6\uff0c\u4ece\u800c\u4fdd\u8bc1CBF\u89e3\u7684\u9002\u5b9a\u6027\u3002", "result": "\u5728\u81ea\u5b9a\u4e49Stewart\u5e73\u53f0\u539f\u578b\u4e0a\u8fdb\u884c\u4e86\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u80fd\u4e0eQP\u65b9\u6cd5\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e76\u884c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b9e\u65f6\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u8ba1\u7b97\u8f7b\u91cf\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2512.11173", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11173", "abs": "https://arxiv.org/abs/2512.11173", "authors": ["Tzu-Hsien Lee", "Fidan Mahmudova", "Karthik Desingh"], "title": "Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance", "comment": null, "summary": "Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u4f53\u4e2d\u5fc3\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u56db\u8db3\u79fb\u52a8\u673a\u68b0\u81c2\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u5b9e\u73b0\u6700\u540e\u4e00\u7c73\u7cbe\u786e\u5bfc\u822a\uff0c\u8fbe\u5230\u53ef\u64cd\u4f5c\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u73b0\u6709RGB\u5bfc\u822a\u7cfb\u7edf\u901a\u5e38\u53ea\u6709\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u4e0d\u9002\u5408\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u7cbe\u786e\u5b9a\u4f4d\u9700\u6c42\uff0c\u5bfc\u81f4\u540e\u7eed\u64cd\u4f5c\u5931\u8d25\u7387\u9ad8\u3002\u9700\u8981\u89e3\u51b3\u6700\u540e\u4e00\u7c73\u5bfc\u822a\u7684\u7cbe\u5ea6\u95ee\u9898", "method": "\u4f7f\u7528\u76ee\u6807\u56fe\u50cf\u3001\u591a\u89c6\u89d2RGB\u89c2\u6d4b\u548c\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u5bfc\u822a\u7b56\u7565\u8f93\u5165\uff0c\u7ed3\u5408\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u6a21\u5757\u548c\u7a7a\u95f4\u5f97\u5206\u77e9\u9635\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u7269\u4f53\u5b9a\u4f4d\u548c\u76f8\u5bf9\u4f4d\u59ff\u63a8\u7406", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5b9e\u4f8b\u4e0a\uff0c\u8fb9\u7f18\u5bf9\u9f50\u6210\u529f\u7387\u8fbe\u523073.47%\uff0c\u7269\u4f53\u5bf9\u9f50\u6210\u529f\u7387\u8fbe\u523096.94%\uff0c\u8bc1\u660e\u65e0\u9700\u6df1\u5ea6\u3001\u6fc0\u5149\u96f7\u8fbe\u6216\u5730\u56fe\u5148\u9a8c\u5373\u53ef\u5b9e\u73b0\u7c7b\u522b\u7ea7\u7cbe\u786e\u5bfc\u822a", "conclusion": "\u4ec5\u4f7f\u7528RGB\u89c2\u6d4b\u5373\u53ef\u5b9e\u73b0\u6700\u540e\u4e00\u7c73\u7cbe\u786e\u5bfc\u822a\uff0c\u4e3a\u7edf\u4e00\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u673a\u68b0\u81c2\u7cbe\u786e\u5b9a\u4f4d\u7684\u5173\u952e\u95ee\u9898"}}
{"id": "2512.11218", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11218", "abs": "https://arxiv.org/abs/2512.11218", "authors": ["Kechun Xu", "Zhenjie Zhu", "Anzhe Chen", "Shuqi Zhao", "Qing Huang", "Yifei Yang", "Haojian Lu", "Rong Xiong", "Masayoshi Tomizuka", "Yue Wang"], "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy", "comment": null, "summary": "The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.", "AI": {"tldr": "BayesVLA\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u89e3\u51b3VLA\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u89c6\u89c9-\u52a8\u4f5c\u5148\u9a8c\u548c\u8bed\u8a00\u6761\u4ef6\u4f3c\u7136\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "VLA\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5e38\u56e0\u6a21\u6001\u4e0d\u5e73\u8861\uff08\u8bed\u8a00\u591a\u6837\u6027\u8fdc\u4f4e\u4e8e\u89c6\u89c9\u548c\u52a8\u4f5c\u591a\u6837\u6027\uff09\u5bfc\u81f4\u89c6\u89c9\u6377\u5f84\u5b66\u4e60\u548c\u8bed\u8a00\u9057\u5fd8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u63a8\u7406\u6570\u636e\u4e14\u9700\u8981\u7ecf\u9a8c\u6027\u8c03\u53c2\u3002", "method": "\u63d0\u51faBayesVLA\uff0c\u91c7\u7528\u8d1d\u53f6\u65af\u5206\u89e3\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u89c6\u89c9-\u52a8\u4f5c\u5148\u9a8c\uff08\u652f\u6301\"\u770b\u5230\u5373\u884c\u52a8\"\uff09\u548c\u8bed\u8a00\u6761\u4ef6\u4f3c\u7136\uff08\u652f\u6301\"\u63d0\u793a\u5373\u6307\u5b9a\"\uff09\uff0c\u5e76\u5f15\u5165\u63a5\u89e6\u524d\u548c\u63a5\u89e6\u540e\u9636\u6bb5\u4ee5\u66f4\u597d\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002", "result": "\u4fe1\u606f\u8bba\u5206\u6790\u6b63\u5f0f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7f13\u89e3\u6377\u5f84\u5b66\u4e60\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\u5728\u672a\u89c1\u6307\u4ee4\u3001\u5bf9\u8c61\u548c\u73af\u5883\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BayesVLA\u901a\u8fc7\u5185\u5728\u7684\u8d1d\u53f6\u65af\u5206\u89e3\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u4f9d\u8d56\u5373\u53ef\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u5e76\u4fc3\u8fdb\u6307\u4ee4\u8ddf\u968f\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5206\u5e03\u5916\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11249", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11249", "abs": "https://arxiv.org/abs/2512.11249", "authors": ["Chandra Raskoti", "Weizi Li"], "title": "Elevation Aware 2D/3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics", "comment": null, "summary": "Reliable testing of autonomous driving systems requires simulation environments that combine large-scale traffic modeling with realistic 3D perception and terrain. Existing tools rarely capture real-world elevation, limiting their usefulness in cities with complex topography. This paper presents an automated, elevation-aware co-simulation framework that integrates SUMO with CARLA using a pipeline that fuses OpenStreetMap road networks and USGS elevation data into physically consistent 3D environments. The system generates smooth elevation profiles, validates geometric accuracy, and enables synchronized 2D-3D simulation across platforms. Demonstrations on multiple regions of San Francisco show the framework's scalability and ability to reproduce steep and irregular terrain. The result is a practical foundation for high-fidelity autonomous vehicle testing in realistic, elevation-rich urban settings.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u3001\u8003\u8651\u9ad8\u7a0b\u7684\u534f\u540c\u4eff\u771f\u6846\u67b6\uff0c\u96c6\u6210SUMO\u548cCARLA\uff0c\u878d\u5408OpenStreetMap\u8def\u7f51\u548cUSGS\u9ad8\u7a0b\u6570\u636e\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u76843D\u73af\u5883\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9ad8\u4fdd\u771f\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u4eff\u771f\u5de5\u5177\u5f88\u5c11\u8003\u8651\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u7a0b\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u5730\u5f62\u57ce\u5e02\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u9700\u8981\u80fd\u591f\u7ed3\u5408\u5927\u89c4\u6a21\u4ea4\u901a\u5efa\u6a21\u4e0e\u771f\u5b9e3D\u611f\u77e5\u548c\u5730\u5f62\u7684\u4eff\u771f\u73af\u5883\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u9ad8\u7a0b\u611f\u77e5\u534f\u540c\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u7ba1\u9053\u5c06OpenStreetMap\u8def\u7f51\u548cUSGS\u9ad8\u7a0b\u6570\u636e\u878d\u5408\u5230\u7269\u7406\u4e00\u81f4\u76843D\u73af\u5883\u4e2d\u3002\u7cfb\u7edf\u751f\u6210\u5e73\u6ed1\u7684\u9ad8\u7a0b\u5256\u9762\uff0c\u9a8c\u8bc1\u51e0\u4f55\u7cbe\u5ea6\uff0c\u5e76\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u540c\u6b652D-3D\u4eff\u771f\u3002", "result": "\u5728\u65e7\u91d1\u5c71\u591a\u4e2a\u533a\u57df\u8fdb\u884c\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u518d\u73b0\u9661\u5ced\u4e0d\u89c4\u5219\u5730\u5f62\u7684\u80fd\u529b\u3002\u4e3a\u5728\u771f\u5b9e\u3001\u9ad8\u7a0b\u4e30\u5bcc\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u4fdd\u771f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u771f\u5b9e\u590d\u6742\u5730\u5f62\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u9ad8\u7a0b\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.11250", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11250", "abs": "https://arxiv.org/abs/2512.11250", "authors": ["Brock Marcinczyk", "Logan E. Beaver"], "title": "Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator", "comment": "6 pages + 18 page appendix", "summary": "This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u964d\u9636\u5e9e\u7279\u91cc\u4e9a\u91d1\u6781\u5927\u503c\u539f\u7406\u63a7\u5236\u5668\u4e0e\u7269\u7406\u4fe1\u606f\u68af\u5ea6\u4e0b\u964d\u9636\u6bb5\u7684\u63a7\u5236\u4e2d\u5fc3\u6846\u67b6\uff0c\u7528\u4e8e4\u81ea\u7531\u5ea6\u521a\u6027\u673a\u68b0\u81c2\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u7684\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u63a7\u5236\u7406\u8bba\u7ed3\u6784\u3002", "motivation": "\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u4e25\u683c\u63a7\u5236\u7406\u8bba\u7ed3\u6784\uff0c\u53c8\u80fd\u5d4c\u5165\u673a\u68b0\u81c2\u7269\u7406\u7ea6\u675f\u548c\u8d1f\u8f7d\u884c\u4e3a\u7684\u8ba1\u7b97\u9ad8\u6548\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u6001\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "1. \u4f7f\u7528\u964d\u9636PMP\u6a21\u578b\u63d0\u4f9b\u5173\u8282\u52a0\u901f\u5ea6\u7684\u95ed\u5f0f\u6700\u4f18\u63a7\u5236\u5f8b\uff1b2. \u68af\u5ea6\u4e0b\u964d\u6a21\u5757\u901a\u8fc7\u6700\u5c0f\u5316\u57fa\u4e8e\u5b8c\u6574\u521a\u4f53\u52a8\u529b\u5b66\u7684\u6210\u672c\u51fd\u6570\u786e\u5b9a\u5bf9\u5e94\u65f6\u95f4\u8303\u56f4\uff1b3. \u4f7f\u7528\u7ed3\u6784\u529b\u5b66\u53cd\u5e94\u5206\u6790\u521d\u59cb\u5316\u53ef\u884c\u5173\u8282\u901f\u5ea6\uff1b4. \u5c06\u8fd0\u52a8\u8f68\u8ff9\u548c\u65f6\u95f4\u8303\u56f4\u8f93\u5165\u7b26\u53f7\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u6a21\u578b\u751f\u6210\u95ed\u5f0f\u9006\u52a8\u529b\u5b66\u8f93\u5165\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u63a7\u5236\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4e25\u683c\u63a7\u5236\u7406\u8bba\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u4ee5\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u5f0f\u5d4c\u5165\u673a\u68b0\u81c2\u7684\u7269\u7406\u7ea6\u675f\u548c\u8d1f\u8f7d\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u4e00\u81f4\u7684\u65f6\u95f4\u8303\u56f4\u548c\u8fd0\u52a8\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u964d\u9636\u6700\u4f18\u63a7\u5236\u4e0e\u7269\u7406\u4fe1\u606f\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u4e3a\u521a\u6027\u673a\u68b0\u81c2\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u4fdd\u6301\u7406\u8bba\u4e25\u8c28\u6027\u53c8\u5177\u6709\u8ba1\u7b97\u6548\u7387\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u63a7\u5236\u7406\u8bba\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u6709\u6548\u878d\u5408\u3002"}}
{"id": "2512.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11275", "abs": "https://arxiv.org/abs/2512.11275", "authors": ["Suchang Chen", "Daqiang Guo"], "title": "Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing", "comment": "8 pages, 2 figures, submitted to the 2026 IFAC World Congress", "summary": "Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple \u03c4, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate \u03c4 and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze \u03c4-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7269\u4f53\u4e2d\u5fc3\u5316\u64cd\u4f5c\u903b\u8f91\u6a21\u5f0f\u03c4\uff0c\u5c06\u63a5\u89e6\u529b/\u963b\u6297\u7b49\u5173\u952e\u53c2\u6570\u4f5c\u4e3a\u77e5\u8bc6\u4fe1\u53f7\uff0c\u5e94\u7528\u4e8e3D\u6253\u5370\u673a\u7ebf\u8f74\u79fb\u9664\u4efb\u52a1\uff0c\u652f\u6301\u6570\u636e\u589e\u5f3a\u548c\u903b\u8f91\u611f\u77e5\u63d0\u793a\u68c0\u7d22", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6ce8\u91cd\u8bed\u4e49\u6cdb\u5316\uff0c\u4f46\u7f3a\u4e4f\u5236\u9020\u573a\u666f\u4e2d\u63a5\u89e6\u5bc6\u96c6\u578b\u52a8\u4f5c\u6240\u9700\u7684\u5173\u952e\u6267\u884c\u53c2\u6570\uff0c\u9700\u8981\u66f4\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5", "method": "\u5b9a\u4e49\u516b\u5b57\u6bb5\u5143\u7ec4\u03c4\u4f5c\u4e3a\u7269\u4f53\u4e2d\u5fc3\u5316\u64cd\u4f5c\u903b\u8f91\u6a21\u5f0f\uff0c\u5305\u542b\u7269\u4f53\u3001\u63a5\u53e3\u3001\u8f68\u8ff9\u3001\u5bb9\u5dee\u3001\u529b/\u963b\u6297\u7b49\u4fe1\u606f\uff0c\u6784\u5efa\u77e5\u8bc6\u5e93\u5e76\u57283D\u6253\u5370\u673a\u7ebf\u8f74\u79fb\u9664\u4efb\u52a1\u4e2d\u5b9e\u4f8b\u5316", "result": "\u03c4\u6a21\u5f0f\u652f\u6301\u8bad\u7ec3\u65f6\u7684\u5206\u7c7b\u6807\u8bb0\u6570\u636e\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u7684\u903b\u8f91\u611f\u77e5\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\uff0c\u4f5c\u4e3a\u667a\u80fd\u5236\u9020\u52a9\u624b\u7cfb\u7edf\u7684\u6784\u5efa\u6a21\u5757\uff0c\u901a\u8fc7\u9002\u5e94VLM/LLM\u89c4\u5212\u57fa\u51c6\u7684\u6307\u6807\u8bc4\u4f30\u89c4\u5212\u8d28\u91cf", "conclusion": "\u7269\u4f53\u4e2d\u5fc3\u5316\u64cd\u4f5c\u903b\u8f91\u6a21\u5f0f\u03c4\u4e3a\u5236\u9020\u573a\u666f\u4e2d\u7684\u63a5\u89e6\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\uff0c\u8fde\u63a5\u4e86\u4eba\u7c7b\u64cd\u4f5c\u5458\u3001VLM\u52a9\u624b\u548c\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u652f\u6301\u66f4\u53ef\u9760\u7684\u667a\u80fd\u5236\u9020\u7cfb\u7edf"}}
{"id": "2512.11351", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11351", "abs": "https://arxiv.org/abs/2512.11351", "authors": ["Steffen Sch\u00e4fer", "Martin Cichon"], "title": "Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains", "comment": null, "summary": "The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8fd0\u884c\u8bbe\u8ba1\u57df\uff08ODD\uff09\u7ec6\u5206\u4e3a\u5faeODD\uff08mODD\uff09\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u611f\u77e5\u7cfb\u7edf\u6d4b\u8bd5\u7528\u4f8b\uff0c\u901a\u8fc7\u7b80\u5316\u969c\u788d\u7269\u8868\u793a\u548c\u95ed\u73af\u4eff\u771f\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u573a\u666f\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u9762\u4e34\u6311\u6218\uff1a\u4eceODD\u5206\u7c7b\u5230\u5177\u4f53\u6d4b\u8bd5\u7528\u4f8b\u7684\u8f6c\u6362\u8fc7\u7a0b\u7f3a\u4e4f\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5b8c\u6574\u6027\u96be\u4ee5\u4fdd\u8bc1\u3002\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u611f\u77e5\u7cfb\u7edf\u5728\u5404\u79cd\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u6b63\u786e\u6027\u3002", "method": "1. \u5c06ODD\u7ec6\u5206\u4e3a\u53ef\u7ba1\u7406\u7684\u5faeODD\uff08mODD\uff09\u90e8\u5206\uff1b2. \u4f7f\u7528\u62bd\u8c61\u5bf9\u8c61\u8868\u793a\uff08\u5982\u4e0d\u540c\u5c3a\u5bf8\u7684\u901a\u7528\u7acb\u65b9\u4f53\uff09\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff1b3. \u5728\u95ed\u73af\u534f\u540c\u4eff\u771f\u865a\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5305\u542b\u903c\u771f\u6e32\u67d3\u548c\u6a21\u62df\u4f20\u611f\u5668\uff08LiDAR\u3001GNSS\u3001\u6444\u50cf\u5934\uff09\uff1b4. \u4ee5\u78b0\u649e\u4e0e\u5b89\u5168\u505c\u6b62\u4f5c\u4e3a\u7ed3\u679c\u6307\u6807\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u969c\u788d\u7269\u68c0\u6d4b\u7684\u8fb9\u7f18\u60c5\u51b5\uff0c\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u8f66\u8f86\u884c\u4e3a\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\u3002\u901a\u8fc7\u53d7\u9650ODD\u4e2d\u7684\u4fa7\u5411\u5f15\u5bfc\u673a\u52a8\u6848\u4f8b\uff08\u6d89\u53ca\u8c03\u8f66\u673a\u8f66\uff09\u5c55\u793a\u4e86mODD\u7684\u5b9a\u4e49\u548c\u7ec6\u5316\u8fc7\u7a0b\uff0c\u6210\u529f\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u5e76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684mODD\u65b9\u6cd5\u548c\u62bd\u8c61\u969c\u788d\u7269\u8868\u793a\u4e3a\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u7684\u5b89\u5168\u8bba\u8bc1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u57fa\u7840\uff0c\u662f\u9a8c\u8bc1\u548c\u6388\u6743\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u7684\u91cd\u8981\u5b9e\u8df5\u6b65\u9aa4\uff0c\u652f\u6301\u7cfb\u7edf\u5316\u7684\u5b89\u5168\u8bc4\u4f30\u3002"}}
{"id": "2512.11362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11362", "abs": "https://arxiv.org/abs/2512.11362", "authors": ["Chao Xu", "Suyu Zhang", "Yang Liu", "Baigui Sun", "Weihong Chen", "Bo Xu", "Qi Liu", "Juncheng Wang", "Shujun Wang", "Shan Luo", "Jan Peters", "Athanasios V. Vasilakos", "Stefanos Zafeiriou", "Jiankang Deng"], "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges", "comment": null, "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/Survery/}{project page}.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u7efc\u8ff0\u6027\u6587\u7ae0\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u5b66\u4e60\u8def\u5f84\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u8be5\u9886\u57df\u7684\u4e94\u5927\u6838\u5fc3\u6311\u6218\u3002", "motivation": "\u968f\u7740VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u65b0\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c42\u51fa\u4e0d\u7a77\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u8ddf\u4e0a\u6700\u65b0\u8fdb\u5c55\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u7ed3\u6784\u5316\u6307\u5357\uff0c\u5e2e\u52a9\u65b0\u4eba\u5feb\u901f\u5165\u95e8\uff0c\u5e76\u4e3a\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u6218\u7565\u8def\u7ebf\u56fe\u3002", "method": "\u91c7\u7528\u81ea\u7136\u5b66\u4e60\u8def\u5f84\u7684\u7ed3\u6784\u8bbe\u8ba1\uff1a\u9996\u5148\u4ecb\u7ecdVLA\u6a21\u578b\u7684\u57fa\u672c\u6a21\u5757\uff0c\u7136\u540e\u8ffd\u6eaf\u5173\u952e\u91cc\u7a0b\u7891\uff0c\u6700\u540e\u6df1\u5165\u5206\u6790\u4e94\u5927\u6838\u5fc3\u6311\u6218\uff08\u8868\u793a\u3001\u6267\u884c\u3001\u6cdb\u5316\u3001\u5b89\u5168\u3001\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\uff09\u3002\u4e3a\u6bcf\u4e2a\u6311\u6218\u56de\u987e\u73b0\u6709\u65b9\u6cd5\u5e76\u6307\u51fa\u672a\u6765\u673a\u4f1a\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684VLA\u9886\u57df\u7efc\u8ff0\u6846\u67b6\uff0c\u5c06\u53d1\u5c55\u8def\u7ebf\u56fe\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u5efa\u7acb\u57fa\u672c\u7684\u611f\u77e5-\u52a8\u4f5c\u5faa\u73af\u3001\u5728\u4e0d\u540c\u5177\u8eab\u548c\u73af\u5883\u4e2d\u7684\u80fd\u529b\u6269\u5c55\u3001\u786e\u4fdd\u53ef\u4fe1\u90e8\u7f72\u3002\u63d0\u4f9b\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u5728\u7ebf\u7248\u672c\u3002", "conclusion": "\u672c\u6587\u65e2\u662f\u65b0\u7814\u7a76\u8005\u7684\u57fa\u7840\u6307\u5357\uff0c\u4e5f\u662f\u7ecf\u9a8c\u4e30\u5bcc\u7814\u7a76\u8005\u7684\u6218\u7565\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u52a0\u901f\u5177\u8eab\u667a\u80fd\u9886\u57df\u7684\u5b66\u4e60\u5e76\u6fc0\u53d1\u65b0\u60f3\u6cd5\u3002\u901a\u8fc7\u5206\u6790\u4e94\u5927\u6838\u5fc3\u6311\u6218\uff0c\u4e3aVLA\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6846\u67b6\u3002"}}
{"id": "2512.11551", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11551", "abs": "https://arxiv.org/abs/2512.11551", "authors": ["J\u00f6rg Gamerdinger", "Sven Teufel", "Simon Roller", "Oliver Bringmann"], "title": "CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios", "comment": null, "summary": "The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u96c6\u4f53\u611f\u77e5\u5bf9\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u5b89\u5168\u6539\u5584\u7684\u6846\u67b6\uff0c\u5305\u542b\u5305\u542b1.1\u4e07\u5e27EuroNCAP\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u53ef\u5c06\u4e8b\u6545\u907f\u514d\u7387\u4ece33%\u63d0\u5347\u81f3100%\u3002", "motivation": "\u9053\u8def\u4f7f\u7528\u8005\u6570\u91cf\u589e\u52a0\u5bfc\u81f4\u4e8b\u6545\u98ce\u9669\u4e0a\u5347\uff0c\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5c24\u5176\u5371\u9669\uff0c\u5e38\u88ab\u505c\u8f66\u6216\u5efa\u7b51\u7269\u906e\u6321\u3002\u81ea\u52a8\u9a7e\u9a76\u548c\u96c6\u4f53\u611f\u77e5\u6280\u672f\u6709\u671b\u964d\u4f4e\u8fd9\u4e9b\u98ce\u9669\uff0c\u4f46\u9700\u8981\u5168\u9762\u7814\u7a76\u548c\u6570\u636e\u96c6\u6765\u8bc1\u660e\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u96c6\u4f53\u611f\u77e5\u5bf9\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u6539\u5584\u6548\u679c\uff0c\u4ee5\u9f13\u52b1\u51b3\u7b56\u8005\u91c7\u7528\u8be5\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u96c6\u4f53\u611f\u77e5\u5bf9\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u5b89\u5168\u6539\u5584\u7684\u6846\u67b6\uff0c\u521b\u5efa\u5305\u542bEuroNCAP\u5b89\u5168\u5173\u952e\u573a\u666f\u7684CarlaNCAP\u6570\u636e\u96c6\uff081.1\u4e07\u5e27\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6\u4eff\u771f\u7814\u7a76\u5206\u6790\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u5355\u5143\u5bf9\u4e8b\u6545\u907f\u514d\u7684\u6548\u679c\u3002", "result": "\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u96c6\u4f53\u611f\u77e5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u80fd\u663e\u8457\u964d\u4f4e\u4e8b\u6545\u7387\uff0c\u76f8\u6bd4\u4ec5\u914d\u5907\u4f20\u611f\u5668\u7684\u8f66\u8f86\uff0833%\u4e8b\u6545\u907f\u514d\u7387\uff09\uff0c\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u53ef\u5b9e\u73b0\u9ad8\u8fbe100%\u7684\u4e8b\u6545\u907f\u514d\u7387\u3002", "conclusion": "\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u96c6\u4f53\u611f\u77e5\u6280\u672f\u80fd\u6709\u6548\u63d0\u9ad8\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u4e25\u91cd\u7684\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u4e3a\u51b3\u7b56\u8005\u91c7\u7528\u8be5\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2512.11571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11571", "abs": "https://arxiv.org/abs/2512.11571", "authors": ["Andreu Matoses Gimenez", "Nils Wilde", "Chris Pek", "Javier Alonso-Mora"], "title": "Cross-Entropy Optimization of Physically Grounded Task and Motion Plans", "comment": "Preprint", "summary": "Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528GPU\u5e76\u884c\u7269\u7406\u6a21\u62df\u5668\u8ba1\u7b97\u8ba1\u5212\u5b9e\u73b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u4f18\u5316\u91c7\u6837\u63a7\u5236\u5668\u53c2\u6570\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u76f4\u63a5\u6267\u884c\u8ba1\u7b97\u51fa\u7684\u8ba1\u5212", "motivation": "\u73b0\u6709TAMP\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u8ba1\u7b97\u6027\u80fd\u3001\u5b8c\u5907\u6027\u6216\u6700\u4f18\u6027\uff0c\u901a\u8fc7\u7b80\u5316\u548c\u62bd\u8c61\u4f7f\u95ee\u9898\u53ef\u5904\u7406\uff0c\u4f46\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8ba1\u5212\u65e0\u6cd5\u8003\u8651\u52a8\u529b\u5b66\u6216\u590d\u6742\u63a5\u89e6\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u6267\u884c\u4efb\u52a1\u65f6\u5931\u8d25\u3002\u540c\u65f6\uff0c\u5ffd\u7565\u5e95\u5c42\u63a7\u5236\u5668\u5f71\u54cd\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\u6700\u4f18\u6216\u53ef\u884c\u7684\u8ba1\u5212\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528GPU\u5e76\u884c\u5316\u7269\u7406\u6a21\u62df\u5668\u8ba1\u7b97\u5e26\u6709\u8fd0\u52a8\u63a7\u5236\u5668\u7684\u8ba1\u5212\u5b9e\u73b0\uff0c\u663e\u5f0f\u8003\u8651\u52a8\u529b\u5b66\u548c\u73af\u5883\u63a5\u89e6\u3002\u901a\u8fc7\u4ea4\u53c9\u71b5\u4f18\u5316\u91c7\u6837\u63a7\u5236\u5668\u53c2\u6570\uff08\u52a8\u4f5c\uff09\uff0c\u83b7\u5f97\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e0e\u5b9e\u9645\u7cfb\u7edf\u76f8\u540c\u7684\u63a7\u5236\u5668\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u76f4\u63a5\u6267\u884c\u8ba1\u7b97\u51fa\u7684\u8ba1\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u5c55\u793a\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5229\u7528\u73af\u5883\u51e0\u4f55\u5f62\u72b6\u79fb\u52a8\u7269\u4f53\u3002\u63d0\u4f9b\u4e86\u7f51\u7ad9\u548c\u4ee3\u7801\u94fe\u63a5\u4f9b\u8fdb\u4e00\u6b65\u53c2\u8003\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408GPU\u5e76\u884c\u7269\u7406\u6a21\u62df\u548c\u4ea4\u53c9\u71b5\u4f18\u5316\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u8003\u8651\u52a8\u529b\u5b66\u548c\u63a5\u89e6\u7684\u53ef\u884c\u8ba1\u5212\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u76f4\u63a5\u6267\u884c\u8fd9\u4e9b\u8ba1\u5212\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7269\u4f53\u64cd\u4f5c\u7684\u4efb\u52a1\u3002"}}
{"id": "2512.11609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11609", "abs": "https://arxiv.org/abs/2512.11609", "authors": ["Tingyu Yuan", "Biaoliang Guan", "Wen Ye", "Ziyan Tian", "Yi Yang", "Weijie Zhou", "Yan Huang", "Peng Wang", "Chaoyang Zhao", "Jinqiao Wang"], "title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations", "comment": null, "summary": "In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.", "AI": {"tldr": "UniBYD\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53d1\u73b0\u7b26\u5408\u673a\u5668\u4eba\u7269\u7406\u7279\u6027\u7684\u64cd\u4f5c\u7b56\u7565\uff0c\u4f7f\u7528\u7edf\u4e00\u5f62\u6001\u8868\u793a\u652f\u6301\u4e0d\u540c\u673a\u68b0\u624b\u5f62\u6001\uff0c\u5728\u6210\u529f\u7387\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534767.90%\u3002", "motivation": "\u5728\u5177\u8eab\u667a\u80fd\u4e2d\uff0c\u673a\u5668\u4eba\u624b\u4e0e\u4eba\u7c7b\u624b\u4e4b\u95f4\u7684\u5177\u8eab\u5dee\u8ddd\u7ed9\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u867d\u7136\u5c1d\u8bd5\u7528\u5f3a\u5316\u5b66\u4e60\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f46\u4ec5\u9650\u4e8e\u590d\u5236\u4eba\u7c7b\u64cd\u4f5c\uff0c\u5bfc\u81f4\u4efb\u52a1\u6027\u80fd\u6709\u9650\u3002", "method": "\u63d0\u51faUniBYD\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u7edf\u4e00\u5f62\u6001\u8868\u793a(UMR)\u5b9e\u73b0\u8de8\u4e0d\u540c\u673a\u68b0\u624b\u5f62\u6001\u7684\u4e00\u81f4\u5efa\u6a21\uff1b2) \u57fa\u4e8eUMR\u8bbe\u8ba1\u5e26\u6709\u9000\u706b\u5956\u52b1\u8c03\u5ea6\u7684\u52a8\u6001PPO\u7b97\u6cd5\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u4ece\u6a21\u4eff\u4eba\u7c7b\u6f14\u793a\u8fc7\u6e21\u5230\u63a2\u7d22\u9002\u5e94\u673a\u5668\u4eba\u5f62\u6001\u7684\u7b56\u7565\uff1b3) \u8bbe\u8ba1\u6df7\u5408\u9a6c\u5c14\u53ef\u592b\u9634\u5f71\u5f15\u64ce\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u7cbe\u7ec6\u6a21\u4eff\u4eba\u7c7b\u64cd\u4f5c\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u6db5\u76d6\u591a\u79cd\u624b\u5f62\u6001\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6UniManip\u3002\u5b9e\u9a8c\u8868\u660e\uff0cUniBYD\u5728\u6210\u529f\u7387\u4e0a\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e8667.90%\u3002", "conclusion": "UniBYD\u6846\u67b6\u80fd\u591f\u8d85\u8d8a\u5355\u7eaf\u6a21\u4eff\u4eba\u7c7b\u624b\uff0c\u53d1\u73b0\u9002\u5e94\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u7684\u64cd\u4f5c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u4e3a\u8de8\u5f62\u6001\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11620", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11620", "abs": "https://arxiv.org/abs/2512.11620", "authors": ["Kanisorn Sangchai", "Methasit Boonpun", "Withawin Kraipetchara", "Paulo Garcia"], "title": "Architecting Large Action Models for Human-in-the-Loop Intelligent Robots", "comment": null, "summary": "The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.", "AI": {"tldr": "\u901a\u8fc7\u7ec4\u5408\u73b0\u6210\u7684\u57fa\u7840\u6a21\u578b\u5e76\u52a0\u5165\u7b26\u53f7\u5316\u5c01\u88c5\u548c\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u5177\u6709\u63a7\u5236\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u795e\u7ecf\u7b26\u53f7\u5316\u5927\u578b\u52a8\u4f5c\u6a21\u578b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u7b26\u53f7\u5316AI\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u4e0a\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7b49\u795e\u7ecf\u65b9\u6cd5\u867d\u7136\u80fd\u529b\u5f3a\u5927\u4f46\u7f3a\u4e4f\u63a7\u5236\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002\u5927\u578b\u52a8\u4f5c\u6a21\u578b\u8bd5\u56fe\u6269\u5c55LLM\u4ee5\u6db5\u76d6\u5b8c\u6574\u7684\u611f\u77e5-\u63a8\u7406-\u52a8\u4f5c\u5faa\u73af\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u4e14\u5b58\u5728\u540c\u6837\u7684\u53ef\u9760\u6027\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u7ec4\u5408\u73b0\u6210\u7684\u57fa\u7840\u6a21\u578b\u6784\u5efa\u5927\u578b\u52a8\u4f5c\u6a21\u578b\uff0c\u5e76\u52a0\u5165\u7b26\u53f7\u5316\u5c01\u88c5\u548c\u5bf9\u8f93\u51fa\u7684\u9a8c\u8bc1\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u96c6\u6210\u9ad8\u6548\u7684\u611f\u77e5\u6a21\u578b\u4e0e\u903b\u8f91\u9a71\u52a8\u6838\u5fc3\uff0c\u901a\u8fc7\u751f\u6210\u89c4\u5212\u57df\u5b9a\u4e49\u8bed\u8a00\uff08PDDL\uff09\u4ee3\u7801\u6765\u9a71\u52a8\u52a8\u4f5c\u6267\u884c\uff0c\u5e76\u5f15\u5165\u4eba\u673a\u4ea4\u4e92\u9a8c\u8bc1\u9636\u6bb5\u3002", "result": "\u5728\u591a\u6a21\u6001\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u578b\u52a8\u4f5c\u6a21\u578b\u7684\u667a\u80fd\u53ef\u4ee5\u901a\u8fc7\u96c6\u6210\u9ad8\u6548\u611f\u77e5\u6a21\u578b\u548c\u903b\u8f91\u9a71\u52a8\u6838\u5fc3\u5b9e\u73b0\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u901a\u8fc7PDDL\u4ee3\u7801\u751f\u6210\u548c\u4eba\u673a\u4ea4\u4e92\u9a8c\u8bc1\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u52a8\u4f5c\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u5316\u65b9\u6cd5\u6784\u5efa\u7684\u5927\u578b\u52a8\u4f5c\u6a21\u578b\u5177\u6709\u63a7\u5236\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u4e3a\u667a\u80fd\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u8fd9\u79cd\u65b9\u6cd5\u652f\u6301\u4ece\u4e1a\u8005\u5728\u5404\u884c\u4e1a\u5f00\u53d1\u673a\u5668\u4eba\u5927\u578b\u52a8\u4f5c\u6a21\u578b\uff0c\u5e76\u6307\u51fa\u4e86\u786e\u4fdd\u73b0\u573a\u5b89\u5168\u9700\u8981\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2512.11736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11736", "abs": "https://arxiv.org/abs/2512.11736", "authors": ["Ninghan Zhong", "Steven Caro", "Megnath Ramesh", "Rishi Bhatnagar", "Avraiem Iskandar", "Stephen L. Smith"], "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots", "comment": "Under review for ICRA 2026", "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.", "AI": {"tldr": "Bench-Push\u662f\u9996\u4e2a\u7528\u4e8e\u63a8\u52a8\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u7684\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u6a21\u62df\u73af\u5883\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u5b9e\u73b0", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9700\u8981\u4e0e\u53ef\u79fb\u52a8\u7269\u4f53\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u63a8\u52a8\u5f0f\u673a\u5668\u4eba\u7814\u7a76\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u548c\u8de8\u7814\u7a76\u6bd4\u8f83", "method": "\u5f00\u53d1\u4e86Bench-Push\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u62ec\uff1a1\uff09\u591a\u79cd\u6a21\u62df\u73af\u5883\uff08\u8ff7\u5bab\u5bfc\u822a\u3001\u51b0\u533a\u8239\u8236\u5bfc\u822a\u3001\u7bb1\u5b50\u9012\u9001\u3001\u533a\u57df\u6e05\u7406\uff09\uff1b2\uff09\u65b0\u9896\u8bc4\u4f30\u6307\u6807\uff08\u6548\u7387\u3001\u4ea4\u4e92\u52aa\u529b\u3001\u90e8\u5206\u4efb\u52a1\u5b8c\u6210\u5ea6\uff09\uff1b3\uff09\u793a\u4f8b\u57fa\u7ebf\u5b9e\u73b0\u8bc4\u4f30", "result": "Bench-Push\u4f5c\u4e3a\u5f00\u6e90Python\u5e93\u53d1\u5e03\uff0c\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u4e86\u4ee3\u7801\u3001\u6587\u6863\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u63a8\u52a8\u5f0f\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6", "conclusion": "Bench-Push\u586b\u8865\u4e86\u63a8\u52a8\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u8de8\u65b9\u6cd5\u6bd4\u8f83"}}
{"id": "2512.11746", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11746", "abs": "https://arxiv.org/abs/2512.11746", "authors": ["Hana Kopecka", "Jose Such"], "title": "The Influence of Human-like Appearance on Expected Robot Explanations", "comment": null, "summary": "A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u5916\u89c2\uff08\u4eba\u5f62\u7a0b\u5ea6\uff09\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u673a\u5668\u4eba\u89e3\u91ca\u7684\u671f\u671b\uff0c\u53d1\u73b0\u4eba\u5f62\u5916\u89c2\u4e0e\u62df\u4eba\u5316\u89e3\u91ca\u5448\u6b63\u76f8\u5173", "motivation": "\u673a\u5668\u4eba\u5916\u89c2\u5df2\u77e5\u4f1a\u5f71\u54cd\u7528\u6237\u5fc3\u667a\u6a21\u578b\u548c\u4eba\u673a\u4ea4\u4e92\uff0c\u4f46\u5c1a\u672a\u7814\u7a76\u5176\u5bf9\u9884\u671f\u673a\u5668\u4eba\u89e3\u91ca\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u673a\u5668\u4eba\u7684\u4eba\u5f62\u5916\u89c2\u662f\u5426\u53ca\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5f15\u53d1\u62df\u4eba\u5316\uff08\u5373\u5fc3\u7406\u80fd\u529b\u5f52\u56e0\uff09\uff0c\u4ee5\u53ca\u62df\u4eba\u5316\u6c34\u5e73\u5982\u4f55\u4f53\u73b0\u5728\u4eba\u4eec\u671f\u671b\u4ece\u673a\u5668\u4eba\u83b7\u5f97\u7684\u89e3\u91ca\u4e2d\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ec4\u95f4\u7814\u7a76\uff0c\u5305\u542b\u4e09\u79cd\u5177\u6709\u4e0d\u540c\u4eba\u5f62\u5916\u89c2\u7684\u5bb6\u7528\u670d\u52a1\u673a\u5668\u4eba\u7684\u89c6\u89c9\u523a\u6fc0\u6761\u4ef6\u3002\u8981\u6c42\u53d7\u8bbf\u8005\u4e3a\u76f8\u540c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u63d0\u4f9b\u4ed6\u4eec\u671f\u671b\u4ece\u673a\u5668\u4eba\u83b7\u5f97\u7684\u89e3\u91ca\u3002\u901a\u8fc7\u5206\u6790\u89e3\u91ca\u5185\u5bb9\u6765\u8bc4\u4f30\u62df\u4eba\u5316\u7a0b\u5ea6\u3002", "result": "\u5728\u6240\u6709\u6761\u4ef6\u4e0b\uff0c\u5927\u591a\u6570\u89e3\u91ca\u90fd\u662f\u62df\u4eba\u5316\u7684\u3002\u4f46\u62df\u4eba\u5316\u89e3\u91ca\u4e0e\u4eba\u5f62\u5916\u89c2\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\u3002\u8fd8\u89c2\u5bdf\u5230\u975e\u62df\u4eba\u5316\u89e3\u91ca\u548c\u673a\u5668\u4eba\u63cf\u8ff0\u4e2d\u66f4\u7ec6\u5fae\u7684\u8d8b\u52bf\u3002", "conclusion": "\u673a\u5668\u4eba\u7684\u4eba\u5f62\u5916\u89c2\u786e\u5b9e\u4f1a\u5f71\u54cd\u7528\u6237\u671f\u671b\u4ece\u673a\u5668\u4eba\u83b7\u5f97\u7684\u89e3\u91ca\u7c7b\u578b\uff0c\u4eba\u5f62\u5916\u89c2\u8d8a\u5f3a\uff0c\u7528\u6237\u8d8a\u503e\u5411\u4e8e\u671f\u671b\u62df\u4eba\u5316\u7684\u89e3\u91ca\u3002\u8fd9\u4e3a\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u548c\u673a\u5668\u4eba\u89e3\u91ca\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2512.11769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11769", "abs": "https://arxiv.org/abs/2512.11769", "authors": ["Xiaoyu Ma", "Zhengqing Yuan", "Zheyuan Zhang", "Kaiwen Shi", "Lichao Sun", "Yanfang Ye"], "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models", "comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model", "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.", "AI": {"tldr": "BLURR\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a8\u7406\u5305\u88c5\u5668\uff0c\u53ef\u52a0\u901f\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63a8\u7406\u5806\u6808\u8fc7\u4e8e\u7b28\u91cd\uff0c\u96be\u4ee5\u5728\u666e\u901aGPU\u4e0a\u5b9e\u73b0\u54cd\u5e94\u5f0f\u7f51\u9875\u6f14\u793a\u6216\u9ad8\u9891\u673a\u5668\u4eba\u63a7\u5236\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848", "method": "BLURR\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u63a8\u7406\u5305\u88c5\u5668\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u66f4\u6539\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u901a\u8fc7\u6307\u4ee4\u524d\u7f00\u952e\u503c\u7f13\u5b58\u3001\u6df7\u5408\u7cbe\u5ea6\u6267\u884c\u548c\u5355\u6b65\u5c55\u5f00\u8c03\u5ea6\u6765\u52a0\u901f\u63a7\u5236", "result": "\u5728SimplerEnv\u8bc4\u4f30\u4e2d\uff0cBLURR\u4fdd\u6301\u4e0e\u539f\u59cb\u63a7\u5236\u5668\u76f8\u5f53\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6709\u6548FLOPs\u548c\u5b9e\u9645\u5ef6\u8fdf\uff0c\u5e76\u6784\u5efa\u4e86\u4ea4\u4e92\u5f0f\u7f51\u9875\u6f14\u793a", "conclusion": "BLURR\u662f\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u90e8\u7f72\u73b0\u4ee3VLA\u7b56\u7565\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u7f51\u9875\u6f14\u793a\u548c\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u573a\u666f"}}
{"id": "2512.11773", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11773", "abs": "https://arxiv.org/abs/2512.11773", "authors": ["Britton Jordan", "Jordan Thompson", "Jesse F. d'Almeida", "Hao Li", "Nithesh Kumar", "Susheela Sharma Stern", "Ipek Oguz", "Robert J. Webster", "Daniel Brown", "Alan Kuntz", "James Ferguson"], "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics", "comment": "9 pages, 5 figures", "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.", "AI": {"tldr": "ProbeMDE\uff1a\u4e00\u79cd\u6210\u672c\u611f\u77e5\u7684\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RGB\u56fe\u50cf\u548c\u7a00\u758f\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\u6765\u6539\u8fdb\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u624b\u672f\u573a\u666f\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6", "motivation": "\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u624b\u672f\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u51c6\u786e\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u7eb9\u7406\u7f3a\u5931\u8868\u9762\u3001\u955c\u9762\u53cd\u5c04\u548c\u906e\u6321\u7b49\u56e0\u7d20\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u7a00\u758f\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6", "method": "\u63d0\u51faProbeMDE\u6846\u67b6\uff0c\u4f7f\u7528MDE\u6a21\u578b\u96c6\u6210\u9884\u6d4b\u57fa\u4e8eRGB\u56fe\u50cf\u548c\u7a00\u758f\u5df2\u77e5\u6df1\u5ea6\u6d4b\u91cf\u7684\u5bc6\u96c6\u6df1\u5ea6\u56fe\u3002\u901a\u8fc7\u96c6\u6210\u65b9\u5dee\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u76f8\u5bf9\u4e8e\u5019\u9009\u6d4b\u91cf\u4f4d\u7f6e\u7684\u68af\u5ea6\u3002\u5229\u7528Stein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u5728\u68af\u5ea6\u56fe\u4e0a\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u672c\u4f53\u611f\u77e5\u4f4d\u7f6e\uff0c\u907f\u514d\u6a21\u5f0f\u5d29\u6e83", "result": "\u5728\u6a21\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u4e2d\u592e\u6c14\u9053\u963b\u585e\u624b\u672f\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5728\u6807\u51c6\u6df1\u5ea6\u4f30\u8ba1\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u540c\u65f6\u6700\u5c0f\u5316\u6240\u9700\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\u6570\u91cf", "conclusion": "ProbeMDE\u901a\u8fc7\u6210\u672c\u611f\u77e5\u7684\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\u6709\u6548\u7ed3\u5408RGB\u56fe\u50cf\u548c\u7a00\u758f\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u6311\u6218\u6027\u624b\u672f\u73af\u5883\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2512.11781", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11781", "abs": "https://arxiv.org/abs/2512.11781", "authors": ["Vineet Pasumarti", "Lorenzo Bianchi", "Antonio Loquercio"], "title": "Agile Flight Emerges from Multi-Agent Competitive Racing", "comment": null, "summary": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent", "AI": {"tldr": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u548c\u8d62\u5f97\u6bd4\u8d5b\u7684\u7a00\u758f\u9ad8\u5c42\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u51fa\u7684\u667a\u80fd\u4f53\u80fd\u591f\u540c\u65f6\u638c\u63e1\u654f\u6377\u98de\u884c\uff08\u5982\u9ad8\u901f\u8fd0\u52a8\uff09\u548c\u7b56\u7565\uff08\u5982\u8d85\u8f66\u6216\u963b\u6321\uff09\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684\u5355\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u8bad\u7ec3\u5355\u667a\u80fd\u4f53\u4f7f\u7528\u89c4\u5b9a\u884c\u4e3a\u7684\u5956\u52b1\u51fd\u6570\uff08\u5982\u6cbf\u8d5b\u9053\u7ebf\u524d\u8fdb\uff09\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u679c\u6709\u9650\u3002\u672c\u7814\u7a76\u63a2\u7d22\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u548c\u7a00\u758f\u9ad8\u5c42\u76ee\u6807\uff08\u8d62\u5f97\u6bd4\u8d5b\uff09\u662f\u5426\u80fd\u591f\u540c\u65f6\u4ea7\u751f\u9ad8\u7ea7\u98de\u884c\u6280\u80fd\u548c\u7b56\u7565\u884c\u4e3a\uff0c\u5e76\u6539\u5584\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u7cfb\u7edf\uff0c\u4ec5\u63d0\u4f9b\u8d62\u5f97\u6bd4\u8d5b\u7684\u7a00\u758f\u9ad8\u5c42\u76ee\u6807\u4f5c\u4e3a\u5956\u52b1\u3002\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u76f8\u540c\u7684\u4eff\u771f\u73af\u5883\u3001\u968f\u673a\u5316\u7b56\u7565\u548c\u786c\u4ef6\u914d\u7f6e\uff0c\u4e0e\u57fa\u4e8e\u8fdb\u5ea6\u5956\u52b1\u7684\u5355\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u6709\u969c\u788d\u7269\u65f6\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u8fdb\u5ea6\u5956\u52b1\u65b9\u6cd5\u3002\u591a\u667a\u80fd\u4f53\u7b56\u7565\u5177\u6709\u66f4\u597d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\uff0c\u5373\u4f7f\u4e24\u79cd\u65b9\u6cd5\u4f7f\u7528\u76f8\u540c\u7684\u4eff\u771f\u8bbe\u7f6e\u3002\u6b64\u5916\uff0c\u591a\u667a\u80fd\u4f53\u7b56\u7565\u8fd8\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6cdb\u5316\u5230\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u5bf9\u624b\u3002", "conclusion": "\u7a00\u758f\u4efb\u52a1\u7ea7\u5956\u52b1\u8db3\u4ee5\u8bad\u7ec3\u51fa\u80fd\u591f\u5728\u7269\u7406\u4e16\u754c\u4e2d\u6267\u884c\u9ad8\u7ea7\u4f4e\u7ea7\u63a7\u5236\u7684\u667a\u80fd\u4f53\u3002\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u4e0d\u4ec5\u80fd\u591f\u540c\u65f6\u4ea7\u751f\u654f\u6377\u98de\u884c\u6280\u80fd\u548c\u7b56\u7565\u884c\u4e3a\uff0c\u8fd8\u80fd\u663e\u8457\u6539\u5584\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u4e2d\u7684\u9ad8\u7ea7\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.11797", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11797", "abs": "https://arxiv.org/abs/2512.11797", "authors": ["Junjie Ye", "Rong Xue", "Basile Van Hoorick", "Pavel Tokmakov", "Muhammad Zubair Irshad", "Yue Wang", "Vitor Guizilini"], "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis", "comment": "Project page: https://jay-ye.github.io/AnchorDream/", "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.", "AI": {"tldr": "AnchorDream\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u8fd0\u52a8\u6e32\u67d3\u4f5c\u4e3a\u6761\u4ef6\uff0c\u4ece\u5c11\u91cf\u4eba\u7c7b\u9065\u64cd\u4f5c\u6f14\u793a\u4e2d\u751f\u6210\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u56f0\u96be\uff0c\u4eff\u771f\u5668\u5b58\u5728\u591a\u6837\u6027\u548c\u4fdd\u771f\u5ea6\u9650\u5236\u4ee5\u53ca\u660e\u663e\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u3002\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u53ea\u6539\u53d8\u89c6\u89c9\u5916\u89c2\u800c\u4e0d\u521b\u9020\u65b0\u884c\u4e3a\uff0c\u8981\u4e48\u5b58\u5728\u672c\u4f53\u4e0d\u4e00\u81f4\u5bfc\u81f4\u4e0d\u5408\u7406\u7684\u8fd0\u52a8\u3002", "method": "\u63d0\u51faAnchorDream\uff0c\u4e00\u79cd\u672c\u4f53\u611f\u77e5\u7684\u4e16\u754c\u6a21\u578b\uff0c\u91cd\u65b0\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u673a\u5668\u4eba\u6570\u636e\u5408\u6210\u3002\u8be5\u65b9\u6cd5\u4ee5\u673a\u5668\u4eba\u8fd0\u52a8\u6e32\u67d3\u4e3a\u6761\u4ef6\uff0c\u951a\u5b9a\u672c\u4f53\u4ee5\u9632\u6b62\u5e7b\u89c9\uff0c\u540c\u65f6\u5408\u6210\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e00\u81f4\u7684\u5bf9\u8c61\u548c\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684\u6570\u636e\u5728\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u4e2d\u5e26\u6765\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u4eff\u771f\u57fa\u51c6\u4e2d\u76f8\u5bf9\u63d0\u534736.4%\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u7814\u7a76\u4e2d\u6027\u80fd\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "\u5c06\u751f\u6210\u4e16\u754c\u6a21\u578b\u57fa\u4e8e\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u4e3a\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
