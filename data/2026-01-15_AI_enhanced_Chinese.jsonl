{"id": "2601.08953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08953", "abs": "https://arxiv.org/abs/2601.08953", "authors": ["Le Liu", "Bangguo Yu", "Nynke Vellinga", "Ming Cao"], "title": "Fairness risk and its privacy-enabled solution in AI-driven robotic applications", "comment": null, "summary": "Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u51b3\u7b56\u7684\u6548\u7528\u611f\u77e5\u516c\u5e73\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u516c\u5e73\u6027\u4e0e\u7528\u6237\u6570\u636e\u9690\u79c1\u4e4b\u95f4\u7684\u8054\u5408\u5173\u7cfb\uff0c\u5efa\u7acb\u4e86\u9690\u79c1\u9884\u7b97\u63a7\u5236\u516c\u5e73\u6027\u6307\u6807\u7684\u6846\u67b6\u3002", "motivation": "\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\u5b58\u5728\u516c\u5e73\u6027\u9690\u60a3\uff0c\u800c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7f3a\u4e4f\u65e2\u8003\u8651\u7528\u6237\u6548\u7528\u53c8\u8003\u8651\u6570\u636e\u968f\u673a\u6027\u7684\u53ef\u5b9e\u65bd\u516c\u5e73\u6027\u5b9a\u4e49\u3002\u540c\u65f6\uff0c\u6cd5\u5f8b\u8981\u6c42\u4e0b\u5927\u591a\u6570\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u8981\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u9700\u8981\u63a2\u7d22\u9690\u79c1\u4e0e\u516c\u5e73\u6027\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u6548\u7528\u611f\u77e5\u7684\u673a\u5668\u4eba\u51b3\u7b56\u516c\u5e73\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u516c\u5e73\u6027\u4e0e\u7528\u6237\u6570\u636e\u9690\u79c1\u7684\u8054\u5408\u5173\u7cfb\uff0c\u63a8\u5bfc\u9690\u79c1\u9884\u7b97\u63a7\u5236\u516c\u5e73\u6027\u6307\u6807\u7684\u6761\u4ef6\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9690\u79c1\u9884\u7b97\u53ef\u4ee5\u8054\u5408\u7528\u4e8e\u6ee1\u8db3\u516c\u5e73\u6027\u76ee\u6807\uff0c\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u80fd\u591f\u540c\u65f6\u4fc3\u8fdb\u516c\u5e73\u6027\uff0c\u8fd9\u4e3a\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u5b9e\u73b0\u516c\u5e73\u51b3\u7b56\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u521b\u9020\u6027\u7ed3\u5408\u9690\u79c1\u8003\u91cf\u6765\u89e3\u51b3\u516c\u5e73\u6027\u95ee\u9898\uff0c\u662f\u8fc8\u5411AI\u4f26\u7406\u4f7f\u7528\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u589e\u5f3a\u65e5\u5e38\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2601.09104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09104", "abs": "https://arxiv.org/abs/2601.09104", "authors": ["Ko Yamamoto", "Kyosuke Ishibashi", "Hiroki Ishikawa", "Osamu Azami"], "title": "Design Methodology of Hydraulically-driven Soft Robotic Gripper for a Large and Heavy Object", "comment": null, "summary": "This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping a large and heavy object -- approximately 10 - 20 kg with 20 - 30 cm diameter. Most existing soft grippers are pneumatically actuated with several hundred kPa pressure, and cannot generate output force sufficient for such a large and heavy object. Instead of pneumatic actuation, hydraulic actuation has a potential to generate much larger power by several MPa pressure. In this study, we develop a hydraulically-driven soft gripper, in which its basic design parameters are determined based on a mathematical model that represents the relationship among the driving pressure, bending angle, object mass and grasping force. Moreover, we selected materials suitable for grasping a heavier object, based on the finite element analysis result of the detailed design. We report experimental results on a 20 kg object grasping and closed-loop control of the finger bending angle.", "AI": {"tldr": "\u5f00\u53d1\u6db2\u538b\u9a71\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u624b\uff0c\u7528\u4e8e\u6293\u53d610-20\u516c\u65a4\u91cd\u7269\uff0c\u76f8\u6bd4\u6c14\u52a8\u65b9\u6848\u80fd\u63d0\u4f9b\u66f4\u5927\u6293\u53d6\u529b", "motivation": "\u73b0\u6709\u6c14\u52a8\u8f6f\u4f53\u6293\u624b\u538b\u529b\u4ec5\u51e0\u767ekPa\uff0c\u65e0\u6cd5\u6293\u53d610-20\u516c\u65a4\u91cd\u7269\uff1b\u6db2\u538b\u9a71\u52a8\u53ef\u8fbe\u51e0MPa\u538b\u529b\uff0c\u80fd\u4ea7\u751f\u66f4\u5927\u529f\u7387", "method": "\u57fa\u4e8e\u6570\u5b66\u6a21\u578b\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff08\u538b\u529b\u3001\u5f2f\u66f2\u89d2\u5ea6\u3001\u7269\u4f53\u8d28\u91cf\u3001\u6293\u53d6\u529b\u5173\u7cfb\uff09\uff1b\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\u9009\u62e9\u5408\u9002\u6750\u6599\uff1b\u5b9e\u73b020\u516c\u65a4\u7269\u4f53\u6293\u53d6\u548c\u5f2f\u66f2\u89d2\u5ea6\u95ed\u73af\u63a7\u5236", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u6db2\u538b\u9a71\u52a8\u8f6f\u4f53\u6293\u624b\uff0c\u80fd\u591f\u6293\u53d620\u516c\u65a4\u7269\u4f53\uff0c\u5e76\u5b9e\u73b0\u624b\u6307\u5f2f\u66f2\u89d2\u5ea6\u7684\u95ed\u73af\u63a7\u5236", "conclusion": "\u6db2\u538b\u9a71\u52a8\u8f6f\u4f53\u6293\u624b\u8bbe\u8ba1\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u6293\u53d6\u4f20\u7edf\u6c14\u52a8\u8f6f\u4f53\u6293\u624b\u65e0\u6cd5\u5904\u7406\u7684\u5927\u578b\u91cd\u7269"}}
{"id": "2601.09163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09163", "abs": "https://arxiv.org/abs/2601.09163", "authors": ["Tong Wu", "Shoujie Li", "Junhao Gong", "Changqing Guo", "Xingting Li", "Shilong Mu", "Wenbo Ding"], "title": "CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space", "comment": null, "summary": "Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \\CEI introduces the concept of \\textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \\CEI transfers data and policies from a Franka Panda robot to \\textbf{16} different embodiments across \\textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \\textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\\%. Finally, we demonstrate that \\CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/", "AI": {"tldr": "CEI\u6846\u67b6\u901a\u8fc7\u529f\u80fd\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u68af\u5ea6\u4f18\u5316\u5b9e\u73b0\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u95f4\u7684\u6f14\u793a\u8fc1\u79fb\uff0c\u652f\u630116\u79cd\u4eff\u771f\u548c6\u79cd\u771f\u5b9e\u4efb\u52a1\u7684\u53cc\u5411\u7b56\u7565\u8f6c\u79fb\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5728\u5927\u89c4\u6a21\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5f80\u5f80\u56e0\u6570\u636e\u96c6\u504f\u5dee\u800c\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u89c6\u89d2\u3001\u673a\u68b0\u81c2\u548c\u5e73\u884c\u5939\u722a\uff0c\u9650\u5236\u4e86\u8de8\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCross-Embodiment Interface (CEI)\u6846\u67b6\uff0c\u5f15\u5165\u529f\u80fd\u76f8\u4f3c\u6027\u6982\u5ff5\u5e76\u4f7f\u7528\u65b9\u5411\u6027Chamfer\u8ddd\u79bb\u91cf\u5316\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u5bf9\u9f50\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u4e3a\u672a\u89c1\u8fc7\u7684\u673a\u68b0\u81c2\u548c\u672b\u7aef\u6267\u884c\u5668\u5408\u6210\u89c2\u6d4b\u548c\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5c06Franka Panda\u673a\u5668\u4eba\u7684\u6570\u636e\u548c\u7b56\u7565\u8fc1\u79fb\u523016\u79cd\u4e0d\u540c\u5f62\u6001\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0UR5+AG95\u4e0eUR5+Xhand\u4e4b\u95f4\u76846\u4e2a\u4efb\u52a1\u53cc\u5411\u8fc1\u79fb\uff0c\u5e73\u5747\u8fc1\u79fb\u7387\u8fbe\u523082.4%\u3002", "conclusion": "CEI\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u5f62\u6001\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u673a\u68b0\u81c2\u548c\u672b\u7aef\u6267\u884c\u5668\u7684\u6f14\u793a\u8fc1\u79fb\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u7a7a\u95f4\u6cdb\u5316\u548c\u591a\u6a21\u6001\u8fd0\u52a8\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2601.09178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09178", "abs": "https://arxiv.org/abs/2601.09178", "authors": ["Paul Brunzema", "Thomas Lew", "Ray Zhang", "Takeru Shirasawa", "John Subosits", "Marcus Greiff"], "title": "Vision-Conditioned Variational Bayesian Last Layer Dynamics Models", "comment": "9 pages, 7 figures, currently under review", "summary": "Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u6761\u4ef6\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u6700\u540e\u4e00\u5c42\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5229\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u9884\u6d4b\u73af\u5883\u53d8\u5316\uff0c\u5e94\u7528\u4e8e\u8f66\u8f86\u8d5b\u8f66\u63a7\u5236\uff0c\u5728\u79ef\u6c34\u8def\u9762\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u65e0\u89c6\u89c9\u6761\u4ef6\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u654f\u6377\u63a7\u5236\u9700\u8981\u9884\u6d4b\u73af\u5883\u5bf9\u7cfb\u7edf\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7cfb\u7edf\u884c\u4e3a\u7684\u7a81\u53d8\uff0c\u800c\u81ea\u9002\u5e94\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u53cd\u5e94\u5f0f\u7684\uff0c\u53ef\u80fd\u65e0\u6cd5\u53ca\u65f6\u9002\u5e94\u4ee5\u786e\u4fdd\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u6761\u4ef6\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u6700\u540e\u4e00\u5c42\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5148\u5b66\u4e60\u540d\u4e49\u8f66\u8f86\u52a8\u529b\u5b66\uff0c\u7136\u540e\u901a\u8fc7\u6f5c\u5728\u7279\u5f81\u7684\u9010\u7279\u5f81\u4eff\u5c04\u53d8\u6362\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u5e76\u5c06\u8be5\u6a21\u578b\u96c6\u6210\u5230\u6700\u4f18\u63a7\u5236\u5668\u4e2d\u3002", "result": "\u5728\u96f7\u514b\u8428\u65afLC500\u8d5b\u8f66\u901a\u8fc7\u79ef\u6c34\u8def\u9762\u7684\u5b9e\u9a8c\u4e2d\uff0c\u89c6\u89c9\u6761\u4ef6\u6a21\u578b\u5728\u53d8\u5316\u6761\u4ef6\u4e0b\u5b8c\u6210\u4e86\u6240\u670912\u6b21\u5c1d\u8bd5\u7684\u5708\u6570\uff0c\u800c\u65e0\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u57fa\u7ebf\u65b9\u6cd5\u5168\u90e8\u5931\u63a7\uff0c\u8bc1\u660e\u4e86\u5728\u9ad8\u6027\u80fd\u5e94\u7528\u4e2d\u4e3b\u52a8\u52a8\u529b\u5b66\u9002\u5e94\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u89c6\u89c9\u6761\u4ef6\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u6700\u540e\u4e00\u5c42\u52a8\u529b\u5b66\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u73af\u5883\u53d8\u5316\uff0c\u5b9e\u73b0\u4e3b\u52a8\u9002\u5e94\uff0c\u5728\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u63a7\u5236\u6027\u80fd\u548c\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2601.09231", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09231", "abs": "https://arxiv.org/abs/2601.09231", "authors": ["Shuoye Li", "Zhiyuan Song", "Yulin Li", "Zhihai Bi", "Jun Ma"], "title": "Online Trajectory Optimization for Arbitrary-Shaped Mobile Robots via Polynomial Separating Hypersurfaces", "comment": null, "summary": "An emerging class of trajectory optimization methods enforces collision avoidance by jointly optimizing the robot's configuration and a separating hyperplane. However, as linear separators only apply to convex sets, these methods require convex approximations of both the robot and obstacles, which becomes an overly conservative assumption in cluttered and narrow environments. In this work, we unequivocally remove this limitation by introducing nonlinear separating hypersurfaces parameterized by polynomial functions. We first generalize the classical separating hyperplane theorem and prove that any two disjoint bounded closed sets in Euclidean space can be separated by a polynomial hypersurface, serving as the theoretical foundation for nonlinear separation of arbitrary geometries. Building on this result, we formulate a nonlinear programming (NLP) problem that jointly optimizes the robot's trajectory and the coefficients of the separating polynomials, enabling geometry-aware collision avoidance without conservative convex simplifications. The optimization remains efficiently solvable using standard NLP solvers. Simulation and real-world experiments with nonconvex robots demonstrate that our method achieves smooth, collision-free, and agile maneuvers in environments where convex-approximation baselines fail.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u591a\u9879\u5f0f\u8d85\u66f2\u9762\u66ff\u4ee3\u7ebf\u6027\u8d85\u5e73\u9762\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\u4e2d\u7684\u78b0\u649e\u907f\u514d\uff0c\u89e3\u51b3\u4e86\u51f8\u8fd1\u4f3c\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4fdd\u5b88\u6027\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ebf\u6027\u8d85\u5e73\u9762\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u5c06\u673a\u5668\u4eba\u548c\u969c\u788d\u7269\u8fd1\u4f3c\u4e3a\u51f8\u96c6\uff0c\u8fd9\u5728\u6742\u4e71\u72ed\u7a84\u73af\u5883\u4e2d\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "1. \u63a8\u5e7f\u7ecf\u5178\u5206\u79bb\u8d85\u5e73\u9762\u5b9a\u7406\uff0c\u8bc1\u660e\u4efb\u610f\u4e24\u4e2a\u4e0d\u76f8\u4ea4\u7684\u6709\u754c\u95ed\u96c6\u90fd\u53ef\u7528\u591a\u9879\u5f0f\u8d85\u66f2\u9762\u5206\u79bb\uff1b2. \u6784\u5efa\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u673a\u5668\u4eba\u8f68\u8ff9\u548c\u5206\u79bb\u591a\u9879\u5f0f\u7cfb\u6570\uff1b3. \u4f7f\u7528\u6807\u51c6NLP\u6c42\u89e3\u5668\u9ad8\u6548\u6c42\u89e3", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u51f8\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5e73\u6ed1\u3001\u65e0\u78b0\u649e\u3001\u654f\u6377\u7684\u673a\u52a8\uff0c\u800c\u57fa\u4e8e\u51f8\u8fd1\u4f3c\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u5931\u8d25", "conclusion": "\u901a\u8fc7\u5f15\u5165\u591a\u9879\u5f0f\u8d85\u66f2\u9762\u5206\u79bb\uff0c\u5f7b\u5e95\u6d88\u9664\u4e86\u8f68\u8ff9\u4f18\u5316\u4e2d\u78b0\u649e\u907f\u514d\u7684\u51f8\u8fd1\u4f3c\u9650\u5236\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u51e0\u4f55\u611f\u77e5\u78b0\u649e\u907f\u514d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2601.09318", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.09318", "abs": "https://arxiv.org/abs/2601.09318", "authors": ["Ro'i Lang", "Elon Rimon"], "title": "Feedback-Based Mobile Robot Navigation in 3-D Environments Using Artificial Potential Functions Technical Report", "comment": null, "summary": "This technical report presents the construction and analysis of polynomial navigation functions for motion planning in 3-D workspaces populated by spherical and cylindrical obstacles. The workspace is modeled as a bounded spherical region, and obstacles are encoded using smooth polynomial implicit functions. We establish conditions under which the proposed navigation functions admit a unique non-degenerate minimum at the target while avoiding local minima, including in the presence of pairwise intersecting obstacles. Gradient and Hessian analyses are provided, and the theoretical results are validated through numerical simulations in obstacle rich 3-D environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e3D\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7403\u5f62\u548c\u5706\u67f1\u5f62\u969c\u788d\u7269\u8fd0\u52a8\u89c4\u5212\u7684\u5bfc\u822a\u51fd\u6570\u6784\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u9690\u51fd\u6570\u7f16\u7801\u969c\u788d\u7269\uff0c\u786e\u4fdd\u76ee\u6807\u70b9\u5b58\u5728\u552f\u4e00\u975e\u9000\u5316\u6700\u5c0f\u503c\u4e14\u65e0\u5c40\u90e8\u6781\u5c0f\u503c\u3002", "motivation": "\u57283D\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u65f6\uff0c\u9700\u8981\u5904\u7406\u7403\u5f62\u548c\u5706\u67f1\u5f62\u969c\u788d\u7269\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u9762\u4e34\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u80fd\u591f\u4fdd\u8bc1\u5168\u5c40\u6536\u655b\u5230\u76ee\u6807\u7684\u5bfc\u822a\u51fd\u6570\uff0c\u5373\u4f7f\u5728\u969c\u788d\u7269\u76f8\u4e92\u4ea4\u53c9\u7684\u590d\u6742\u73af\u5883\u4e2d\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "method": "\u91c7\u7528\u591a\u9879\u5f0f\u5bfc\u822a\u51fd\u6570\u65b9\u6cd5\uff0c\u5c06\u5de5\u4f5c\u7a7a\u95f4\u5efa\u6a21\u4e3a\u6709\u754c\u7403\u5f62\u533a\u57df\uff0c\u4f7f\u7528\u5149\u6ed1\u591a\u9879\u5f0f\u9690\u51fd\u6570\u7f16\u7801\u7403\u5f62\u548c\u5706\u67f1\u5f62\u969c\u788d\u7269\u3002\u901a\u8fc7\u6570\u5b66\u5206\u6790\u5efa\u7acb\u6761\u4ef6\uff0c\u786e\u4fdd\u5bfc\u822a\u51fd\u6570\u5728\u76ee\u6807\u70b9\u5b58\u5728\u552f\u4e00\u975e\u9000\u5316\u6700\u5c0f\u503c\uff0c\u540c\u65f6\u907f\u514d\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u5305\u62ec\u5728\u969c\u788d\u7269\u76f8\u4e92\u4ea4\u53c9\u7684\u60c5\u51b5\u4e0b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6240\u63d0\u51fa\u7684\u5bfc\u822a\u51fd\u6570\u5728\u76ee\u6807\u70b9\u5177\u6709\u552f\u4e00\u975e\u9000\u5316\u6700\u5c0f\u503c\uff0c\u80fd\u591f\u907f\u514d\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\u3002\u63d0\u4f9b\u4e86\u68af\u5ea6\u548cHessian\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5728\u969c\u788d\u7269\u4e30\u5bcc\u76843D\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u6784\u5efa\u4e86\u9002\u7528\u4e8e3D\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7403\u5f62\u548c\u5706\u67f1\u5f62\u969c\u788d\u7269\u7684\u591a\u9879\u5f0f\u5bfc\u822a\u51fd\u6570\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u8bc1\u8fd0\u52a8\u89c4\u5212\u7cfb\u7edf\u5168\u5c40\u6536\u655b\u5230\u76ee\u6807\u4f4d\u7f6e\uff0c\u5373\u4f7f\u5728\u590d\u6742\u969c\u788d\u7269\u914d\u7f6e\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u4e3a3D\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2601.09377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09377", "abs": "https://arxiv.org/abs/2601.09377", "authors": ["Xuemei Yao", "Xiao Yang", "Jianbin Sun", "Liuwei Xie", "Xuebin Shao", "Xiyu Fang", "Hang Su", "Kewei Yang"], "title": "ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving", "comment": "Accepted by AAAI 2026", "summary": "Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.", "AI": {"tldr": "ReflexDiffusion\uff1a\u4e00\u79cd\u901a\u8fc7\u53cd\u5c04\u8c03\u6574\u589e\u5f3a\u6269\u6563\u8f68\u8ff9\u89c4\u5212\u5668\u7684\u63a8\u7406\u9636\u6bb5\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u573a\u666f\uff08\u5982\u6025\u8f6c\u5f2f\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u8c03\u6574\u673a\u5236\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u653e\u5927\u5173\u952e\u6761\u4ef6\u4fe1\u53f7\uff0c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6781\u9650\u5de5\u51b5\u4e0b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u89c4\u5212\u5668\u5728\u957f\u5c3e\u573a\u666f\uff08\u7279\u522b\u662f\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u7684\u6025\u8f6c\u5f2f\u7b49\u5173\u952e\u5b89\u5168\u573a\u666f\uff09\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\u5bf9\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u9053\u8def\u51e0\u4f55\u548c\u73af\u5883\u7ea6\u675f\u7684\u5efa\u6a21\u4e0d\u8db3\uff0c\u5f53\u8f66\u8f86\u63a5\u8fd1\u7269\u7406\u6781\u9650\u65f6\u4f1a\u4ea7\u751f\u6b21\u4f18\u6216\u4e0d\u5b89\u5168\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "method": "\u63d0\u51faReflexDiffusion\u6846\u67b6\uff0c\u5728\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5f15\u5165\u57fa\u4e8e\u68af\u5ea6\u7684\u8c03\u6574\u673a\u5236\uff1a\u5728\u6bcf\u6b21\u6807\u51c6\u8f68\u8ff9\u66f4\u65b0\u540e\uff0c\u8ba1\u7b97\u6761\u4ef6\u566a\u58f0\u9884\u6d4b\u548c\u65e0\u6761\u4ef6\u566a\u58f0\u9884\u6d4b\u4e4b\u95f4\u7684\u68af\u5ea6\uff0c\u663e\u5f0f\u653e\u5927\u5173\u952e\u6761\u4ef6\u4fe1\u53f7\uff08\u5305\u62ec\u9053\u8def\u66f2\u7387\u548c\u6a2a\u5411\u8f66\u8f86\u52a8\u529b\u5b66\uff09\uff0c\u4ece\u800c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u9075\u5b88\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5728nuPlan Test14-hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReflexDiffusion\u5728\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u573a\u666f\u4e0b\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e8614.1%\u7684\u9a7e\u9a76\u5206\u6570\u63d0\u5347\uff0c\u8bc1\u660e\u63a8\u7406\u65f6\u8f68\u8ff9\u4f18\u5316\u53ef\u4ee5\u6709\u6548\u8865\u507f\u8bad\u7ec3\u6570\u636e\u7a00\u758f\u6027\uff0c\u5728\u63a5\u8fd1\u64cd\u63a7\u6781\u9650\u65f6\u52a8\u6001\u5f3a\u5316\u5b89\u5168\u5173\u952e\u7ea6\u675f\u3002", "conclusion": "ReflexDiffusion\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5176\u67b6\u6784\u65e0\u5173\u7684\u8bbe\u8ba1\u53ef\u4ee5\u76f4\u63a5\u90e8\u7f72\u5230\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u5668\u4e2d\uff0c\u6709\u6548\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6311\u6218\u6027\u9a7e\u9a76\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u7684\u6781\u9650\u5de5\u51b5\u4e0b\u3002"}}
{"id": "2601.09444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09444", "abs": "https://arxiv.org/abs/2601.09444", "authors": ["Lauri Suomela", "Naoki Takahata", "Sasanka Kuruppu Arachchige", "Harry Edelman", "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"], "title": "Data Scaling for Navigation in Unknown Environments", "comment": null, "summary": "Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.\n  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.", "AI": {"tldr": "\u5927\u89c4\u6a21\u7814\u7a76\u8868\u660e\uff0c\u6570\u636e\u591a\u6837\u6027\u6bd4\u6570\u636e\u91cf\u5bf9\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u66f4\u4e3a\u5173\u952e\uff0c\u4f7f\u7528\u6765\u81ea35\u4e2a\u56fd\u5bb6161\u4e2a\u5730\u70b9\u76844,565\u5c0f\u65f6\u4f17\u5305\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7279\u5b9a\u73af\u5883\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\u5728\u8bad\u7ec3\u672a\u89c1\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\u63a2\u7d22\u6570\u636e\u91cf\u548c\u6570\u636e\u591a\u6837\u6027\u5bf9\u7aef\u5230\u7aef\u3001\u65e0\u5730\u56fe\u89c6\u89c9\u5bfc\u822a\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u6536\u96c6\u6765\u81ea35\u4e2a\u56fd\u5bb6161\u4e2a\u5730\u70b9\u76844,565\u5c0f\u65f6\u4f17\u5305\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u70b9\u76ee\u6807\u5bfc\u822a\u7b56\u7565\uff0c\u5728\u56db\u4e2a\u56fd\u5bb6\u7684\u4fa7\u9053\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u95ed\u73af\u63a7\u5236\u6027\u80fd\u8bc4\u4f30\uff0c\u8986\u76d6125\u516c\u91cc\u81ea\u4e3b\u9a7e\u9a76\uff0c\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u91cf\u548c\u6570\u636e\u591a\u6837\u6027\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u4f7f\u7b56\u7565\u80fd\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u5bfc\u822a\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u73af\u5883\u7279\u5b9a\u6f14\u793a\u8bad\u7ec3\u7684\u7b56\u7565\u3002\u6570\u636e\u591a\u6837\u6027\u6bd4\u6570\u636e\u91cf\u66f4\u91cd\u8981\uff1a\u8bad\u7ec3\u96c6\u4e2d\u5730\u7406\u4f4d\u7f6e\u6570\u91cf\u7ffb\u500d\u53ef\u4f7f\u5bfc\u822a\u9519\u8bef\u51cf\u5c11\u7ea615%\uff0c\u800c\u4ece\u73b0\u6709\u4f4d\u7f6e\u6dfb\u52a0\u6570\u636e\u7684\u6027\u80fd\u6536\u76ca\u5728\u6570\u636e\u91cf\u5f88\u5c11\u65f6\u5c31\u9971\u548c\u3002\u5728\u566a\u58f0\u4f17\u5305\u6570\u636e\u4e0b\uff0c\u7b80\u5355\u56de\u5f52\u6a21\u578b\u4f18\u4e8e\u751f\u6210\u5f0f\u548c\u5e8f\u5217\u67b6\u6784\u3002", "conclusion": "\u6570\u636e\u591a\u6837\u6027\u662f\u5b9e\u73b0\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u80fd\u591f\u652f\u6301\u96f6\u6837\u672c\u5bfc\u822a\uff0c\u800c\u7b80\u5355\u56de\u5f52\u6a21\u578b\u5728\u566a\u58f0\u4f17\u5305\u6570\u636e\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2601.09512", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09512", "abs": "https://arxiv.org/abs/2601.09512", "authors": ["Ralf R\u00f6mer", "Yi Zhang", "Angela P. Schoellig"], "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion", "comment": "Project page: https://tum-lsy.github.io/clare. 9 pages, 5 figures", "summary": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.", "AI": {"tldr": "CLARE\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u514d\u793a\u4f8b\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u9002\u914d\u5668\u548c\u81ea\u52a8\u7f16\u7801\u5668\u8def\u7531\u673a\u5236\u5b9e\u73b0\u65b0\u4efb\u52a1\u5b66\u4e60\u800c\u4e0d\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5b58\u50a8\u5148\u524d\u6570\u636e\u3001\u96be\u4ee5\u5904\u7406\u957f\u4efb\u52a1\u5e8f\u5217\u6216\u4f9d\u8d56\u4efb\u52a1\u6807\u8bc6\u7b26\u90e8\u7f72\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u957f\u671f\u8fd0\u884c\u7684\u9700\u6c42\u3002", "method": "CLARE\u5728\u9009\u5b9a\u524d\u9988\u5c42\u5f15\u5165\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u9002\u914d\u5668\uff0c\u57fa\u4e8e\u5c42\u95f4\u7279\u5f81\u76f8\u4f3c\u6027\u81ea\u4e3b\u6269\u5c55\u6a21\u578b\uff1b\u90e8\u7f72\u65f6\u4f7f\u7528\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u8def\u7531\u673a\u5236\u52a8\u6001\u6fc0\u6d3b\u6700\u76f8\u5173\u9002\u914d\u5668\uff0c\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLARE\u5728\u65b0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u4e0d\u707e\u96be\u6027\u9057\u5fd8\u65e9\u671f\u4efb\u52a1\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u793a\u4f8b\u7684\u65b9\u6cd5\u3002", "conclusion": "CLARE\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53c2\u6570\u9ad8\u6548\u7684\u514d\u793a\u4f8b\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u957f\u671f\u8fd0\u884c\u4e2d\u7684\u77e5\u8bc6\u4fdd\u7559\u4e0e\u9002\u5e94\u95ee\u9898\u3002"}}
{"id": "2601.09518", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09518", "abs": "https://arxiv.org/abs/2601.09518", "authors": ["Wei-Jin Huang", "Yue-Yi Zhang", "Yi-Lin Wei", "Zhi-Wei Xia", "Juantao Tan", "Yuan-Ming Li", "Zhilin Zhao", "Wei-Shi Zheng"], "title": "Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations", "comment": null, "summary": "Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.", "AI": {"tldr": "PAIR+D-STAR\uff1a\u57fa\u4e8e\u7269\u7406\u611f\u77e5\u4ea4\u4e92\u91cd\u5b9a\u5411\u548c\u65f6\u7a7a\u89e3\u8026\u63a8\u7406\u7684\u5206\u5c42\u7b56\u7565\uff0c\u4ece\u4eba\u7c7b\u4ea4\u4e92\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u4eba\u5f62\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u5e76\u5b66\u4e60\u590d\u6742\u5168\u8eab\u534f\u4f5c\u884c\u4e3a", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u9700\u8981\u4e0e\u4eba\u7c7b\u8fdb\u884c\u7269\u7406\u4ea4\u4e92\uff0c\u4f46\u9ad8\u8d28\u91cf\u7684\u4eba-\u4eba\u5f62\u4ea4\u4e92\u6570\u636e\u7a00\u7f3a\u3002\u867d\u7136\u5229\u7528\u4e30\u5bcc\u7684\u4eba-\u4eba\u4ea4\u4e92\u6570\u636e\u662f\u53ef\u884c\u65b9\u6848\uff0c\u4f46\u6807\u51c6\u91cd\u5b9a\u5411\u65b9\u6cd5\u4f1a\u7834\u574f\u5173\u952e\u63a5\u89e6\u70b9\uff0c\u800c\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4ec5\u6a21\u4eff\u8f68\u8ff9\u7f3a\u4e4f\u4ea4\u4e92\u7406\u89e3", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) PAIR\uff08\u7269\u7406\u611f\u77e5\u4ea4\u4e92\u91cd\u5b9a\u5411\uff09- \u57fa\u4e8e\u63a5\u89e6\u4e2d\u5fc3\u7684\u7ba1\u9053\uff0c\u8de8\u5f62\u6001\u5dee\u5f02\u4fdd\u6301\u63a5\u89e6\u8bed\u4e49\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u4eba-\u4eba\u5f62\u4ea4\u4e92\u6570\u636e\uff1b2) D-STAR\uff08\u89e3\u8026\u65f6\u7a7a\u52a8\u4f5c\u63a8\u7406\u5668\uff09- \u5206\u5c42\u7b56\u7565\uff0c\u901a\u8fc7\u76f8\u4f4d\u6ce8\u610f\u529b\uff08\u4f55\u65f6\u884c\u52a8\uff09\u548c\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6a21\u5757\uff08\u4f55\u5904\u884c\u52a8\uff09\u89e3\u8026\u65f6\u7a7a\u63a8\u7406\uff0c\u7531\u6269\u6563\u5934\u878d\u5408\u751f\u6210\u8d85\u8d8a\u6a21\u4eff\u7684\u540c\u6b65\u5168\u8eab\u884c\u4e3a", "result": "\u901a\u8fc7\u5e7f\u6cdb\u4e25\u683c\u7684\u4eff\u771f\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u4ece\u4eba-\u4eba\u4ea4\u4e92\u6570\u636e\u5b66\u4e60\u590d\u6742\u5168\u8eab\u4ea4\u4e92\u7684\u5b8c\u6574\u6709\u6548\u7ba1\u9053", "conclusion": "PAIR\u548cD-STAR\u5171\u540c\u6784\u6210\u4e86\u4ece\u4e30\u5bcc\u4eba-\u4eba\u4ea4\u4e92\u6570\u636e\u5b66\u4e60\u590d\u6742\u4eba-\u4eba\u5f62\u4ea4\u4e92\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u7a7a\u63a8\u7406\u5b9e\u73b0\u4e86\u8d85\u8d8a\u7b80\u5355\u6a21\u4eff\u7684\u54cd\u5e94\u5f0f\u540c\u6b65\u534f\u4f5c"}}
{"id": "2601.09578", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09578", "abs": "https://arxiv.org/abs/2601.09578", "authors": ["Jiajun Sun", "Yangyi Ou", "Haoyuan Zheng", "Chao yang", "Yue Ma"], "title": "Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping", "comment": "5 pages,7 figures. Under review", "summary": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u589e\u5f3a3D\u70b9\u4e91\u5730\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\uff0c\u5c06\u70ed\u4fe1\u606f\u4f5c\u4e3a\u8bed\u4e49\u5c42\u6dfb\u52a0\u52303D\u5730\u56fe\u4e2d\uff0c\u5b9e\u73b0\u73af\u5883\u51e0\u4f55\u4e0e\u70ed\u6e90\u8bed\u4e49\u7684\u53cc\u91cd\u611f\u77e5\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u548c\u73af\u5883\u611f\u77e5\u5bf9SLAM\u6280\u672f\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002\u73b0\u6709SLAM\u7cfb\u7edf\u901a\u5e38\u7f3a\u4e4f\u5bf9\u73af\u5883\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bc6\u522b\u70ed\u6e90\u76ee\u6807\uff08\u5982\u706b\u707e\u3001\u8bbe\u5907\u6545\u969c\uff09\u7684\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u6e29\u5ea6\u76f8\u5173\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "method": "1. \u9996\u5148\u5728\u50cf\u7d20\u7ea7\u522b\u878d\u5408\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\uff1b2. \u5c06\u5b9e\u65f6LiDAR\u70b9\u4e91\u6295\u5f71\u5230\u878d\u5408\u540e\u7684\u56fe\u50cf\u6d41\u4e0a\uff1b3. \u5728\u70ed\u901a\u9053\u4e2d\u5206\u5272\u70ed\u6e90\u7279\u5f81\uff0c\u5b9e\u65f6\u8bc6\u522b\u9ad8\u6e29\u76ee\u6807\uff1b4. \u5c06\u6e29\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u8bed\u4e49\u5c42\u5e94\u7528\u5230\u6700\u7ec8\u76843D\u5730\u56fe\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5730\u56fe\u4e0d\u4ec5\u5177\u6709\u7cbe\u786e\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd8\u5305\u542b\u5bf9\u73af\u5883\u7684\u5173\u952e\u8bed\u4e49\u7406\u89e3\uff0c\u80fd\u591f\u5b9e\u65f6\u8bc6\u522b\u9ad8\u6e29\u76ee\u6807\u5e76\u5c06\u6e29\u5ea6\u4fe1\u606f\u6574\u5408\u52303D\u5730\u56fe\u4e2d\u3002", "conclusion": "\u8fd9\u79cd\u70ed\u4fe1\u606f\u8bed\u4e49\u589e\u5f3a\u76843D\u70b9\u4e91\u5730\u56fe\u65b9\u6cd5\u5bf9\u4e8e\u5feb\u901f\u707e\u5bb3\u8bc4\u4f30\u548c\u5de5\u4e1a\u9884\u9632\u6027\u7ef4\u62a4\u7b49\u7279\u5b9a\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
