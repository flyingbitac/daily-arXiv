<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为\method的样本高效人机交互强化学习框架，通过主动选择信息量大的样本来减少人类干预需求，在真实世界操作任务中实现了更高的成功率和更低的人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互强化学习框架样本效率低，需要大量人类干预才能收敛，导致高昂的劳动力成本。需要开发一种更高效的框架来减少人类干预需求。

Method: 通过策略熵的稳定减少来改善探索与利用的权衡。首先构建不同样本对策略熵的影响函数，通过动作概率和策略软优势的协方差高效估计。然后选择影响函数值适中的样本，剔除导致熵急剧下降的捷径样本和影响可忽略的噪声样本。

Result: 在四个真实世界操作任务上的广泛实验表明，\method相比最先进的人机交互强化学习方法实现了42.1%更高的成功率，同时需要10.1%更少的人类干预。

Conclusion: \method框架通过主动选择信息量大的样本，有效提高了人机交互强化学习的样本效率，减少了人类干预需求，在真实世界操作任务中表现出优越性能。

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [2] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法通过即时模块和运动性能模块改进高维机器人路径规划，在复杂环境中比传统采样方法表现更好


<details>
  <summary>Details</summary>
Motivation: 传统采样方法在高维机器人路径规划中难以在复杂多障碍环境中高效找到可行且最优的路径，特别是对于机器人操纵器，运动奇点和自碰撞风险进一步增加了运动效率和安全性挑战

Method: 提出JIT*算法，包含两个核心模块：即时模块（包括动态优化边连接的"即时边"和调整瓶颈区域采样密度的"即时采样"）和运动性能模块（通过动态切换平衡可操作性和轨迹成本）

Result: JIT*在R^4到R^16维度上持续优于传统采样规划器，在单臂和双臂操纵任务中表现出有效性

Conclusion: JIT*算法通过即时优化和运动性能平衡，有效解决了高维机器人路径规划中的效率和安全性问题

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [3] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 提出一种高斯过程模型更新方法，通过预计算雅可比和海森矩阵，利用测量位置误差数据进行二阶校正，提高预测精度和计算效率


<details>
  <summary>Details</summary>
Motivation: 现实世界中移动机器人进行标量场映射时，不完美的定位会引入状态不确定性，导致测量位置估计误差，从而降低高斯过程的均值和协方差估计精度

Method: 利用核函数的可微性，开发二阶校正算法，使用预计算的GP均值和协方差函数的雅可比和海森矩阵，基于测量位置误差数据进行实时精炼

Result: 仿真结果表明，与完整模型重新训练相比，该方法提高了预测精度和计算效率

Conclusion: 该方法能够有效处理移动机器人标量场映射中的定位不确定性，提供了一种高效的高斯过程模型更新方案

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [4] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER框架通过纹理鲁棒的视觉推理链和可变形物体细化，解决可变形物体操作中语义指令与物理交互点对齐的挑战，显著提升功能区域识别精度和长时程任务成功率。


<details>
  <summary>Details</summary>
Motivation: 可变形物体操作的核心挑战在于复杂外观和纹理变化下，高层语义指令与物理交互点的对齐。现有基于视觉的功能预测方法由于自由度近无限、复杂动力学和异质模式，常出现边界溢出和功能区域碎片化问题。

Method: 提出TRACER框架：1) 树状功能推理链(TA-CoT)将高层任务意图分解为层次化子任务语义；2) 空间约束边界细化(SCBR)机制抑制预测溢出；3) 交互收敛细化流(ICRF)聚合受外观噪声污染的离散像素。

Result: 在Fine-AGDDO15数据集和真实机器人平台上进行广泛实验，TRACER显著提升了跨不同纹理和模式的功能定位精度，更重要的是提高了长时程任务的成功率。

Conclusion: TRACER有效弥合了高层语义推理与低层物理执行之间的鸿沟，为可变形物体操作提供了纹理鲁棒的功能识别框架。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [5] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide：一种新颖的跨策略视觉-触觉融合范式，通过两阶段推理引导预训练的视觉运动策略，利用触觉反馈优化接触丰富的精细操作任务


<details>
  <summary>Details</summary>
Motivation: 机器人精细操作和接触丰富的任务仍然具有挑战性，主要原因是触觉反馈利用不足。现有方法未能充分利用触觉信息来指导动作生成，导致物理接触约束无法得到有效满足。

Method: 提出TouchGuide两阶段范式：1）早期采样阶段使用视觉输入生成粗略的视觉合理动作；2）通过任务特定的接触物理模型（CPM）提供触觉指导，使用对比学习训练CPM生成触觉感知的可行性分数，引导采样过程满足物理接触约束。同时开发TacUMI数据收集系统，使用刚性指尖获取直接触觉反馈，平衡精度与成本。

Result: 在五个具有挑战性的接触丰富任务（如系鞋带、芯片交接等）上的广泛实验表明，TouchGuide在性能上持续且显著优于最先进的视觉-触觉策略。

Conclusion: TouchGuide通过将触觉反馈融入动作空间，有效解决了精细操作中的物理接触约束问题，为机器人接触丰富任务提供了有效的解决方案，同时TacUMI系统为高质量触觉数据收集提供了经济高效的途径。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [6] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi是一个知识蒸馏框架，通过将VLA模型的Transformer层数从18层压缩到6层，实现2倍以上推理加速，成功率下降小于1%，并在工业级机器人平台上验证


<details>
  <summary>Details</summary>
Motivation: 实时机器人部署需要快速、设备端推理的视觉-语言-动作模型。现有研究主要关注token级效率（如视觉token剪枝），而系统性的Transformer层缩减在基于流的VLA模型中尚未在知识蒸馏框架下探索

Method: 提出Shallow-pi知识蒸馏框架，激进地减少VLM主干和基于流的动作头的Transformer深度，将模型从18层压缩到6层

Result: 实现超过2倍的推理加速，在标准操作基准测试中成功率绝对下降小于1%，在缩减的VLA模型中达到最先进性能。在Jetson Orin和Jetson Thor等工业级平台上，在复杂动态操作场景中验证了方法

Conclusion: Shallow-pi通过系统性的Transformer层缩减和知识蒸馏，为实时机器人部署提供了高效的VLA模型压缩方案，在保持性能的同时显著提升推理速度

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [7] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: TaF-VLA框架通过触觉-力对齐而非传统的触觉-视觉对齐，将高维触觉观测与物理交互力关联，显著提升了VLA模型在接触密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖视觉模态，缺乏对接触密集型任务所需的物理直觉和精确力调节能力。现有将触觉传感融入VLA的方法通常将触觉输入视为辅助视觉纹理，忽略了表面变形与交互动力学之间的内在关联。

Method: 提出触觉-力对齐范式，开发自动触觉-力数据采集设备并创建TaF-Dataset。核心是Tactile-Force Adapter（TaF-Adapter），这是一种触觉传感器编码器，提取离散化潜在信息来编码触觉观测，确保学习到的表示捕捉历史依赖、噪声不敏感的物理动态而非静态视觉纹理。

Result: TaF-VLA策略在真实世界实验中显著优于最先进的触觉-视觉对齐和纯视觉基线，在接触密集型任务中表现出色，验证了其通过跨模态物理推理实现稳健、力感知操作的能力。

Conclusion: 通过将触觉观测与物理交互力明确对齐，TaF-VLA框架为VLA模型提供了物理直觉，使其能够更好地处理需要精确力调节和物理推理的接触密集型任务，代表了从触觉-视觉对齐到触觉-力对齐的范式转变。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [8] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: FAEA将前沿LLM智能体框架直接应用于机器人操作，无需任务演示或微调，在多个基准测试中达到接近VLA模型的性能水平


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型需要任务特定的演示和微调，且领域迁移能力差，研究是否通用大语言模型智能体框架可以作为机器人操作的替代控制范式

Method: FAEA将未修改的Claude Agent SDK直接应用于机器人操作，利用软件工程中调试代码的迭代推理能力，让机器人智能体能够推理操作策略

Result: 在LIBERO、ManiSkill3和MetaWorld基准测试中，FAEA分别达到84.9%、85.7%和96%的成功率，接近需要少于100次演示训练的VLA模型性能；加入一轮人类反馈后，LIBERO上性能提升至88.2%

Conclusion: 通用智能体足以处理一类以审慎任务级规划为主的机器人操作任务，为机器人系统利用前沿模型基础设施开辟了新路径，具有即时实用价值

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [9] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID：首个开源、大规模、宽频带、几何多样的射频数据集，用于细粒度材料识别，包含16个细粒度类别、142k样本，覆盖4-43.5GHz频率范围，并建立了多设置、多协议的深度学习基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的材料识别受光学传感器固有限制，而基于射频的方法能揭示材料内在特性但缺乏大规模公开数据集和学习方法的系统基准测试，阻碍了该领域的发展。

Method: 创建RF-MatID数据集，包含16个细粒度材料类别（分为5个超类），覆盖4-43.5GHz宽频带，包含142k个样本的频域和时域表示，系统引入入射角和距离等几何扰动控制变量。

Result: 建立了多设置、多协议基准测试，评估了最先进的深度学习模型在分布内性能和分布外鲁棒性（跨角度和跨距离偏移），通过5个频率分配协议支持频率和区域级分析。

Conclusion: RF-MatID旨在促进可重复研究、加速算法进步、增强跨领域鲁棒性，并支持射频材料识别在实际应用中的发展，填补了该领域的数据集空白。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [10] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一个用于多机器人月球勘探任务评估的结构化KPI框架，旨在解决现有野外试验因平台和实验设置差异而难以比较的问题，强调效率、鲁棒性和精度的场景依赖性优先级。


<details>
  <summary>Details</summary>
Motivation: 月球关键资源（如钛铁矿、稀土元素和水冰）的机器人勘探需要稳健的探索方法，但现有的野外模拟试验由于机器人平台和实验设置的差异，结果难以比较。这些任务通常使用特定场景的工程指标评估性能，未能建立野外性能与科学目标之间的明确联系。

Method: 从三个现实的多机器人月球场景中推导出结构化KPI框架，这些场景反映了科学目标和操作约束。该框架强调效率、鲁棒性和精度的场景依赖性优先级，并专门为野外部署的实际应用而设计。

Result: 在多机器人野外测试中验证了该框架，发现效率和鲁棒性相关的KPI实用且易于应用，而精度导向的KPI需要可靠的基准数据，这在户外模拟环境中并不总是可行。

Conclusion: 该框架可作为通用评估标准，实现多机器人野外试验的一致、目标导向比较，并支持未来行星探索机器人系统的系统化开发。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [11] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 通过低成本压电麦克风和音频谱图Transformer实现机器人全身触觉定位，静态条件下定位误差小于5毫米，材料特性对定位性能有显著影响


<details>
  <summary>Details</summary>
Motivation: 传统触觉皮肤成本高且集成复杂，需要一种可扩展、低成本的替代方案来实现机器人全身接触感知

Method: 使用七个低成本压电麦克风装备机械手，利用音频谱图Transformer解码物理交互过程中产生的振动信号

Result: 静态条件下定位误差小于5毫米，不同材料特性对定位性能有不同影响：刚性材料在脉冲响应定位中表现优异，纹理材料在轨迹跟踪中提供更好的摩擦特征

Conclusion: 复杂物理接触动力学可以通过简单振动信号有效解码，为实现广泛、经济实惠的机器人接触感知提供了可行途径，所有数据集、模型和实验设置已开源

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [12] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo是一个基于相似性感知的多机器人协作框架，通过"缓存与重用"机制减少重复计算，提高LLM驱动的多机器人系统效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多机器人协作方法在遇到相同或相似任务时需要重新规划，忽略了任务级相似性，导致计算效率低下

Method: 提出MeCo框架，引入相似性测试方法检索先前解决的高相关性任务，实现无需重新调用LLM的有效规划重用

Result: 实验结果显示，MeCo相比最先进方法显著降低了规划成本并提高了成功率

Conclusion: MeCo通过相似性感知和缓存重用机制，为多机器人协作提供了更高效的解决方案，并建立了首个相似任务协作场景基准MeCoBench

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [13] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO是一种用于足式机器人强化学习的训练框架，通过时变动作变换限制早期动作空间，促进有效数据收集，然后逐步扩展动作空间以增强探索，最终获得更高回报。


<details>
  <summary>Details</summary>
Motivation: 足式机器人的强化学习训练面临高维连续动作、硬件限制和有限探索的挑战。现有方法在位置控制中表现良好，但在力矩控制中效果较差，因为力矩控制需要更充分的动作空间探索和更有信息量的梯度信号。

Method: 提出Growing Policy Optimization (GPO)框架，应用时变动作变换：在训练早期限制有效动作空间以促进有效数据收集和策略学习，然后逐步扩展动作空间以增强探索。证明该变换保持了PPO更新规则，仅引入有界、渐近消失的梯度失真。

Result: 在四足和六足机器人上评估GPO，包括将仿真训练的策略零样本部署到硬件上。使用GPO训练的策略始终获得更好的性能。

Conclusion: GPO为学习足式运动提供了一个通用的、与环境无关的优化框架，能够有效解决力矩控制中的探索难题。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [14] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 提出一种基于肌腱位移和张力估计关节位置的计算方法，用于无关节编码器的仿人机器人手，通过非线性优化求解关节角度，并用于基于雅可比矩阵的闭环控制实现手势跟踪。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动的仿人机器人手通常缺乏直接关节角度传感，因为集成关节编码器会影响机械紧凑性和灵巧性。需要一种从肌腱状态间接估计关节位置的方法。

Method: 1. 基于Denavit-Hartenberg约定建立仿人手的运动学建模框架；2. 使用简化肌腱模型推导肌腱状态与关节位置的非线性方程组；3. 通过非线性优化方法求解关节角度；4. 采用基于雅可比矩阵的比例积分控制器加前馈项进行闭环控制。

Result: 在MuJoCo仿真环境中使用解剖学正确的生物机电手（每个长指5自由度，拇指6自由度）验证了所提出的估计和控制框架的有效性和局限性。

Conclusion: 该方法能够在不使用直接关节传感的情况下实现手势跟踪，为肌腱驱动仿人机器人手提供了一种可行的关节位置估计和控制解决方案。

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [15] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO提出了一种单步生成策略框架，通过MeanFlow实现无需知识蒸馏的单步推理，结合分散正则化和RL微调，在机器人控制任务中实现实时性能（>120Hz）。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流匹配的生成策略需要多步采样，限制了在时间关键场景中的部署。实时机器人控制需要快速的动作生成，但多步采样方法无法满足实时性要求。

Method: DMPO包含三个关键组件：1) MeanFlow - 数学推导的单步推理方法，无需知识蒸馏；2) 分散正则化 - 防止表示崩溃；3) 强化学习微调 - 超越专家演示。采用轻量级模型架构。

Result: 在RoboMimic操作和OpenAI Gym运动基准测试中，DMPO表现优于或与多步基线方法相当。推理速度提升5-20倍，达到>120Hz的实时控制要求，在高性能GPU上可达数百赫兹。在Franka-Emika-Panda机器人上的物理部署验证了实际应用性。

Conclusion: DMPO框架通过单步生成策略解决了实时机器人控制的瓶颈问题，结合数学推导的单步推理、正则化和RL微调，实现了高性能的实时控制，为时间关键场景提供了可行的解决方案。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [16] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 提出了一种在ROS 2系统中实现知识图谱的综合方法，旨在提升自主机器人任务的效率和智能性，并在Aerostack2框架中通过模拟搜救任务进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了增强自主机器人任务的效率和智能性，需要一种系统化的方法来整合知识图谱到ROS 2系统中，以改善决策制定和任务性能。

Method: 该方法包含五个关键步骤：定义初始和目标条件、结构化任务和子任务、规划任务序列、在知识图谱中表示任务相关数据、使用高级语言设计任务。每个步骤都建立在前一步的基础上，确保从初始设置到最终执行的连贯过程。

Result: 在Aerostack2框架中实现了该方法，通过Gazebo环境中的模拟搜救任务（无人机自主定位目标）展示了其有效性。实施结果表明该方法能够显著改善决策制定和任务性能。

Conclusion: 提出的知识图谱实施方法能够有效提升ROS 2系统中自主机器人任务的效率和智能性，通过结构化任务表示和规划增强了系统的决策能力。

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [17] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出一种基于神经风格迁移的强化学习策略从仿真到现实的迁移方法，通过变分自编码器学习特征表示并生成弱配对轨迹，在机器人切割未知材料任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人控制中面临仿真与现实之间的领域差距问题，且真实世界数据有限，限制了实际部署。需要一种能够利用有限真实数据实现有效策略迁移的方法。

Method: 将图像处理中的神经风格迁移重新解释并应用于强化学习策略迁移，使用变分自编码器联合学习自监督特征表示，生成弱配对的源-目标轨迹，提高合成轨迹的物理真实性。

Result: 在机器人切割未知材料案例中，相比基线方法（包括先前工作、CycleGAN和条件变分自编码器时间序列翻译），本方法在任务完成时间和行为稳定性方面表现更好，仅需少量真实数据，对几何和材料变化具有鲁棒性。

Conclusion: 该方法证明了在真实世界奖励信息不可用的接触密集型任务中，策略适应的可行性，为解决仿真到现实迁移问题提供了一种有效框架。

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
