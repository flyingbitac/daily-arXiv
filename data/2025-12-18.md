<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots](https://arxiv.org/abs/2512.14952)
*Iddo Yehoshua Wald,Amber Maimon,Shiyao Zhang,Dennis Küster,Robert Porzel,Tanja Schultz,Rainer Malaka*

Main category: cs.RO

TL;DR: 研究探索通过实时呼吸同步增强用户在机器人系统中的具身感，发现呼吸同步能显著提高身体所有权感


<details>
  <summary>Details</summary>
Motivation: 在远程呈现和远程操作中，同步的视觉运动反馈可以唤起身体所有权感和能动性，但现有研究主要关注视觉和运动反馈，缺乏对生理信号（如呼吸）作为内感受通路的探索

Method: 采用被试内实验设计，让参与者控制机械臂，机械臂运动要么与参与者自身呼吸同步，要么不同步，比较两种条件下身体所有权感的差异

Result: 呼吸同步显著增加了身体所有权感，大多数参与者更喜欢同步条件

Conclusion: 生理信号（如呼吸）可以作为人机交互的新内感受通路，对远程呈现、假肢、机器人协作和共享自主性有重要应用价值

Abstract: Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.

</details>


### [2] [ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision](https://arxiv.org/abs/2512.15020)
*Wenlong Xia,Jinhao Zhang,Ce Zhang,Yaojia Wang,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: ISS Policy是一种基于3D点云观测的扩散策略，通过隐式场景监督模块提升模仿学习的效率和泛化能力，在单臂和灵巧手操作任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的模仿学习过度依赖物体外观而忽略3D场景结构，导致训练效率低、泛化能力差。需要一种能更好利用3D几何信息的方法来提升机器人操作技能。

Method: 提出ISS Policy，这是一种基于DiT的3D视觉运动扩散策略，从点云观测预测连续动作序列。核心创新是隐式场景监督模块，使模型输出与场景几何演化保持一致。

Result: 在单臂操作任务（MetaWorld）和灵巧手操作（Adroit）上达到最先进性能。真实世界实验显示强大的泛化能力和鲁棒性。消融研究表明方法在数据和参数规模上都能有效扩展。

Conclusion: ISS Policy通过隐式场景监督有效利用3D几何信息，显著提升了模仿学习的效率和泛化能力，在模拟和真实环境中都表现出色，具有很好的可扩展性。

Abstract: Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.

</details>


### [3] [HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles](https://arxiv.org/abs/2512.15047)
*Yunheng Wang,Yixiao Feng,Yuetong Fang,Shuning Zhang,Tan Jing,Jian Li,Xiangrui Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: HERO框架通过构建层次化可通行3D场景图，将可操作障碍物建模为通路，显著提升机器人在复杂环境中的导航效率和可达性。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图导航方法基于静态世界假设，将可交互障碍物视为不可通行区域，导致在真实场景中可达性有限、效率低下且扩展性差。

Method: 提出HERO框架，构建层次化可通行3D场景图，重新定义可通行性：将可操作障碍物建模为通路，捕捉其物理交互性、功能语义和场景关系层次结构。

Result: 相比基线方法，HERO在部分阻塞环境中减少路径长度35.1%，在完全阻塞环境中提升成功率79.4%，显著提高了导航效率和可达性。

Conclusion: HERO通过建模可操作障碍物的交互特性，克服了传统静态世界假设的局限性，为智能体在复杂真实环境中的导航提供了更有效的解决方案。

Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.

</details>


### [4] [NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles](https://arxiv.org/abs/2512.15080)
*Gaurav Bansal*

Main category: cs.RO

TL;DR: NAP3D是一种利用NeRF进行3D-3D姿态对齐的新方法，通过将当前深度图像与预训练NeRF合成的3D点云对齐来校正姿态估计误差，无需重访先前位置，在长距离环境中表现优于传统2D-3D方法。


<details>
  <summary>Details</summary>
Motivation: 自主车辆定位中传感器噪声和时间漂移会导致显著的姿态估计误差，特别是在长距离环境中。传统的视觉闭环方法需要重访先前位置并依赖多传感器数据融合，存在局限性。

Method: 提出NeRF-Assisted 3D-3D Pose Alignment (NAP3D)，利用智能体当前深度图像与预训练NeRF之间的3D-3D对应关系。通过将观测场景的3D点与NeRF合成的3D点直接对齐，即使在新的视角下也能精炼估计姿态，无需重访先前观察位置。

Result: 在自定义数据集上，NAP3D实现了5厘米内的相机姿态校正，稳健地优于2D-3D Perspective-N-Point基线。在TUM RGB-D数据集上，NAP3D在不同噪声条件下持续改善3D对齐RMSE约6厘米，尽管PnP在某些情况下获得更低的原始旋转和平移参数误差，但NAP3D在3D空间中表现出更好的几何一致性。

Conclusion: NAP3D提供了一种轻量级、数据集无关的工具，当传统闭环不可用时，可以补充现有的SLAM和定位流程。其3D-3D公式相比传统2D-3D定位方法具有优势，同时在准确性和适用性方面保持可比性。

Abstract: Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.
  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.
  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.

</details>


### [5] [BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization](https://arxiv.org/abs/2512.15111)
*Dongmyeong Lee,Jesse Quattrociocchi,Christian Ellis,Rwik Rana,Amanda Adkins,Adam Uccello,Garrett Warnell,Joydeep Biswas*

Main category: cs.RO

TL;DR: BEV-Patch-PF是一种无需GPS的序列化地理定位系统，通过粒子滤波器结合鸟瞰图和航拍特征图，在越野环境中实现高精度实时定位。


<details>
  <summary>Details</summary>
Motivation: 解决在无GPS环境（如茂密树冠下）中机器人的精确定位问题，传统GPS方法在遮挡条件下失效，需要一种鲁棒的视觉定位系统。

Method: 从机载RGB和深度图像构建BEV特征图，通过粒子滤波器生成3自由度位姿假设，从局部航拍图像中裁剪对应补丁，通过特征匹配计算粒子对数似然。

Result: 在两个真实世界越野数据集上，相比基于检索的基线方法，在已见路线上绝对轨迹误差降低7.5倍，在未见路线上降低7.0倍，在茂密树冠和阴影下仍保持精度，在NVIDIA Tesla T4上以10Hz实时运行。

Conclusion: BEV-Patch-PF系统实现了无GPS环境下的高精度实时定位，在挑战性越野条件下表现出色，具备实际机器人部署的实用性。

Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.

</details>


### [6] [EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving](https://arxiv.org/abs/2512.15195)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Lukas Marc Listl,Oliver Bringmann*

Main category: cs.RO

TL;DR: 提出一种新的感知系统安全评估框架，结合物体和车道检测的安全指标，用于评估自动驾驶感知系统的安全性而非仅关注传统检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统感知评估指标（如精确率、召回率、F1分数）只关注整体检测精度，但未考虑感知错误可能引发的安全风险。高精度系统仍可能因特定误检导致严重事故，因此需要专门的安全评估方法。

Method: 提出一个联合评估框架，包含：1）轻量级物体安全指标，量化物体检测错误相关的潜在风险；2）车道安全指标，考虑物体与车道检测任务间的相互依赖关系；3）结合两者的统一安全评分。

Result: 在DeepAccident数据集上验证，该方法能识别传统性能指标无法捕捉的安全关键感知错误，证明安全中心评估方法的重要性。

Conclusion: 感知系统的安全评估至关重要，提出的联合安全评估框架为自动驾驶感知系统提供了统一、可解释的安全性能衡量标准，弥补了传统精度指标的不足。

Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.

</details>


### [7] [Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives](https://arxiv.org/abs/2512.15215)
*Erik Brorsson,Kristian Ceder,Ze Zhang,Sabino Francesco Roselli,Endre Erős,Martin Dahl,Beatrice Alenljung,Jessica Lindblom,Thanh Bui,Emmanuel Dean,Lennart Svensson,Kristofer Bengtsson,Per-Lage Götvall,Knut Åkesson*

Main category: cs.RO

TL;DR: 本文全面综述了基于基础设施的自主移动机器人系统，提出了参考架构，并在重型车辆制造环境中进行了实际部署验证。


<details>
  <summary>Details</summary>
Motivation: 目前自主移动机器人在内部物流中的应用主要依赖分散的机载智能，而利用基础设施（外部传感器和计算资源）支持AMR的系统在文献中尚未得到充分探索，特别是在复杂工业环境中需要更可扩展、鲁棒且人机兼容的解决方案。

Method: 提出结合基础设施感知、本地云计算和机载自主性的参考架构，基于该架构综述定位、感知和规划等核心技术，并在重型车辆制造环境中进行实际部署和用户体验评估。

Result: 建立了基于基础设施的AMR系统参考架构，在实际工业环境中验证了该方法的可行性，并通过用户体验评估总结了相关发现，为未来可扩展、鲁棒且人机兼容的AMR系统开发提供了基础。

Conclusion: 基于基础设施的AMR系统为复杂工业环境提供了有前景的解决方案，本文提出的参考架构和实际部署经验为未来开发可扩展、鲁棒且人机兼容的AMR系统提供了全面的基础。

Abstract: The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.

</details>


### [8] [VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/abs/2512.15258)
*Yuze Wu,Mo Zhu,Xingxing Li,Yuheng Du,Yuxin Fan,Wenjun Li,Xin Zhou,Fei Gao*

Main category: cs.RO

TL;DR: VLA-AN是一个高效的机载视觉-语言-动作框架，用于无人机在复杂环境中的自主导航，解决了现有大型空中导航模型的四个主要限制：数据域差距、时间导航推理不足、生成动作策略的安全问题以及机载部署约束。


<details>
  <summary>Details</summary>
Motivation: 现有大型空中导航模型存在四个主要问题：1) 数据域差距导致模型泛化能力差；2) 缺乏足够的时间导航推理能力；3) 生成动作策略存在安全隐患；4) 难以在资源受限的无人机上部署。需要开发一个高效、安全、可机载部署的自主导航框架。

Method: 1) 使用3D高斯泼溅构建高保真数据集以弥合域差距；2) 采用渐进式三阶段训练框架：场景理解→核心飞行技能→复杂导航能力；3) 设计轻量级实时动作模块，结合几何安全校正确保碰撞自由；4) 深度优化机载部署流程。

Result: VLA-AN在资源受限的无人机上实现了8.3倍的推理吞吐量提升，最大单任务成功率达到98.1%，显著改善了空间定位、场景推理和长时程导航能力，为轻量级空中机器人提供了高效实用的全链闭环自主解决方案。

Conclusion: VLA-AN通过创新的数据集构建、渐进式训练框架、安全动作模块和深度部署优化，成功解决了现有空中导航模型的关键限制，为无人机在复杂环境中的自主导航提供了高效、安全、实用的解决方案，推动了轻量级空中机器人的全链闭环自主化发展。

Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

</details>


### [9] [A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies](https://arxiv.org/abs/2512.15282)
*Martijn IJtsma,Salvatore Hargis*

Main category: cs.RO

TL;DR: 提出用于人机系统联合工作策略分析的计算框架，整合功能建模与图论表示，支持概念设计阶段协调动态推理


<details>
  <summary>Details</summary>
Motivation: 现有HRI框架要么强调实时执行计算支持，要么依赖静态设计表示，缺乏对早期概念设计阶段协调动态的推理支持

Method: 整合功能建模与图论表示，通过系统功能关系和工作环境物理信息结构表征集体工作，显式捕捉协调需求随时间演化

Result: 通过灾难机器人案例研究展示框架在概念设计中的应用，支持早期人机协调策略权衡探索，识别支持协调开销灵活管理的合作能力

Conclusion: 框架使协调需求及其时间演化显式化，支持在实施前对合作能力需求和工作需求进行设计时推理

Abstract: Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.

</details>


### [10] [GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments](https://arxiv.org/abs/2512.15309)
*Kai Zhang,Shoubin Chen,Dong Li,Baiyang Zhang,Tao Huang,Zehao Wu,Jiasheng Chen,Bo Zhang*

Main category: cs.RO

TL;DR: 本文介绍了GuangMing-Explorer，一个完全集成的自主探索平台，旨在在各种环境中实现稳健操作，包括硬件设计、软件堆栈、算法部署和实验配置的完整系统架构。


<details>
  <summary>Details</summary>
Motivation: 自主探索是紧密集成感知、规划、控制和运动执行的基本能力，在室内目标搜索、极端环境测绘、资源勘探等广泛应用中发挥关键作用。尽管各个组件取得了显著进展，但涵盖硬件和软件的完全自主探索系统的整体实用描述仍然稀缺。

Method: 提出了GuangMing-Explorer平台，提供了完整的系统架构概述，包括硬件设计、软件堆栈、算法部署和实验配置，实现了完全集成的自主探索系统。

Result: 通过大量真实世界实验证明了该平台在执行自主探索任务中的有效性和效率，突显了其在复杂和非结构化环境中实际部署的潜力。

Conclusion: GuangMing-Explorer是一个完全集成的自主探索平台，为复杂和非结构化环境中的实际应用提供了完整的硬件和软件解决方案，展示了自主探索系统在实际部署中的可行性和有效性。

Abstract: Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.

</details>


### [11] [Remotely Detectable Robot Policy Watermarking](https://arxiv.org/abs/2512.15379)
*Michael Amir,Manon Flageat,Amanda Prorok*

Main category: cs.RO

TL;DR: 该论文提出了首个用于机器人策略远程检测的水印方法CoNoCo，通过利用策略的固有随机性在机器人运动中嵌入频谱信号，解决了物理观察差距问题，无需访问机器人内部状态即可从外部观测中检测水印。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在机器人系统中的成功应用，训练好的策略成为一种新型知识产权，需要验证所有权并检测未经授权的使用。现有水印方法需要访问机器人内部状态，但审计者通常只能通过外部观测（如视频）进行检测，存在"物理观察差距"。

Method: 提出了Colored Noise Coherency (CoNoCo)水印策略，通过利用策略的固有随机性在机器人运动中嵌入频谱信号。该方法使用"一瞥序列"概念形式化远程检测挑战，并证明CoNoCo保持了边缘动作分布，不会降低性能。

Result: 实验表明CoNoCo在各种远程模态中都能实现强大、稳健的检测，包括动作捕捉和侧视/俯视视频，在模拟和真实机器人实验中均有效。这是首个使用纯远程观测非侵入式验证物理策略来源的方法。

Conclusion: 该工作为保护机器人知识产权提供了必要步骤，首次实现了通过纯远程观测非侵入式验证物理策略来源的方法，解决了物理观察差距这一独特挑战。

Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.

</details>


### [12] [MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training](https://arxiv.org/abs/2512.15411)
*Zhenhan Yin,Xuanhan Wang,Jiahao Jiang,Kaiyuan Deng,Pengqi Chen,Shuangle Li,Chong Liu,Xing Xu,ingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: MiVLA是一种通过人机相互模仿预训练增强泛化能力的视觉语言动作模型，利用人手与机械臂的行为相似性建立行为先验，在仿真和真实机器人任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作模型（VLA）在利用丰富的人类视频和仿真机器人数据时，由于相机视角、视觉外观和形态结构的不匹配，泛化能力受限。需要克服这些限制以提升模型在真实机器人控制任务中的表现。

Method: 提出MiVLA方法，基于人机相互模仿预训练，利用运动学规则和左右手坐标系实现人类与机器人动作空间的双向对齐。模型训练时，给定人类或仿真机器人演示，能够预测一个形态的行为轨迹，并模仿另一个未见形态的行为，从而整合真实人类数据的行为保真度和仿真机器人数据的操作多样性。

Result: 在三个机器人平台（ARX、PiPer和LocoMan）的仿真和真实世界实验中，MiVLA表现出显著提升的泛化能力，在仿真任务中优于现有VLA方法（如π₀、π₀.₅和H-RDT）25%，在真实机器人控制任务中优于14%。

Conclusion: 通过人机相互模仿预训练，MiVLA成功整合了人类数据的真实行为特征和仿真数据的操作多样性，构建了一个统一的模型，显著提升了视觉语言动作模型在跨形态、跨环境任务中的泛化能力。

Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.

</details>


### [13] [Load-Based Variable Transmission Mechanism for Robotic Applications](https://arxiv.org/abs/2512.15448)
*Sinan Emre,Victor Barasuol,Matteo Villa,Claudio Semini*

Main category: cs.RO

TL;DR: 提出了一种基于负载的可变传动比机制，通过预紧弹簧和四连杆机构被动调整传动比，无需额外执行器即可根据外部扭矩需求动态改变传动比，提高机器人关节驱动性能。


<details>
  <summary>Details</summary>
Motivation: 现有可变传动系统通常需要额外的执行器进行主动控制，增加了机器人关节驱动系统的复杂性。本研究旨在开发一种能够被动响应负载变化、无需额外执行器的轻量高效传动系统，特别适用于需要动态扭矩适应的腿式机器人应用。

Method: 提出基于负载的可变传动（LBVT）机制，利用预紧弹簧和四连杆机构实现被动传动比调整。当外部扭矩超过预设阈值时，系统自动改变传动比，无需主动控制。通过仿真分析评估系统性能。

Result: 仿真结果显示，当达到预设扭矩阈值时，系统传动比可增加高达40%，有效放大关节扭矩。当施加力超过18N时触发扭矩放大效应，系统能够自主响应变化的负载条件。

Conclusion: LBVT机制为机器人应用提供了轻量、高效、自适应的传动系统解决方案，特别适用于需要动态扭矩适应的腿式机器人，通过被动调整传动比减少了系统复杂性，提高了驱动效率。

Abstract: This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.

</details>


### [14] [OMCL: Open-vocabulary Monte Carlo Localization](https://arxiv.org/abs/2512.15557)
*Evgenii Kruzhkov,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 该研究扩展了蒙特卡洛定位方法，通过引入视觉-语言特征来实现跨模态的鲁棒机器人定位，能够在不同传感器创建的地图中进行特征关联。


<details>
  <summary>Details</summary>
Motivation: 机器人定位是导航规划的重要前提，当环境地图由不同传感器创建时，需要鲁棒地将机器人测量与地图特征关联起来。传统方法在处理跨模态数据时存在局限性。

Method: 扩展蒙特卡洛定位方法，使用视觉-语言特征。这些开放词汇特征能够根据相机位姿和3D地图（由RGB-D图像或对齐点云创建）鲁棒计算视觉观测的似然度。抽象视觉-语言特征支持不同模态的观测和地图元素关联。

Result: 在Matterport3D和Replica室内场景中评估了该方法，并在SemanticKITTI室外场景中展示了泛化能力。

Conclusion: 视觉-语言特征能够实现跨模态的鲁棒机器人定位，通过自然语言描述初始化全局定位，在不同室内外场景中表现出良好的性能。

Abstract: Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.

</details>


### [15] [An Open Toolkit for Underwater Field Robotics](https://arxiv.org/abs/2512.15597)
*Giacomo Picardi,Saverio Iacoponi,Matias Carandell,Jorge Aguirregomezcorta,Mrudul Chellapurath,Joaquin del Rio,Marcello Calisti,Iacopo Aguzzi*

Main category: cs.RO

TL;DR: 开源水下机器人关节工具包：包含防水关节、控制电子设备和ROS2软件栈，旨在降低水下操作研究门槛，已在40米深度测试验证


<details>
  <summary>Details</summary>
Motivation: 水下机器人技术对海洋科学和环境监测日益重要，但当前水下操作和驱动系统开发受到高成本、专有设计和缺乏模块化研究硬件的限制。虽然开源项目已使车辆构建和控制软件民主化，但关节驱动系统（特别是需要防水、带反馈功能的执行器）仍存在巨大差距，导致研究周期长、可重复性差、难以从实验室原型过渡到现场平台。

Method: 开发了一个开源、经济高效的水下操作研究硬件和软件工具包，包括：1）深度额定水下机器人关节（URJ），具有早期泄漏检测功能；2）紧凑的控制和电源管理电子设备；3）基于ROS2的软件栈，用于传感和多模式驱动。所有CAD模型、制造文件、PCB源文件、固件和ROS2软件包都开源发布。

Result: 工具包经过广泛的实验室测试和多次现场部署，在40米深度下可靠运行，已成功应用于多种应用：3自由度水下机械臂、肌腱驱动软夹爪和欠驱动沉积物采样器。这些结果验证了工具包在真实海洋环境中的鲁棒性、多功能性和可重用性。

Conclusion: 通过提供完全开源、经过现场测试的平台，这项工作旨在降低水下操作研究的入门门槛，提高可重复性，并加速水下现场机器人技术的创新。

Abstract: Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.
  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.
  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.
  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.

</details>
