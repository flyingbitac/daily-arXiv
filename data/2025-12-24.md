<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 将ESGVI算法扩展到矩阵李群以处理方向状态估计，并引入因子处理重尾和偏斜噪声分布，在UWB定位实验中验证了改进的精度和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决状态估计中两个关键问题：1）处理包含方向组件的状态估计时尊重底层群结构；2）处理UWB定位中常见的非视距和多径效应导致的噪声分布重尾和偏斜问题。

Method: 将ESGVI算法扩展到矩阵李群上操作，同时引入能够处理重尾和偏斜噪声分布的因子，保持算法的稀疏性和无导数结构。

Result: 在包含丰富非视距测量的UWB定位实验中验证了方法的有效性，展示了改进的精度和可比较的一致性，并提供了开源Python实现。

Conclusion: 成功将ESGVI框架扩展到矩阵李群并增强了噪声处理能力，为UWB定位等应用提供了更鲁棒的估计方法，开源实现促进了更广泛的研究应用。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


### [2] [A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones](https://arxiv.org/abs/2512.19914)
*Sujan Warnakulasooriya,Andreas Willig,Xiaobing Wu*

Main category: cs.RO

TL;DR: 提出一种基于优先级的调度算法，通过计算延迟时间解决无人机集群初始编队中的碰撞问题，相比现有方法在性能和计算效率上都有提升


<details>
  <summary>Details</summary>
Motivation: 无人机集群编队应用日益广泛，但初始编队过程中存在效率低、可扩展性差的问题，特别是当潜在碰撞迫使无人机采取次优轨迹时，现有算法难以应对大规模集群

Method: 提出时间高效的优先级调度算法，根据无人机的潜在碰撞数量和到达目标位置的可能性分配优先级，利用这种层级结构为每架无人机计算适当的延迟时间，确保无碰撞路径

Result: 仿真结果显示，该算法能够为多达5000架无人机的集群生成无碰撞轨迹，在性能和计算效率上都优于基于耦合度的启发式优先级规划方法（CDH-PP）

Conclusion: 提出的优先级调度算法有效解决了大规模无人机集群初始编队中的碰撞问题，提高了编队效率和可扩展性

Abstract: Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.

</details>


### [3] [Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting](https://arxiv.org/abs/2512.20014)
*Sangoh Lee,Sangwoo Mo,Wook-Shin Han*

Main category: cs.RO

TL;DR: VAP是一种无需训练的视觉注意力提示方法，为冻结的VLA模型添加自上而下的选择性注意力，以解决个性化物体操纵问题


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在通用指令上表现良好，但在处理个性化命令（如"拿我的杯子"）时表现不佳，因为需要从视觉相似物体中识别特定实例

Method: 提出视觉注意力提示（VAP），将参考图像作为非参数化视觉记忆，通过开放词汇检测和嵌入匹配在场景中定位个性化物体，然后通过高亮物体和重写指令将其作为视觉提示注入

Result: 在模拟基准（Personalized-SIMPLER和Personalized-VLABench）和真实世界桌面基准上的实验表明，VAP在成功率和正确物体操纵方面持续优于通用策略和基于token学习的方法

Conclusion: VAP有助于弥合语义理解和实例级控制之间的差距，为冻结的VLA模型提供有效的个性化物体操纵能力

Abstract: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.

</details>


### [4] [LoLA: Long Horizon Latent Action Learning for General Robot Manipulation](https://arxiv.org/abs/2512.20166)
*Xiaofan Wang,Xingyu Gao,Jianlong Fu,Zuolei Li,Dean Fortier,Galen Mullins,Andrey Kolobov,Baining Guo*

Main category: cs.RO

TL;DR: LoLA框架通过整合长期多视角观测和机器人本体感知，实现多步推理和动作生成，在长时程机器人操作任务中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型往往忽视历史信息和连贯动作序列生成能力，难以处理长时程、语言引导的机器人操作任务

Method: 提出LoLA框架，使用视觉语言模型编码历史序列和多视角观测的上下文特征，引入状态感知潜在重表示模块，将视觉输入和语言指令转换为可执行的机器人动作空间

Result: 在仿真基准测试和真实世界任务中，LoLA显著优于现有最先进方法，特别是在长时程操作任务中表现突出

Conclusion: LoLA通过显式地将视觉语言表示在物理尺度上锚定到机器人状态，有效解决了长时程机器人操作任务中的多步推理和动作生成问题

Abstract: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable "embodiment-anchored" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.

</details>


### [5] [Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation](https://arxiv.org/abs/2512.20188)
*Teqiang Zou,Hongliang Zeng,Yuxuan Nong,Yifan Li,Kehui Liu,Haotian Yang,Xinyang Ling,Xin Li,Lianyang Ma*

Main category: cs.RO

TL;DR: DuoCore-FS提出异步快慢VLA框架，通过分离高频动作生成和慢速VLM推理，实现30Hz全身动作生成，比同步VLA模型快约3倍


<details>
  <summary>Details</summary>
Motivation: 传统VLA系统将视觉语言模型和动作专家统一在单一频率下运行，受限于大型VLM的低推理速度，限制了全身机器人操作的稳定性和实时性能

Method: 采用异步快慢框架：慢速路径进行丰富的VLM推理，快速路径进行高频动作生成；使用潜在表示缓冲区连接两个系统，存储指令语义和动作推理表示；采用全身动作分词器提供紧凑统一的动作表示；VLM和动作专家仍进行端到端联合训练

Result: 支持30亿参数VLM的同时实现30Hz全身动作块生成，比同类模型快约3倍；真实世界全身操作实验显示任务成功率提高，响应性显著增强

Conclusion: DuoCore-FS通过异步执行解决了VLA系统的频率限制问题，在保持端到端策略学习的同时显著提升了实时性能，已作为Astribot机器人平台的一部分提供给商业用户

Abstract: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.

</details>


### [6] [UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas](https://arxiv.org/abs/2512.20224)
*Qijun Qin,Ziqi Zhang,Yihan Zhong,Feng Huang,Xikun Liu,Runzhi Hu,Hang Chen,Wei Hu,Dongzhe Su,Jun Zhang,Hoi-Fung Ng,Weisong Wen*

Main category: cs.RO

TL;DR: UrbanV2X是一个用于车路协同导航的多传感器数据集，包含车辆和路边基础设施在密集城市环境中的同步数据，支持智能交通应用研究。


<details>
  <summary>Details</summary>
Motivation: 由于单车自动驾驶存在局限性，C-V2X技术通过传感器信息共享为实现完全自动驾驶提供了新途径，但目前缺乏支持复杂城市环境中车路协同导航的真实世界数据集。

Method: 在香港C-V2X测试场收集车辆和路边基础设施的多传感器数据，包括工业相机、LiDAR、4D雷达、UWB、IMU和高精度GNSS-RTK/INS导航系统，使用PTP协议进行时间同步并提供传感器标定数据。

Result: 创建了UrbanV2X数据集，包含车辆和基础设施的同步多传感器数据，并提供了多种导航算法的基准测试来评估协同数据性能，数据集已公开可用。

Conclusion: UrbanV2X填补了复杂城市环境中车路协同导航数据集的空白，为智能交通应用研究提供了全面的多传感器数据集支持。

Abstract: Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.

</details>


### [7] [KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System](https://arxiv.org/abs/2512.20299)
*Zhongyu Xia,Wenhao Chen,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.RO

TL;DR: KnowVal：通过开放世界感知与知识检索协同整合实现视觉语言推理的自动驾驶系统，结合驾驶知识图谱和基于LLM的检索机制，提升规划性能并保持价值对齐


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶方法主要依赖数据驱动学习，难以通过模仿或有限强化奖励捕捉决策背后的复杂逻辑，需要解决视觉语言推理、驾驶知识和价值对齐的整合问题

Method: 构建包含交通法规、防御性驾驶原则和道德规范的全面驾驶知识图谱；开发基于LLM的高效检索机制；创建人类偏好数据集并训练价值模型以指导可解释的价值对齐轨迹评估

Result: 在nuScenes数据集上达到最低碰撞率，在Bench2Drive基准测试中获得最先进结果，显著提升规划性能，同时保持与现有架构的兼容性

Conclusion: KnowVal通过整合开放世界感知、知识检索和价值对齐，有效解决了自动驾驶中的复杂逻辑推理问题，为高级自动驾驶系统提供了新的解决方案

Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.

</details>


### [8] [Pneumatic bladder links with wide range of motion joints for articulated inflatable robots](https://arxiv.org/abs/2512.20322)
*Katsu Uchiyama,Ryuma Niiyama*

Main category: cs.RO

TL;DR: 本文提出了一种新型充气机器人，采用多层气囊连杆和滚动接触关节，实现了大范围运动和高负载能力，并展示了在机械臂和腿部运动中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索充气机器人的各种应用是当前研究的前沿。传统充气关节运动范围有限，需要开发具有更大运动范围和更高负载能力的新型关节结构。

Method: 提出了一种由多个气囊连杆通过滚动接触关节（Hillberry关节）连接的铰接式机器人。气囊连杆采用帆布和聚氨酯片材的双层结构，既气密又形状灵活。将Hillberry关节集成到充气机器人中是一种新方法。

Result: 滚动接触关节实现了±150°的大范围运动，是传统充气关节中最大的。使用3自由度机械臂移动500g负载，2自由度和1自由度机械臂分别提升3.4kg和5kg负载。通过安装在推车上的3自由度充气腿展示了腿部运动功能。

Conclusion: 提出的充气机器人结构在运动范围、负载能力和应用多样性方面表现出色，为充气机器人在各种实际应用中的发展提供了新思路。

Abstract: Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\pm 150 ^{\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.

</details>


### [9] [FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration](https://arxiv.org/abs/2512.20355)
*Hao Wei,Peiji Wang,Qianhao Wang,Tong Qin,Fei Gao,Yulin Si*

Main category: cs.RO

TL;DR: FAR-AVIO是一种基于舒尔补的紧耦合声学-视觉-惯性里程计框架，专为水下机器人设计，通过EKF实现联合位姿-地标优化，同时保持恒定时间更新，并包含自适应传感器健康评估模块。


<details>
  <summary>Details</summary>
Motivation: 水下环境对视觉惯性里程计系统提出严峻挑战：强光衰减、海洋雪、浑浊度以及弱激励运动导致惯性可观测性下降和频繁跟踪失败。虽然紧耦合声学-视觉-惯性融合能提供准确状态估计，但基于图的优化计算量大，难以在资源受限平台上实时部署。

Method: 1. 将舒尔补公式嵌入扩展卡尔曼滤波器(EKF)，实现联合位姿-地标优化，同时通过高效边缘化地标状态保持恒定时间更新；2. 引入自适应权重调整和可靠性评估(AWARE)模块，在线评估视觉、惯性和DVL测量的可靠性并自适应调整其权重；3. 开发高效的在线标定方案，联合估计DVL-IMU外参，无需专用标定操作。

Result: 数值模拟和真实水下实验一致表明，FAR-AVIO在定位精度和计算效率方面均优于最先进的水下SLAM基线方法，能够在低功耗嵌入式平台上实现鲁棒运行。

Conclusion: FAR-AVIO框架成功解决了水下环境中视觉惯性里程计的挑战，通过舒尔补EKF实现了高精度和计算效率的平衡，自适应传感器健康评估增强了系统鲁棒性，在线标定简化了部署流程，为资源受限的水下机器人提供了实用的状态估计解决方案。

Abstract: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.

</details>


### [10] [Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing](https://arxiv.org/abs/2512.20475)
*Maulana Bisyir Azhari,Donghun Han,Je In You,Sungjun Park,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本文介绍了为阿布扎比自主赛车联盟(A2RL) x 无人机冠军联赛(DCL)开发的单目视觉自主无人机竞速系统，该系统使用卡尔曼滤波器融合视觉惯性里程计和基于YOLO的门检测器，配合感知感知规划器，在比赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 在A2RL x DCL竞赛中，无人机只能使用单摄像头和低质量惯性测量单元进行高速自主竞速，这种最小传感器配置容易导致视觉惯性里程计漂移，特别是在长时间高速飞行和激进机动时。需要解决VIO漂移问题以实现稳定可靠的自主飞行。

Method: 系统采用卡尔曼滤波器将VIO输出与基于YOLO的门检测器获得的全局位置测量进行融合，以校正VIO漂移。同时开发了感知感知规划器，生成平衡速度和保持门对感知系统可见性的轨迹。

Result: 系统在比赛中表现出色：AI大挑战赛第三名（最高速度43.2 km/h），AI直线加速赛第二名（超过59 km/h），AI多无人机赛第二名。系统展示了单目视觉自主无人机飞行的高性能。

Conclusion: 本文详细介绍了完整的系统架构，并基于比赛实验数据进行了性能分析，为构建成功的单目视觉自主无人机飞行系统提供了有价值的见解和经验。

Abstract: The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.

</details>


### [11] [LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing](https://arxiv.org/abs/2512.20591)
*Changyi Lin,Boda Huo,Mingyang Yu,Emily Ruppel,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: LightTact是一种新型视觉触觉指尖传感器，采用光学原理直接可视化接触，无需依赖表面变形，能够感知液体、半液体和超软材料的轻接触。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器大多依赖表面变形来推断接触，难以可靠感知与液体、半液体或超软材料的轻接触交互。

Method: 采用环境光阻挡光学配置，抑制非接触区域的外部光和内部照明，仅传输真实接触处产生的漫射光，从而产生高对比度原始图像。

Result: LightTact实现了精确的像素级接触分割，对材料特性、接触力、表面外观和环境光照具有鲁棒性，非接触像素保持近黑色（平均灰度值<3），接触像素保留接触表面的自然外观。

Conclusion: 该传感器能够驱动机器人执行极轻接触的操纵行为，其空间对齐的视觉触觉图像可直接由现有视觉语言模型解释，实现机器人分类中的电阻值推理。

Abstract: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.

</details>
