<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles](https://arxiv.org/abs/2602.21259)
*Ricardo B. Grando,Victor A. Kich,Alisson H. Kolling,Junior C. D. Jesus,Rodrigo S. Guerra,Paulo L. J. Drews-Jr*

Main category: cs.RO

TL;DR: 该研究提出了一种基于深度强化学习和迁移学习的混合无人空中水下车辆持续监测方法，使用统一的策略处理空中和水下环境。


<details>
  <summary>Details</summary>
Motivation: 混合无人空中水下车辆（HUAUVs）能够在空中和水下环境中操作，适用于检查、测绘、搜索和救援等应用，但由于空气和水域的不同动力学特性和约束，开发新方法面临重大挑战。

Method: 结合深度强化学习（DRL）和迁移学习，采用共享的DRL架构，使用激光雷达传感器数据（空中）和声纳数据（水下）进行训练，实现跨域适应性。

Result: 该方法展示了为两种环境使用统一策略的可行性，并在考虑环境不确定性和多个移动目标动态的情况下取得了有希望的结果。

Conclusion: 该框架为基于深度强化学习的混合空中水下车辆可扩展自主持续监测解决方案奠定了基础。

Abstract: Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles.

</details>


### [2] [Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints](https://arxiv.org/abs/2602.21266)
*Mor Levenhar,Itzik Klein*

Main category: cs.RO

TL;DR: 提出一种双分支信息辅助框架，融合等式和不等式运动约束，通过方差加权方案提升城市环境中低成本惯性导航系统的鲁棒性，无需额外传感器或硬件。


<details>
  <summary>Details</summary>
Motivation: 城市环境中卫星信号频繁被遮挡，低成本惯性传感器在长时间信号中断时误差快速累积。现有非完整约束等辅助方法采用刚性等式假设，在动态城市驾驶条件下可能被违反，限制了其鲁棒性。

Method: 提出双分支信息辅助框架，融合等式和不等式运动约束，通过方差加权方案将两种约束结合。该方法只需对现有导航滤波器进行软件修改，无需额外传感器或硬件。

Result: 在四个公开城市数据集上评估，总时长4.3小时。在完全GNSS可用时，垂直位置误差减少16.7%，高度精度提高50.1%；在GNSS不可用时，垂直漂移减少24.2%，高度精度提高20.2%。

Conclusion: 用物理驱动的有界不等式替代刚性运动等式假设，是一种实用且无成本的策略，可在不依赖额外传感器、地图数据或学习模型的情况下，提高导航系统的韧性、连续性和漂移鲁棒性。

Abstract: Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models.

</details>


### [3] [Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control](https://arxiv.org/abs/2602.21302)
*Krishna Suresh,Chris Atkeson*

Main category: cs.RO

TL;DR: 论文提出了一种任务级迭代学习控制方法，用于可变形物体的动态操作，在"飞行绳结"任务中通过少量演示和简化模型实现了快速学习


<details>
  <summary>Details</summary>
Motivation: 可变形物体（如绳子）的动态操作具有挑战性，因为它们具有无限自由度且表现出欠驱动动力学特性。传统方法需要大量演示数据或仿真，难以直接在实际硬件上学习

Method: 提出任务级迭代学习控制方法，使用单个人类演示和简化绳子模型，通过求解二次规划构建机器人和绳子的局部逆模型，将任务空间误差传播到动作更新中

Result: 在7种不同类型的绳子上测试（包括链条、乳胶手术管、编织绳和绞合绳，厚度7-25mm，密度0.013-0.5 kg/m），所有绳子在10次试验内达到100%成功率，且在不同绳子类型间转移学习仅需2-5次试验

Conclusion: 该方法能够在实际硬件上直接学习可变形物体的动态操作，无需大量演示数据或仿真，展示了在多种材料上的鲁棒性和快速学习能力

Abstract: Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io

</details>


### [4] [Unified Complementarity-Based Contact Modeling and Planning for Soft Robots](https://arxiv.org/abs/2602.21316)
*Milad Azizkhani,Yue Chen*

Main category: cs.RO

TL;DR: 本文提出了CUSP框架，一个基于互补性的统一方法，用于解决软体机器人接触建模和规划中的挑战，包括冗余约束、病态条件等问题，并通过三阶段条件化流程和运动学引导的预热策略实现了有效的接触丰富操作。


<details>
  <summary>Details</summary>
Motivation: 软体机器人旨在实现与环境的安全自适应交互，这依赖于接触。然而，软体机器人的接触丰富交互建模和规划仍然具有挑战性：身体上的密集接触候选点会产生冗余约束和秩亏线性互补问题，而高刚度和低摩擦之间的差异导致严重的病态条件。现有方法依赖于特定问题的近似或基于惩罚的处理。

Method: 提出了一个统一的基于互补性的软体机器人接触建模和规划框架。开发了针对离散化软体机器人的鲁棒线性互补问题模型，采用三阶段条件化流程：惯性秩选择去除冗余接触、Ruiz均衡校正尺度差异和病态条件、对法向块的轻量级Tikhonov正则化。基于相同公式，引入了运动学引导的预热策略，使用带互补约束的数学规划进行动态轨迹优化。

Result: CUSP框架为软体机器人提供了统一的接触建模、模拟和规划基础。在接触丰富的球体操作任务中展示了其有效性，能够处理冗余约束和病态条件问题，实现了物理一致的接触交互。

Conclusion: CUSP为软体机器人中的接触建模、模拟和规划统一提供了新的基础，解决了现有方法在接触丰富交互中的局限性，为软体机器人的实际应用提供了更可靠的理论和计算框架。

Abstract: Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics.

</details>


### [5] [Autonomous Sea Turtle Robot for Marine Fieldwork](https://arxiv.org/abs/2602.21389)
*Zach J. Patterson,Emily Sologuren,Levi Cai,Daniel Kim,Alaa Maalouf,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 研究人员开发了一款海龟仿生自主水下机器人，结合了生物启发式运动与现场就绪的自主能力，能够在复杂珊瑚礁环境中安全导航并追踪移动目标。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在海洋生态系统观测中具有变革潜力，但在珊瑚礁等复杂栖息地近距离操作面临挑战。现有仿生平台在自主部署能力方面有限，需要开发既能安全操作又能应对复杂环境的机器人系统。

Method: 开发了海龟仿生自主水下机器人，采用紧密集成的视觉驱动控制堆栈，结合深度-航向稳定控制、避障和目标中心控制。系统包括新颖的硬件设计和低计算量的机载跟踪模式。

Result: 在受控水池实验和新英格兰水族馆活珊瑚礁展区验证了系统性能：实现了稳定操作，可靠追踪快速移动的海洋动物和潜水员；在脱离线缆实验中，水族馆展区的避障成功率达到91%。

Conclusion: 这是首个结合新型硬件、控制和现场实验的集成仿生机器人系统，能够追踪和监测真实海洋动物。为软-硬混合仿生水下机器人提供了一条实用路径，使其能够在敏感生态系统中进行最小干扰的探索和近距离监测。

Abstract: Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems.

</details>


### [6] [Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups](https://arxiv.org/abs/2602.21450)
*Felipe Bartelt,Vinicius M. Gonçalves,Luciano C. A. Pimenta*

Main category: cs.RO

TL;DR: 本文提出了一种用于控制连通矩阵李群上完全驱动系统的新颖向量场策略，确保系统收敛到并沿着群上定义的曲线运动。该方法推广了先前工作，并在SE(3)群上具有特别应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有向量场控制方法主要针对欧几里得空间中的平移群，需要扩展到更一般的李群结构，特别是SE(3)群，以控制能够自由移动和旋转的物体（如全向无人机）。同时希望控制输入能够非冗余，与李群维度匹配而非嵌入空间的更大维度。

Method: 提出基于李群性质的向量场控制策略，利用李群特性扩展先前证明结果。特别针对SE(3)群提供了高效计算向量场的算法。控制输入维度与李群维度匹配，避免了冗余输入。

Result: 成功将向量场控制方法推广到连通矩阵李群，在SE(3)群上实现了非冗余控制输入（对应物体的机械扭转）。通过机器人机械臂实验验证了方法的有效性。

Conclusion: 该方法为李群上的系统控制提供了通用框架，特别适用于SE(3)群上的物体控制，如全向无人机。非冗余控制输入使其在实际应用中更具实用性。

Abstract: This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness.

</details>


### [7] [LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies](https://arxiv.org/abs/2602.21531)
*Yue Yang,Shuo Cheng,Yu Fang,Homanga Bharadhwaj,Mingyu Ding,Gedas Bertasius,Daniel Szafir*

Main category: cs.RO

TL;DR: LiLo-VLA是一个模块化框架，通过将运输与交互解耦来解决长时程操作任务，实现了对新任务的零样本泛化，在仿真和真实世界中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要掌握涉及多个运动结构变化的长时程操作任务。虽然视觉-语言-动作模型能掌握多样原子技能，但在组合这些技能时面临组合复杂性，且容易因环境敏感性导致级联失败。

Method: 提出LiLo-VLA模块化框架，将运输与交互解耦：Reaching Module处理全局运动，Interaction Module使用以对象为中心的VLA处理孤立的目标对象，确保对无关视觉特征的鲁棒性和空间配置的不变性。模块化设计支持动态重规划和技能重用，有效缓解端到端方法中的级联错误。

Result: 在包含LIBERO-Long++和Ultra-Long两个挑战性套件的21任务仿真基准测试中，LiLo-VLA平均成功率达到69%，比Pi0.5高41%，比OpenVLA-OFT高67%。在8个真实世界长时程任务评估中，平均成功率达到85%。

Conclusion: LiLo-VLA通过模块化设计成功解决了长时程操作任务的挑战，实现了对新任务的零样本泛化，在仿真和真实环境中均显著优于现有方法，展示了其在实际机器人应用中的潜力。

Abstract: General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.

</details>


### [8] [Learning Agile and Robust Omnidirectional Aerial Motion on Overactuated Tiltable-Quadrotors](https://arxiv.org/abs/2602.21583)
*Wentao Zhang,Zhaoqi Ma,Jinjie Li,Huayi Wang,Haokun Liu,Junichiro Sugihara,Chen Chen,Yicheng Chen,Moju Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的倾斜旋翼无人机全向运动控制框架，通过系统辨识和物理一致的域随机化实现可靠的仿真到现实迁移，相比传统NMPC控制器在保持精度的同时显著提升了鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 倾斜旋翼无人机通过推力矢量实现全向机动，但关节和旋翼动力学的强耦合带来了显著的控制挑战。基于模型的控制器在标称条件下可实现高精度运动，但在干扰和建模不确定性存在时，其鲁棒性和响应性会下降。

Method: 提出了一个学习型控制框架，通过强化学习高效获取协调的旋翼-关节行为以实现SE(3)空间中的目标姿态跟踪。采用系统辨识结合最小化和物理一致的域随机化方法，确保可靠的仿真到现实迁移。

Result: 与最先进的NMPC控制器相比，该方法实现了相当的六自由度姿态跟踪精度，同时在各种任务中表现出更优的鲁棒性和泛化能力，能够在真实硬件上实现零样本部署。

Conclusion: 强化学习方法能够有效解决倾斜旋翼无人机控制中的耦合动力学挑战，通过结合系统辨识和域随机化技术，实现了高精度、高鲁棒性的全向运动控制，为复杂无人机系统的控制提供了新思路。

Abstract: Tilt-rotor aerial robots enable omnidirectional maneuvering through thrust vectoring, but introduce significant control challenges due to the strong coupling between joint and rotor dynamics. While model-based controllers can achieve high motion accuracy under nominal conditions, their robustness and responsiveness often degrade in the presence of disturbances and modeling uncertainties. This work investigates reinforcement learning for omnidirectional aerial motion control on over-actuated tiltable quadrotors that prioritizes robustness and agility. We present a learning-based control framework that enables efficient acquisition of coordinated rotor-joint behaviors for reaching target poses in the $SE(3)$ space. To achieve reliable sim-to-real transfer while preserving motion accuracy, we integrate system identification with minimal and physically consistent domain randomization. Compared with a state-of-the-art NMPC controller, the proposed method achieves comparable six-degree-of-freedom pose tracking accuracy, while demonstrating superior robustness and generalization across diverse tasks, enabling zero-shot deployment on real hardware.

</details>


### [9] [SPOC: Safety-Aware Planning Under Partial Observability And Physical Constraints](https://arxiv.org/abs/2602.21595)
*Hyungmin Kim,Hobeom Jeon,Dohyung Kim,Minsu Jang,Jeahong Kim*

Main category: cs.RO

TL;DR: SPOC是一个安全感知的具身任务规划基准，强调部分可观测性和物理约束下的安全性评估，覆盖多种家庭危险场景。


<details>
  <summary>Details</summary>
Motivation: 现有具身任务规划基准往往忽视现实环境中的安全挑战，特别是部分可观测性和物理约束，这限制了评估规划可行性和安全性的能力。

Method: 引入SPOC基准，整合严格的部分可观测性、物理约束、逐步规划和基于目标条件的评估，覆盖火灾、液体、伤害、物体损坏和污染等家庭危险。

Result: 实验表明，当前最先进的LLM在安全感知规划方面表现不佳，特别是在隐式约束下难以确保安全性。

Conclusion: SPOC基准填补了具身任务规划中安全性评估的空白，揭示了现有模型在安全约束下的局限性，为未来研究提供了重要工具。

Abstract: Embodied Task Planning with large language models faces safety challenges in real-world environments, where partial observability and physical constraints must be respected. Existing benchmarks often overlook these critical factors, limiting their ability to evaluate both feasibility and safety. We introduce SPOC, a benchmark for safety-aware embodied task planning, which integrates strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation. Covering diverse household hazards such as fire, fluid, injury, object damage, and pollution, SPOC enables rigorous assessment through both state and constraint-based online metrics. Experiments with state-of-the-art LLMs reveal that current models struggle to ensure safety-aware planning, particularly under implicit constraints. Code and dataset are available at https://github.com/khm159/SPOC

</details>


### [10] [Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control](https://arxiv.org/abs/2602.21599)
*Weisheng Xu,Qiwei Wu,Jiaxi Zhang,Tan Jing,Yangfan Li,Yuetong Fang,Jiaqi Xiong,Kai Wu,Rong Ou,Renjing Xu*

Main category: cs.RO

TL;DR: 提出闭环自动运动数据生成与迭代框架，通过物理指标和客观评估实现策略与数据难度迭代，仅用AMASS数据集约1/10规模将测试集平均失败率降低45%


<details>
  <summary>Details</summary>
Motivation: 基于物理的人形控制依赖多样运动数据集训练，但固定难度分布限制了策略性能上限，且专业动捕系统获取高质量数据成本高、难以大规模扩展

Method: 提出闭环自动运动数据生成与迭代框架，生成包含武术、舞蹈、战斗、体育、体操等丰富动作语义的高质量运动数据，通过物理指标和客观评估实现策略与数据难度迭代

Result: 在PHC单基元跟踪器上，仅使用AMASS数据集约1/10规模，测试集（2201个片段）平均失败率相比基线降低45%

Conclusion: 该框架能够突破原始难度限制，通过全面消融和对比实验验证了其合理性和优势

Abstract: Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.

</details>


### [11] [Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization](https://arxiv.org/abs/2602.21612)
*Xuanqi Zeng,Lingwei Zhang,Linzhu Yue,Zhitao Song,Hongbo Zhang,Tianlin Zhang,Yun-Hui Liu*

Main category: cs.RO

TL;DR: 提出了一种用于四足轮腿机器人的新型运动控制框架，结合NMPC进行运动控制和DE算法进行跳跃轨迹优化，实现了垂直跳跃、前向跳跃和后空翻等敏捷动作。


<details>
  <summary>Details</summary>
Motivation: 四足轮腿机器人结合了腿式和轮式运动的优势，具有卓越的机动性，但由于轮腿引入了额外的自由度，执行动态跳跃仍然是一个重大挑战。

Method: 开发了一种小型轮腿机器人，并提出了一种新颖的运动控制框架，该框架集成了非线性模型预测控制（NMPC）用于运动控制，以及基于差分进化（DE）的轨迹优化用于跳跃控制。

Result: 通过广泛的仿真和真实世界实验验证了该框架的有效性，实现了越过0.12米障碍物的前向跳跃和达到0.5米高度的垂直跳跃。

Conclusion: 该控制器利用轮子运动和运动控制来增强跳跃性能，实现了垂直跳跃、前向跳跃和后空翻等多种敏捷动作，为轮腿机器人的动态跳跃控制提供了有效解决方案。

Abstract: Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.

</details>


### [12] [ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation](https://arxiv.org/abs/2602.21622)
*Enyi Wang,Wen Fan,Dandan Zhang*

Main category: cs.RO

TL;DR: ADM-DP是一个多智能体机器人操作框架，通过融合视觉、触觉和图结构模态，采用自适应注意力机制实现协调控制，在7个多智能体任务中性能提升12-25%。


<details>
  <summary>Details</summary>
Motivation: 多智能体机器人操作面临协调、抓取稳定性和共享工作空间碰撞避免的挑战，需要整合多种感知模态并实现自适应融合。

Method: 提出ADM-DP框架：1) 增强视觉编码器通过FiLM融合RGB和点云特征；2) 触觉引导抓取策略使用FSR反馈检测接触不足并触发修正；3) 基于图的碰撞编码器利用多智能体TCP位置作为结构化运动学上下文；4) 自适应模态注意力机制动态调整模态权重；采用解耦训练范式。

Result: 在7个多智能体任务中，ADM-DP相比最先进基线获得12-25%的性能提升。消融研究表明在需要多种感知模态的任务中改进最大，验证了自适应融合策略的有效性。

Conclusion: ADM-DP通过多模态融合和自适应注意力机制，有效解决了多智能体机器人操作的协调、抓取稳定性和碰撞避免问题，展示了在多样化操作场景中的鲁棒性。

Abstract: Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios.

</details>


### [13] [Self-Correcting VLA: Online Action Refinement via Sparse World Imagination](https://arxiv.org/abs/2602.21633)
*Chenyv Liu,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: SC-VLA通过稀疏世界想象和在线动作精炼实现自我改进，在机器人操作任务中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖统计数据先验，缺乏对物理动态的鲁棒理解；强化学习依赖外部奖励信号，与世界模型隔离；世界动作模型缺乏显式自我改进机制

Method: 设计稀疏世界想象模块，集成辅助预测头来预测当前任务进度和未来轨迹趋势；引入在线动作精炼模块，基于预测的稀疏未来状态调整轨迹方向

Result: 在仿真基准和真实世界机器人操作任务中达到最先进性能，比最佳基线减少16%步骤，成功率提高9%，真实世界实验提升14%

Conclusion: SC-VLA通过稀疏想象和内在指导的动作精炼实现了自我改进，显著提升了机器人操作的效率和成功率

Abstract: Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.

</details>


### [14] [DAGS-SLAM: Dynamic-Aware 3DGS SLAM via Spatiotemporal Motion Probability and Uncertainty-Aware Scheduling](https://arxiv.org/abs/2602.21644)
*Li Zhang,Yu-An Liu,Xijia Jiang,Conghao Huang,Danyang Li,Yanyong Zhang*

Main category: cs.RO

TL;DR: DAGS-SLAM：一种动态感知的3D高斯泼溅SLAM系统，通过时空运动概率状态和按需语义调度，在保持实时性能的同时提升动态环境下的重建和跟踪鲁棒性


<details>
  <summary>Details</summary>
Motivation: 移动机器人和物联网设备需要在有限的计算和能耗预算下实现实时定位和密集重建。虽然3D高斯泼溅（3DGS）能够实现高效的密集SLAM，但动态物体和遮挡仍然会降低跟踪和建图质量。现有的动态3DGS-SLAM方法通常依赖计算量大的光流和逐帧分割，这对于移动部署来说成本过高，且在挑战性光照条件下表现脆弱。

Method: DAGS-SLAM为每个高斯泼溅点维护一个时空运动概率（MP）状态，并通过不确定性感知调度器按需触发语义处理。系统融合轻量级YOLO实例先验与几何线索来估计和更新MP，将MP传播到前端用于动态感知的对应点选择，并通过MP引导的优化在后端抑制动态伪影。

Result: 在公开的动态RGB-D基准测试中，DAGS-SLAM显示出改进的重建质量和鲁棒的跟踪性能，同时在商用GPU上保持实时吞吐量，实现了实用的速度-精度权衡，并减少了语义调用的次数，更适合移动部署。

Conclusion: DAGS-SLAM通过创新的运动概率状态管理和按需语义调度机制，为动态环境下的3D高斯泼溅SLAM提供了一种高效实用的解决方案，在保持实时性能的同时显著提升了系统的鲁棒性和重建质量，为移动机器人等资源受限设备的部署提供了可行的技术路径。

Abstract: Mobile robots and IoT devices demand real-time localization and dense reconstruction under tight compute and energy budgets. While 3D Gaussian Splatting (3DGS) enables efficient dense SLAM, dynamic objects and occlusions still degrade tracking and mapping. Existing dynamic 3DGS-SLAM often relies on heavy optical flow and per-frame segmentation, which is costly for mobile deployment and brittle under challenging illumination. We present DAGS-SLAM, a dynamic-aware 3DGS-SLAM system that maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. DAGS-SLAM fuses lightweight YOLO instance priors with geometric cues to estimate and temporally update MP, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization. Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU, demonstrating a practical speed-accuracy tradeoff with reduced semantic invocations toward mobile deployment.

</details>


### [15] [Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits](https://arxiv.org/abs/2602.21666)
*Luying Feng,Yaochu Jin,Hanze Hu,Wei Chen*

Main category: cs.RO

TL;DR: 提出GDAF框架系统量化人形机器人与人类步态差异，发现现代控制器虽能生成视觉上类人运动，但在步态对称性、能量分布和关节协调方面仍存在显著生物力学差异。


<details>
  <summary>Details</summary>
Motivation: 由于生物结构与机械结构存在根本差异，实现类人步态对腿式机器人仍具挑战。虽然模仿学习是生成自然机器人运动的有效方法，但单纯复制关节角度轨迹无法捕捉人类运动的底层原理。

Method: 提出步态差异分析框架(GDAF)，这是一个统一的生物力学评估框架，系统量化人类与双足机器人之间的运动学和动力学差异。在28种步行速度下系统比较人类和人形机器人步态，收集并发布速度连续的人形机器人步态数据集，并提供GDAF开源实现。

Result: 结果显示，尽管现代人形控制器能生成视觉上类人的运动，但在不同速度下仍存在显著的生物力学差异。机器人表现出步态对称性、能量分布和关节协调方面的系统性偏差，表明人形机器人步态的生物力学保真度和能量效率仍有很大改进空间。

Conclusion: 该工作为评估人形机器人步态提供了量化基准，并提供数据和多功能工具支持开发更类人且能量效率更高的步态控制器。论文接受后将公开数据和代码。

Abstract: It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.

</details>


### [16] [Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning](https://arxiv.org/abs/2602.21670)
*Tomoya Kawabe,Rin Takano*

Main category: cs.RO

TL;DR: 提出了一种分层多智能体LLM规划器，结合提示优化，用于多机器人任务规划，在MAT-THOR基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统PDDL规划器虽然提供严格保证，但难以处理模糊或长时程任务；而大语言模型能解释指令但可能产生幻觉或不可行动作。需要结合两者优势来解决多机器人任务规划问题。

Method: 采用分层多智能体LLM架构：上层分解任务并分配给下层智能体，下层生成PDDL问题由经典规划器求解。当规划失败时，使用TextGrad启发的文本梯度更新优化每个智能体的提示。此外，在同一层内的智能体间学习和共享元提示，实现高效的多智能体提示优化。

Result: 在MAT-THOR基准测试中，复合任务成功率0.95，复杂任务0.84，模糊任务0.60，分别比之前的SOTA方法LaMMA-P提高了2、7和15个百分点。消融研究表明，分层结构、提示优化和元提示共享分别贡献约+59、+37和+4个百分点的总体成功率提升。

Conclusion: 提出的分层多智能体LLM规划器结合提示优化，有效解决了多机器人任务规划问题，显著提高了规划成功率，特别是在处理复杂和模糊任务方面表现出色。

Abstract: Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate.

</details>


### [17] [SunnyParking: Multi-Shot Trajectory Generation and Motion State Awareness for Human-like Parking](https://arxiv.org/abs/2602.21682)
*Jishu Miao,Han Chen,Jiankun Zhai,Qi Liu,Tsubasa Hirakawa,Takayoshi Yamashita,Hironobu Fujiyoshi*

Main category: cs.RO

TL;DR: SunnyParking是一个新颖的双分支端到端架构，通过联合预测空间轨迹和离散运动状态序列（如前向/倒车）实现运动状态感知，解决传统方法在自主泊车中的物理不可行轨迹问题。


<details>
  <summary>Details</summary>
Motivation: 现有端到端规划方法将泊车任务简化为几何路径回归问题，忽略了车辆运动状态的显式建模，导致"维度不足"问题，容易产生物理不可行的轨迹，与真实人类驾驶行为不符，特别是在多段泊车场景的关键换挡点。

Method: 提出SunnyParking双分支架构：1）联合预测空间轨迹和离散运动状态序列（如前向/倒车）；2）引入基于傅里叶特征的目标停车位表示，克服传统鸟瞰图方法的分辨率限制，实现高精度目标交互。

Result: 实验结果表明，该框架在复杂多段泊车场景中生成更鲁棒、更接近人类驾驶的轨迹，同时相比最先进方法显著提高了换挡点定位精度。开源了专门设计的CARLA模拟器泊车数据集。

Conclusion: SunnyParking通过运动状态感知和傅里叶特征表示，有效解决了自主泊车中的物理可行性和换挡点精度问题，为复杂泊车场景提供了更优的端到端解决方案。

Abstract: Autonomous parking fundamentally differs from on-road driving due to its frequent direction changes and complex maneuvering requirements. However, existing End-to-End (E2E) planning methods often simplify the parking task into a geometric path regression problem, neglecting explicit modeling of the vehicle's kinematic state. This "dimensionality deficiency" easily leads to physically infeasible trajectories and deviates from real human driving behavior, particularly at critical gear-shift points in multi-shot parking scenarios. In this paper, we propose SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (e.g., forward/reverse). Additionally, we introduce a Fourier feature-based representation of target parking slots to overcome the resolution limitations of traditional bird's-eye view (BEV) approaches, enabling high-precision target interactions. Experimental results demonstrate that our framework generates more robust and human-like trajectories in complex multi-shot parking scenarios, while significantly improving gear-shift point localization accuracy compared to state-of-the-art methods. We open-source a new parking dataset of the CARLA simulator, specifically designed to evaluate full prediction capabilities under complex maneuvers.

</details>


### [18] [Primary-Fine Decoupling for Action Generation in Robotic Imitation](https://arxiv.org/abs/2602.21684)
*Xiaohan Lei,Min Wang,Wengang Zhou,Xingyu Lu,Houqiang Li*

Main category: cs.RO

TL;DR: PF-DAG：两阶段动作生成框架，通过粗粒度模式选择和细粒度动作生成解决机器人模仿学习中多模态分布问题，在56个任务上超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 机器人操作动作序列中的多模态分布给模仿学习带来挑战。现有方法要么将动作离散化为token丢失细粒度变化，要么单阶段生成连续动作导致模式转换不稳定。需要一种既能保持粗粒度一致性又能生成高保真细粒度动作的方法。

Method: 提出Primary-Fine Decoupling for Action Generation (PF-DAG)两阶段框架：1）将动作块压缩为少量离散模式，轻量级策略选择一致的粗粒度模式避免模式跳跃；2）学习模式条件MeanFlow策略生成高保真连续动作。理论证明两阶段设计比单阶段生成策略有更低的MSE上界。

Result: 在Adroit、DexArt和MetaWorld基准的56个任务上超越最先进基线方法，并进一步推广到真实世界的触觉灵巧操作任务。

Conclusion: 显式的模式级解耦能够同时实现鲁棒的多模态建模和机器人操作的响应式闭环控制，为机器人模仿学习提供了有效的两阶段动作生成框架。

Abstract: Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.

</details>


### [19] [Trajectory Generation with Endpoint Regulation and Momentum-Aware Dynamics for Visually Impaired Scenarios](https://arxiv.org/abs/2602.21691)
*Yuting Zeng,Manping Fan,You Zhou,Yongbin Yu,Zhiwen Zheng,Jingtao Zhang,Liyong Ren,Zhenglin Yang*

Main category: cs.RO

TL;DR: 提出一种用于视障场景的轨迹生成方法，通过端点调节和动量感知动力学来改善轨迹平滑性和一致性，减少加速度峰值和急动度。


<details>
  <summary>Details</summary>
Motivation: 传统基于急动度的启发式轨迹采样方法在结构化、低速动态环境中存在终端行为不稳定和状态不连续的问题，特别是在频繁重新生成轨迹时。

Method: 提出一种轨迹生成方法，集成了端点调节来稳定每个轨迹段的终端状态，以及动量感知动力学来正则化速度和加速度的演化，确保段间一致性。

Result: 实验结果显示：降低了加速度峰值和急动度水平，减少了离散度；获得了更平滑的速度和加速度曲线；端点分布更稳定；不可行轨迹候选数量减少。

Conclusion: 该方法通过端点调节和动量感知动力学有效改善了视障场景下轨迹生成的平滑性、一致性和稳定性，优于基线规划器。

Abstract: Trajectory generation for visually impaired scenarios requires smooth and temporally consistent state in structured, low-speed dynamic environments. However, traditional jerk-based heuristic trajectory sampling with independent segment generation and conventional smoothness penalties often lead to unstable terminal behavior and state discontinuities under frequent regenerating. This paper proposes a trajectory generation approach that integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize the evolution of velocity and acceleration for segment consistency. Endpoint regulation is incorporated into trajectory sampling to stabilize terminal behavior, while a momentum-aware dynamics enforces consistent velocity and acceleration evolution across consecutive trajectory segments. Experimental results demonstrate reduced acceleration peaks and lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared with a baseline planner.

</details>


### [20] [LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations](https://arxiv.org/abs/2602.21723)
*Yutang Lin,Jieming Cui,Yixuan Li,Baoxiong Jia,Yixin Zhu,Siyuan Huang*

Main category: cs.RO

TL;DR: LessMimic提出了一种基于距离场（DF）的统一交互表示方法，使人形机器人无需参考运动或任务特定奖励，就能实现几何泛化、长时程技能组合和视觉部署


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖参考运动或任务特定奖励，将策略与特定物体几何紧密耦合，阻碍了多技能泛化。需要一种统一的交互表示，能在单一策略中实现无参考推理、几何泛化和长时程技能组合

Method: 使用距离场（DF）作为几何线索（表面距离、梯度、速度分解）来调节全身策略，无需运动参考。通过变分自编码器（VAE）编码交互潜在变量，在强化学习（RL）下使用对抗交互先验（AIP）进行后训练。通过DAgger式蒸馏将DF潜在变量与自中心深度特征对齐，实现纯视觉部署

Result: 单一LessMimic策略在0.4x到1.6x物体尺度范围内PickUp和SitStand任务上达到80-100%成功率（基线方法急剧下降），在5个任务实例轨迹上达到62.1%成功率，在40个顺序组合任务中保持可行性

Conclusion: 通过将交互建立在局部几何而非演示上，LessMimic为人形机器人在非结构化环境中实现泛化、技能组合和故障恢复提供了可扩展的路径

Abstract: Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments.

</details>


### [21] [Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild](https://arxiv.org/abs/2602.21736)
*Hao Luo,Ye Wang,Wanpeng Zhang,Haoqi Yuan,Yicheng Feng,Haiweng Xu,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: JALA框架通过联合对齐潜在动作，利用混合人类视频数据（实验室+野外）进行VLA预训练，无需完整视觉动态重建，显著提升机器人操作性能


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型受限于大规模多样化机器人数据的稀缺性，现有方法必须在小型精确标注数据集和大量野外视频（手部追踪标签不可靠）之间做出选择

Method: 提出JALA预训练框架，学习联合对齐潜在动作，绕过完整视觉动态重建，学习与逆动力学和真实动作对齐的预测性动作嵌入，创建过渡感知、行为中心的潜在空间

Result: JALA在受控和非受控场景中生成更真实的手部动作，在仿真和现实世界任务中显著提升下游机器人操作性能

Conclusion: 联合对齐潜在动作为从人类数据中进行VLA预训练提供了可扩展的途径

Abstract: Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.

</details>


### [22] [Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control](https://arxiv.org/abs/2602.21783)
*Beatrice Luciani,Alex van den Berg,Matti Lang,Alexandre L. Ratschat,Laura Marchal-Crespo*

Main category: cs.RO

TL;DR: 研究人员开发了一种触觉远程操作系统，让训练师可以通过手持触觉设备远程引导佩戴臂部外骨骼的受训者，相比传统视觉演示，该系统显著减少了运动完成时间、提高了平滑度，并减少了口头指令需求。


<details>
  <summary>Details</summary>
Motivation: 机器人系统虽然能增强物理引导运动训练的数量和可重复性，但在实际应用中受到限制，部分原因是训练师/治疗师与受训者/患者之间的交互不够直观。为了解决这一问题，研究人员希望开发一种更直观的远程交互系统。

Method: 开发了一种触觉远程操作系统，训练师可以通过商用手持触觉设备与受训者佩戴的臂部外骨骼进行物理交互。系统通过外骨骼肘部和腕部的虚拟接触点实现直观引导。32名参与者以训练师-受训者模式测试了该系统，将触觉演示与传统视觉演示进行比较，评估引导受训者执行臂部姿势的效果。

Result: 定量分析显示，触觉演示显著减少了运动完成时间并提高了平滑度。使用大型语言模型进行语音分析，自动转录和分类口头指令，发现触觉演示减少了口头指令需求。与视觉演示相比，触觉演示没有导致训练师报告更高的心理和身体负担，同时训练师报告了更高的能力感，受训者报告了更低的身体需求。

Conclusion: 研究结果支持所提出的界面在有效远程人机物理交互方面的可行性。未来工作应评估其在临床人群中的可用性和有效性，以恢复临床医生在机器人辅助治疗中的能动感。

Abstract: Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy.

</details>


### [23] [DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations](https://arxiv.org/abs/2602.21811)
*Qingtao Liu,Zhengnan Sun,Yu Cui,Haoming Li,Gaofeng Li,Lin Shao,Jiming Chen,Qi Ye*

Main category: cs.RO

TL;DR: DexRep是一种新颖的手-物体交互表示方法，通过捕捉物体表面特征和手-物体空间关系来提升灵巧操作技能学习，在抓取、重定向和双手交接任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法主要关注高维动作空间的样本效率，但忽视了表示学习在复杂手-物体交互输入空间中对策略泛化能力的重要作用。

Method: 提出DexRep表示方法，专门设计用于捕捉物体表面特征和手-物体之间的空间关系，基于此表示学习灵巧操作策略。

Result: 在抓取任务中，使用40个物体训练的策略在5000多个未见物体上达到87.9%成功率；在重定向和交接任务中，相比现有表示方法提升20-40%成功率；真实世界部署显示较小的仿真到现实差距。

Conclusion: DexRep表示方法能有效提升灵巧操作策略的泛化能力，在多种任务中显著超越现有方法，且具有良好的仿真到现实迁移性能。

Abstract: Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.

</details>


### [24] [Self-Curriculum Model-based Reinforcement Learning for Shape Control of Deformable Linear Objects](https://arxiv.org/abs/2602.21816)
*Zhaowei Liang,Song Wang,Zhao Jin,Shirui Wu,Dan Wu*

Main category: cs.RO

TL;DR: 提出结合强化学习和视觉伺服的DLOS形状控制两阶段框架，显著提升复杂大变形任务的处理效率和精度


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂大变形任务（特别是涉及相反曲率）时面临挑战，缺乏效率和精度，需要更有效的DLOS形状控制方案

Method: 两阶段框架：1）大变形阶段采用基于模型的强化学习，使用动态模型集成提高样本效率，设计自课程目标生成机制；2）小变形阶段采用基于雅可比矩阵的视觉伺服控制器确保高精度收敛

Result: 仿真结果显示方法显著优于主流基线，成功率达到100%（30个案例），能够零样本适应从仿真到真实世界的策略迁移，适用于不同尺寸和材料的DLOS

Conclusion: 提出的两阶段框架有效解决了DLOS复杂形状控制问题，实现了高效策略学习和精确控制，具有实际应用价值

Abstract: Precise shape control of Deformable Linear Objects (DLOs) is crucial in robotic applications such as industrial and medical fields. However, existing methods face challenges in handling complex large deformation tasks, especially those involving opposite curvatures, and lack efficiency and precision. To address this, we propose a two-stage framework combining Reinforcement Learning (RL) and online visual servoing. In the large-deformation stage, a model-based reinforcement learning approach using an ensemble of dynamics models is introduced to significantly improve sample efficiency. Additionally, we design a self-curriculum goal generation mechanism that dynamically selects intermediate-difficulty goals with high diversity through imagined evaluations, thereby optimizing the policy learning process. In the small-deformation stage, a Jacobian-based visual servo controller is deployed to ensure high-precision convergence. Simulation results show that the proposed method enables efficient policy learning and significantly outperforms mainstream baselines in shape control success rate and precision. Furthermore, the framework effectively transfers the policy trained in simulation to real-world tasks with zero-shot adaptation. It successfully completes all 30 cases with diverse initial and target shapes across DLOs of different sizes and materials. The project website is available at: https://anonymous.4open.science/w/sc-mbrl-dlo-EB48/

</details>


### [25] [Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments](https://arxiv.org/abs/2602.21967)
*Xiangqi Meng,Pengxu Hou,Zhenjun Zhao,Javier Civera,Daniel Cremers,Hesheng Wang,Haoang Li*

Main category: cs.RO

TL;DR: Dream-SLAM：一种基于跨时空图像生成和语义结构预测的单目主动SLAM方法，通过融合生成图像与真实观测提升定位和建图精度，实现长远规划


<details>
  <summary>Details</summary>
Motivation: 现有主动SLAM方法存在三个主要限制：1）受限于底层SLAM模块的限制；2）运动规划策略短视，缺乏长远视野；3）难以处理动态场景。需要一种能够克服这些限制的新方法。

Method: 提出Dream-SLAM方法，基于跨时空图像生成和语义合理的动态环境结构预测。将生成的跨时空图像与真实观测融合以减少噪声和数据不完整性，从而提高相机姿态估计精度和3D场景表示一致性。同时整合生成和观测的场景结构，实现长远规划，产生具有远见的轨迹以促进高效全面探索。

Result: 在公开和自收集数据集上的大量实验表明，Dream-SLAM在定位精度、建图质量和探索效率方面均优于现有最先进方法。

Conclusion: Dream-SLAM通过跨时空图像生成和语义结构预测，有效解决了传统主动SLAM方法的限制，实现了更准确的定位、更高质量的建图和更高效的探索。

Abstract: In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance.

</details>


### [26] [Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots](https://arxiv.org/abs/2602.21983)
*Jingchao Wei,Jingkai Qin,Yuxiao Cao,Jingcheng Huang,Xiangrui Zeng,Min Li,Zhouping Yin*

Main category: cs.RO

TL;DR: 提出了一个机器人注视转移框架，通过视觉语言模型推理注视目标，结合条件VQ-VAE生成眼头协调的自然注视转移动作


<details>
  <summary>Details</summary>
Motivation: 在无约束的人机交互中，使人形机器人能够执行自然且符合情境的注视转移具有挑战性，需要将认知注意机制与仿生运动生成相结合

Method: 提出机器人注视转移框架，包含两个组件：1) 基于视觉语言模型的注视推理管道，从多模态交互线索推断情境适当的注视目标；2) 条件向量量化变分自编码器模型，用于眼头协调的注视转移运动生成

Result: 实验验证RGS能够有效复制类人的目标选择，并生成真实、多样化的注视转移动作

Conclusion: 该框架成功整合了认知注意机制和仿生运动生成，实现了自然的人机交互注视转移

Abstract: Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.

</details>


### [27] [Parallel Continuous-Time Relative Localization with Augmented Clamped Non-Uniform B-Splines](https://arxiv.org/abs/2602.22006)
*Jiadong Lu,Zhehan Li,Tao Han,Miao Xu,Chao Xu,Yanjun Cao*

Main category: cs.RO

TL;DR: CT-RIO：一种基于夹紧非均匀B样条的新型连续时间相对惯性里程计框架，用于机器人群体中的高精度、低延迟相对定位，解决了异步测量和时间偏移问题。


<details>
  <summary>Details</summary>
Motivation: 机器人群体中的精确相对定位对于多机器人协作至关重要，但现有连续时间方法在处理异步测量时存在查询时间延迟和高计算成本的问题，难以满足高精度、低延迟和高频率的性能要求。

Method: 首次使用夹紧非均匀B样条表示机器人状态以消除查询时间延迟，并引入闭式扩展和收缩操作保持样条形状；提出knot-keyknot策略支持高频扩展同时保留稀疏关键节点；采用滑动窗口相对定位问题，通过增量异步块坐标下降法将紧密耦合优化分解为机器人级子问题进行并行求解。

Result: CT-RIO能够从高达263毫秒的时间偏移在3秒内收敛到亚毫秒级别，实现0.046米和1.8°的RMSE；在高速度运动下相比最先进方法提升高达60%的性能。

Conclusion: CT-RIO通过创新的夹紧非均匀B样条表示和并行优化策略，成功解决了机器人群体中连续时间相对定位的挑战，实现了高精度、低延迟和高频率的性能，为大规模机器人协作提供了有效的解决方案。

Abstract: Accurate relative localization is critical for multi-robot cooperation. In robot swarms, measurements from different robots arrive asynchronously and with clock time-offsets. Although Continuous-Time (CT) formulations have proved effective for handling asynchronous measurements in single-robot SLAM and calibration, extending CT methods to multi-robot settings faces great challenges to achieve high-accuracy, low-latency, and high-frequency performance. Especially, existing CT methods suffer from the inherent query-time delay of unclamped B-splines and high computational cost. This paper proposes CT-RIO, a novel Continuous-Time Relative-Inertial Odometry framework. We employ Clamped Non-Uniform B-splines (C-NUBS) to represent robot states for the first time, eliminating the query-time delay. We further augment C-NUBS with closed-form extension and shrinkage operations that preserve the spline shape, making it suitable for online estimation and enabling flexible knot management. This flexibility leads to the concept of knot-keyknot strategy, which supports spline extension at high-frequency while retaining sparse keyknots for adaptive relative-motion modeling. We then formulate a sliding-window relative localization problem that operates purely on relative kinematics and inter-robot constraints. To meet the demanding computation required at swarm scale, we decompose the tightly-coupled optimization into robot-wise sub-problems and solve them in parallel using incremental asynchronous block coordinate descent. Extensive experiments show that CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 s, and achieves RMSEs of 0.046 m and 1.8 °. It consistently outperforms state-of-the-art methods, with improvements of up to 60% under high-speed motion.

</details>


### [28] [World Guidance: World Modeling in Condition Space for Action Generation](https://arxiv.org/abs/2602.22010)
*Yue Su,Sijin Chen,Haixin Shi,Mingyu Liu,Zhengshen Zhang,Ningyuan Huang,Weiheng Zhong,Zhengbang Zhu,Yuxiao Liu,Xihui Liu*

Main category: cs.RO

TL;DR: 提出WoG框架，通过将未来观测映射为紧凑条件并注入动作推理流程，在保持高效预测性的同时保留细粒度信息，以增强视觉-语言-动作模型的行动生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡高效可预测的未来表示与保留足够细粒度信息来指导精确动作生成之间的矛盾，需要一种既能有效建模世界又能指导动作推理的方法。

Method: 提出WoG框架，将未来观测映射为紧凑条件并注入动作推理流程，训练VLA模型同时预测这些压缩条件和未来动作，在条件空间中实现有效的世界建模。

Result: 该方法不仅促进了细粒度动作生成，还展现出优越的泛化能力，并能有效从大量人类操作视频中学习。在仿真和真实环境中的广泛实验验证了其显著优于基于未来预测的现有方法。

Conclusion: WoG框架通过将未来观测建模为紧凑条件空间，成功解决了现有方法在平衡预测效率与细粒度信息保留方面的局限性，为VLA模型的动作生成提供了有效解决方案。

Abstract: Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/

</details>


### [29] [System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot](https://arxiv.org/abs/2602.22118)
*Benjamin Bokser,Daniel Gonzalez,Surya Singh,Aaron Preston,Alex Bahner,Annika Wollschläger,Arianna Ilvonen,Asa Eckert-Erdheim,Ashwin Khadke,Bilal Hammoud,Dean Molinaro,Fabian Jenelten,Henry Mayne,Howie Choset,Igor Bogoslavskyi,Itic Tinman,James Tigue,Jan Preisig,Kaiyu Zheng,Kenny Sharma,Kim Ang,Laura Lee,Liana Margolese,Nicole Lin,Oscar Frias,Paul Drews,Ravi Boggavarapu,Rick Burnham,Samuel Zapolsky,Sangbae Kim,Scott Biddlestone,Sean Mayorga,Shamel Fahmi,Tyler McCollum,Velin Dimitrov,William Moyne,Yu-Ming Chen,Farbod Farshidian,Marco Hutter,David Perry,Al Rizzi,Gabe Nelson*

Main category: cs.RO

TL;DR: 研究人员受山地车运动员启发，开发了UMV机器人平台，结合自行车和反作用质量块，通过强化学习实现了多种动态运动能力


<details>
  <summary>Details</summary>
Motivation: 受山地自行车运动员能够在各种地形上完成跳跃、平衡、单轮骑行等复杂动作的启发，研究人员希望开发一个能够实现类似动态运动能力的机器人平台

Method: 采用仿真驱动的设计优化过程，合成空间连杆拓扑结构；使用约束强化学习框架训练控制器，实现零样本迁移多种运动行为

Result: 23.5公斤的UMV机器人能够实现8米/秒的高速运动，跳跃1米高的障碍物（相当于机器人标称高度的130%），并能完成定车、跳跃、翘头、后轮跳跃和前空翻等动作

Conclusion: 通过结合自行车设计和反作用质量块，配合强化学习控制，成功开发出具有出色动态运动能力的机器人平台，为机器人运动控制提供了新思路

Abstract: Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to synthesize a spatial linkage topology with a focus on vertical jump height and momentum-based balancing on a single wheel contact. Using a constrained Reinforcement Learning (RL) framework, we demonstrate zero-shot transfer of diverse athletic behaviors, including track-stands, jumps, wheelies, rear wheel hopping, and front flips. This 23.5 kg robot is capable of high speeds (8 m/s) and jumping on and over large obstacles (1 m tall, or 130% of the robot's nominal height).

</details>


### [30] [Position-Based Flocking for Persistent Alignment without Velocity Sensing](https://arxiv.org/abs/2602.22154)
*Hossein B. Jond,Veli Bakırcıoğlu,Logan E. Beaver,Nejat Tükenmez,Adel Akbarimajd,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种基于位置而非速度感知的群体运动模型，通过位置变化估算相对速度差异，实现持久的速度对齐和紧凑编队


<details>
  <summary>Details</summary>
Motivation: 受鸟群和鱼群协调集体运动启发，为现实机器人群体开发算法。现实机器人群体中速度测量通常不可靠、有噪声或不可用，需要不依赖速度传感的群体运动模型

Method: 基于位置的群体运动模型：通过当前与初始相对位置的变化近似相对速度差异；采用时间和密度相关的对齐增益，并设置非零最小阈值以维持持久对齐

Result: 50个智能体的仿真显示，该模型比基于速度对齐的基线方法获得更快、更持久的方向对齐，形成更紧凑的编队；9个真实轮式移动机器人的实验验证了模型有效性

Conclusion: 该基于位置的群体运动模型特别适合现实机器人群体应用，能够在没有可靠速度测量的情况下实现持久、协调的集体运动

Abstract: Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent collective motion over extended periods. Simulations with a collective of 50 agents demonstrate that the position-based flocking model attains faster and more sustained directional alignment and results in more compact formations than a velocity-alignment-based baseline. This position-based flocking model is particularly well-suited for real-world robotic swarms, where velocity measurements are unreliable, noisy, or unavailable. Experimental results using a team of nine real wheeled mobile robots are also presented.

</details>
