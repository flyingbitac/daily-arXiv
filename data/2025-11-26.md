<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories](https://arxiv.org/abs/2511.19528)
*Rushuai Yang,Zhiyuan Feng,Tianxiang Zhang,Kaixin Wang,Chuheng Zhang,Li Zhao,Xiu Su,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 提出DLR框架，通过信息论模式发现生成多样化的行为模式，用于视觉语言动作模型的预训练数据生成，相比标准强化学习能产生更丰富的轨迹数据。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言动作模型预训练需要大量多样化高质量操作轨迹数据的问题，当前人类遥操作数据获取成本高且难以扩展，标准强化学习训练会收敛到单一执行模式，限制了其在大规模预训练中的实用性。

Method: 提出Discover、Learn和Reinforce（DLR）框架，基于信息论模式发现，生成多个不同但高成功率的行为模式，用于VLA预训练数据生成。

Result: 在LIBERO基准测试中，DLR生成了明显更多样化的轨迹语料库，学习到同一任务的多个不同高成功率策略，覆盖了更广泛的状态-动作空间。在未见下游任务中，基于DLR多样化RL数据预训练的VLA模型优于基于等规模标准RL数据训练的模型。

Conclusion: 多模式强化学习可作为实用的、可扩展的具身基础模型数据引擎，DLR展现出标准单模式RL所缺乏的正向数据扩展特性。

Abstract: Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.

</details>


### [2] [A Virtual Mechanical Interaction Layer Enables Resilient Human-to-Robot Object Handovers](https://arxiv.org/abs/2511.19543)
*Omar Faris,Sławomir Tadeja,Fulvio Forni*

Main category: cs.RO

TL;DR: 提出了一种使用虚拟模型控制和增强现实技术的人机物体交接方法，能够适应交接过程中物体姿态的复杂变化，并通过用户研究验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 物体交接是协作任务中的常见交互形式，但实现高效交接仍具挑战性。需要确保机器人在人机物体交接过程中能够适应物体姿态的复杂变化。

Method: 使用虚拟模型控制创建交互层来控制机器人并适应交接过程中的动态变化，同时使用增强现实技术促进人机双向通信。

Result: 实验表明控制器对各种不确定性具有韧性，包括交接过程中物体姿态的复杂变化。用户研究显示参与者普遍偏好所提出的方法。

Conclusion: 该方法在人机物体交接中表现出良好的适应性和用户接受度，为未来交互适配的进一步发展提供了指导。

Abstract: Object handover is a common form of interaction that is widely present in collaborative tasks. However, achieving it efficiently remains a challenge. We address the problem of ensuring resilient robotic actions that can adapt to complex changes in object pose during human-to-robot object handovers. We propose the use of Virtual Model Control to create an interaction layer that controls the robot and adapts to the dynamic changes in the handover process. Additionally, we propose the use of augmented reality to facilitate bidirectional communication between humans and robots during handovers. We assess the performance of our controller in a set of experiments that demonstrate its resilience to various sources of uncertainties, including complex changes to the object's pose during the handover. Finally, we performed a user study with 16 participants to understand human preferences for different robot control profiles and augmented reality visuals in object handovers. Our results showed a general preference for the proposed approach and revealed insights that can guide further development in adapting the interaction with the user.

</details>


### [3] [Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation](https://arxiv.org/abs/2511.19647)
*Jennifer Grannen,Michelle Pan,Kenneth Llontop,Cherie Ho,Mark Zolotas,Jeannette Bohg,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本文提出了机器人驱动的数据飞轮框架，将机器人从基础模型消费者转变为数据生成器，通过在真实环境中部署机器人收集数据来改进基础模型的领域特定适应性和领域相邻泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基础模型在视觉和语言任务中展现出强大的零样本能力，但依赖互联网预训练数据使其在非结构化真实世界环境中表现脆弱。机器人作为具身智能体，能够通过物理环境交互收集大规模真实世界数据，弥补现有数据集的不足。

Method: 开发了机器人驱动的数据飞轮框架，部署名为Scanford的移动机械臂在东亚图书馆进行为期2周的自主操作。机器人使用视觉语言模型识别书籍，并利用图书馆目录自动标注图像，无需人工标注。

Result: 从2103个书架收集数据后，书籍识别准确率从32.0%提升至71.8%，领域相邻的多语言OCR性能从24.8%提升至46.6%（英文）和30.8%提升至38.0%（中文），同时节省约18.7小时人工时间。

Conclusion: 机器人驱动的数据飞轮既能减少实际部署中的人工努力，又能为持续适应现实世界复杂性开辟新途径，展示了机器人作为数据生成器的潜力。

Abstract: Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: https://scanford-robot.github.io

</details>


### [4] [Online Learning-Enhanced High Order Adaptive Safety Control](https://arxiv.org/abs/2511.19651)
*Lishuo Pan,Mattia Catellani,Thales C. Silva,Lorenzo Sabattini,Nora Ayanian*

Main category: cs.RO

TL;DR: 提出了一种基于神经ODE的在线学习增强高阶自适应控制屏障函数方法，能够在复杂时变模型扰动下实时提高CBF认证系统的安全性，并在38克纳米四旋翼无人机上成功验证，在18公里/小时风速下保持与障碍物的安全距离。


<details>
  <summary>Details</summary>
Motivation: 控制屏障函数(CBFs)是形式化认证系统安全性的有效工具，但其安全保证在实际系统中的成功转移严重依赖于模型精度。有效载荷或风扰动等复杂时变扰动会显著影响飞行器动力学并使安全保证失效。

Method: 使用神经ODE构建高效的在线学习增强高阶自适应控制屏障函数，结合混合自适应CBF控制器，在复杂时变模型扰动下实时提高系统安全性。

Result: 在38克纳米四旋翼无人机上成功部署，在18公里/小时风速下能够保持与障碍物的安全距离，验证了方法在真实环境中的有效性。

Conclusion: 提出的自适应CBF方法能够有效应对复杂时变模型扰动，为CBF认证系统在真实世界中的安全应用提供了可行解决方案。

Abstract: Control barrier functions (CBFs) are an effective model-based tool to formally certify the safety of a system. With the growing complexity of modern control problems, CBFs have received increasing attention in both optimization-based and learning-based control communities as a safety filter, owing to their provable guarantees. However, success in transferring these guarantees to real-world systems is critically tied to model accuracy. For example, payloads or wind disturbances can significantly influence the dynamics of an aerial vehicle and invalidate the safety guarantee. In this work, we propose an efficient yet flexible online learning-enhanced high-order adaptive control barrier function using Neural ODEs. Our approach improves the safety of a CBF-certified system on the fly, even under complex time-varying model perturbations. In particular, we deploy our hybrid adaptive CBF controller on a 38g nano quadrotor, keeping a safe distance from the obstacle, against 18km/h wind.

</details>


### [5] [Development of a Testbed for Autonomous Vehicles: Integrating MPC Control with Monocular Camera Lane Detection](https://arxiv.org/abs/2511.19655)
*Shantanu Rahman,Nayeb Hasin,Mainul Islam*

Main category: cs.RO

TL;DR: 本文提出了一种结合车道识别与模型预测控制(MPC)的新方法，用于提高自动驾驶汽车轨迹跟踪的精度和稳定性。通过边缘识别、滑动窗口直线识别等技术提取车道线，并基于自行车车辆动力学模型构建MPC控制器，在ROS Gazebo仿真中验证了控制器的性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在道路行驶、工业自动化等领域的普及，Ackermann转向机制成为大型车辆的标准。为提高自动驾驶汽车轨迹跟踪的精度和稳定性，需要开发更有效的低层控制器。

Method: 使用边缘识别、滑动窗口直线识别方法进行车道线提取，结合动态感兴趣区域(ROI)提取技术。基于自行车车辆动力学模型构建模型预测控制(MPC)控制器，在ROS Gazebo中构建单车道道路仿真模型进行验证。

Result: 仿真结果显示，最优跟踪轨迹与目标轨迹之间的均方根误差降低了27.65%，证明了所开发控制器的高度鲁棒性和灵活性。

Conclusion: 提出的结合车道识别与MPC的方法能有效提高自动驾驶汽车的轨迹跟踪性能，在仿真环境中表现出良好的控制效果和鲁棒性。

Abstract: Autonomous vehicles are becoming popular day by day not only for autonomous road traversal but also for industrial automation, farming and military. Most of the standard vehicles follow the Ackermann style steering mechanism. This has become to de facto standard for large and long faring vehicles. The local planner of an autonomous vehicle controls the low-level vehicle movement upon which the vehicle will perform its motor actuation. In our work, we focus on autonomous vehicles in road and perform experiments to analyze the effect of low-level controllers in the simulation and a real environment. To increase the precision and stability of trajectory tracking in autonomous cars, a novel method that combines lane identification with Model Predictive Control (MPC) is presented. The research focuses on camera-equipped autonomous vehicles and uses methods like edge recognition, sliding window-based straight-line identification for lane line extraction, and dynamic region of interest (ROI) extraction. Next, to follow the identified lane line, an MPC built on a bicycle vehicle dynamics model is created. A single-lane road simulation model is built using ROS Gazebo and tested in order to verify the controller's performance. The root mean square error between the optimal tracking trajectory and the target trajectory was reduced by 27.65% in the simulation results, demonstrating the high robustness and flexibility of the developed controller.

</details>


### [6] [Whole-Body Inverse Dynamics MPC for Legged Loco-Manipulation](https://arxiv.org/abs/2511.19709)
*Lukas Molnar,Jin Cheng,Gabriele Fadini,Dongho Kang,Fatemeh Zargarbashi,Stelian Coros*

Main category: cs.RO

TL;DR: 提出了一种全身模型预测控制框架，通过全阶逆动力学直接优化关节扭矩，在单个预测层内实现统一的运动和力规划与执行，在四足机器人上实现了80Hz的实时性能。


<details>
  <summary>Details</summary>
Motivation: 全身操作需要协调的全身运动来有效操纵物体同时保持运动稳定性，这对规划和控制都提出了重大挑战。

Method: 使用全身模型预测控制框架，通过全阶逆动力学直接优化关节扭矩，采用Pinocchio、CasADi软件框架和Fatrop求解器实现。

Result: 在配备机械臂的Unitree B2四足机器人上实现了80Hz的实时性能，成功完成了拉动重物、推箱子和擦白板等实际交互任务。

Conclusion: 该方法能够实现物理一致的全身行为，考虑了系统动力学和物理约束，为全身操作任务提供了有效的解决方案。

Abstract: Loco-manipulation demands coordinated whole-body motion to manipulate objects effectively while maintaining locomotion stability, presenting significant challenges for both planning and control. In this work, we propose a whole-body model predictive control (MPC) framework that directly optimizes joint torques through full-order inverse dynamics, enabling unified motion and force planning and execution within a single predictive layer. This approach allows emergent, physically consistent whole-body behaviors that account for the system's dynamics and physical constraints. We implement our MPC formulation using open software frameworks (Pinocchio and CasADi), along with the state-of-the-art interior-point solver Fatrop. In real-world experiments on a Unitree B2 quadruped equipped with a Unitree Z1 manipulator arm, our MPC formulation achieves real-time performance at 80 Hz. We demonstrate loco-manipulation tasks that demand fine control over the end-effector's position and force to perform real-world interactions like pulling heavy loads, pushing boxes, and wiping whiteboards.

</details>


### [7] [Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation](https://arxiv.org/abs/2511.19859)
*Xiangkai Ma,Lekai Xing,Han Zhang,Wenzhong Li,Sanglu Lu*

Main category: cs.RO

TL;DR: VITA框架通过构建视觉和动作的共享离散潜在空间，解决了视觉观察与低级动作之间的模态差距问题，同时通过隐式视觉思维链将视觉动态内化为运动规划的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 现有基于CoT的VLA模型在复杂空间环境中难以充分捕捉场景细节，且面临视觉观察与动作之间的模态差距以及视觉预测与动作生成目标冲突导致的训练不稳定问题。

Method: 提出VITA框架，学习视觉和动作的共享离散潜在空间，引入隐式视觉CoT：自回归生成的token同时解码为未来帧预测和机器人动作，从而将视觉动态内化为运动规划的归纳偏置。

Result: 在CALVIN、LIBERO和SimplerEnv基准上分别比现有基线提升14.5%、9.6%和12.1%，在六项真实世界任务中平均成功率达到80.5%。

Conclusion: VITA框架在模拟和真实环境中均表现出最先进的性能，展示了其作为通用机器人操作模型的潜力。

Abstract: Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\%, 9.6\% and 12.1\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.

</details>


### [8] [Human-Centered Cooperative Control Coupling Autonomous and Haptic Shared Control via Control Barrier Function](https://arxiv.org/abs/2511.19869)
*Eito Sato,Takahiro Wada*

Main category: cs.RO

TL;DR: 提出了一种结合独立操纵杆自主控制器与触觉共享控制的协作框架，通过控制屏障函数在安全区域内忽略操纵杆输入，在虚拟环境中的水下机器人模拟任务中验证了比传统触觉共享控制更高的精度和更短的操作时间。


<details>
  <summary>Details</summary>
Motivation: 当完全自主控制受限于不确定性或感知约束时，触觉共享控制(HSC)在遥操作中很有效，但最大化HSC强度实现的自主控制性能有限，因为操纵杆和人体手臂的动力学会影响机器人行为。

Method: 提出协作框架，将独立于操纵杆的自主控制器与HSC耦合。使用控制屏障函数在由人类操作员实时确定的安全区域内忽略操纵杆输入，否则启用HSC。

Result: 在虚拟环境中对遥操作水下机器人进行的模拟任务初步实验表明，相比传统HSC，该方法提高了精度并减少了所需时间。

Conclusion: 所提出的协作框架通过结合自主控制器和触觉共享控制，有效克服了传统HSC的性能限制，在遥操作任务中实现了更好的性能表现。

Abstract: Haptic shared control (HSC) is effective in teleoperation when full autonomy is limited by uncertainty or sensing constraints. However, autonomous control performance achieved by maximizing HSC strength is limited because the dynamics of the joystick and human arm affect the robot's behavior. We propose a cooperative framework coupling a joystick-independent autonomous controller with HSC. A control barrier function ignores joystick inputs within a safe region determined by the human operator in real-time, while HSC is engaged otherwise. A pilot experiment on simulated tasks with tele-operated underwater robot in virtual environment demonstrated improved accuracy and reduced required time over conventional HSC.

</details>


### [9] [CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model](https://arxiv.org/abs/2511.19914)
*Dapeng Zhang,Fei Shen,Rui Zhao,Yinda Chen,Peng Zhi,Chenyang Li,Rui Zhou,Qingguo Zhou*

Main category: cs.RO

TL;DR: 提出CoC-VLA框架，通过对抗迁移学习将仿真环境中的长尾场景处理能力迁移到现实世界自动驾驶系统中


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖真实世界数据，要么依赖仿真数据，很少能有效整合两种数据源的优势来提升自动驾驶系统在复杂长尾场景中的表现

Method: 基于共享的因果链视觉语言模型架构，包含教师VLM、学生VLM和判别器，通过对抗训练和新型反向传播策略实现从仿真到现实的迁移学习

Result: 开发了端到端的对抗迁移框架，能够将仿真环境中的长尾处理能力有效迁移到现实世界部署

Conclusion: CoC-VLA框架成功整合了仿真和真实世界数据的优势，提升了自动驾驶系统在复杂长尾场景中的推理能力和性能

Abstract: Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.

</details>


### [10] [Collaborate sim and real: Robot Bin Packing Learning in Real-world and Physical Engine](https://arxiv.org/abs/2511.19932)
*Lidi Zhang,Han Wu,Liyu Zhang,Ruofeng Liu,Haotian Wang,Chao Li,Desheng Zhang,Yunhuai Liu,Tian He*

Main category: cs.RO

TL;DR: 本文提出了一种混合强化学习框架，通过物理模拟与真实世界数据反馈相结合，解决3D装箱问题中的模拟到现实差距问题，显著降低了包装坍塌率。


<details>
  <summary>Details</summary>
Motivation: 现有3D装箱方法通常将其建模为离散静态过程，而实际应用涉及连续重力驱动交互，这种理想化简化导致实际部署不可行（如不稳定包装）。物理模拟提供了模拟连续重力效应的机会，但存在模拟到现实的差距。

Method: 提出混合强化学习框架：1）在模拟中应用领域随机化，使智能体接触各种物理参数以增强泛化能力；2）利用真实世界部署反馈对RL智能体进行微调，进一步降低坍塌率。

Result: 大量实验表明，该方法在模拟和真实世界场景中都实现了更低的坍塌率。在物流系统中的大规模部署验证了实际有效性，与基准方法相比包装坍塌减少了35%。

Conclusion: 所提出的混合RL框架通过结合物理模拟和真实世界反馈，有效缩小了模拟到现实的差距，显著提高了3D装箱的稳定性和实际部署可行性。

Abstract: The 3D bin packing problem, with its diverse industrial applications, has garnered significant research attention in recent years. Existing approaches typically model it as a discrete and static process, while real-world applications involve continuous gravity-driven interactions. This idealized simplification leads to infeasible deployments (e.g., unstable packing) in practice. Simulations with physical engine offer an opportunity to emulate continuous gravity effects, enabling the training of reinforcement learning (RL) agents to address such limitations and improve packing stability. However, a simulation-to-reality gap persists due to dynamic variations in physical properties of real-world objects, such as various friction coefficients, elasticity, and non-uniform weight distributions. To bridge this gap, we propose a hybrid RL framework that collaborates with physical simulation with real-world data feedback. Firstly, domain randomization is applied during simulation to expose agents to a spectrum of physical parameters, enhancing their generalization capability. Secondly, the RL agent is fine-tuned with real-world deployment feedback, further reducing collapse rates. Extensive experiments demonstrate that our method achieves lower collapse rates in both simulated and real-world scenarios. Large-scale deployments in logistics systems validate the practical effectiveness, with a 35\% reduction in packing collapse compared to baseline methods.

</details>


### [11] [ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation](https://arxiv.org/abs/2511.19955)
*Jinxuan Zhu,Zihao Yan,Yangyu Xiao,Jingxiang Guo,Chenrui Tie,Xinyi Cao,Yuhang Zheng,Lin Shao*

Main category: cs.RO

TL;DR: ShapeForce是一种低成本、即插即用的软腕部装置，通过将外力转换为可测量的变形来提供类似力的信号，用于接触丰富的机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 六轴力扭矩传感器成本高且易碎，限制了在接触丰富任务中的应用。ShapeForce旨在提供更经济、易获取的接触反馈方案。

Method: 通过柔性核心将外力和扭矩转换为可测量的变形，使用基于标记的姿态跟踪估计变形，并将其转换为类似力的信号，无需校准或专用电子设备。

Result: 在多种接触丰富任务和操作策略的广泛实验中，ShapeForce的性能与六轴力扭矩传感器相当，但成本极低。

Conclusion: ShapeForce提供了一种低成本、高性能的接触反馈解决方案，能够替代昂贵的六轴力扭矩传感器，促进接触丰富机器人操作的发展。

Abstract: Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six- axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.

</details>


### [12] [Active3D: Active High-Fidelity 3D Reconstruction via Hierarchical Uncertainty Quantification](https://arxiv.org/abs/2511.20050)
*Yan Li,Yingzhao Li,Gim Hee Lee*

Main category: cs.RO

TL;DR: 提出了一种用于高保真3D重建的主动探索框架，通过增量构建多级不确定性空间和不确定性驱动的运动规划器来选择最佳视角。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法在主动探索和不确定性量化方面存在不足，需要一种能够同时捕捉全局结构先验和局部细节，并能有效指导探索过程的框架。

Method: 采用混合隐式-显式表示融合神经场与高斯基元；构建分层不确定性体积；提出不确定性驱动的关键帧选择策略；将最佳视角选择建模为期望混合信息增益问题；结合风险敏感路径规划器。

Result: 在具有挑战性的基准测试中，该方法在准确性、完整性和渲染质量方面均达到最先进水平。

Conclusion: 该方法在真实世界主动重建和机器人感知任务中表现出色，证明了其有效性。

Abstract: In this paper, we present an active exploration framework for high-fidelity 3D reconstruction that incrementally builds a multi-level uncertainty space and selects next-best-views through an uncertainty-driven motion planner. We introduce a hybrid implicit-explicit representation that fuses neural fields with Gaussian primitives to jointly capture global structural priors and locally observed details. Based on this hybrid state, we derive a hierarchical uncertainty volume that quantifies both implicit global structure quality and explicit local surface confidence. To focus optimization on the most informative regions, we propose an uncertainty-driven keyframe selection strategy that anchors high-entropy viewpoints as sparse attention nodes, coupled with a viewpoint-space sliding window for uncertainty-aware local refinement. The planning module formulates next-best-view selection as an Expected Hybrid Information Gain problem and incorporates a risk-sensitive path planner to ensure efficient and safe exploration. Extensive experiments on challenging benchmarks demonstrate that our approach consistently achieves state-of-the-art accuracy, completeness, and rendering quality, highlighting its effectiveness for real-world active reconstruction and robotic perception tasks.

</details>


### [13] [Hibikino-Musashi@Home 2025 Team Description Paper](https://arxiv.org/abs/2511.20180)
*Ryohei Kobayashi,Kosei Isomoto,Kosei Yamao,Soma Fumoto,Koshun Arimura,Naoki Yamaguchi,Akinobu Mizutani,Tomoya Shiba,Kouki Kimizuka,Yuta Ohno,Ryo Terashima,Hiromasa Yamaguchi,Tomoaki Fujino,Ryoga Maruno,Wataru Yoshimura,Kazuhito Mine,Tang Phu Thien Nhan,Yuga Yano,Yuichiro Tanaka,Takeshi Nishida,Takashi Morie,Hakaru Tamukoh*

Main category: cs.RO

TL;DR: 本文概述了Hibikino-Musashi@Home团队在家庭服务机器人领域的技术方法，包括数据集生成器、开源开发环境、大语言模型任务规划器、脑启发记忆模型以及导航系统复用。


<details>
  <summary>Details</summary>
Motivation: 设计能够在家中协助人类的家庭服务机器人，并通过持续参与比赛来评估和改进开发的系统。

Method: 开发了用于训练机器人视觉系统的数据集生成器、基于Human Support Robot模拟器的开源开发环境、大语言模型驱动的任务规划器、脑启发记忆模型用于适应个体家庭环境，以及复用Pumas在RoboCup2024中开发的导航系统。

Result: 构建了完整的家庭服务机器人技术栈，包括视觉训练、任务规划、环境适应和导航系统。

Conclusion: 该团队通过多技术融合的方法，旨在提供直观和个性化的家庭服务机器人辅助，并通过竞赛不断优化系统性能。

Abstract: This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.

</details>


### [14] [Toward generic control for soft robotic systems](https://arxiv.org/abs/2511.20226)
*Yu Sun,Yaosheng Deng,Wenjie Mei,Xiaogang Xiong,Yang Bai,Masaki Ogura,Zeyu Zhou,Mir Feroskhan,Michael Yu Wang,Qiyang Zuo,Yao Li,Yunjiang Lou*

Main category: cs.RO

TL;DR: 提出了一种基于控制顺应性的通用软体机器人控制框架，通过利用而非抑制近似动作表示来实现鲁棒性和适应性，验证了该框架在不同形态和驱动机制软体机器人上的跨平台可转移性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人控制方法仍处于碎片化状态，不同形态和驱动方案需要特定控制器，阻碍了理论整合和大规模部署。刚性控制逻辑依赖精确模型和严格低级执行，不适用于软体机器人，而控制顺应性（容忍和利用近似动作表示）才是鲁棒性和适应性的基础。

Method: 受人类运动控制启发，提出基于控制顺应性的通用软体机器人控制框架，通过高层运动倾向表达意图，而反射和生物力学机制自主解决局部细节，不计算精确动力学或发出详细肌肉级命令。

Result: 在不同形态和驱动机制的软体机器人上验证了该框架，结果表明能够实现稳定、安全且跨平台可转移的行为。

Conclusion: 拥抱而非抵抗控制顺应性，可能为统一软体机器人控制提供广泛应用基础，实现鲁棒性、灵活性和跨任务泛化能力。

Abstract: Soft robotics has advanced rapidly, yet its control methods remain fragmented: different morphologies and actuation schemes still require task-specific controllers, hindering theoretical integration and large-scale deployment. A generic control framework is therefore essential, and a key obstacle lies in the persistent use of rigid-body control logic, which relies on precise models and strict low-level execution. Such a paradigm is effective for rigid robots but fails for soft robots, where the ability to tolerate and exploit approximate action representations, i.e., control compliance, is the basis of robustness and adaptability rather than a disturbance to be eliminated. Control should thus shift from suppressing compliance to explicitly exploiting it. Human motor control exemplifies this principle: instead of computing exact dynamics or issuing detailed muscle-level commands, it expresses intention through high-level movement tendencies, while reflexes and biomechanical mechanisms autonomously resolve local details. This architecture enables robustness, flexibility, and cross-task generalization. Motivated by this insight, we propose a generic soft-robot control framework grounded in control compliance and validate it across robots with diverse morphologies and actuation mechanisms. The results demonstrate stable, safe, and cross-platform transferable behavior, indicating that embracing control compliance, rather than resisting it, may provide a widely applicable foundation for unified soft-robot control.

</details>


### [15] [Dynamic-ICP: Doppler-Aware Iterative Closest Point Registration for Dynamic Scenes](https://arxiv.org/abs/2511.20292)
*Dong Wang,Daniel Casado Herraez,Stefan May,Andreas Nüchter*

Main category: cs.RO

TL;DR: Dynamic-ICP是一种基于多普勒感知的激光雷达里程计方法，专门针对高动态环境设计，通过结合几何残差和多普勒残差来提高在动态场景中的定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统ICP方法假设场景基本静态，在高度动态环境中性能会显著下降，特别是在重复纹理或低纹理几何环境中。

Method: 该方法包括四个步骤：(i)通过稳健回归从点级多普勒速度估计自身运动并构建速度滤波器；(ii)聚类动态物体并从自身补偿的径向测量重建物体级平移速度；(iii)使用恒定速度模型预测动态点；(iv)结合点对面几何残差和仅旋转的多普勒残差来对齐扫描。

Result: 在三个数据集（HeRCULES、HeLiPR、AevaScenes）上的评估显示，Dynamic-ICP在旋转稳定性和平移精度方面持续优于现有最先进方法。

Conclusion: Dynamic-ICP提供了一种无需外部传感器或传感器-车辆校准的轻量级解决方案，能够实时运行，并且易于集成到现有流程中。

Abstract: Reliable odometry in highly dynamic environments remains challenging when it relies on ICP-based registration: ICP assumes near-static scenes and degrades in repetitive or low-texture geometry. We introduce Dynamic-ICP, a Doppler-aware registration framework. The method (i) estimates ego motion from per-point Doppler velocity via robust regression and builds a velocity filter, (ii) clusters dynamic objects and reconstructs object-wise translational velocities from ego-compensated radial measurements, (iii) predicts dynamic points with a constant-velocity model, and (iv) aligns scans using a compact objective that combines point-to-plane geometry residual with a translation-invariant, rotation-only Doppler residual. The approach requires no external sensors or sensor-vehicle calibration and operates directly on FMCW LiDAR range and Doppler velocities. We evaluate Dynamic-ICP on three datasets-HeRCULES, HeLiPR, AevaScenes-focusing on highly dynamic scenes. Dynamic-ICP consistently improves rotational stability and translation accuracy over the state-of-the-art methods. Our approach is also simple to integrate into existing pipelines, runs in real time, and provides a lightweight solution for robust registration in dynamic environments. To encourage further research, the code is available at: https://github.com/JMUWRobotics/Dynamic-ICP.

</details>


### [16] [How Robot Kinematics Influence Human Performance in Virtual Robot-to-Human Handover Tasks](https://arxiv.org/abs/2511.20299)
*Róisín Keenan,Joost C. Dessing*

Main category: cs.RO

TL;DR: 本研究使用VR技术探索人机协作中的物体交接任务，分析了任务动态和机器人运动学对人类运动表现的影响，发现提供早期视觉信息和类人平滑轨迹能改善预测准确性和同步性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统越来越多地融入人类工作环境，需要优化人机协作中的协调性，特别是在物体交接任务中。

Method: 通过VR机器人交接模拟，在安全受控环境中研究四个影响因素：任务启动控制和机器人运动同步性、伙伴外观、机器人速度曲线、旋转物体运动时机。

Result: 实验表明，机器人提供早期视觉信息和类人平滑轨迹能提高人类的预测准确性和交互同步性，让人类能更好地利用生物运动检测能力。

Conclusion: 人机交互设计应允许人类利用其自然能力检测生物运动，这可以减少机器人计算成本和人类认知适应需求。

Abstract: Recent advancements in robotics have increased the possibilities for integrating robotic systems into human-involved workplaces, highlighting the need to examine and optimize human-robot coordination in collaborative settings. This study explores human-robot interactions during handover tasks using Virtual Reality (VR) to investigate differences in human motor performance across various task dynamics and robot kinematics. A VR-based robot handover simulation afforded safe and controlled assessments of human-robot interactions. In separate experiments, four potential influences on human performance were examined (1) control over task initiation and robot movement synchrony (temporal and spatiotemporal); (2) partner appearance (human versus robotic); (3) robot velocity profiles (minimum jerk, constant velocity, constant acceleration, and biphasic); and (4) the timing of rotational object motion. Findings across experiments emphasize humans benefit from robots providing early and salient visual information about task-relevant object motion, and advantages of human-like smooth robot trajectories. To varying degrees, these manipulations improved predictive accuracy and synchronization during interaction. This suggests that human-robot interactions should be designed to allow humans to leverage their natural capabilities for detecting biological motion, which conversely may reduce the need for costly robotic computations or added cognitive adaptation on the human side.

</details>


### [17] [ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation](https://arxiv.org/abs/2511.20330)
*Yuhan Wu,Tiantian Wei,Shuo Wang,ZhiChao Wang,Yanyong Zhang,Daniel Cremers,Yan Xia*

Main category: cs.RO

TL;DR: 提出了ArtiBench基准测试和ArtiBrain框架，用于解决关节物体操作中的泛化挑战，结合高层推理和自适应底层控制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言和基于扩散的策略在跨部件、实例和类别的关节操作中泛化能力不足，需要解决长时程、多步骤交互的物理一致性挑战。

Method: 提出ArtiBrain模块化框架：使用VLM任务推理器分解和验证子目标，结合几何感知关键帧执行和可操作性引导扩散的混合控制器，通过可操作性记忆库积累和传播成功经验。

Result: 在ArtiBench基准测试上的大量实验表明，ArtiBrain在鲁棒性和泛化能力上显著优于最先进的多模态和基于扩散的方法。

Conclusion: ArtiBrain框架通过统一高层推理和自适应底层控制，有效解决了关节物体操作的泛化挑战，为交互式关节操作提供了新的解决方案。

Abstract: Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.

</details>


### [18] [Quality-guided UAV Surface Exploration for 3D Reconstruction](https://arxiv.org/abs/2511.20353)
*Benjamin Sportich,Kenza Boubakri,Olivier Simonin,Alessandro Renzaglia*

Main category: cs.RO

TL;DR: 本文提出了一种用于无人机的新型模块化Next-Best-View规划框架，通过重建质量目标指导探索规划，在视图生成和候选视点选择方面引入高效方法，适应不同质量需求，并在仿真环境中验证了其优于传统NBV策略的性能。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在未知环境中的建图需求广泛，但在规划策略开发中常被忽视。快速信息收集和建筑结构全面评估有不同的要求，需要不同的方法学。

Method: 提出模块化NBV规划框架，使用重建质量目标指导探索规划，引入高效的视图生成和候选视点选择方法，充分利用TSDF表示中的不确定性信息，实现针对预定目标的信息化高效探索决策。

Result: 在真实环境中的广泛仿真验证表明，该方法成功根据用户目标调整行为，在覆盖范围、最终3D地图质量和路径效率方面持续优于传统NBV策略。

Conclusion: 所提出的NBV规划框架能够有效适应不同质量需求，实现更高效的探索和更高质量的3D重建，为自主机器人的环境建图提供了更优的解决方案。

Abstract: Reasons for mapping an unknown environment with autonomous robots are wide-ranging, but in practice, they are often overlooked when developing planning strategies. Rapid information gathering and comprehensive structural assessment of buildings have different requirements and therefore necessitate distinct methodologies. In this paper, we propose a novel modular Next-Best-View (NBV) planning framework for aerial robots that explicitly uses a reconstruction quality objective to guide the exploration planning. In particular, our approach introduces new and efficient methods for view generation and selection of viewpoint candidates that are adaptive to the user-defined quality requirements, fully exploiting the uncertainty encoded in a Truncated Signed Distance field (TSDF) representation of the environment. This results in informed and efficient exploration decisions tailored towards the predetermined objective. Finally, we validate our method via extensive simulations in realistic environments. We demonstrate that it successfully adjusts its behavior to the user goal while consistently outperforming conventional NBV strategies in terms of coverage, quality of the final 3D map and path efficiency.

</details>


### [19] [Improved adaptive wind driven optimization algorithm for real-time path planning](https://arxiv.org/abs/2511.20394)
*Shiqian Liu,Azlan Mohd Zain,Le-le Mao*

Main category: cs.RO

TL;DR: 本文提出了一种多层级自适应风驱动优化算法(MAWDO)，通过分层引导机制改进动态环境下的适应性和鲁棒性，在动态路径规划中实现了更短、更平滑的避障轨迹。


<details>
  <summary>Details</summary>
Motivation: 动态环境中实时适应性是自主导航的关键挑战，传统风驱动优化算法在动态路径规划中存在不稳定和早熟收敛问题，需要改进其适应性和鲁棒性。

Method: 基于风驱动优化原理，提出多层级自适应风驱动优化(MAWDO)，采用分层引导机制将种群分为多个组，由个体、区域和全局领导者引导，平衡探索与开发。

Result: 在16个基准函数上评估显示MAWDO获得优越的优化精度、收敛稳定性和适应性；在动态路径规划中，路径长度缩短至469.28像素，比MEWDO、AWDO和WDO分别提升3.51%、11.63%和14.93%，最优性差距最小(1.01)，平滑度0.71优于AWDO(13.50)和WDO(15.67)。

Conclusion: MAWDO算法能够生成更平滑、更短且无碰撞的轨迹，证实了其在复杂环境中实时路径规划的有效性。

Abstract: Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\%, 11.63\% and 14.93\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.

</details>
