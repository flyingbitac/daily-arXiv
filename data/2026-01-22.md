<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RoboBrain 2.5: Depth in Sight, Time in Mind](https://arxiv.org/abs/2601.14352)
*Huajie Tan,Enshen Zhou,Zhiyu Li,Yijie Xu,Yuheng Ji,Xiansheng Chen,Cheng Chi,Pengwei Wang,Huizhu Jia,Yulong Ao,Mingyu Cao,Sixiang Chen,Zhe Li,Mengzhen Liu,Zixiao Wang,Shanyu Rong,Yaoxu Lyu,Zhongxia Zhao,Peterson Co,Yibo Li,Yi Han,Shaoxuan Xie,Guocai Yao,Songjing Wang,Leiduo Zhang,Xi Yang,Yance Jiao,Donghai Shi,Kunchang Xie,Shaokai Nie,Chunlei Men,Yonghua Lin,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboBrain 2.5是一个新一代具身AI基础模型，通过高质量时空监督训练，提升了通用感知、空间推理和时间建模能力，专注于复杂精细操作任务。


<details>
  <summary>Details</summary>
Motivation: 构建更物理接地、执行感知的具身智能系统，以处理复杂精细的操作任务，需要从2D像素相对定位转向精确的3D空间推理和密集时间价值估计。

Method: 引入两大能力升级：1）精确3D空间推理：从2D像素相对定位转向深度感知坐标预测和绝对度量约束理解，生成完整3D操作轨迹作为有序关键点序列；2）密集时间价值估计：提供密集、步骤感知的进度预测和执行状态理解，产生稳定反馈信号。

Result: 模型实现了更物理接地和更执行感知的具身智能框架，能够处理复杂精细的操作任务，代码和检查点已公开。

Conclusion: RoboBrain 2.5通过精确3D空间推理和密集时间价值估计两大升级，推进了具身AI基础模型在物理接地和执行感知方面的发展，为复杂精细操作提供了更强大的能力。

Abstract: We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io

</details>


### [2] [Agentic AI Meets Edge Computing in Autonomous UAV Swarms](https://arxiv.org/abs/2601.14437)
*Thuan Minh Nguyen,Vu Tuan Truong,Long Bao Le*

Main category: cs.RO

TL;DR: 本文研究将基于大语言模型的智能AI与边缘计算集成到无人机集群中，以实现可扩展和弹性的自主性，特别针对野火搜救等高风险场景。


<details>
  <summary>Details</summary>
Motivation: 将具备自主推理、规划和执行能力的大语言模型智能AI集成到无人机集群中，为无人机物联网带来新的操作可能性，但基础设施限制、动态环境和多智能体协调的计算需求限制了其在野火和灾难响应等高风险场景的实际部署。

Method: 首先讨论了支持无人机集群的三种架构：独立部署、边缘使能和边缘-云混合部署，每种架构针对不同的自主性和连接水平进行优化。然后设计了一个野火搜救用例来展示边缘使能架构的效率。

Result: 边缘使能架构相比传统方法，能够实现更高的搜救覆盖率、减少任务完成时间，并提高自主性水平。

Conclusion: 本文强调了将大语言模型和边缘计算集成到任务关键型无人机集群应用中面临的开放挑战。

Abstract: The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.

</details>


### [3] [Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery](https://arxiv.org/abs/2601.14445)
*Aiden Mazidi,Majid Roshanfar,Amir Sayadi,Javad Dargahi,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: NIMA方法通过非线性阻抗匹配显著提升机器人辅助手术的触觉反馈精度，减少95%的力反馈误差，并消除触觉"回弹"现象。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术中的触觉反馈长期面临力渲染精度和系统安全性的挑战，需要开发鲁棒、高保真的触觉系统来提升远程操作手术工具的精确性和可靠性。

Method: 提出非线性阻抗匹配方法(NIMA)，在先前验证的阻抗匹配方法(IMA)基础上，引入非线性动力学来准确建模和渲染工具-组织相互作用力。

Result: NIMA将力反馈的平均绝对误差降至0.01N(SD 0.02)，相比IMA减少了95%的误差；有效消除了触觉"回弹"现象，确保用户释放手柄时触觉设备不施加力，提升了患者安全和用户体验。

Conclusion: NIMA通过考虑工具-组织相互作用的非线性特性，在各种手术条件下显著提升了力反馈的保真度、响应性和精确性，推动了机器人手术触觉反馈系统的发展。

Abstract: Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic "kickback" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.

</details>


### [4] [UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries](https://arxiv.org/abs/2601.14492)
*Malak Mansour,Ali Abouzeid,Zezhou Sun,Qinbo Sun,Dezhen Song,Abdalla Swikir*

Main category: cs.RO

TL;DR: 该研究提出了一种不确定性感知的草莓抓取方法，通过蒙特卡洛dropout生成多个形状假设，使用力闭合指标评估抓取可行性，并采用保守的下置信界准则决定是否执行抓取，在部分遮挡条件下实现可靠抓取与安全弃权。


<details>
  <summary>Details</summary>
Motivation: 机器人草莓采摘在部分遮挡条件下面临挑战，叶片遮挡导致显著的几何不确定性，基于单一确定性形状估计的抓取决策不可靠。从单一局部观测中，可能存在多个不兼容的3D补全结果，导致在一个补全上可行的抓取在另一个补全上失败。

Method: 使用蒙特卡洛dropout进行点云补全以采样多个形状假设，为每个补全生成候选抓取，使用基于力闭合的物理基础指标评估抓取可行性。通过聚合所有补全的可行性，应用保守的下置信界准则决定是否执行抓取或安全弃权。

Result: 在模拟和物理机器人实验中，该方法在合成和真实叶片遮挡条件下均表现出色。不确定性感知决策能够在严重遮挡下可靠地弃权高风险抓取尝试，同时在几何置信度足够时保持稳健的抓取执行，优于确定性基线方法。

Conclusion: 提出的不确定性感知抓取管道通过显式建模遮挡和学习形状重建带来的补全不确定性，实现了在部分遮挡草莓采摘中的可靠抓取决策，能够在高风险情况下安全弃权，在置信度足够时稳健执行抓取。

Abstract: Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.

</details>


### [5] [TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks](https://arxiv.org/abs/2601.14550)
*Tailai Cheng,Kejia Chen,Lingyun Chen,Liding Zhang,Yue Zhang,Yao Ling,Mahdi Hamad,Zhenshan Bing,Fan Wu,Karan Sharma,Alois Knoll*

Main category: cs.RO

TL;DR: TacUMI系统通过集成多种传感器（ViTac、力扭矩、姿态跟踪）到紧凑的机器人兼容夹爪中，实现了多模态数据采集，并提出基于时序模型的多模态分割框架，在电缆安装任务中达到90%以上的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 对于涉及丰富物理交互的复杂长时程操作任务，仅依赖视觉观察和机器人本体感受信息往往无法揭示底层的事件转换，需要高质量多模态数据的高效收集和鲁棒的分割方法。

Method: 基于手持演示设备UMI，引入TacUMI多模态数据采集系统，集成ViTac传感器、力扭矩传感器和姿态跟踪器到紧凑的机器人兼容夹爪设计中；提出多模态分割框架，利用时序模型检测序列操作中的语义有意义的事件边界。

Result: 在具有挑战性的电缆安装任务评估中，展示了超过90%的分割准确率，并且随着模态数量的增加显示出显著改进，验证了TacUMI为接触丰富任务中多模态演示的可扩展收集和分割建立了实用基础。

Conclusion: TacUMI系统成功解决了复杂操作任务中多模态数据采集和分割的挑战，为接触丰富的长时程操作任务提供了有效的多模态演示收集和分割解决方案。

Abstract: Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.

</details>


### [6] [UniCon: A Unified System for Efficient Robot Learning Transfers](https://arxiv.org/abs/2601.14617)
*Yunfeng Lin,Li Xu,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: UniCon是一个轻量级框架，通过标准化状态、控制流和工具来简化异构机器人上学习控制器的部署，采用模块化、数据导向的设计实现高效跨平台工作流。


<details>
  <summary>Details</summary>
Motivation: 异构机器人部署学习控制器面临平台差异、接口不一致和中间件效率低下的挑战，需要统一的解决方案来简化部署过程并提高效率。

Method: UniCon将工作流分解为执行图，使用可重用组件，分离系统状态和控制逻辑，采用批处理向量化数据流最小化通信开销，支持即插即用部署。

Result: UniCon减少了工作流迁移时的代码冗余，相比ROS系统提高了推理效率，已在7个制造商的12个机器人模型上成功部署，并集成到实际研究项目中。

Conclusion: UniCon通过模块化、数据导向的方法有效解决了异构机器人控制器部署的挑战，实现了高效的sim-to-real迁移，在实际应用中证明了其有效性。

Abstract: Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.

</details>


### [7] [Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models](https://arxiv.org/abs/2601.14622)
*Ling Xiao,Toshihiko Yamasaki*

Main category: cs.RO

TL;DR: 该研究探索了社交机器人导航中基于认知理论的提示设计，发现竞争性动机框架和系统指导提示能显著提升小视觉语言模型的决策准确性，而非仅依赖微调。


<details>
  <summary>Details</summary>
Motivation: 当前社交机器人导航基准大多忽视基于认知理论的提示设计，而实际部署中常使用计算效率高但决策能力较弱的小视觉语言模型，因此需要有效的提示设计来提升其社交合规导航性能。

Method: 研究从两个维度探索提示设计：系统指导（动作聚焦、推理导向、感知-推理提示）和动机框架（与人类竞争、与其他AI系统竞争、与自身过去版本竞争）。在两个社交合规导航数据集上进行实验，比较不同提示设计对微调前后模型性能的影响。

Result: 1) 对于未微调的GPT-4o，与人类竞争表现最佳，与其他AI竞争最差；对于微调模型，与自身过去竞争效果最好。2) 不当的系统提示设计会显著降低性能，甚至比直接微调更差。3) 直接微调主要提升语义级指标（感知、预测、推理），但对动作准确性改善有限；而系统提示设计对动作准确性提升更大，主要起决策级约束作用而非表征增强。

Conclusion: 基于认知理论的提示设计（特别是竞争性动机框架）能有效提升小视觉语言模型在社交导航中的决策准确性，提示设计主要作为决策级约束机制，为实际部署提供了重要的工程指导。

Abstract: Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.

</details>


### [8] [A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control](https://arxiv.org/abs/2601.14628)
*Weiyu Guo,He Zhang,Pengteng Li,Tiefu Cai,Ziyang Chen,Yandong Guo,Xiao He,Yongkui Yang,Ying Sun,Hui Xiong*

Main category: cs.RO

TL;DR: NeuroVLA是一个受生物神经系统启发的机器人控制框架，通过模拟大脑皮层、小脑和脊髓的结构组织，实现了动态稳定性、反射响应和时间记忆能力，在物理机器人上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略难以复制生物运动的动态稳定性、反射响应性和时间记忆能力，而生物系统却能从稀疏经验中快速学习技能。需要开发能模拟生物神经系统结构的框架来解决这些问题。

Method: 采用系统级生物启发设计：高层模型规划目标，自适应小脑模块利用高频传感器反馈稳定运动，生物启发的脊髓层执行快速动作生成。这是首个在物理机器人上部署的神经形态VLA框架。

Result: 实现了最先进的性能，观察到了生物运动特性的涌现：消除了机械臂抖动、显著节能（神经形态处理器仅0.4w功耗）、展示了时间记忆能力、在20毫秒内触发安全反射。

Conclusion: NeuroVLA框架成功模拟了生物神经系统的结构组织，在物理机器人上实现了生物运动特性，为快速学习、动态稳定和节能的机器人控制提供了新途径。

Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.

</details>


### [9] [Landing-Induced Viscoelastic Changes in an Anthropomimetic Foot Joint Structure are Modulated by Foot Structure and Posture](https://arxiv.org/abs/2601.14634)
*Satoru Hashimoto,Yinlai Jiang,Hiroshi Yokoi,Shunta Togo*

Main category: cs.RO

TL;DR: 开发仿人足部关节结构研究骨骼结构对落地冲击衰减的影响，发现多关节拱形结构比简化平足有更高阻尼比，踝背屈和脚趾伸展会降低阻尼比，表明骨骼结构和姿势可调节衰减与回弹的平衡。


<details>
  <summary>Details</summary>
Motivation: 尸体研究难以重复测试落地冲击后的姿势依赖性粘弹性响应，导致骨骼结构对落地动力学的贡献不完全清楚。需要开发仿人足部结构来研究骨骼结构和姿势如何调节冲击后的粘弹性响应。

Method: 开发了仿人足部关节结构，复制人类足部骨骼几何形状。使用模拟落地的垂直下落装置和粘弹性系统辨识模型，研究骨骼结构和姿势如何调节冲击后的表观粘弹性响应。

Result: 多关节仿人结构比简化的平足和刚性足表现出更高的阻尼比。踝关节背屈和脚趾伸展系统地改变了辨识参数，在测试条件下降低了阻尼比。拱形多关节骨骼结构可以增强仿人机械足的冲击衰减。

Conclusion: 骨骼结构和被动姿势可以调节衰减与回弹的平衡。观察到的姿势依赖性趋势与人类落地策略的差异一致，表明骨骼结构可能部分解释这种调节。解剖学指导的骨骼复制具有工程优势，可通过姿势调整实现类人表观粘弹性行为。

Abstract: Cadaveric studies have provided important insights into the mechanics of the human foot arch and plantar fascia. However, repeatedly probing posture-dependent viscoelastic responses immediately after landing impact is difficult in biological specimens, leaving the contribution of skeletal architecture to landing dynamics incompletely understood. In this study, we developed an anthropomimetic foot joint structure aimed at replicating the skeletal geometry of the human foot. Using a vertical drop apparatus that simulates landing and a viscoelastic system-identification model, we investigated how skeletal structure and posture modulate the apparent post-impact viscoelastic response. The results show that the multi-jointed anthropomimetic structure exhibited a higher damping ratio than simplified flat and rigid feet. Moreover, ankle dorsiflexion and toe extension systematically shifted the identified parameters, reducing the damping ratio under the tested conditions. Taken together, these findings indicate that an arch-like, multi-jointed skeletal architecture can enhance impact attenuation in an anthropomimetic mechanical foot, and that morphology and passive posture alone can tune the trade-off between attenuation and rebound. The observed posture-dependent trends are qualitatively consistent with reported differences in human landing strategies, suggesting that skeletal architecture may partly account for the modulation. Furthermore, these results highlight the engineering advantage of anatomically informed skeletal replication for achieving human-like apparent viscoelastic behavior through postural adjustment during landing.

</details>


### [10] [FARE: Fast-Slow Agentic Robotic Exploration](https://arxiv.org/abs/2601.14681)
*Shuhao Liao,Xuxin Lv,Jeric Lew,Shizhe Zhang,Jingsong Liang,Peizhuo Li,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: FARE框架通过大语言模型全局推理与强化学习局部决策的层次化结合，实现了自主机器人探索的语义推理与快速控制的集成


<details>
  <summary>Details</summary>
Motivation: 现有自主探索方法缺乏高级语义推理能力，无法有效理解环境结构并制定全局策略，需要将语义推理与局部控制相结合

Method: 采用快慢思维范式：慢思维LLM模块解析环境文本描述并生成探索策略，通过拓扑图生成全局路径点；快思维RL模块基于局部观测执行探索，通过奖励机制遵循全局路径点

Result: 在模拟环境中显著优于现有基线方法，并在200m×130m大规模建筑环境中成功部署验证

Conclusion: FARE框架通过分离语义推理与几何决策，实现了高效、鲁棒的自主探索，为机器人智能探索提供了新范式

Abstract: This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\times130m$ building environment.

</details>


### [11] [Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications](https://arxiv.org/abs/2601.14809)
*Muhammad Adel Yusuf,Ali Nasir,Zeeshan Hameed Khan*

Main category: cs.RO

TL;DR: 提出一种基于随机建模的人机协作决策方法，通过概率模型和控制策略预测人类行为和情绪，使协作机器人能相应调整行为。


<details>
  <summary>Details</summary>
Motivation: 当前协作机器人需要与人类高效安全地协作，但现有研究主要关注检测人类意图，缺乏对人类动机水平和攻击性水平等人类因素的推理能力。

Method: 采用随机建模方法，结合概率模型和控制策略，构建能够预测人类行为和情绪的理论框架，实现双边协作决策。

Result: 通过仿真验证了该方法的有效性，展示了在协作机器人环境中提高安全性和效率的潜力。

Conclusion: 提出的基于随机建模的双边协作方法能够使协作机器人更好地适应人类行为和情绪，为人机协作环境中的安全性和效率提供了理论框架和应用策略。

Abstract: Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.

</details>


### [12] [Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies](https://arxiv.org/abs/2601.14837)
*B. Calmé,N. J. Greenidge,A. Metcalf,A. Bacchetti,G. Loza,D. Kpeglo,P. Lloyd,V. Pensabene,J. H. Chandler,P. Valdastri*

Main category: cs.RO

TL;DR: 研究人员开发了一种直径1.47毫米的模块化软体机器人导管，集成了传感、驱动和治疗功能，能够在腔内导航中实现半自主操作，特别适用于胰腺导管等难以到达的区域。


<details>
  <summary>Details</summary>
Motivation: 软体机器人器械比刚性工具能更安全地导航精细曲折的解剖结构，但临床应用受到尖端功能化不足和实时组织界面反馈有限的限制。目前缺乏足够紧凑、稳健且适应性强的传感和治疗模块来测量和响应腔内手术中的细微生理信号。

Method: 开发了一种直径1.47毫米的模块化软体机器人导管架构，支持最多四个独立控制的功能单元，可定制组合锚定、操作、传感和靶向给药功能。系统结合了学习模型、磁驱动、板载形状传感和视觉标记跟踪的闭环自主/共享控制系统。

Result: 在活体猪模型中，成功演示了半自主部署到胰腺导管内，并在其中进行了7.5厘米的内窥镜导航，这是标准导管目前无法到达的区域。闭环控制系统进一步提高了插管准确性。

Conclusion: 该研究建立了一个可扩展的多功能软体机器人导管平台，为复杂的腔内介入提供了新范式，有望减少辐射暴露、缩短培训时间并加速软体机器人技术的临床转化。

Abstract: Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.

</details>


### [13] [On-the-fly hand-eye calibration for the da Vinci surgical robot](https://arxiv.org/abs/2601.14871)
*Zejian Cui,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: 提出一个用于机器人辅助微创手术的在线手眼标定框架，通过特征关联和标定算法提高电缆驱动机器人工具定位精度


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术中，电缆驱动机器人（如达芬奇机器人）由于编码器读数错误导致工具定位不准确，影响患者安全和手术成功，需要一种有效的在线标定方法

Method: 提出一个包含两个相互关联算法的标定框架：特征关联模块（无需预训练即可从单目图像中检测关键点并提供鲁棒对应关系）和手眼标定模块（采用多种滤波方法适应不同手术场景）

Result: 在公开视频数据集上测试表明，该框架显著降低了工具定位误差，精度与最先进方法相当但更高效

Conclusion: 提出的在线手眼标定框架能够有效提高电缆驱动手术机器人的工具定位精度，适应不同手术场景，具有时间效率优势

Abstract: In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.

</details>


### [14] [HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation](https://arxiv.org/abs/2601.14874)
*Yara Mahmoud,Yasheerah Yaqoot,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HumanoidVLM：基于视觉语言模型的检索框架，让Unitree G1人形机器人通过第一人称RGB图像自动选择任务相关的笛卡尔阻抗参数和夹爪配置，实现自适应操作。


<details>
  <summary>Details</summary>
Motivation: 目前大多数人形机器人控制器依赖固定的、手动调优的阻抗增益和夹爪设置，难以适应多样化的物体和任务需求。需要一种能够根据视觉语义感知自动调整控制参数的方法。

Method: 结合视觉语言模型进行语义任务推断，使用基于FAISS的检索增强生成模块从两个自定义数据库中检索经过实验验证的刚度-阻尼对和物体特定的抓取角度，通过任务空间阻抗控制器执行。

Result: 在14个视觉场景中达到93%的检索准确率。实际实验中表现出稳定的交互动力学，z轴跟踪误差通常在1-3.5厘米内，虚拟力与任务相关的阻抗设置一致。

Conclusion: 将语义感知与基于检索的控制相结合，为人形机器人的自适应操作提供了一条可解释的路径，证明了这种方法的可行性。

Abstract: Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.

</details>


### [15] [Vision-Language Models on the Edge for Real-Time Robotic Perception](https://arxiv.org/abs/2601.14921)
*Sarat Ahmad,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.RO

TL;DR: 该研究探索在6G边缘智能架构（ORAN/MEC）上部署视觉语言模型，使用人形机器人作为测试平台，比较边缘与云端部署的性能差异


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在机器人感知和交互中具有重要作用，但实际部署面临延迟、资源限制和云端隐私风险等挑战，需要探索边缘智能解决方案

Method: 使用Unitree G1人形机器人作为测试平台，设计基于WebRTC的多模态数据流管道，在ORAN/MEC边缘节点部署LLaMA-3.2-11B-Vision-Instruct模型，并与云端部署进行实时性能比较，同时评估轻量级模型Qwen2-VL-2B-Instruct

Result: 边缘部署在保持接近云端准确性的同时，将端到端延迟降低了5%；轻量级模型Qwen2-VL-2B-Instruct实现了亚秒级响应，延迟减少超过一半，但准确性有所下降

Conclusion: 边缘智能架构为视觉语言模型的实时机器人应用提供了可行的解决方案，在延迟和准确性之间提供了权衡选择，轻量级模型适合资源受限环境

Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.

</details>


### [16] [TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control](https://arxiv.org/abs/2601.14945)
*Yuteng Sun,Haoran Wang,Ruofei Bai,Zhengguo Li,Jun Li,Meng Yee,Chuah,Wei Yun Yau*

Main category: cs.RO

TL;DR: TIDAL是一个分层框架，通过解耦语义推理与高频执行来解决VLA模型延迟问题，实现约9Hz控制更新，在动态拦截任务中性能提升2倍。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言-动作模型存在高推理延迟，只能采用低频批处理执行模式，导致在动态环境中目标移动时出现执行盲点，限制了实时控制能力。

Method: 提出TIDAL框架：1）双频架构分离语义推理与高频执行；2）低频宏意图循环缓存语义嵌入；3）高频微控制循环交错单步流集成与执行；4）时间错位训练策略学习预测补偿；5）加入差分运动预测器增强速度感知。

Result: 在边缘硬件上实现约9Hz控制更新（基线约2.4Hz），动态拦截任务性能提升2倍，反馈频率提高4倍，语义嵌入有效范围超出原生动作块大小，在非暂停推理协议下保持鲁棒性。

Conclusion: TIDAL通过架构级解耦解决了VLA模型的延迟问题，实现了语义推理与高频执行的平衡，显著提升了动态环境中的控制性能，为实时视觉-语言-动作系统提供了可行方案。

Abstract: Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.

</details>


### [17] [HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV](https://arxiv.org/abs/2601.14973)
*Faryal Batool,Iana Zhura,Valerii Serpiva,Roohan Ahmed Khan,Ivan Valuev,Issatay Tokmurziyev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HumanDiffusion：基于RGB图像的轻量级扩散规划器，用于无人机在紧急场景中实现人类感知导航，无需先验地图或复杂规划管道


<details>
  <summary>Details</summary>
Motivation: 在紧急救援场景中，需要能够检测人类、推断导航目标并在动态环境中安全操作的自主系统，以实现可靠的人机协作

Method: 结合YOLO-11人类检测与扩散驱动的轨迹生成，直接从RGB图像生成人类感知的导航轨迹，在像素空间预测轨迹以确保平滑运动和安全距离

Result: 在300个样本测试集上实现像素空间轨迹重建的均方误差0.02；真实世界实验中，在部分遮挡的应急响应和搜索定位任务中总体任务成功率80%

Conclusion: 人类条件扩散规划为时间关键型援助场景中的人类感知无人机导航提供了实用且鲁棒的解决方案

Abstract: Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.

</details>


### [18] [Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)](https://arxiv.org/abs/2601.14998)
*Adip Ranjan Das,Maria Koskinopoulou*

Main category: cs.RO

TL;DR: 提出eGRAP系统，通过视觉、动态规划和双机械臂执行实现电子设备的自主拆解，在硬盘驱动器上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 电子废弃物快速增长而回收率低，需要自动化拆解解决方案来提高回收效率。

Method: 使用带摄像头的机械臂识别零件和姿态估计，有向图编码拆解顺序，调度器基于拓扑排序选择下一步操作并分配给两个机械臂并行执行。

Result: 在3.5英寸硬盘驱动器上成功演示，实现了完整的自主拆解，具有高成功率和高效的循环时间。

Conclusion: eGRAP系统能够实时自适应协调双机械臂任务，为电子废弃物自动化拆解提供了有效解决方案。

Abstract: E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method's ability to adaptively coordinate dual-arm tasks in real time.

</details>


### [19] [DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints](https://arxiv.org/abs/2601.15006)
*Fumiya Ohnishi,Masaki Takahashi*

Main category: cs.RO

TL;DR: DWPP方法通过重新设计速度指令计算过程，在速度空间中考虑速度加速度约束，避免违反约束的速度指令，提升路径跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统纯追踪方法未显式考虑速度和加速度约束，导致指令速度与实际速度存在差异，造成超调和跟踪性能下降。

Method: 提出动态窗口纯追踪(DWPP)，在速度空间(v-ω平面)中重新设计速度指令计算过程，从动态窗口中选择最接近ω=κv线的速度指令点。

Result: 实验结果表明DWPP能够避免违反约束的指令，相比传统纯追踪方法获得更优的路径跟踪精度，已集成到Nav2官方仓库。

Conclusion: DWPP通过显式纳入速度加速度约束，有效解决了传统纯追踪方法的局限性，提升了移动机器人路径跟踪性能。

Abstract: Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$ω$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $ω= κv$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).

</details>


### [20] [Risk Estimation for Automated Driving](https://arxiv.org/abs/2601.15018)
*Leon Tolksdorf,Arturo Tejada,Jonas Bauernfeind,Christian Birkner,Nathan van de Wouw*

Main category: cs.RO

TL;DR: 该论文提出了一种结合碰撞概率估计和碰撞严重性的通用风险估计方法，用于自动驾驶车辆的运动规划和安全评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全评估需要准确的风险估计，现有方法要么依赖经验模型，要么采用严重近似，缺乏通用性和准确性。风险的两个关键方面是状态估计的不确定性和碰撞事件的严重性。

Method: 结合碰撞概率估计的最新进展与碰撞严重性概念，开发通用风险估计方法。该方法可为不同碰撞类型（如正面或侧面碰撞）分配单独的严重性函数。

Result: 提出的方法计算效率高，适合实时运动规划应用。提供了高斯不确定性示例实现的编程代码。

Conclusion: 该方法为自动驾驶车辆提供了准确、通用的风险估计框架，能够基于不确定性和潜在碰撞严重性来约束或最小化风险，实现安全导航。

Abstract: Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.

</details>


### [21] [CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2601.15039)
*Jiyao Zhang,Zhiyuan Ma,Tianhao Wu,Zeyuan Chen,Hao Dong*

Main category: cs.RO

TL;DR: CADGrasp是一个两阶段算法，使用单视角点云输入实现通用灵巧抓取，通过预测稀疏IBS表示和优化生成高质量、无碰撞的抓取姿态。


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中进行灵巧抓取面临巨大挑战，包括灵巧手的高自由度、遮挡问题，以及由于物体几何形状多样和布局复杂导致的潜在碰撞风险。

Method: 提出两阶段算法：第一阶段预测稀疏IBS（场景解耦、接触感知和碰撞感知的表示）作为优化目标；第二阶段基于稀疏IBS开发能量函数和排序策略进行优化，生成高质量灵巧抓取姿态。使用占用扩散模型增强高维表示预测。

Result: 在模拟和真实世界环境中进行了大量实验，验证了方法的有效性，证明其能够在保持高抓取成功率的同时减少碰撞，适用于多种物体和复杂场景。

Conclusion: CADGrasp算法通过稀疏IBS表示和两阶段优化策略，成功解决了杂乱环境中灵巧抓取的挑战，实现了稳定、无碰撞的高质量抓取。

Abstract: Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.

</details>


### [22] [Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations](https://arxiv.org/abs/2601.15056)
*Maria T. Tagliaferri,Inseung Kang*

Main category: cs.RO

TL;DR: 研究系统调节髋关节外骨骼辅助力矩大小和持续时间对滑倒扰动中稳定性的影响，发现时间参数决定辅助效果，稳定性优化参数比能量优化参数能减少25.7%的全身角动量范围，存在显著个体差异。


<details>
  <summary>Details</summary>
Motivation: 跌倒对老年人危害极大，现有外骨骼控制器主要优化行走能量消耗而非稳定性，特定参数（如辅助大小和持续时间）对稳定性的影响尚不明确，需要填补这一研究空白。

Method: 使用双侧髋关节外骨骼，在8名健康成年人滑倒扰动实验中系统调节辅助力矩的大小和持续时间，通过全身角动量（WBAM）量化稳定性，并与现有能量优化控制器进行比较。

Result: 辅助大小和持续时间存在显著交互作用，持续时间决定外骨骼辅助是稳定还是失稳；实验确定的稳定性最优参数比能量优化参数平均减少25.7%的WBAM范围；观察到显著的个体间变异性。

Conclusion: 仅优化能量消耗不足以改善步态扰动中的反应稳定性；稳定性导向的外骨骼控制应优先考虑时间参数并包含用户个性化定制，这对改善老年人稳定性和降低跌倒风险有直接意义。

Abstract: Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.

</details>


### [23] [Influence of Operator Expertise on Robot Supervision and Intervention](https://arxiv.org/abs/2601.15069)
*Yanran Jiang,Pavan Sikka,Leimin Tian,Dana Kuliic,Cecile Paris*

Main category: cs.RO

TL;DR: 研究探索不同专业水平的用户如何监督自主机器人，发现新手、中级和专家用户在干预时机和决策策略上存在差异模式


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主性提高，监督机器人的用户群体越来越多样化，需要理解不同专业水平的用户如何执行监督任务，以及这对人机团队绩效的影响

Method: 通过用户研究(N=27)，让参与者监督机器人在模拟器中自主探索四个未知隧道环境，当他们认为机器人遇到困难时提供航点进行干预，分析交互数据和问卷回答

Result: 识别出新手、中级和专家用户在干预时机和决策策略上的不同模式

Conclusion: 用户专业水平影响监督自主机器人的方式，理解这些差异有助于设计更有效的机器人监督系统

Abstract: With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.

</details>


### [24] [V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks](https://arxiv.org/abs/2601.15164)
*Yaru Liu,Ao-bo Wang,Nanyang Ye*

Main category: cs.RO

TL;DR: V-CAGE是一个闭环框架，通过上下文感知的场景实例化和分层指令分解，结合VLM验证循环，生成物理合理且语义对齐的机器人操作数据集，显著提升下游策略性能。


<details>
  <summary>Details</summary>
Motivation: 当前从合成数据学习长时程具身行为面临三大挑战：生成的场景物理上不合理、语言驱动程序经常"成功"但不满足任务语义、高层指令需要接地到可执行动作序列。这些限制导致数据集质量不高，影响下游策略学习效果。

Method: 1. 上下文感知实例化机制：通过动态维护禁止空间区域地图，确保物体放置时的几何一致性，防止穿透并保证可到达的无冲突配置。2. 分层指令分解模块：将高层目标分解为组合动作基元，促进连贯的长时程规划。3. VLM验证循环：作为视觉评判器，在每个子任务后进行严格的拒绝采样，过滤掉代码执行但未达到视觉目标的"静默失败"。

Result: 实验表明，V-CAGE生成的数椐集具有优越的物理和语义保真度，与非验证基线相比，显著提高了下游策略的成功率和泛化能力。

Conclusion: V-CAGE通过闭环验证框架解决了合成数据生成中的物理合理性和语义对齐问题，为大规模生成高质量的机器人操作数据集提供了有效解决方案，能够显著提升下游策略的学习效果。

Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently "succeed" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., "get ready for work") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out "silent failures" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.

</details>
