{"id": "2512.03166", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03166", "abs": "https://arxiv.org/abs/2512.03166", "authors": ["Aya Taourirte", "Md Sohag Mia"], "title": "Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments", "comment": null, "summary": "The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u548c\u5e73\u5747\u573a\u7406\u8bba\u89e3\u51b3\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u3001\u590d\u6742\u534f\u4f5c\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u8db3\u7403\u7b49\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u90e8\u7f72\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u5b9e\u65f6\u51b3\u7b56\u3001\u590d\u6742\u534f\u4f5c\u548c\u53ef\u6269\u5c55\u7b97\u6cd5\u4ee5\u907f\u514d\u7ef4\u5ea6\u707e\u96be\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u591a\u7c92\u5ea6\u4efb\u52a1\uff08\u957f\u671f\u7b56\u7565vs\u5373\u65f6\u52a8\u4f5c\uff09\u548c\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4ea4\u4e92\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "1. \u57fa\u4e8e\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\u4f7f\u7528PPO\u5efa\u7acb\u5b9e\u65f6\u52a8\u4f5c\u8c03\u5ea6\u57fa\u7ebf\uff1b2. \u5f15\u5165\u57fa\u4e8e\u9009\u9879\u6846\u67b6\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7ed3\u6784\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u9ad8\u5c42\u8f68\u8ff9\u89c4\u5212\u5c42\uff08\u5efa\u6a21\u4e3a\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff09\u548c\u4f4e\u5c42\u52a8\u4f5c\u6267\u884c\u5c42\uff1b3. \u5c06\u5e73\u5747\u573a\u7406\u8bba\u96c6\u6210\u5230HRL\u6846\u67b6\u4e2d\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7b80\u5316\u4e3a\u5355\u4e2a\u667a\u80fd\u4f53\u4e0e\u7fa4\u4f53\u5e73\u5747\u7684\u4ea4\u4e92\u3002", "result": "PPO\u57fa\u7ebf\u8868\u73b0\u4f18\u5f02\uff08\u5e73\u57474.32\u4e2a\u8fdb\u7403\uff0c82.9%\u63a7\u7403\u7387\uff09\uff1bHRL\u7ed3\u6784\u63d0\u9ad8\u4e86\u5168\u5c40\u7b56\u7565\uff08\u5e73\u5747\u8fdb\u7403\u589e\u81f35.26\uff09\uff1b\u5e73\u5747\u573a\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u5e73\u57475.93\u4e2a\u8fdb\u7403\uff0c89.1%\u63a7\u7403\u7387\uff0c92.3%\u4f20\u7403\u51c6\u786e\u7387\uff09\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u589e\u5f3a\u3002", "conclusion": "\u5728Webots\u73af\u5883\u4e2d\u8fdb\u884c\u76844v4\u6bd4\u8d5b\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u9886\u57df\u4e2d\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u548c\u534f\u4f5c\u884c\u4e3a\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03256", "abs": "https://arxiv.org/abs/2512.03256", "authors": ["Albert H. Li", "Ivan Dario Jimenez Rodriguez", "Joel W. Burdick", "Yisong Yue", "Aaron D. Ames"], "title": "KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems", "comment": null, "summary": "Long-horizon dynamical prediction is fundamental in robotics and control, underpinning canonical methods like model predictive control. Yet, many systems and disturbance phenomena are difficult to model due to effects like nonlinearity, chaos, and high-dimensionality. Koopman theory addresses this by modeling the linear evolution of embeddings of the state under an infinite-dimensional linear operator that can be approximated with a suitable finite basis of embedding functions, effectively trading model nonlinearity for representational complexity. However, explicitly computing a good choice of basis is nontrivial, and poor choices may cause inaccurate forecasts or overfitting. To address this, we present Kalman-Implicit Koopman Operator (KALIKO) Learning, a method that leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. KALIKO produces interpretable representations consistent with both theory and prior works, yielding high-quality reconstructions and inducing a globally linear latent dynamics. Evaluated on wave data generated by a high-dimensional PDE, KALIKO surpasses several baselines in open-loop prediction and in a demanding closed-loop simulated control task: stabilizing an underactuated manipulator's payload by predicting and compensating for strong wave disturbances.", "AI": {"tldr": "KALIKO\u65b9\u6cd5\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u9690\u5f0f\u5b66\u4e60\u5d4c\u5165\u8868\u793a\uff0c\u5b9e\u73b0\u957f\u65f6\u57df\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u5728\u6ce2\u6d6a\u6270\u52a8\u8865\u507f\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u957f\u65f6\u57df\u52a8\u529b\u5b66\u9884\u6d4b\u5728\u673a\u5668\u4eba\u5b66\u548c\u63a7\u5236\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u7cfb\u7edf\u7531\u4e8e\u975e\u7ebf\u6027\u3001\u6df7\u6c8c\u548c\u9ad8\u7ef4\u7279\u6027\u96be\u4ee5\u5efa\u6a21\u3002Koopman\u7406\u8bba\u901a\u8fc7\u7ebf\u6027\u7b97\u5b50\u5efa\u6a21\u72b6\u6001\u5d4c\u5165\u7684\u6f14\u5316\uff0c\u4f46\u663e\u5f0f\u8ba1\u7b97\u5408\u9002\u7684\u57fa\u51fd\u6570\u5f88\u56f0\u96be\uff0c\u4e14\u9009\u62e9\u4e0d\u5f53\u4f1a\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u6216\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faKALIKO\uff08Kalman-Implicit Koopman Operator Learning\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u9690\u5f0f\u5b66\u4e60\u4e0e\u6f5c\u5728\u52a8\u529b\u5b66\u5bf9\u5e94\u7684\u5d4c\u5165\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f\u7f16\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u4fdd\u6301\u5168\u5c40\u7ebf\u6027\u6f5c\u5728\u52a8\u529b\u5b66\u3002", "result": "\u5728\u9ad8\u7ef4PDE\u751f\u6210\u7684\u6ce2\u6d6a\u6570\u636e\u4e0a\u8bc4\u4f30\uff0cKALIKO\u5728\u5f00\u73af\u9884\u6d4b\u548c\u95ed\u73af\u63a7\u5236\u4efb\u52a1\u4e2d\u5747\u8d85\u8d8a\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002\u7279\u522b\u5728\u7a33\u5b9a\u6b20\u9a71\u52a8\u673a\u68b0\u81c2\u8f7d\u8377\u7684\u4efb\u52a1\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u548c\u8865\u507f\u5f3a\u6ce2\u6d6a\u6270\u52a8\u3002", "conclusion": "KALIKO\u901a\u8fc7\u9690\u5f0f\u5b66\u4e60\u5d4c\u5165\u8868\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86Koopman\u7b97\u5b50\u5b66\u4e60\u4e2d\u7684\u57fa\u51fd\u6570\u9009\u62e9\u95ee\u9898\uff0c\u5728\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u9884\u6d4b\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5904\u7406\u975e\u7ebf\u6027\u3001\u9ad8\u7ef4\u7cfb\u7edf\u7684\u957f\u65f6\u57df\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2512.03347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03347", "abs": "https://arxiv.org/abs/2512.03347", "authors": ["William van den Bogert", "Gregory Linkowski", "Nima Fazeli"], "title": "GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation", "comment": "8 pages, 8 figures, 2 tables", "summary": "Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GOMPsite.", "AI": {"tldr": "GOMP\u662f\u4e00\u79cd\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u975e\u521a\u6027\u6293\u53d6\u7269\u4f53\u7ea6\u675f\u5230\u4f4e\u7ef4\u6d41\u5f62\u6765\u51cf\u5c11\u7d2f\u79ef\u8bef\u5dee\uff0c\u63d0\u9ad8\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5de5\u4e1a\u88c5\u914d\u7b49\u91cd\u590d\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u901a\u5e38\u56e0\u7d2f\u79ef\u8bef\u5dee\u5bfc\u81f4\u8f68\u8ff9\u7cbe\u5ea6\u4e0d\u8db3\u800c\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e9b\u8bef\u5dee\uff0c\u7279\u522b\u662f\u5728\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51faGrasped Object Manifold Projection (GOMP)\u65b9\u6cd5\uff0c\u5c06\u975e\u521a\u6027\u6293\u53d6\u7269\u4f53\u7ea6\u675f\u5230\u4f4e\u7ef4\u6d41\u5f62\u3002\u6240\u6709\u589e\u5f3a\u90fd\u4ece\u8bad\u7ec3\u57fa\u7840\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u540c\u4e00\u4e13\u5bb6\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7n\u81c2\u8001\u864e\u673a\u4ea4\u4e92\u7ec4\u4ef6\u8fdb\u884c\u8c03\u6574\u3002", "result": "\u5728\u56db\u4e2a\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u4e0a\u4f7f\u7528\u89e6\u89c9\u53cd\u9988\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u6307\u51fa\u8be5\u65b9\u6cd5\u4fdd\u6301\u6a21\u6001\u65e0\u5173\u6027\u3002\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660eGOMP\u6539\u8fdb\u4e86\u6a21\u4eff\u5b66\u4e60\u6587\u732e\u4e2d\u5df2\u77e5\u7684\u7d2f\u79ef\u8bef\u5dee\u754c\u9650\u3002", "conclusion": "GOMP\u901a\u8fc7\u6d41\u5f62\u6295\u5f71\u548c\u4ea4\u4e92\u5f0f\u8c03\u6574\u6709\u6548\u51cf\u5c11\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03397", "abs": "https://arxiv.org/abs/2512.03397", "authors": ["Seungwon Choi", "Dong-Gyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing", "comment": null, "summary": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.", "AI": {"tldr": "Surfel-LIO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u4f53\u7d20\u7ed3\u6784\u548c\u9884\u8ba1\u7b97\u9762\u5143\u8868\u793a\u7684\u6fc0\u5149\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7O(1)\u5bf9\u5e94\u5173\u7cfb\u68c0\u7d22\u548cZ-order\u66f2\u7ebf\u7f16\u7801\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709LIO\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6700\u8fd1\u90bb\u641c\u7d22\u9700\u8981\u68c0\u67e5\u591a\u4e2a\u7a7a\u95f4\u5355\u5143\u4ee5\u83b7\u53d6\u8db3\u591f\u7684\u70b9\u8fdb\u884c\u5e73\u9762\u62df\u5408\uff1b2\uff09\u5c3d\u7ba1\u5730\u56fe\u51e0\u4f55\u672a\u53d8\uff0c\u4f46\u5e73\u9762\u53c2\u6570\u901a\u5e38\u5728\u6bcf\u4e2a\u8fed\u4ee3\u4e2d\u91cd\u65b0\u8ba1\u7b97\u3002\u8fd9\u4e9b\u56e0\u7d20\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faSurfel-LIO\u65b9\u6cd5\uff0c\u91c7\u7528\u5c42\u6b21\u4f53\u7d20\u7ed3\u6784\uff08hVox\uff09\u548c\u9884\u8ba1\u7b97\u7684\u9762\u5143\u8868\u793a\u3002\u8be5\u8bbe\u8ba1\u5b9e\u73b0\u4e86O(1)\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u5bf9\u5e94\u5173\u7cfb\u68c0\u7d22\uff0c\u65e0\u9700\u8fd0\u884c\u65f6\u90bb\u5c45\u679a\u4e3e\u6216\u5e73\u9762\u62df\u5408\uff0c\u5e76\u7ed3\u5408Z-order\u66f2\u7ebf\u7f16\u7801\u5b9e\u73b0\u7f13\u5b58\u53cb\u597d\u7684\u7a7a\u95f4\u7d22\u5f15\u3002", "result": "\u5728M3DGR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u8fd1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5feb\u7684\u5904\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "Surfel-LIO\u901a\u8fc7\u521b\u65b0\u7684\u5c42\u6b21\u4f53\u7d20\u7ed3\u6784\u548c\u9884\u8ba1\u7b97\u9762\u5143\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LIO\u7cfb\u7edf\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\uff0c\u4e3a\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03422", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03422", "abs": "https://arxiv.org/abs/2512.03422", "authors": ["Tianchen Deng", "Yue Pan", "Shenghai Yuan", "Dong Li", "Chen Wang", "Mingrui Li", "Long Chen", "Lihua Xie", "Danwei Wang", "Jingchuan Wang", "Javier Civera", "Hesheng Wang", "Weidong Chen"], "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models", "comment": null, "summary": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u4f20\u7edf\u65b9\u6cd5\uff08\u70b9\u4e91\u3001\u4f53\u7d20\u3001SDF\u3001\u573a\u666f\u56fe\uff09\u548c\u795e\u7ecf\u8868\u793a\uff08NeRF\u30013DGS\u3001\u57fa\u7840\u6a21\u578b\uff09\uff0c\u5206\u6790\u5b83\u4eec\u5728\u673a\u5668\u4eba\u4e94\u5927\u6a21\u5757\uff08\u611f\u77e5\u3001\u5efa\u56fe\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u3001\u64cd\u4f5c\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba83D\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u672a\u6765\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u5f53\u524dSLAM\u548c\u5b9a\u4f4d\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u8868\u793a\uff08\u5982\u70b9\u4e91\u548c\u4f53\u7d20\uff09\uff0c\u4f46\u5bc6\u96c6\u573a\u666f\u8868\u793a\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5bfc\u822a\u548c\u907f\u969c\uff09\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff08\u5982NeRF\u30013DGS\u548c\u57fa\u7840\u6a21\u578b\uff09\u80fd\u591f\u6574\u5408\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u548c\u8bed\u8a00\u5148\u9a8c\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u76843D\u573a\u666f\u7406\u89e3\u548c\u5177\u8eab\u667a\u80fd\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5173\u4e8e3D\u573a\u666f\u8868\u793a\u5728\u673a\u5668\u4eba\u4e2d\u5e94\u7528\u7684\u5168\u9762\u8d44\u6e90\u3002", "method": "\u5c06\u673a\u5668\u4eba\u6838\u5fc3\u6a21\u5757\u5206\u4e3a\u4e94\u4e2a\u90e8\u5206\uff08\u611f\u77e5\u3001\u5efa\u56fe\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u3001\u64cd\u4f5c\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u5448\u73b0\u4e0d\u540c\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7684\u6807\u51c6\u516c\u5f0f\u5316\uff0c\u5e76\u6bd4\u8f83\u5404\u6a21\u5757\u4e2d\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002\u901a\u8fc7\u5f00\u6e90\u9879\u76ee\u6301\u7eed\u6536\u96c6\u548c\u66f4\u65b0\u76f8\u5173\u5de5\u4f5c\u548c\u6280\u672f\u3002", "result": "\u63d0\u4f9b\u4e86\u673a\u5668\u4eba\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7684\u5168\u9762\u5206\u7c7b\u548c\u6bd4\u8f83\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u5728\u4e0d\u540c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u548c\u5c40\u9650\u6027\u3002\u7279\u522b\u5f3a\u8c03\u4e863D\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u672a\u6765\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "3D\u57fa\u7840\u6a21\u578b\u6709\u671b\u6210\u4e3a\u672a\u6765\u673a\u5668\u4eba\u5e94\u7528\u7684\u7edf\u4e00\u573a\u666f\u8868\u793a\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6574\u5408\u8bed\u4e49\u7279\u5f81\u548c\u8bed\u8a00\u5148\u9a8c\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u548c\u667a\u80fd\u884c\u4e3a\u3002\u7136\u800c\uff0c\u5b8c\u5168\u5b9e\u73b0\u8fd9\u4e00\u6a21\u578b\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u672c\u6587\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u9879\u76ee\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2512.03522", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03522", "abs": "https://arxiv.org/abs/2512.03522", "authors": ["Gihyeon Lee", "Jungwoo Lee", "Juwon Kim", "Young-Sik Shin", "Younggun Cho"], "title": "MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization", "comment": "Accepted in IEEE Robotics and Automation Letters (2025)", "summary": "Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6807\u7b7e\u4f3c\u7136\u7684\u8bed\u4e49\u56fe\u5339\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6807\u7b7e\u56fe\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u4f3c\u7136\u4f20\u64ad\u6765\u63d0\u5347\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb", "motivation": "\u5728\u672a\u77e5\u7269\u4f53\u7c7b\u522b\u548c\u8bed\u4e49\u6a21\u7cca\u7684\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u8fdb\u884c\u5168\u5c40\u5b9a\u4f4d\u65f6\uff0c\u9ad8\u8bed\u4e49\u6a21\u7cca\u6027\u4f1a\u52a0\u5267\u7269\u4f53\u8bef\u5206\u7c7b\u548c\u9519\u8bef\u5173\u8054\uff0c\u5bfc\u81f4\u59ff\u6001\u4f30\u8ba1\u51fa\u73b0\u663e\u8457\u8bef\u5dee", "method": "\u63d0\u51fa\u591a\u6807\u7b7e\u4f3c\u7136\u8bed\u4e49\u56fe\u5339\u914d\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u6807\u7b7e\u56fe\u8868\u793a\u800c\u975e\u5355\u6807\u7b7e\u8868\u793a\u6765\u6355\u6349\u548c\u5229\u7528\u7269\u4f53\u89c2\u6d4b\u7684\u56fa\u6709\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u4f3c\u7136\u4f20\u64ad\u7ed3\u5408\u8282\u70b9\u4f3c\u7136\u4e0e\u5176\u90bb\u5c45\u7684\u6700\u5927\u4f3c\u7136\u6765\u589e\u5f3a\u56fe\u95f4\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb", "result": "\u5728\u95ed\u96c6\u548c\u5f00\u96c6\u68c0\u6d4b\u914d\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u6570\u636e\u5173\u8054\u548c\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u548c\u5408\u6210\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u5bf9\u5927\u8bcd\u6c47\u91cf\u7269\u4f53\u7c7b\u522b\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u63d0\u51fa\u7684\u591a\u6807\u7b7e\u56fe\u5339\u914d\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u7269\u4f53\u7c7b\u522b\u548c\u8bed\u4e49\u6a21\u7cca\u7684\u73af\u5883\u4e2d"}}
{"id": "2512.03538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03538", "abs": "https://arxiv.org/abs/2512.03538", "authors": ["Yuhang Huang", "Shilong Zou", "Jiazhao Zhang", "Xinwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation", "comment": null, "summary": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.", "AI": {"tldr": "AdaPower\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5c06\u901a\u7528\u4e16\u754c\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u4e13\u4e1a\u4e16\u754c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u7684\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\uff0c\u5728LIBERO\u57fa\u51c6\u4e0a\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u8d85\u8fc741%", "motivation": "\u4e16\u754c\u57fa\u7840\u6a21\u578b\uff08WFMs\uff09\u5177\u6709\u5f3a\u5927\u7684\u89c6\u89c9\u52a8\u6001\u6a21\u62df\u80fd\u529b\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u7cbe\u786e\u673a\u5668\u4eba\u63a7\u5236\u65f6\u5b58\u5728\u751f\u6210\u771f\u5b9e\u6027\u4e0e\u63a7\u5236\u5bfc\u5411\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u73b0\u6709\u65b9\u6cd5\u5c06WFMs\u7528\u4f5c\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u7684VLA\u7b56\u7565", "method": "\u63d0\u51faAdaPower\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TS-TTT\uff09\u7528\u4e8e\u63a8\u7406\u65f6\u9002\u914d\uff1b2\uff09\u8bb0\u5fc6\u6301\u4e45\u6027\uff08MP\uff09\u7528\u4e8e\u957f\u671f\u4e00\u81f4\u6027\u4fdd\u6301\u3002\u8be5\u6846\u67b6\u96c6\u6210\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u5185\uff0c\u5c06\u901a\u7528WFMs\u8f6c\u5316\u4e3a\u4e13\u4e1a\u4e16\u754c\u6a21\u578b", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u8d85\u8fc741%\uff0c\u65e0\u9700\u7b56\u7565\u91cd\u65b0\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u901a\u7528\u80fd\u529b", "conclusion": "AdaPower\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u914d\u65b9\u6cd5\uff0c\u6709\u6548\u5f25\u5408\u4e86\u4e16\u754c\u57fa\u7840\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u7684\u6027\u80fd"}}
{"id": "2512.03548", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03548", "abs": "https://arxiv.org/abs/2512.03548", "authors": ["Zexin Lin", "Yebin Zhong", "Hanwen Wan", "Jiu Cheng", "Zhenglong Sun", "Xiaoqiang Ji"], "title": "A Learning-based Control Methodology for Transitioning VTOL UAVs", "comment": null, "summary": "Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u8fc7\u6e21\u63a7\u5236\u65b9\u6cd5ST3M\uff0c\u5c06\u5de1\u822a\u6a21\u5f0f\u89c6\u4e3a\u60ac\u505c\u7279\u4f8b\uff0c\u6709\u6548\u51cf\u5c11VTOL\u65e0\u4eba\u673a\u8fc7\u6e21\u8fc7\u7a0b\u4e2d\u7684\u632f\u52a8\u5e76\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd", "motivation": "VTOL\u65e0\u4eba\u673a\u5728\u8fc7\u6e21\u8fc7\u7a0b\u4e2d\u7531\u4e8e\u503e\u659c\u8f6c\u5b50\u673a\u5236\u5bfc\u81f4\u91cd\u5fc3\u548c\u63a8\u529b\u65b9\u5411\u53d8\u5316\uff0c\u73b0\u6709\u89e3\u8026\u63a7\u5236\u65b9\u6cd5\u4f1a\u4ea7\u751f\u663e\u8457\u632f\u52a8\uff0c\u4e14\u9650\u5236\u4e86\u4ea4\u4e92\u8003\u8651\u548c\u9002\u5e94\u6027", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u8fc7\u6e21\u63a7\u5236\u65b9\u6cd5ST3M\uff0c\u5c06\u5de1\u822a\u6a21\u5f0f\u89c6\u4e3a\u60ac\u505c\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u91c7\u7528\u65b0\u7684\u63a7\u5236\u89c6\u89d2", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a7\u5236\u5668\u5f00\u53d1\u548c\u8fc1\u79fb\uff0c\u7cbe\u786e\u63a7\u5236\u65e0\u4eba\u673a\u4f4d\u7f6e\u548c\u59ff\u6001\uff0c\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u548c\u51cf\u5c11\u7684\u8fc7\u6e21\u632f\u52a8", "conclusion": "ST3M\u65b9\u6cd5\u4e3aVTOL\u65e0\u4eba\u673a\u8fc7\u6e21\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u8026\u5408\u63a7\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u632f\u52a8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8fc7\u6e21\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027"}}
{"id": "2512.03630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03630", "abs": "https://arxiv.org/abs/2512.03630", "authors": ["Shifa Sulaiman", "Amarnath H", "Simon Bogh", "Naresh Marturi"], "title": "Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations", "comment": null, "summary": "Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u96c5\u53ef\u6bd4\u65b9\u6cd5\u5b9e\u73b0\u4e863\u79cd\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u7528\u4e8e\u5197\u4f59\u673a\u68b0\u81c2\u4e0e\u8026\u5408\u624b\u6307\u5939\u722a\u7684\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u6bd4\u8f83\u4e86\u96c5\u53ef\u6bd4\u8f6c\u7f6e\u3001\u4f2a\u9006\u548c\u963b\u5c3c\u6700\u5c0f\u4e8c\u4e58\u4e09\u79cd\u9006\u89e3\u65b9\u6cd5\u5728\u8f68\u8ff9\u5e73\u6ed1\u6027\u3001\u8bef\u5dee\u7b49\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u5197\u4f59\u673a\u68b0\u81c2\u4e0e\u8026\u5408\u624b\u6307\u5939\u722a\u7cfb\u7edf\u5bfb\u627e\u9ad8\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u96c5\u53ef\u6bd4\u9006\u89e3\u65b9\u6cd5\u6765\u786e\u5b9a\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u9006\u89e3\u6280\u672f\u3002", "method": "\u4f7f\u7528RRT*\u7b97\u6cd5\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u6c42\u89e3\u6b63\u5411\u8fd0\u52a8\u5b66\u65b9\u7a0b\uff0c\u5206\u522b\u91c7\u7528\u96c5\u53ef\u6bd4\u8f6c\u7f6e(JT)\u3001\u4f2a\u9006(PI)\u548c\u963b\u5c3c\u6700\u5c0f\u5e73\u65b9(DLS)\u4e09\u79cd\u65b9\u6cd5\u8ba1\u7b97\u9006\u89e3\uff0c\u5e76\u901a\u8fc7\u87ba\u65cb\u7406\u8bba\u516c\u5f0f\u83b7\u5f97\u7a7a\u95f4\u96c5\u53ef\u6bd4\u548c\u53ef\u64cd\u4f5c\u6027\u5ea6\u91cf\u3002", "result": "\u5206\u6790\u4e86\u751f\u6210\u8f68\u8ff9\u7684\u5e73\u6ed1\u6027\u548cRMSE\u8bef\u5dee\uff0c\u4ee5\u53ca\u5173\u8282\u8fd0\u52a8\u7684\u8fde\u7eed\u6027\u3001\u52a0\u901f\u5ea6\u5206\u5e03\u3001\u6025\u52a8\u5ea6\u548c\u51b2\u51fb\u503c\uff0c\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u786e\u5b9a\u4e86\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u6700\u4f18\u9006\u89e3\u6280\u672f\uff0c\u4e3a\u5197\u4f59\u673a\u68b0\u81c2\u4e0e\u8026\u5408\u5939\u722a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u9009\u62e9\u4f9d\u636e\u3002"}}
{"id": "2512.03639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03639", "abs": "https://arxiv.org/abs/2512.03639", "authors": ["Kilian Schweppe", "Anne-Kathrin Schmuck"], "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction", "comment": null, "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u89e6\u53d1\u5e94\u6025\u535a\u5f08\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u7684\u6218\u7565\u535a\u5f08\u4e0e\u5b9e\u65f6\u52a8\u6001\u5e94\u6025\u535a\u5f08\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u9760\u9ad8\u6548\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u957f\u671f\u6218\u7565\u76ee\u6807\u4e0e\u77ed\u671f\u52a8\u6001\u9002\u5e94\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u4e0d\u786e\u5b9a\u3001\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u8fdb\u5c55\u6027\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a1) \u57fa\u4e8e\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u751f\u6210\u6218\u7565\u535a\u5f08\u7684\u7b56\u7565\u6a21\u677f\uff0c\u4fdd\u8bc1\u9ad8\u5c42\u76ee\u6807\u6ee1\u8db3\uff1b2) \u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u65b0\u6c42\u89e3\u5668\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5904\u7406\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5bfc\u822a\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u3001\u81ea\u9002\u5e94\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u89e6\u53d1\u5e94\u6025\u535a\u5f08\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u6218\u7565\u89c4\u5212\u4e0e\u5b9e\u65f6\u63a7\u5236\uff0c\u4e3a\u4e0d\u786e\u5b9a\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65e2\u5b89\u5168\u53c8\u5177\u8fdb\u5c55\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03684", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03684", "abs": "https://arxiv.org/abs/2512.03684", "authors": ["Shahid Ansari", "Mahendra Kumar Gohil", "Yusuke Maeda", "Bishakh Bhattacharya"], "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "comment": null, "summary": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u756a\u8304\u91c7\u6458\u7cfb\u7edf\uff0c\u91c7\u7528\u7ed3\u5408\u516d\u4e2a\u8f6f\u8d1f\u6cca\u677e\u6bd4\u624b\u6307\u4e0e\u521a\u6027\u5916\u9aa8\u9abc\u7684\u6df7\u5408\u5939\u722a\uff0c\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u548c\u95ed\u73af\u529b\u63a7\u5b9e\u73b0\u8f7b\u67d4\u91c7\u6458\uff0c\u5e73\u5747\u91c7\u6458\u5468\u671f24.34\u79d2\uff0c\u6210\u529f\u7387\u7ea680%\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u6742\u4e71\u73af\u5883\u4e2d\u53ef\u9760\u91c7\u6458\u756a\u8304\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u9700\u8981\u89e3\u51b3\u8f7b\u67d4\u6293\u53d6\u3001\u679c\u5b9e\u5206\u79bb\u3001\u679c\u6897\u5207\u5272\u7b49\u6311\u6218\uff0c\u540c\u65f6\u5e94\u5bf9\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u7b49\u89c6\u89c9\u96be\u9898\u3002", "method": "1. \u6df7\u5408\u5939\u722a\u8bbe\u8ba1\uff1a\u516d\u4e2a\u8f6f\u8d1f\u6cca\u677e\u6bd4\u624b\u6307+\u521a\u6027\u5916\u9aa8\u9abc+\u4e73\u80f6\u7bee\u7b50\uff0c\u5f62\u6210\u7b3c\u5f0f\u6293\u53d6\uff1b2. \u89c6\u89c9\u7cfb\u7edf\uff1aRGB-D\u76f8\u673a+Detectron2\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff08\u6210\u719f/\u672a\u6210\u719f\u756a\u8304\uff09\u548c\u5173\u952e\u70b9\u5b9a\u4f4d\uff08\u679c\u6897\u548c\u679c\u5b9e\u4e2d\u5fc3\uff09\uff1b3. \u529b\u63a7\uff1a\u57fa\u4e8e\u865a\u62df\u529f\u539f\u7406\u5efa\u7acb\u4f3a\u670d\u626d\u77e9\u4e0e\u6293\u53d6\u529b\u5173\u7cfb\u6a21\u578b\uff0c\u4f7f\u7528\u5e26\u529b\u654f\u7535\u963b\u53cd\u9988\u7684PID\u63a7\u5236\u5668\u8fdb\u884c\u95ed\u73af\u529b\u8c03\u8282\uff1b4. \u8f68\u8ff9\u89c4\u5212\uff1a\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u76845\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u7cfb\u7edf\u5b8c\u6210\u5b8c\u6574\u91c7\u6458\u5468\u671f\uff08\u63a5\u8fd1\u3001\u5206\u79bb\u3001\u5207\u5272\u3001\u6293\u53d6\u3001\u8fd0\u8f93\u3001\u91ca\u653e\uff09\uff0c\u5e73\u5747\u5468\u671f\u65f6\u95f424.34\u79d2\uff0c\u603b\u4f53\u6210\u529f\u7387\u7ea680%\uff0c\u6293\u53d6\u529b\u4fdd\u6301\u57280.20-0.50N\u7684\u4f4e\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u5939\u722a\u548c\u96c6\u6210\u89c6\u89c9\u63a7\u5236\u7ba1\u9053\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u5939\u722a\u8bbe\u8ba1\u548c\u96c6\u6210\u89c6\u89c9\u63a7\u5236\u7ba1\u9053\u80fd\u591f\u5b9e\u73b0\u756a\u8304\u7684\u53ef\u9760\u3001\u8f7b\u67d4\u91c7\u6458\uff0c\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u519c\u4e1a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03707", "abs": "https://arxiv.org/abs/2512.03707", "authors": ["Sundas Rafat Mulkana", "Ronyu Yu", "Tanaya Guha", "Emma Li"], "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration", "comment": "8 pages, 7 figures", "summary": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.", "AI": {"tldr": "ContactRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u5c06\u63a5\u89e6\u5b89\u5168\u76f4\u63a5\u7eb3\u5165\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u673a\u5668\u4eba\u5b66\u4e60\u81ea\u9002\u5e94\u8fd0\u52a8\u7b56\u7565\u4ee5\u6700\u5c0f\u5316\u4eba\u673a\u63a5\u89e6\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6548\u7387\u3002", "motivation": "\u5728\u534f\u4f5c\u4eba\u673a\u4efb\u52a1\u4e2d\uff0c\u5b89\u5168\u4e0d\u4ec5\u9700\u8981\u907f\u514d\u78b0\u649e\uff0c\u8fd8\u9700\u8981\u786e\u4fdd\u5b89\u5168\u3001\u6709\u610f\u7684\u7269\u7406\u63a5\u89e6\u3002\u5f53\u524d\u65b9\u6cd5\u5728\u63a5\u89e6\u5b89\u5168\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u5904\u7406\u63a5\u89e6\u5b89\u5168\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faContactRL\u6846\u67b6\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u5c06\u63a5\u89e6\u5b89\u5168\u76f4\u63a5\u7eb3\u5165\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u673a\u5668\u4eba\u5b66\u4e60\u81ea\u9002\u5e94\u8fd0\u52a8\u7b56\u7565\u3002\u4e3a\u786e\u4fdd\u90e8\u7f72\u5b89\u5168\uff0c\u8fd8\u4f7f\u7528\u57fa\u4e8e\u52a8\u80fd\u7684\u63a7\u5236\u5c4f\u969c\u51fd\u6570(eCBF)\u5c4f\u853d\u6765\u589e\u5f3a\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cContactRL\u5b9e\u73b0\u4e860.2%\u7684\u4f4e\u5b89\u5168\u8fdd\u89c4\u7387\u548c87.7%\u7684\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7ea6\u675fRL\u57fa\u51c6\u3002\u5728UR3e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684360\u6b21\u5c0f\u7269\u4f53\u4ece\u4eba\u624b\u4ea4\u63a5\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u6d4b\u91cf\u7684\u6cd5\u5411\u529b\u59cb\u7ec8\u4f4e\u4e8e10N\uff0c\u8bc1\u5b9e\u4e86\u5b89\u5168\u63a5\u89e6\u3002", "conclusion": "ContactRL\u5b9e\u73b0\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u7269\u7406\u534f\u4f5c\uff0c\u63a8\u52a8\u4e86\u534f\u4f5c\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u90e8\u7f72\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u5b89\u5168\u6846\u67b6\u3002"}}
{"id": "2512.03729", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03729", "abs": "https://arxiv.org/abs/2512.03729", "authors": ["Samantha Chapin", "Kenneth Stewart", "Roxana Leontie", "Carl Glen Henshaw"], "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing", "comment": "iSpaRo 2025, Best Paper Award in Orbital Robotics", "summary": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.", "AI": {"tldr": "APIARY\u5b9e\u9a8c\u9996\u6b21\u5728\u592a\u7a7a\u96f6\u91cd\u529b\u73af\u5883\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\uff0c\u901a\u8fc7NASA Astrobee\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u9a8c\u8bc1\u4e86RL\u5728\u592a\u7a7a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u592a\u7a7a\u96f6\u91cd\u529b\u73af\u5883\u4e0b\u63a7\u5236\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u7684\u6f5c\u529b\uff0c\u4e3a\u592a\u7a7a\u63a2\u7d22\u3001\u7269\u6d41\u548c\u5b9e\u65f6\u4efb\u52a1\u9700\u6c42\u5f00\u53d1\u5feb\u901f\u90e8\u7f72\u7684\u81ea\u4e3b\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6f14\u5458-\u8bc4\u8bba\u5bb6PPO\u7b97\u6cd5\u76846\u81ea\u7531\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u5728NVIDIA Isaac Lab\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u901a\u8fc7\u968f\u673a\u5316\u76ee\u6807\u4f4d\u59ff\u548c\u8d28\u91cf\u5206\u5e03\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u7136\u540e\u8fdb\u884c\u5730\u9762\u6d4b\u8bd5\u548c\u592a\u7a7a\u98de\u884c\u9a8c\u8bc1\u3002", "result": "2025\u5e745\u670827\u65e5\u6210\u529f\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u4f7f\u7528NASA Astrobee\u673a\u5668\u4eba\u5b9e\u73b0\u4e86\u9996\u6b21\u592a\u7a7a\u4e2d\u7684RL\u63a7\u5236\u81ea\u7531\u98de\u884c\u5668\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86RL\u5728\u592a\u7a7a\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u9879\u5728\u8f68\u6f14\u793a\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u592a\u7a7a\u63a2\u7d22\u3001\u7269\u6d41\u548c\u5b9e\u65f6\u4efb\u52a1\u9700\u6c42\u5feb\u901f\u5f00\u53d1\u548c\u90e8\u7f72\u5b9a\u5236\u5316\u884c\u4e3a\u3002"}}
{"id": "2512.03743", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03743", "abs": "https://arxiv.org/abs/2512.03743", "authors": ["Kehlani Fay", "Darin Anthony Djapri", "Anya Zorin", "James Clinton", "Ali El Lahib", "Hao Su", "Michael T. Tolley", "Sha Yi", "Xiaolong Wang"], "title": "Cross-embodied Co-design for Dexterous Hands", "comment": null, "summary": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u673a\u5668\u4eba\u624b\u5f62\u6001\u4e0e\u63a7\u5236\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u572824\u5c0f\u65f6\u5185\u5b8c\u6210\u4ece\u8bbe\u8ba1\u3001\u8bad\u7ec3\u3001\u5236\u9020\u5230\u90e8\u7f72\u7684\u5168\u6d41\u7a0b", "motivation": "\u7075\u5de7\u64cd\u4f5c\u53d7\u9650\u4e8e\u63a7\u5236\u548c\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u5173\u4e8e\u4ec0\u4e48\u4f7f\u673a\u68b0\u624b\u6700\u9002\u5408\u6267\u884c\u7075\u5de7\u4efb\u52a1\u7684\u5171\u8bc6\uff0c\u9700\u8981\u89e3\u51b3\u5982\u4f55\u8bbe\u8ba1\u548c\u63a7\u5236\u9488\u5bf9\u7075\u5de7\u6027\u4f18\u5316\u7684\u673a\u5668\u4eba\u673a\u68b0\u624b\u8fd9\u4e00\u6839\u672c\u6311\u6218", "method": "\u63d0\u51fa\u4e00\u4e2a\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u624b\u5f62\u6001\u548c\u4e92\u8865\u7684\u7075\u5de7\u63a7\u5236\u7b56\u7565\uff0c\u652f\u6301\uff1a1\uff09\u5305\u62ec\u5173\u8282\u3001\u624b\u6307\u548c\u624b\u638c\u751f\u6210\u7684\u5e7f\u6cdb\u5f62\u6001\u641c\u7d22\u7a7a\u95f4\uff1b2\uff09\u901a\u8fc7\u5f62\u6001\u6761\u4ef6\u8de8\u5b9e\u4f53\u63a7\u5236\u5b9e\u73b0\u5e7f\u6cdb\u8bbe\u8ba1\u7a7a\u95f4\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\uff1b3\uff09\u4f7f\u7528\u53ef\u8bbf\u95ee\u7ec4\u4ef6\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u5236\u9020", "result": "\u5728\u591a\u4e2a\u7075\u5de7\u4efb\u52a1\u4e0a\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u5305\u62ec\u6a21\u62df\u548c\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u624b\u5185\u65cb\u8f6c\u4efb\u52a1\uff0c\u6846\u67b6\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u80fd\u591f\u572824\u5c0f\u65f6\u5185\u8bbe\u8ba1\u3001\u8bad\u7ec3\u3001\u5236\u9020\u548c\u90e8\u7f72\u65b0\u7684\u673a\u5668\u4eba\u624b", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u624b\u5f62\u6001\u4e0e\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8fed\u4ee3\u548c\u90e8\u7f72\uff0c\u5b8c\u6574\u6846\u67b6\u5c06\u5f00\u6e90\u5e76\u5728\u7f51\u7ad9\u4e0a\u63d0\u4f9b"}}
{"id": "2512.03756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03756", "abs": "https://arxiv.org/abs/2512.03756", "authors": ["Marlon Steiner", "Royden Wagner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models", "comment": "In Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, AUSTRALIA, 18-21 November 2025", "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u5bfc\u822a\u4fe1\u606f\u96c6\u6210\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u4ee5\u5f25\u5408\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e0e\u57fa\u4e8e\u76ee\u6807\u7684\u8fd0\u52a8\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u591a\u79cd\u5bfc\u822a\u96c6\u6210\u7b56\u7565\u3002", "motivation": "\u7ed3\u5408\u8fd0\u52a8\u9884\u6d4b\u548c\u8fd0\u52a8\u89c4\u5212\u4e3a\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4e24\u4e2a\u6311\u6218\uff1a\u5982\u4f55\u57fa\u4e8e\u5bfc\u822a\u76ee\u6807\u8fdb\u884c\u9884\u6d4b\uff0c\u4ee5\u53ca\u5982\u4f55\u786e\u4fdd\u7a33\u5b9a\u4e14\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\u3002\u672c\u6587\u4e3b\u8981\u89e3\u51b3\u7b2c\u4e00\u4e2a\u6311\u6218\uff0c\u5373\u5982\u4f55\u5c06\u5bfc\u822a\u4fe1\u606f\u96c6\u6210\u5230\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u5c06\u5bfc\u822a\u4fe1\u606f\u6269\u5c55\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u81ea\u8f66\u7684\u9884\u671f\u8def\u7ebf\u548c\u76ee\u6807\u59ff\u6001\u96c6\u6210\u5230\u6a21\u578b\u67b6\u6784\u4e2d\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u67b6\u6784\u5bfc\u822a\u96c6\u6210\u7b56\u7565\uff0c\u5e76\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u5bf9\u8fd9\u4e9b\u7b56\u7565\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9884\u6d4b\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u5177\u6709\u6f5c\u529b\uff0c\u5bfc\u822a\u4fe1\u606f\u53ef\u4ee5\u540c\u65f6\u589e\u5f3a\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u3002\u4f5c\u8005\u5728GitHub\u4e0a\u516c\u5f00\u4e86\u5b9e\u73b0\u4ee3\u7801\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5bfc\u822a\u4fe1\u606f\u96c6\u6210\u5230\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5f25\u5408\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e0e\u57fa\u4e8e\u76ee\u6807\u7684\u8fd0\u52a8\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u9884\u6d4b\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03772", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03772", "abs": "https://arxiv.org/abs/2512.03772", "authors": ["Gabriele Fadini", "Deepak Ingole", "Tong Duy Son", "Alisa Rupenyan"], "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control", "comment": "6 pages, 7 figures, 3 tables", "summary": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u626d\u77e9\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u81ea\u52a8\u8c03\u53c2\u6846\u67b6\uff0c\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u4f18\u5316MPC\u53c2\u6570\uff0c\u5728UR10e\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u672b\u7aef\u8f68\u8ff9\u7cbe\u786e\u8ddf\u8e2a", "motivation": "\u4f20\u7edf\u624b\u52a8\u8c03\u53c2MPC\u53c2\u6570\u8017\u65f6\u4e14\u96be\u4ee5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u4f18\u5316\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u672b\u7aef\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6", "method": "\u91c7\u7528\u7a00\u758f\u8f74\u5bf9\u9f50\u5b50\u7a7a\u95f4\u8d1d\u53f6\u65af\u4f18\u5316(SAASBO)\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\uff0c\u4f18\u5316MPC\u6210\u672c\u51fd\u6570\u6743\u91cd\u548c\u5e95\u5c42\u63a7\u5236\u5668\u589e\u76ca\uff0c\u901a\u8fc7\u4eff\u771f\u6a21\u578b\u5b89\u5168\u63a2\u7d22\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8ddf\u8e2a\u6027\u80fd\u63d0\u534741.9%\uff0c\u6c42\u89e3\u65f6\u95f4\u51cf\u5c112.5%\uff1b\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u8ddf\u8e2a\u6027\u80fd\u63d0\u534725.8%\uff0c\u8bc1\u660e\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u6570\u5b57\u5b6a\u751f\u652f\u6301\u7684\u81ea\u52a8\u53c2\u6570\u4f18\u5316\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6027\u80fd"}}
{"id": "2512.03774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03774", "abs": "https://arxiv.org/abs/2512.03774", "authors": ["Johannes Fischer", "Marlon Steiner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving", "comment": null, "summary": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.", "AI": {"tldr": "\u5c06\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7ed3\u5408\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e3aMPC\u63d0\u4f9b\u65b0\u7684\u5b89\u5168\u53c2\u8003\u8f68\u8ff9\uff0c\u7a81\u7834\u4f20\u7edf\u51f8\u8fd1\u4f3c\u9650\u5236\uff0c\u5bfb\u627e\u5168\u5c40\u6700\u4f18\u89e3\u3002", "motivation": "\u4f20\u7edfMPC\u5728\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4e2d\u91c7\u7528\u51f8\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u6027\uff0c\u4f46\u5c06\u89e3\u9650\u5236\u5728\u5b50\u7a7a\u95f4\u4e2d\uff0c\u53ef\u80fd\u65e0\u6cd5\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\u3002", "method": "\u63d0\u51faSRL-MPC\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u786e\u4fdd\u5b89\u5168\u6027\uff1b2\uff09\u91c7\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u4e8e\u80fd\u91cf\u51fd\u6570\u7684\u5b89\u5168\u6307\u6570\u4f5c\u4e3a\u7ea6\u675f\u76ee\u6807\uff1b3\uff09\u5b66\u4e60\u72b6\u6001\u76f8\u5173\u7684\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u4e0e\u5b89\u5168\u7b56\u7565\uff1b4\uff09\u5c06\u5b66\u4e60\u5230\u7684\u5b89\u5168\u8f68\u8ff9\u4f5c\u4e3aMPC\u7684\u53c2\u8003\u8f68\u8ff9\u3002", "result": "\u5728\u9ad8\u901f\u516c\u8def\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5355\u72ec\u7684MPC\u548cSRL\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408SRL\u548cMPC\u80fd\u591f\u7a81\u7834\u4f20\u7edf\u51f8\u8fd1\u4f3c\u7684\u9650\u5236\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u63a2\u7d22\u66f4\u4f18\u7684\u5168\u5c40\u89e3\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\u3002"}}
{"id": "2512.03795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03795", "abs": "https://arxiv.org/abs/2512.03795", "authors": ["Jia Hu", "Zhexi Lian", "Xuerun Yan", "Ruiang Bi", "Dou Shen", "Yu Ruan", "Haoran Wang"], "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving", "comment": "17 pages, 18 figures", "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.", "AI": {"tldr": "MPCFormer\uff1a\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u53ef\u89e3\u91ca\u793e\u4ea4\u611f\u77e5\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u901a\u8fc7Transformer\u5b66\u4e60\u591a\u8f66\u793e\u4ea4\u4ea4\u4e92\u52a8\u529b\u5b66\uff0c\u5728MPC\u6846\u67b6\u4e0b\u751f\u6210\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u80fd\u529b\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u4ea4\u4e92\u6027\u5f3a\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u8868\u73b0\u51fa\u7c7b\u4eba\u884c\u4e3a\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u7f3a\u4e4f\u5bf9\u793e\u4ea4\u4ea4\u4e92\u5e95\u5c42\u673a\u5236\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u4e0e\u5468\u56f4\u8f66\u8f86\u7684\u4ea4\u4e92\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faMPCFormer\u65b9\u6cd5\uff1a1\uff09\u5c06\u793e\u4ea4\u4ea4\u4e92\u52a8\u529b\u5b66\u5efa\u6a21\u4e3a\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u5d4c\u5165\u7269\u7406\u5148\u9a8c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff1b2\uff09\u901a\u8fc7Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4ece\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u4e2d\u5b66\u4e60\u52a8\u529b\u5b66\u7cfb\u6570\uff1b3\uff09\u5728MPC\u6846\u67b6\u4e0b\u5229\u7528\u5b66\u4e60\u5230\u7684\u793e\u4ea4\u4ea4\u4e92\u52a8\u529b\u5b66\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728NGSIM\u6570\u636e\u96c6\u4e0a\u7684\u5f00\u73af\u8bc4\u4f30\u663e\u793a\uff0cMPCFormer\u5177\u6709\u6700\u4f18\u7684\u793e\u4ea4\u4ea4\u4e92\u611f\u77e5\u80fd\u529b\uff0c5\u79d2\u957f\u65f6\u9884\u6d4b\u7684ADE\u4f4e\u81f30.86\u7c73\u3002\u95ed\u73af\u5b9e\u9a8c\u4e2d\uff0c\u5728\u9ad8\u5f3a\u5ea6\u4ea4\u4e92\u573a\u666f\uff08\u8fde\u7eed\u53d8\u9053\u9a76\u51fa\u531d\u9053\uff09\u4e2d\uff0c\u89c4\u5212\u6210\u529f\u738794.67%\uff0c\u9a7e\u9a76\u6548\u7387\u63d0\u534715.75%\uff0c\u78b0\u649e\u7387\u4ece21.25%\u964d\u81f30.5%\uff0c\u4f18\u4e8e\u524d\u6cbf\u7684\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u5668\u3002", "conclusion": "MPCFormer\u662f\u9996\u4e2a\u663e\u5f0f\u5efa\u6a21\u591a\u8f66\u793e\u4ea4\u4ea4\u4e92\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u5728MPC\u6846\u67b6\u4e0b\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u3001\u5b89\u5168\u4e14\u7c7b\u4eba\u7684\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.03828", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03828", "abs": "https://arxiv.org/abs/2512.03828", "authors": ["Dominykas Strazdas", "Magnus Jung", "Jan Marquenie", "Ingo Siegert", "Ayoub Al-Hamadi"], "title": "IM HERE: Interaction Model for Human Effort Based Robot Engagement", "comment": "8 pages, 5 figures", "summary": "The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.", "AI": {"tldr": "\u63d0\u51faIM HERE\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u52aa\u529b\u7684\u63cf\u8ff0\u5efa\u6a21\u4eba-\u4eba\u3001\u4eba-\u673a\u3001\u673a-\u673a\u4ea4\u4e92\u4e2d\u7684\u53c2\u4e0e\u5ea6\uff0c\u5b9e\u73b0\u793e\u4f1a\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u5206\u6790\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u793e\u4f1a\u89c4\u8303\u9075\u4ece", "motivation": "\u73b0\u6709\u53c2\u4e0e\u5ea6\u5b9a\u4e49\u548c\u6a21\u578b\u8981\u4e48\u8fc7\u4e8e\u6a21\u7cca\uff0c\u8981\u4e48\u7f3a\u4e4f\u8de8\u60c5\u5883\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u96be\u4ee5\u652f\u6301\u6709\u610f\u4e49\u7684\u4eba\u673a\u4ea4\u4e92\u3002\u9700\u8981\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u8ba4\u77e5\u53c2\u4e0e\u52a8\u6001\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u7cfb\u7edf\u7684\u793e\u4f1a\u6574\u5408", "method": "\u5f15\u5165IM HERE\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u52aa\u529b\u7684\u63cf\u8ff0\u65b9\u6cd5\u5206\u6790\u5b9e\u4f53\u95f4\u7684\u53cc\u8fb9\u5173\u7cfb\uff0c\u5c06\u5173\u7cfb\u6a21\u5f0f\u7b80\u5316\u4e3a\u7126\u70b9\u653e\u7f6e\u548c\u56db\u4e2a\u5173\u952e\u72b6\u6001\u3002\u8be5\u6846\u67b6\u80fd\u6355\u6349\u76f8\u4e92\u5173\u7cfb\u3001\u7fa4\u4f53\u884c\u4e3a\u548c\u793e\u4f1a\u89c4\u8303\u9075\u4ece\uff0c\u6574\u5408\u4e3b\u89c2\u611f\u77e5\u548c\u5ba2\u89c2\u72b6\u6001", "result": "\u6846\u67b6\u80fd\u51c6\u786e\u5206\u89e3\u5173\u7cfb\u6a21\u5f0f\uff0c\u8bc6\u522b\u548c\u63cf\u8ff0\u6c9f\u901a\u8bef\u89e3\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u5177\u4f53\u7684\u884c\u4e3a\u6307\u4ee4\u3002\u5b9e\u73b0\u4e86\u793e\u4f1a\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u5206\u6790\u3001\u5efa\u6a21\u548c\u63cf\u8ff0", "conclusion": "IM HERE\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u53c2\u4e0e\u5ea6\uff0c\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u65e2\u80fd\u9075\u4ece\u793e\u4f1a\u89c4\u8303\u5b9e\u73b0\u5b8c\u5168\u793e\u4f1a\u6574\u5408\uff0c\u53c8\u80fd\u8ffd\u6c42\u81ea\u8eab\u7684\u793e\u4f1a\u76ee\u6807\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2512.03874", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03874", "abs": "https://arxiv.org/abs/2512.03874", "authors": ["Lei Zhang", "Diwen Zheng", "Kaixin Bai", "Zhenshan Bing", "Zoltan-Csaba Marton", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance", "comment": "Project Website: https://sites.google.com/view/omnidexvlg, 16 pages", "summary": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.", "AI": {"tldr": "OmniDexVLG\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8bed\u4e49\u611f\u77e5\u7684\u7075\u5de7\u6293\u53d6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u6307\u5bfc\u751f\u6210\u7ed3\u6784\u591a\u6837\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u7075\u5de7\u6293\u53d6\u59ff\u52bf\u3002", "motivation": "\u5f53\u524d\u7075\u5de7\u6293\u53d6\u751f\u6210\u96be\u4ee5\u5b9e\u73b0\u8bed\u4e49\u53ef\u63a7\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u7f3a\u4e4f\u5bf9\u6293\u53d6\u5206\u7c7b\u5b66\u3001\u63a5\u89e6\u8bed\u4e49\u548c\u529f\u80fd\u53ef\u4f9b\u6027\u7b49\u591a\u4e2a\u8bed\u4e49\u7ef4\u5ea6\u7684\u7edf\u4e00\u5efa\u6a21\u3002", "method": "1) OmniDexDataGen\uff1a\u8bed\u4e49\u4e30\u5bcc\u7684\u7075\u5de7\u6293\u53d6\u6570\u636e\u96c6\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u6293\u53d6\u5206\u7c7b\u5b66\u5f15\u5bfc\u7684\u914d\u7f6e\u91c7\u6837\u3001\u529f\u80fd\u53ef\u4f9b\u6027\u63a5\u89e6\u70b9\u91c7\u6837\u3001\u5206\u7c7b\u5b66\u611f\u77e5\u7684\u5fae\u5206\u529b\u95ed\u5408\u6293\u53d6\u91c7\u6837\u3001\u57fa\u4e8e\u7269\u7406\u7684\u4f18\u5316\u9a8c\u8bc1\uff1b2) OmniDexReasoner\uff1a\u591a\u6a21\u6001\u6293\u53d6\u7c7b\u578b\u8bed\u4e49\u63a8\u7406\u6a21\u5757\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u601d\u7ef4\u94fe\u63a8\u7406\uff1b3) \u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6293\u53d6\u751f\u6210\u6a21\u578b\uff0c\u663e\u5f0f\u7ed3\u5408\u6293\u53d6\u5206\u7c7b\u5b66\u3001\u63a5\u89e6\u7ed3\u6784\u548c\u529f\u80fd\u53ef\u4f9b\u6027\u8bed\u4e49\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u6293\u53d6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u591a\u6837\u6027\u3001\u63a5\u89e6\u8bed\u4e49\u591a\u6837\u6027\u3001\u529f\u80fd\u53ef\u4f9b\u6027\u591a\u6837\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "OmniDexVLG\u901a\u8fc7\u7edf\u4e00\u5efa\u6a21\u591a\u4e2a\u8bed\u4e49\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u8bed\u4e49\u611f\u77e5\u7075\u5de7\u6293\u53d6\u751f\u6210\uff0c\u4e3a\u4efb\u52a1\u9700\u6c42\u5bf9\u9f50\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6293\u53d6\u8bed\u4e49\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03886", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03886", "abs": "https://arxiv.org/abs/2512.03886", "authors": ["Brais Fontan-Costas", "M. Diaz-Cacho", "Ruben Fernandez-Boullon", "Manuel Alonso-Carracedo", "Javier Perez-Robles"], "title": "A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments", "comment": null, "summary": "This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c01\u95ed\u8d5b\u9053\u8f66\u8f86\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3001\u8def\u5f84\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6d41\u6c34\u7ebf\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u4e3a\u5728\u5c01\u95ed\u8d5b\u9053\u73af\u5883\u4e2d\u5b9e\u73b0\u8f66\u8f86\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6267\u884c\u7cbe\u786e\u4efb\u52a1\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ec\u73af\u5883\u611f\u77e5\u3001\u7cbe\u786e\u5b9a\u4f4d\u3001\u8f68\u8ff9\u89c4\u5212\u548c\u8f66\u8f86\u63a7\u5236\u7b49\u529f\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5404\u5b50\u7cfb\u7edf\u72ec\u7acb\u8fd0\u884c\u4f46\u901a\u8fc7\u6d41\u6c34\u7ebf\u67b6\u6784\u8fde\u63a5\u6570\u636e\u3002\u7cfb\u7edf\u5305\u542b\u8ba1\u7b97\u673a\u89c6\u89c9\u7528\u4e8e\u73af\u5883\u611f\u77e5\u3001\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7528\u4e8e\u7cbe\u786e\u5b9a\u4f4d\u3001\u8def\u5f84\u89c4\u5212\u7528\u4e8e\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u3001\u63a7\u5236\u6a21\u5757\u7528\u4e8e\u7cbe\u786e\u8f66\u8f86\u6267\u884c\u3002", "result": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7ed3\u5408\u6700\u5148\u8fdb\u6280\u672f\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\uff0c\u5404\u5b50\u7cfb\u7edf\u534f\u540c\u5de5\u4f5c\u5b8c\u6210\u7cbe\u786e\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6d41\u6c34\u7ebf\u6570\u636e\u8fde\u63a5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5c01\u95ed\u8d5b\u9053\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u81ea\u4e3b\u5bfc\u822a\uff0c\u4e3a\u53d7\u63a7\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03891", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03891", "abs": "https://arxiv.org/abs/2512.03891", "authors": ["Ying-Kuan Tsai", "Yi-Ping Chen", "Vispi Karkaria", "Wei Chen"], "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning", "comment": "28 pages, 17 figures", "summary": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u4ee3\u8bbe\u8ba1\u6982\u5ff5\uff0c\u4f18\u5316\u6574\u8f66\u4e3b\u52a8\u60ac\u67b6\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u9002\u5e94\u4e0d\u540c\u9a7e\u9a76\u884c\u4e3a\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u60ac\u67b6\u7cfb\u7edf\u53d7\u9650\u4e8e\u56fa\u5b9a\u786c\u4ef6\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u786e\u5b9a\u548c\u52a8\u6001\u7684\u8fd0\u884c\u6761\u4ef6\u3002\u6570\u5b57\u5b6a\u751f\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e3a\u5b9e\u65f6\u6570\u636e\u9a71\u52a8\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5c06\u8fd9\u4e9b\u6280\u672f\u96c6\u6210\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6570\u5b57\u5b6a\u751f\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c06\u81ea\u52a8\u5fae\u5206\u96c6\u6210\u5230\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8054\u5408\u4f18\u5316\u7269\u7406\u60ac\u67b6\u7ec4\u4ef6\u548c\u63a7\u5236\u7b56\u7565\u3002\u91c7\u7528\u591a\u4ee3\u8bbe\u8ba1\u6982\u5ff5\uff0c\u901a\u8fc7\u5206\u4f4d\u6570\u5b66\u4e60\u8fdb\u884c\u6a21\u578b\u66f4\u65b0\u4ee5\u6355\u6349\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff0c\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "result": "\u5728\u6e29\u548c\u548c\u6fc0\u8fdb\u4e24\u79cd\u9a7e\u9a76\u8bbe\u7f6e\u4e0b\uff0c\u4f18\u5316\u7cfb\u7edf\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\uff0c\u63a7\u5236\u52aa\u529b\u5206\u522b\u51cf\u5c11\u7ea643%\u548c52%\uff0c\u540c\u65f6\u4fdd\u6301\u4e58\u5750\u8212\u9002\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5f00\u53d1\u4e86\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\u66f4\u65b0\u7684\u6570\u5b57\u5b6a\u751f\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5f15\u5165\u591a\u4ee3\u8bbe\u8ba1\u7b56\u7565\u5b9e\u73b0\u81ea\u6539\u8fdb\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u9488\u5bf9\u4e0d\u540c\u9a7e\u9a76\u7c7b\u578b\u7684\u4e3b\u52a8\u60ac\u67b6\u7cfb\u7edf\u4e2a\u6027\u5316\u4f18\u5316\u3002"}}
{"id": "2512.03913", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03913", "abs": "https://arxiv.org/abs/2512.03913", "authors": ["Jeongeun Park", "Jihwan Yoon", "Byungwoo Jeon", "Juhan Park", "Jinwoo Shin", "Namhoon Cho", "Kyungjae Lee", "Sangdoo Yun", "Sungjoon Choi"], "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations", "comment": "https://vine-vla.github.io/", "summary": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.", "AI": {"tldr": "VINE\u6a21\u578b\u5229\u7528\u6df7\u5408\u8d28\u91cf\u6570\u636e\u96c6\uff08\u6210\u529f\u4e0e\u5931\u8d25\u6f14\u793a\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u5c06\u9ad8\u5c42\u63a8\u7406\u4e0e\u5e95\u5c42\u63a7\u5236\u5206\u79bb\uff0c\u5229\u7528\u5931\u8d25\u6570\u636e\u4f5c\u4e3a\u7ed3\u6784\u5316\u5b66\u4e60\u4fe1\u53f7\u6765\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u901a\u5e38\u53ea\u4f7f\u7528\u6210\u529f\u7684\u9065\u64cd\u4f5c\u6f14\u793a\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u5927\u91cf\u81ea\u7136\u6536\u96c6\u8fc7\u7a0b\u4e2d\u7684\u5931\u8d25\u5c1d\u8bd5\u3002\u8fd9\u4e9b\u5931\u8d25\u6570\u636e\u5305\u542b\u4e86\u7b56\u7565\u8106\u5f31\u6027\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u53ef\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faVINE\u5206\u5c42\u6a21\u578b\uff1aSystem 2\u8fdb\u884c\u9ad8\u5c42\u63a8\u7406\uff0c\u57282D\u573a\u666f\u56fe\u62bd\u8c61\u4e0a\u6267\u884c\u53ef\u884c\u6027\u5f15\u5bfc\u7684\u6811\u641c\u7d22\uff0c\u63d0\u51fa\u5b50\u76ee\u6807\u8f6c\u6362\uff0c\u4ece\u6210\u529f\u548c\u5931\u8d25\u4e2d\u9884\u6d4b\u6210\u529f\u6982\u7387\uff0c\u5e76\u5728\u6267\u884c\u524d\u4fee\u526a\u8106\u5f31\u5206\u652f\uff1bSystem 1\u6267\u884c\u5e95\u5c42\u52a8\u4f5c\u800c\u4e0d\u4fee\u6539\u6838\u5fc3\u6280\u80fd\u3002\u5b8c\u5168\u57fa\u4e8e\u79bb\u7ebf\u9065\u64cd\u4f5c\u6570\u636e\u8bad\u7ec3\uff0c\u5c06\u8d1f\u9762\u7ecf\u9a8c\u76f4\u63a5\u6574\u5408\u5230\u51b3\u7b56\u5faa\u73af\u4e2d\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u5931\u8d25\u6570\u636e\u662f\u5c06VLA\u6a21\u578b\u7684\u5e7f\u6cdb\u80fd\u529b\u8f6c\u5316\u4e3a\u9c81\u68d2\u6267\u884c\u7684\u91cd\u8981\u8d44\u6e90\u3002", "conclusion": "\u5931\u8d25\u6570\u636e\u662f\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5173\u952e\u8d44\u6e90\uff0c\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u5c06\u5931\u8d25\u4f5c\u4e3a\u7ed3\u6784\u5316\u5b66\u4e60\u4fe1\u53f7\u800c\u975e\u566a\u58f0\u76d1\u7763\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2512.03936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03936", "abs": "https://arxiv.org/abs/2512.03936", "authors": ["Aron Distelzweig", "Yiwei Wang", "Faris Janjo\u0161", "Marcel Hallgarten", "Mihai Dobre", "Alexander Langmann", "Joschka Boedecker", "Johannes Betz"], "title": "Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response", "comment": null, "summary": "Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.", "AI": {"tldr": "BIBeR\u6846\u67b6\u5c06\u8fd0\u52a8\u9884\u6d4b\u4e0e\u535a\u5f08\u8bba\u89c4\u5212\u7edf\u4e00\u4e3a\u4ea4\u4e92\u611f\u77e5\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8fed\u4ee3\u6700\u4f18\u54cd\u5e94\u5faa\u73af\u5b9e\u73b0\u53cc\u5411\u9002\u5e94\uff0c\u5728\u4ea4\u4e92\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7cfb\u7edf\u5728\u5e38\u89c4\u573a\u666f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5bc6\u96c6\u57ce\u5e02\u4ea4\u901a\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7b80\u5355\u4e22\u5f03\u4e0d\u5b89\u5168\u8ba1\u5212\uff0c\u8981\u4e48\u91c7\u7528\u7aef\u5230\u7aef\u5355\u5411\u96c6\u6210\uff0c\u7f3a\u4e4f\u5bf9\u8054\u5408\u9884\u6d4b\u4e0e\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\u7684\u5efa\u6a21\u3002\u535a\u5f08\u8bba\u65b9\u6cd5\u867d\u6709\u7406\u8bba\u4f18\u52bf\u4f46\u5e94\u7528\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u8fed\u4ee3\u6700\u4f18\u54cd\u5e94\uff08BIBeR\uff09\u6846\u67b6\uff0c\u5c06\u5148\u8fdb\u9884\u6d4b\u5668\u96c6\u6210\u5230\u8fed\u4ee3\u6700\u4f18\u54cd\u5e94\u5faa\u73af\u4e2d\uff0c\u901a\u8fc7\u91cd\u590d\u4f18\u5316\u81ea\u8f66\u548c\u5468\u56f4\u8f66\u8f86\u7b56\u7565\u6765\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u3002\u5f15\u5165\u8d1d\u53f6\u65af\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u91cf\u5316\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u6839\u636e\u7f6e\u4fe1\u5ea6\u8c03\u8282\u66f4\u65b0\u5f3a\u5ea6\u3002", "result": "\u5728\u9ad8\u5ea6\u4ea4\u4e92\u7684interPlan\u53d8\u9053\u573a\u666f\u4e2d\uff0cBIBeR\u6bd4\u6700\u5148\u8fdb\u89c4\u5212\u5668\u63d0\u534711%\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6807\u51c6nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BIBeR\u6210\u529f\u7edf\u4e00\u4e86\u8fd0\u52a8\u9884\u6d4b\u548c\u535a\u5f08\u8bba\u89c4\u5212\uff0c\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u89c4\u5212\u7684\u900f\u660e\u6027\u548c\u5b66\u4e60\u6a21\u578b\u7684\u7075\u6d3b\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5728\u5bc6\u96c6\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03958", "abs": "https://arxiv.org/abs/2512.03958", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation", "comment": null, "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.", "AI": {"tldr": "MDE-AgriVLN\u65b9\u6cd5\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u589e\u5f3a\u519c\u4e1a\u673a\u5668\u4eba\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5728\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u5bfc\u822a\u8bef\u5dee\u3002", "motivation": "\u5f53\u524d\u519c\u4e1a\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\u6216\u8f68\u9053\u7cfb\u7edf\u79fb\u52a8\uff0c\u4e14\u901a\u5e38\u53ea\u914d\u5907\u5355\u76ee\u6444\u50cf\u5934\uff0c\u5bfc\u81f4\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u6709\u9650\u3002\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08AgriVLN\uff09\u867d\u7136\u5c06VLN\u6269\u5c55\u5230\u519c\u4e1a\u9886\u57df\uff0c\u4f46\u5355\u76ee\u89c6\u89c9\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6027\u80fd\u3002", "method": "\u63d0\u51faMDE-AgriVLN\u65b9\u6cd5\uff0c\u5f15\u5165\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u6a21\u5757\uff0c\u4eceRGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u7279\u5f81\uff0c\u8f85\u52a9\u51b3\u7b56\u6a21\u5757\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\uff0c\u4ece\u800c\u589e\u5f3a\u519c\u4e1a\u673a\u5668\u4eba\u7684\u5bfc\u822a\u80fd\u529b\u3002", "result": "\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMDE-AgriVLN\u5c06\u6210\u529f\u7387\u4ece0.23\u63d0\u5347\u52300.32\uff0c\u5bfc\u822a\u8bef\u5dee\u4ece4.43\u7c73\u964d\u4f4e\u52304.08\u7c73\uff0c\u5b9e\u73b0\u4e86\u519c\u4e1aVLN\u9886\u57df\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u80fd\u6709\u6548\u589e\u5f3a\u519c\u4e1a\u673a\u5668\u4eba\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u4e3a\u519c\u4e1a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2512.03995", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03995", "abs": "https://arxiv.org/abs/2512.03995", "authors": ["Levi Burner", "Guido de Croon", "Yiannis Aloimonos"], "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter", "comment": "29 pages, 5 figures, 2 tables, under review", "summary": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.", "AI": {"tldr": "\u63d0\u51fa\"\u4eba\u5de5\u5fae\u626b\u89c6\u8865\u507f\"\u65b9\u6cd5\uff0c\u901a\u8fc7SO(3)\u8868\u793a\u4f18\u53163D\u65cb\u8f6c\u6765\u7a33\u5b9a\u89c6\u9891\uff0c\u7279\u522b\u9002\u7528\u4e8e12-20Hz\u6296\u52a8\u7684\u65e0\u5c3e\u6251\u7ffc\u673a\u62cd\u6444\uff0c\u5b9e\u73b0\u5b9e\u65f6\u65e0\u5931\u771f\u7a33\u5b9a\u6548\u679c\uff0c\u4f18\u4e8eAdobe Premiere Pro\u7684\u53d8\u5f62\u7a33\u5b9a\u5668\u3002", "motivation": "\u53d7\u4eba\u7c7b\u7b49\u52a8\u7269\u5fae\u626b\u89c6\u73b0\u8c61\u7684\u542f\u53d1\uff0c\u89e3\u51b3\u65e0\u5c3e\u6251\u7ffc\u673a\u56e012-20Hz\u6296\u52a8\u800c\u65e0\u6cd5\u4f7f\u7528\u76f8\u673a\u4f20\u611f\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u5b9e\u65f6\u89c6\u9891\u7a33\u5b9a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4f18\u5316SO(3)\u8868\u793a\u76843D\u65cb\u8f6c\u6765\u6700\u5c0f\u5316\u56fe\u50cf\u5f3a\u5ea6\u53d8\u5316\uff0c\u5b9e\u73b0\u89c6\u9891\u7a33\u5b9a\uff1b\u91c7\u7528\u9012\u5f52\u66f4\u65b0\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u53ef\u8c03\u6574\u4e3a\u56fa\u5b9a\u89c6\u89d2\u65b9\u5411\u3002", "result": "\u65b9\u6cd5\u80fd\u5b9e\u65f6\u751f\u6210\u65e0\u5931\u771f\u7684\u7a33\u5b9a\u89c6\u9891\uff0c\u663e\u8457\u51cf\u5c11\u5e27\u95f4\u8fd0\u52a8\uff1b\u4e0eAdobe Premiere Pro\u53d8\u5f62\u7a33\u5b9a\u5668\u76f8\u6bd4\uff0c\u8d28\u91cf\u66f4\u9ad8\u4e14\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u4eba\u5de5\u5fae\u626b\u89c6\u8865\u507f\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6251\u7ffc\u673a\u89c6\u9891\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u5b9e\u65f6\u7a33\u5b9a\u6548\u679c\uff0c\u5728\u89c6\u9891\u7a33\u5b9a\u9886\u57df\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
