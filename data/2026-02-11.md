<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 38]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm](https://arxiv.org/abs/2602.09046)
*Mohammad Jabari,Carmen Visconte,Giuseppe Quaglia,Med Amine Laribi*

Main category: cs.RO

TL;DR: 本文提出了一种基于可行静态工作空间优化的肌腱驱动连续体机器人设计方法，使用遗传算法优化肌腱力以最大化工作空间，并在外部载荷条件下验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动连续体机器人的工作空间受肌腱力配置影响，需要找到最优的肌腱力设计来最大化机器人在外部载荷条件下的可行静态工作空间。

Method: 采用两段八肌腱（每段四肌腱）的TDCR结构，将肌腱力作为设计变量，可行静态工作空间作为优化目标，使用遗传算法优化肌腱力配置以最大化末端执行器位置的欧几里得范数。

Result: 仿真结果表明，该方法能有效识别最优肌腱力配置，即使在外部力和扭矩作用下也能最大化可行静态工作空间。

Conclusion: 提出的基于可行静态工作空间优化的方法为肌腱驱动连续体机器人的设计提供了有效工具，能够在考虑外部载荷的情况下优化肌腱力配置。

Abstract: This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.

</details>


### [2] [Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception](https://arxiv.org/abs/2602.09076)
*Nhat Le,Daeun Song,Xuesu Xiao*

Main category: cs.RO

TL;DR: 该研究探索了利用人体骨骼特征（特别是下肢3D关键点）来提升多智能体轨迹预测精度，在JRDB数据集上实现了13%的误差降低，为社交机器人导航提供了有效感知方案。


<details>
  <summary>Details</summary>
Motivation: 在拥挤环境中预测人类轨迹对社交机器人导航至关重要。现有方法大多将人视为质点，而本研究旨在利用人体骨骼特征（特别是2D/3D关键点和生物力学线索）来提高轨迹预测的准确性。

Method: 系统评估了2D和3D骨骼关键点以及衍生的生物力学线索作为额外输入的预测效用。在JRDB数据集和新的360度全景视频社交导航数据集上进行了全面研究，特别关注下肢3D关键点，并探索了结合生物力学线索的改进效果。

Result: 聚焦于下肢3D关键点使平均位移误差降低了13%，将3D关键点输入与相应的生物力学线索结合可进一步改善1-4%。即使使用从等距圆柱全景图像提取的2D关键点输入，性能提升仍然存在，表明单目环绕视觉能够捕捉到运动预测的信息线索。

Conclusion: 机器人通过观察人类腿部可以高效预测其运动，这为设计社交机器人导航的感知能力提供了可行的见解。研究结果表明，利用下肢骨骼特征可以显著提升轨迹预测性能，特别是在单目环绕视觉系统中。

Abstract: Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>


### [3] [Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality](https://arxiv.org/abs/2602.09123)
*Jackson Habala,Gabriel B. Margolis,Tianyu Wang,Pratyush Bhatt,Juntao He,Naheel Naeem,Zhaochen Xu,Pulkit Agrawal,Daniel I. Goldman,Di Luo,Baxi Chong*

Main category: cs.RO

TL;DR: 本文提出了一种用于多足机器人运动控制的几何力学框架，通过对称性破缺和自旋模型对偶性发现新的控制结构，使六足机器人速度比传统步态提高50%。


<details>
  <summary>Details</summary>
Motivation: 当前多足机器人研究主要集中在双足和四足机器人，而更多腿数的机器人具有提升运动性能的潜力，但缺乏解释何时以及如何利用额外腿部提升性能的理论控制框架。现有方法难以协调多腿同时接触带来的维度灾难，通常只是简单套用双足或四足的低维步态，无法利用高维系统的新对称性和控制机会。

Method: 采用几何力学方法将接触丰富的运动规划简化为图优化问题，并提出统计力学中的自旋模型对偶框架来利用对称性破缺并指导最优步态重组。通过该框架识别六足机器人的非对称运动策略。

Result: 发现了一种非对称运动策略，使六足机器人达到每周期0.61体长的前进速度，比传统步态提高50%。这种非对称性体现在控制和硬件两个层面：控制层面，身体方向在快速顺时针和慢速逆时针转向阶段之间非对称振荡；硬件层面，同侧两条腿可以保持非驱动状态，用刚性部件替换而不影响性能。

Conclusion: 数值模拟和机器人物理实验验证了该框架的有效性，揭示了在高维具身系统中通过对称性重构出现的新型运动行为。该研究为多足机器人运动控制提供了原则性框架，能够发现传统方法无法获得的优化控制结构。

Abstract: Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>


### [4] [Elements of Robot Morphology: Supporting Designers in Robot Form Exploration](https://arxiv.org/abs/2602.09203)
*Amy Koike,Ge,Guo,Xinning He,Callie Y. Kim,Dakota Sullivan,Bilge Mutlu*

Main category: cs.RO

TL;DR: 提出了机器人形态学框架和实体化工具包，用于系统化探索机器人形态设计


<details>
  <summary>Details</summary>
Motivation: 机器人形态是HRI中的关键设计空间，但目前缺乏系统化的设计框架来指导形态探索

Method: 1. 提出机器人形态学五要素框架（感知、关节、末端执行器、运动、结构）；2. 开发形态探索积木（MEB）实体化工具包；3. 通过案例研究和设计工作坊进行评估

Result: 框架和工具包支持机器人形态的分析、构思、反思和协作设计，能够促进多样化的机器人形态探索

Conclusion: 机器人形态学框架和实体化工具包填补了机器人形态系统化设计方法的空白，为HRI研究和实践提供了新工具

Abstract: Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>


### [5] [Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications](https://arxiv.org/abs/2602.09204)
*Ozan Kaya,Emir Cem Gezer,Roger Skjetne,Ingrid Bouwer Utne*

Main category: cs.RO

TL;DR: 提出了一种混合风险感知导航架构，结合概率障碍物建模与平滑轨迹优化，用于自主水面舰艇在动态环境中的安全导航。


<details>
  <summary>Details</summary>
Motivation: 在变化的海洋环境中实现鲁棒导航需要能够感知、推理和在不确定性下行动的自主系统。传统基于LIDAR或视觉的导航方法在动态环境和风险感知方面存在局限。

Method: 1. 构建概率风险地图，捕捉障碍物接近度和动态对象行为；2. 使用风险偏置的RRT规划器生成无碰撞路径；3. 采用B样条算法优化轨迹连续性；4. 实现三种RRT*重连模式：最小化路径长度、最小化风险、优化路径长度与总风险组合。

Result: 在包含静态和动态障碍物的实验场景中评估，系统能够安全导航、保持平滑轨迹并动态适应环境风险变化。相比传统方法，在操作安全性和自主性方面有改进。

Conclusion: 该混合风险感知导航架构为不确定和动态环境中的自主车辆任务提供了有前景的解决方案，提高了安全性和自主性。

Abstract: Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>


### [6] [From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers](https://arxiv.org/abs/2602.09227)
*Ananya Yammanuru,Maria Lusardi,Nancy M. Amato,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 本文提出了混合动机有限可观测性可读运动规划问题，要求机器人生成对友好观察者可读、对敌对观察者不可读的轨迹，同时考虑各观察者的视野限制。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，机器人可能同时面对具有不同动机的观察者（友好和敌对），且每个观察者只能看到环境的一部分。传统方法要么追求完全可读（合作环境），要么追求完全不可读（对抗环境），无法处理这种混合动机场景。

Method: 提出了MMLO-LMP问题框架，并开发了DUBIOUS轨迹优化器来解决该问题。该方法考虑观察者的动机（正/负）和有限视野区域，生成平衡可读性的轨迹。

Result: DUBIOUS能够生成在友好观察者看来可读、在敌对观察者看来不可读的轨迹，同时有效处理各观察者的视野限制。展示了智能体实现该目标的多种策略。

Conclusion: MMLO-LMP问题为混合动机环境中的运动规划提供了新框架，DUBIOUS证明了解决该问题的可行性。未来工作包括移动观察者、观察者协作等扩展场景。

Abstract: In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>


### [7] [STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory](https://arxiv.org/abs/2602.09255)
*Mingfeng Yuan,Hao Zhang,Mahan Mohammadi,Runhao Li,Jinjun Shan,Steven L. Waslander*

Main category: cs.RO

TL;DR: STaR是一个用于移动机器人的智能推理框架，通过构建任务无关的多模态长期记忆和基于信息瓶颈原则的可扩展检索算法，支持在开放动态场景中进行长时程规划和推理。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在开放动态场景中长期部署时面临核心挑战：需要构建可扩展的长时程记忆系统，支持基于开放指令的规划、检索和推理，同时产生精确、可操作的导航答案。

Method: 提出STaR框架：(1)构建任务无关的多模态长期记忆，能够泛化到未见查询并保留细粒度环境语义；(2)基于信息瓶颈原则设计可扩展的任务条件检索算法，从长期记忆中提取紧凑、非冗余、信息丰富的候选记忆集进行上下文推理。

Result: 在NaVQA（混合室内外校园场景）和WH-VQA（仓库基准）两个数据集上，STaR始终优于强基线，实现更高的成功率和显著更低的空间误差。在真实Husky轮式机器人上的部署验证了其鲁棒的长时程推理能力、可扩展性和实用性。

Conclusion: STaR框架为移动机器人在开放动态场景中的长期部署提供了有效的解决方案，通过智能记忆构建和检索机制支持复杂的导航推理任务，在仿真和真实环境中都表现出优越性能。

Abstract: Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>


### [8] [Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation](https://arxiv.org/abs/2602.09259)
*Yizhou Li,Shuyuan Yang,Jiaji Su,Zonghe Chua*

Main category: cs.RO

TL;DR: 研究探讨机器人辅助微创手术中，不同监督来源（专家水平与感知模式）如何影响注意力模型学习，通过配对主动-被动多任务手术注视数据集评估被动注视作为监督的可行性。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术中触觉反馈和深度线索减少，增加了对专家视觉感知的依赖，但专家注视数据收集成本高，需要了解不同监督来源（专家水平和感知模式）如何影响注意力模型学习。

Method: 在da Vinci SimNow模拟器上收集配对主动-被动多任务手术注视数据集，主动注视通过VR头显眼动追踪记录任务执行，相同视频作为刺激材料收集被动观察者注视。使用注视密度重叠分析和单帧显著性建模评估被动注视作为监督的替代性。

Result: MSI-Net产生稳定可解释的预测，而SalGAN不稳定且与人类注视对齐差。被动注视训练的模型能恢复大部分中级主动注意力，但有可预测的退化，主动与被动目标间的转移不对称。新手被动标签能近似中级被动目标，在高质量演示中损失有限。

Conclusion: 新手被动注视标签可以近似中级被动目标，为手术指导和感知建模中的可扩展、众包注视监督提供了实用路径，但主动与被动注视监督不完全可互换，需要针对性选择。

Abstract: In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>


### [9] [Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction](https://arxiv.org/abs/2602.09287)
*Minja Axelsson,Henry Shevlin*

Main category: cs.RO

TL;DR: 本文初步区分了人机交互中的"拟人化"和"拟人模仿"概念，前者指用户感知机器人的人类特质，后者指开发者设计机器人的人类特征。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互和社会机器人学领域对"拟人化"和"拟人模仿"这两个理论概念存在混淆，需要明确区分以指导未来的机器人设计和评估研究。

Method: 通过概念分析和理论澄清的方法，对两个术语进行初步区分和定义：拟人化关注用户感知层面，拟人模仿关注设计层面。

Result: 明确区分了两个概念：拟人化是用户感知机器人的人类特质（感知者视角），拟人模仿是开发者设计机器人的人类特征（设计者视角）。

Conclusion: 这种概念区分为人机交互研究提供了更清晰的理论基础，有助于未来机器人设计和评估工作的开展，明确了不同责任主体（用户感知者 vs. 机器人设计者）。

Abstract: In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.

</details>


### [10] [CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments](https://arxiv.org/abs/2602.09367)
*Jinghan Yang,Jingyi Hou,Xinbo Yu,Wei He,Yifan Wu*

Main category: cs.RO

TL;DR: CAPER框架通过责任分离结构解决机器人科学实验中的程序化长时程操作问题，在低数据环境下提升成功率和程序正确性


<details>
  <summary>Details</summary>
Motivation: 科学实验室中的机器人辅助需要程序正确的长时程操作、有限监督下的可靠执行以及低演示数据下的鲁棒性，这些挑战了端到端视觉-语言-动作模型，因为其可恢复错误和数据驱动策略学习的假设在协议敏感实验中常常失效

Method: CAPER采用责任分离结构：任务级推理在明确约束下生成程序有效的动作序列，中层多模态基础实现子任务而不将空间决策委托给大语言模型，低层控制通过最小演示的强化学习适应物理不确定性

Result: 在科学工作流基准和公共长时程操作数据集上的实验显示，在成功率和程序正确性方面有持续改进，特别是在低数据和长时程设置下

Conclusion: 通过可解释的中间表示编码程序承诺，CAPER防止执行时违反实验逻辑，提高了可控性、鲁棒性和数据效率，为机器人科学实验提供了更可靠的框架

Abstract: Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>


### [11] [Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes](https://arxiv.org/abs/2602.09368)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 该论文提出了一种结合平滑动力学与集合值鲁棒控制的方法，用于接触丰富操作的梯度优化控制器，在保证真实混合动力学约束满足的同时利用平滑梯度信息。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的控制器优化方法在接触丰富的操作任务中面临挑战，因为混合接触动力学会产生不连续或消失的梯度。平滑动力学虽然能提供连续梯度，但会导致模型失配，在真实系统上执行时可能失败。

Method: 提出一种基于凸优化的可微分模拟器，平滑接触动力学和几何形状，将真实动力学的差异量化为集合值偏差。通过分析系统可达集的边界，优化时变仿射反馈策略，为真实闭环混合动力学提供鲁棒约束满足保证。

Result: 在平面推动、物体旋转和手内灵巧操作等多个接触丰富任务上进行评估，相比基线方法实现了更低的安规违反和目标误差，同时保证约束满足。

Conclusion: 该方法首次实现了接触丰富操作的认证梯度策略合成，通过桥接可微分物理与集合值鲁棒控制，在利用平滑梯度信息的同时为真实混合动力学提供形式化保证。

Abstract: Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.

</details>


### [12] [LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration](https://arxiv.org/abs/2602.09472)
*Shuyuan Hu,Tao Lin,Kai Ye,Yang Yang,Tianwei Zhang*

Main category: cs.RO

TL;DR: 提出神经符号框架，将LLM推理与分层LTL规范结合，解决多机器人任务分配与规划问题，通过滚动时域规划处理动态环境变化。


<details>
  <summary>Details</summary>
Motivation: LLM虽然能让非专家指定开放世界多机器人任务，但生成的计划往往缺乏运动学可行性且效率不高，特别是在长时域场景中。形式化方法如LTL提供正确性和最优性保证，但通常局限于静态离线设置且计算可扩展性差。

Method: 提出神经符号框架，将LLM推理基础化为分层LTL规范，解决相应的同时任务分配与规划问题。通过滚动时域规划循环结合实时感知，在分层状态空间中动态优化计划，处理移动用户或更新指令等随机环境变化。

Result: 广泛的真实世界实验表明，该方法在成功率和交互流畅度方面显著优于基线方法，同时最小化规划延迟。

Conclusion: 该框架成功桥接了LLM的开放世界任务指定能力与LTL的形式化保证，通过神经符号方法实现了动态环境下的高效多机器人规划。

Abstract: While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.

</details>


### [13] [Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization](https://arxiv.org/abs/2602.09563)
*Lucas Palazzolo,Mickaël Binois,Laëtitia Giraldi*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化的轨迹跟踪方法，用于微游泳机器人在低雷诺数环境下的控制，该方法结合B样条参数化，无需复杂梯度计算，适用于不同精度模型。


<details>
  <summary>Details</summary>
Motivation: 微游泳机器人的轨迹跟踪是一个关键挑战，特别是在低雷诺数动力学环境下，控制设计尤为复杂。现有方法在处理高计算成本和控制复杂性方面存在困难。

Method: 将轨迹跟踪问题表述为最优控制问题，采用B样条参数化与贝叶斯优化相结合的方法，避免了复杂的梯度计算，能够处理高计算成本问题。

Result: 该方法成功应用于鞭毛磁游泳器和三球游泳器模型，能够重现多种目标轨迹（包括生物启发路径），并能适应并部分补偿壁面诱导的流体动力学效应。

Conclusion: 贝叶斯优化作为一种多功能工具，在复杂流体-结构相互作用下的微尺度运动最优控制策略中具有巨大潜力，该方法在不同精度模型间具有一致性和鲁棒性。

Abstract: Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>


### [14] [Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows](https://arxiv.org/abs/2602.09580)
*Chenyu Yang,Denis Tarasov,Davide Liconti,Hehui Zheng,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: SOFT-FLOW：一种基于归一化流的样本高效离线策略微调框架，用于解决灵巧操作中多模态动作分布和长时域信用分配问题


<details>
  <summary>Details</summary>
Motivation: 现实世界灵巧操作策略的微调面临两大挑战：1）真实交互预算有限，需要样本高效的方法；2）动作分布高度多模态，传统高斯策略会崩溃，而基于扩散的策略无法进行基于似然的保守更新

Method: 提出SOFT-FLOW框架：1）使用归一化流策略生成多模态动作块的确切似然，通过似然正则化实现保守稳定的策略更新；2）设计动作块级评论家评估整个动作序列，使价值估计与策略的时间结构对齐，改善长时域信用分配

Result: 在两项真实机器人灵巧操作任务上验证：从盒中取出剪刀剪胶带、掌心向下握持的立方体旋转。SOFT-FLOW实现了稳定、样本高效的适应，而标准方法在这些任务上表现不佳

Conclusion: SOFT-FLOW首次在真实机器人硬件上展示了基于似然的多模态生成策略与块级价值学习的结合，为灵巧操作的现实世界微调提供了有效的解决方案

Abstract: Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.

</details>


### [15] [Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation](https://arxiv.org/abs/2602.09583)
*Marco Moletta,Michael C. Welle,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出RKO方法，结合RPO和KTO框架优势，通过有限演示调整预训练视觉运动扩散策略，实现机器人对布料折叠等可变形物体操作任务的个性化偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 人类在操作任务中通常有微妙、个性化且难以言喻的偏好，这对机器人个性化操作很重要，但在可变形物体（如衣物布料）操作领域尚未充分探索。

Method: 提出RKO偏好对齐方法，结合RPO和KTO两种框架的优势，利用有限演示调整预训练的视觉运动扩散策略，适应个性化操作偏好。

Result: 在多种衣物和偏好设置的真实布料折叠任务中，偏好对齐策略（特别是RKO）相比标准扩散策略微调表现出更优的性能和样本效率。

Conclusion: 结构化偏好学习对于扩展复杂可变形物体操作任务中的个性化机器人行为具有重要意义和可行性。

Abstract: Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>


### [16] [AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception](https://arxiv.org/abs/2602.09617)
*Ruoxuan Feng,Yuxuan Zhou,Siyu Mei,Dongzhan Zhou,Pengwei Wang,Shaowei Cui,Bin Fang,Guocai Yao,Di Hu*

Main category: cs.RO

TL;DR: ToucHD大规模触觉数据集与AnyTouch 2统一框架，通过分层触觉动态感知提升机器人触觉交互能力


<details>
  <summary>Details</summary>
Motivation: 现有光学触觉传感器虽能提供丰富信息，但数据集和模型主要关注物体级属性（如材质），忽略了物理交互过程中的细粒度触觉时间动态特性，需要系统性的动态感知能力层次来指导数据收集和模型设计

Method: 提出ToucHD大规模分层触觉数据集，涵盖触觉原子动作、真实世界操作和触觉-力配对数据；基于此提出AnyTouch 2通用触觉表示学习框架，统一物体级理解和细粒度、力感知的动态感知，捕捉像素级和动作特定变形，显式建模物理力动态

Result: 实验结果表明，该模型在涵盖静态物体属性和动态物理属性的基准测试中，以及在跨越多个动态感知能力层次（从基本物体级理解到力感知灵巧操作）的真实世界操作任务中，均表现出一致且强大的性能

Conclusion: 通过建立全面的触觉动态数据生态系统和统一的表示学习框架，显著推进了动态触觉感知能力，为光学触觉传感器提供了通用的解决方案

Abstract: Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.

</details>


### [17] [AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild](https://arxiv.org/abs/2602.09657)
*Xiaolou Sun,Wufei Si,Wenhui Ni,Yuntian Li,Dongming Wu,Fei Xie,Runwei Guan,He-Yang Xu,Henghui Ding,Yuan Wu,Yutao Yue,Yongming Huang,Hui Xiong*

Main category: cs.RO

TL;DR: AutoFly是一个用于无人机自主导航的端到端视觉-语言-动作模型，通过伪深度编码器和渐进式两阶段训练策略，在缺乏详细指令的真实户外环境中实现自主导航和避障。


<details>
  <summary>Details</summary>
Motivation: 当前无人机视觉语言导航研究依赖详细的预设指令和预定路线，但真实户外探索通常在未知环境中进行，只能提供粗粒度的位置或方向指导，需要无人机通过连续规划和避障实现自主导航。

Method: 1. 提出AutoFly端到端视觉-语言-动作模型；2. 引入伪深度编码器从RGB输入提取深度感知特征以增强空间推理；3. 采用渐进式两阶段训练策略，有效对齐视觉、深度和语言表示与动作策略；4. 构建新的自主导航数据集，从指令跟随转向自主行为建模。

Result: AutoFly相比最先进的VLA基线实现了3.9%的成功率提升，在模拟和真实环境中均表现一致。构建的新数据集解决了现有VLN数据集对显式指令跟随的过度依赖和真实数据不足的问题。

Conclusion: AutoFly通过创新的模型架构和训练策略，成功解决了真实户外环境中无人机自主导航的挑战，为具身AI中的视觉语言导航任务提供了更实用的解决方案。

Abstract: Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>


### [18] [RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination](https://arxiv.org/abs/2602.09661)
*Ameer Alhashemi,Layan Abdulhadi,Karam Abuodeh,Tala Baghdadi,Suryanarayana Datla*

Main category: cs.RO

TL;DR: RANT是一个受蚂蚁启发的多机器人探索框架，用于噪声不确定环境中的热点探测和覆盖


<details>
  <summary>Details</summary>
Motivation: 在噪声和不确定环境中，多机器人系统需要有效的协调机制来探索未知区域并探测热点，同时避免重复访问和资源浪费

Method: 结合粒子滤波定位、基于梯度的热点利用行为控制器，以及基于虚拟信息素阻塞的轻量级防重复访问协调机制

Result: 粒子滤波对可靠热点参与至关重要，协调机制显著减少重叠，增加团队规模能提高覆盖率但存在收益递减现象

Conclusion: RANT框架在噪声环境中有效平衡了探索效率、热点探测精度和团队协调，为多机器人系统在不确定环境中的探索提供了实用解决方案

Abstract: This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>


### [19] [Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments](https://arxiv.org/abs/2602.09714)
*Alejandro Gonzalez-Garcia,Sebastiaan Wyns,Sonia De Santis,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: 提出一个用于非完整自主移动机器人在复杂结构化环境中快速运动规划的完整框架，通过确定性自由空间分解创建紧凑的矩形走廊图，显著减少搜索空间，实现高效大规模导航。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格的规划器在可扩展性方面存在困难，而许多运动学可行的规划器由于搜索空间复杂度过高而带来显著计算负担。需要一种既能处理复杂环境又能高效计算的运动规划方法。

Method: 采用确定性自由空间分解方法，创建重叠矩形走廊的紧凑图，显著减少搜索空间而不牺牲路径分辨率。然后在线运动规划通过寻找矩形序列，并使用解析规划器生成近似时间最优、运动学可行的轨迹。

Result: 通过大量仿真和物理机器人实验验证了框架的有效性，实现了高效的大规模导航解决方案。该实现已作为开源软件公开发布。

Conclusion: 该框架为复杂结构化环境中的非完整移动机器人提供了一种高效的运动规划解决方案，通过创新的空间分解方法平衡了计算效率和路径质量，适用于大规模导航任务。

Abstract: We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>


### [20] [NavDreamer: Video Models as Zero-Shot 3D Navigators](https://arxiv.org/abs/2602.09765)
*Xijie Huang,Weiqi Gai,Tianyue Wu,Congyu Wang,Zhiyang Liu,Xin Zhou,Yuze Wu,Fei Gao*

Main category: cs.RO

TL;DR: NavDreamer是一个基于视频的3D导航框架，利用生成视频模型作为语言指令和导航轨迹之间的通用接口，通过视频编码时空信息和物理动态实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在导航任务中存在数据稀缺、收集成本高以及静态表示无法捕捉时间动态和物理规律等关键限制。

Method: 提出NavDreamer框架：1) 使用生成视频模型作为语言指令到导航轨迹的接口；2) 引入基于采样的优化方法，利用VLM进行轨迹评分和选择以缓解生成预测的随机性；3) 使用逆动力学模型从生成的视频计划解码可执行的导航路径点。

Result: 建立了涵盖物体导航、精确导航、空间定位、语言控制和场景推理的综合基准，实验显示在未见过的物体和环境上具有强大的泛化能力，消融研究表明导航的高层决策特性特别适合基于视频的规划。

Conclusion: 视频编码时空信息和物理动态的能力，结合互联网规模的数据可用性，能够实现导航任务的强零样本泛化，视频模型可作为导航规划的有效通用接口。

Abstract: Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>


### [21] [Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning](https://arxiv.org/abs/2602.09767)
*Ruopeng Cui,Yifei Bi,Haojie Luo,Wei Li*

Main category: cs.RO

TL;DR: 提出OMoE架构和多判别器框架，解决无监督技能发现中的表示重叠和奖励欺骗问题，在四足机器人上实现多样化的运动技能学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习需要专家精心设计奖励函数，模仿学习需要昂贵的任务特定数据。现有无监督技能发现方法存在两个关键限制：1）依赖单一策略学习多样化行为，没有建模行为间的共享结构和差异，导致学习效率低；2）容易发生奖励欺骗，奖励信号快速增加收敛但实际技能多样性不足。

Method: 提出正交混合专家（OMoE）架构，防止多样化行为在表示空间中重叠，使单一策略能够掌握广泛运动技能。设计多判别器框架，不同判别器在不同观测空间上操作，有效缓解奖励欺骗问题。

Result: 在12自由度Unitree A1四足机器人上评估，展示了多样化的运动技能。实验表明，所提框架提升了训练效率，与基线相比状态空间覆盖度扩展了18.3%。

Conclusion: OMoE架构和多判别器框架能够有效解决无监督技能发现中的表示重叠和奖励欺骗问题，提高学习效率并扩展技能多样性。

Abstract: Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>


### [22] [BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation](https://arxiv.org/abs/2602.09849)
*Yucheng Hu,Jianke Zhang,Yuanfei Luo,Yanjiang Guo,Xiaoyu Chen,Xinshu Sun,Kun Feng,Qingzhou Lu,Sheng Chen,Yangang Zhang,Wei Li,Jianyu Chen*

Main category: cs.RO

TL;DR: BagelVLA是一个统一的视觉-语言-动作模型，通过集成语言规划、视觉预测和动作生成于单一框架，提升复杂长视野操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常单独处理语言规划或视觉预测，缺乏两者的协同整合，导致在复杂长视野操作任务中表现不佳。需要一种能同时整合语言推理、视觉预测和动作生成的统一框架。

Method: 提出BagelVLA统一模型，基于预训练的统一理解和生成模型初始化，训练时将文本推理和视觉预测交织到动作执行循环中。引入残差流引导(RFG)技术，从当前观察初始化，利用单步去噪提取预测性视觉特征，以最小延迟指导动作生成。

Result: 在多个模拟和现实世界基准测试中，BagelVLA显著优于现有基线方法，特别是在需要多阶段推理的任务上表现突出。

Conclusion: BagelVLA通过统一语言规划、视觉预测和动作生成，有效提升了具身智能体在复杂操作任务中的推理和行动能力，为通用操作提供了有前景的解决方案。

Abstract: Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>


### [23] [TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback](https://arxiv.org/abs/2602.09888)
*Zihao Li,Yanan Zhou,Ranpeng Qiu,Hangyu Wu,Guoqiang Ren,Weiming Zhi*

Main category: cs.RO

TL;DR: TriPilot-FF是一个用于双臂移动机械臂的开源全身遥操作系统，通过脚控踏板配合激光雷达触觉反馈，结合上半身双手主从遥操作，实现更自然的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 移动机械臂扩大了机器人操作范围，但全身遥操作仍然困难：操作者需要同时协调轮式底座和双臂，并考虑障碍物和接触。现有界面主要是手控的，脚控通道未被充分利用。

Method: 开发了TriPilot-FF系统，包括：1) 带激光雷达驱动触觉反馈的脚控踏板，通过障碍物接近度信号提供阻力提示；2) 上半身双手主从遥操作；3) 臂侧力反馈用于接触感知；4) 实时力与视觉引导显示双臂可操作性，提示底座重新定位。

Result: 系统能有效"共同驾驶"操作者完成长时间任务和需要精确底座移动协调的任务。将遥操作反馈信号整合到ACT策略中，显示额外信息可提升性能。进行了大量真实世界评估并开源了设计。

Conclusion: TriPilot-FF通过创新的脚控踏板触觉反馈和全身遥操作界面，解决了移动机械臂遥操作的协调难题，提高了操作效率和安全性，并为学习策略提供了有价值的反馈信号。

Abstract: Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>


### [24] [TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data](https://arxiv.org/abs/2602.09893)
*Zhengxue Cheng,Yan Zhao,Keyu Wang,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: TaCo是首个触觉数据编解码器综合基准，评估了30种压缩方法在5个数据集上的表现，并开发了专门针对触觉数据训练的数据驱动编解码器TaCo-LL和TaCo-L。


<details>
  <summary>Details</summary>
Motivation: 触觉传感对具身智能至关重要，但在严格带宽限制下的实时机器人应用中，高效的触觉数据压缩仍然研究不足。触觉数据固有的异质性和时空复杂性进一步加剧了这一挑战。

Method: 引入TaCo基准，评估30种压缩方法（包括现成压缩算法和神经编解码器）在5个不同传感器类型的数据集上。系统评估无损和有损压缩方案在四个关键任务上的表现：无损存储、人类可视化、材料和物体分类、灵巧机器人抓取。开创性地开发了专门在触觉数据上训练的数据驱动编解码器TaCo-LL（无损）和TaCo-L（有损）。

Result: 验证了TaCo-LL和TaCo-L的优越性能。该基准为理解压缩效率与任务性能之间的关键权衡提供了基础框架。

Conclusion: TaCo基准为触觉数据压缩研究奠定了基础，为未来触觉感知的进展铺平了道路。

Abstract: Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>


### [25] [Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning](https://arxiv.org/abs/2602.09972)
*Zixuan Wang,Huang Fang,Shaoan Wang,Yuanfei Luo,Heng Dong,Wei Li,Yiming Gan*

Main category: cs.RO

TL;DR: Hydra-Nav是一个用于目标导航的统一视觉语言模型架构，通过自适应切换慢速推理系统和快速执行系统，解决了现有方法成功率低和计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的目标导航方法存在成功率低、对未见物体定位效率差的问题，主要原因是时空推理能力弱。虽然已有尝试通过注入推理能力来改进，但带来了巨大的计算开销。

Method: 提出Hydra-Nav统一架构，包含一个用于分析探索历史和制定高层计划的慢速推理系统，以及一个用于高效执行的快速反应系统。通过三阶段课程训练：1) 空间-动作对齐以加强轨迹规划；2) 记忆-推理集成以增强长时域探索的时空推理；3) 迭代拒绝微调以实现关键决策点的选择性推理。

Result: 在HM3D、MP3D和OVON基准测试中达到最先进性能，分别比第二好的方法高出11.1%、17.4%和21.2%。同时提出了SOT（按操作时间加权的成功率）新指标来衡量不同推理强度下的搜索效率，结果显示自适应推理显著优于固定频率基线。

Conclusion: Hydra-Nav通过自适应切换慢速推理和快速执行系统，有效解决了目标导航中推理效果和效率的平衡问题，在多个基准测试中实现了显著的性能提升和搜索效率改进。

Abstract: While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>


### [26] [RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation](https://arxiv.org/abs/2602.09973)
*Hao Li,Ziqin Wang,Zi-han Ding,Shuai Yang,Yilun Chen,Yang Tian,Xiaolin Hu,Tai Wang,Dahua Lin,Feng Zhao,Si Liu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: RoboInter Manipulation Suite：一个包含数据、基准和中间表示模型的统一资源，用于提升机器人操作的泛化能力，包含230k+场景标注和VLA框架。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作数据集成本高、特定于具体实现、覆盖范围有限，且缺乏中间监督标注，限制了视觉-语言-动作模型的泛化能力。

Method: 开发了RoboInter-Tool（轻量级GUI半自动标注工具）和RoboInter-Data（大规模数据集），提供10+类中间表示的密集标注，并构建了RoboInter-VQA基准和RoboInter-VLA框架。

Result: 创建了包含571个场景、超过230k个片段的大规模数据集，标注质量和规模远超先前工作，并建立了系统的VQA基准和VLA框架。

Conclusion: RoboInter为通过细粒度、多样化的中间表示推进鲁棒且可泛化的机器人学习提供了实用基础。

Abstract: Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>


### [27] [A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging](https://arxiv.org/abs/2602.10007)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.RO

TL;DR: 提出MARL-MASS控制器，结合多智能体安全防护罩(MASS)与强化学习，解决密集交通中变道时的安全与效率平衡问题


<details>
  <summary>Details</summary>
Motivation: 现有变道控制器要么只关注安全，要么只关注效率，无法同时处理这两个冲突目标。在密集交通中，联网自动驾驶车辆(CAVs)需要既能保证安全又能协作提升效率的变道方案

Method: 1. 设计多智能体安全防护罩(MASS)，使用控制屏障函数(CBFs)确保安全；2. 通过简单算法构建交互拓扑图捕捉多智能体交互；3. 将MASS集成到最先进的多智能体强化学习(MARL)变道控制器中；4. 定义定制奖励函数优先提升效率

Result: 在拥堵匝道合流模拟中，MASS能够实现协作变道并严格保证安全约束。定制奖励函数提高了带安全防护罩的MARL策略稳定性。MARL-MASS有效平衡了安全保证与交通效率提升的权衡

Conclusion: MARL-MASS通过在尊重安全约束的同时鼓励协作变道策略探索，有效解决了密集交通中变道时的安全与效率平衡问题，代码已开源

Abstract: Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS

</details>


### [28] [Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper](https://arxiv.org/abs/2602.10013)
*Xuhui Kang,Tongxuan Tian,Sung-Wook Lee,Binghao Huang,Yunzhu Li,Yen-Ling Kuo*

Main category: cs.RO

TL;DR: 本文介绍了TF-Gripper低成本的力控夹爪和RETAF框架，用于机器人对易碎物体的精确力控制


<details>
  <summary>Details</summary>
Motivation: 人类能够通过触觉反馈精确调节抓取力，但现有商业夹爪成本高或最小力过大，不适合研究日常易碎物体的力控制策略学习

Method: 开发了TF-Gripper低成本力控夹爪（~150美元），并设计了配套的遥操作设备记录人力数据；提出RETAF框架，将力控制与臂姿预测解耦，使用手腕图像和触觉反馈进行高频力调节

Result: 在五个需要精确力控制的实际任务中，直接力控制相比位置控制显著提高了抓取稳定性和任务性能；触觉反馈对力调节至关重要，RETAF始终优于基线方法

Conclusion: 该工作为扩展机器人操作中力控制策略的学习开辟了新路径，展示了低成本硬件与智能控制框架结合在精细操作任务中的潜力

Abstract: Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .

</details>


### [29] [RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments](https://arxiv.org/abs/2602.10015)
*Dharmendra Sharma,Archit Sharma,John Reberio,Vaibhav Kesharwani,Peeyush Thakur,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: RoboSubtaskNet：一个多阶段的人机协作子任务分割框架，结合注意力增强的I3D特征和改进的MS-TCN，用于从未修剪的长视频中定位和分类细粒度子任务，实现从感知到执行的端到端机器人操作。


<details>
  <summary>Details</summary>
Motivation: 在长视频中精确定位和分类细粒度子任务对于安全的人机协作至关重要。与通用活动识别不同，协作操作需要可直接由机器人执行的子任务标签，以弥合视觉基准与控制之间的差距。

Method: 提出RoboSubtaskNet框架，结合注意力增强的I3D特征（RGB+光流）和改进的MS-TCN（采用斐波那契膨胀计划以捕捉短时程转换）。使用包含交叉熵和时间正则化器（截断MSE和转换感知项）的复合目标函数训练，以减少过分割并鼓励有效的子任务进展。

Result: 在GTEA数据集上达到F1@50=79.5%、Edit=88.6%、Acc=78.9%；在Breakfast数据集上达到F1@50=30.4%、Edit=52.0%、Acc=53.5%；在RoboSubtask数据集上达到F1@50=94.2%、Edit=95.6%、Acc=92.2%。在7-DoF Kinova Gen3机械臂上实现端到端物理试验，总体任务成功率约91.25%。

Conclusion: RoboSubtaskNet在多个基准测试中优于MS-TCN和MS-TCN++，并通过物理实验验证了从子任务级视频理解到实际机器人操作的实用路径，为人机协作提供了有效的解决方案。

Abstract: Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>


### [30] [A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation](https://arxiv.org/abs/2602.10035)
*Marc-Philip Ecker,Christoph Fröhlich,Johannes Huemer,David Gruber,Bernhard Bischof,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 提出首个用于林业起重机的碰撞避免与载荷摆振阻尼统一MPC控制器，集成LiDAR环境建图实现实时环境适应


<details>
  <summary>Details</summary>
Motivation: 林业起重机在动态、非结构化户外环境中作业，需要同时解决碰撞避免和载荷摆振控制问题。现有方法要么专注于预定义无碰撞路径的摆振阻尼，要么只在全局规划层面进行碰撞避免，缺乏统一解决方案。

Method: 开发了首个碰撞避免与摆振阻尼统一的模型预测控制器（MPC），将基于LiDAR的环境建图通过在线欧几里得距离场（EDF）直接集成到MPC中，实现实时环境适应。控制器同时强制执行碰撞约束和阻尼载荷摆振。

Result: 在真实林业起重机上的实验验证表明，该方法能有效阻尼摆振并成功避免障碍物。控制器能够：（i）在准静态环境变化时重新规划，（ii）在干扰下保持无碰撞操作，（iii）当无绕行路径时提供安全停止。

Conclusion: 该研究首次实现了林业起重机碰撞避免与载荷摆振阻尼的统一控制框架，通过集成实时环境感知的MPC方法，显著提升了林业起重机在动态户外环境中的安全性和操作效率。

Abstract: Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.

</details>


### [31] [UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking](https://arxiv.org/abs/2602.10093)
*Baijun Chen,Weijie Wan,Tianxing Chen,Xianda Guo,Congsheng Xu,Yuanyang Qi,Haojie Zhang,Longyan Wu,Tianling Xu,Zixuan Li,Yizhe Wu,Rui Li,Xiaokang Yang,Ping Luo,Wei Sui,Yao Mu*

Main category: cs.RO

TL;DR: UniVTAC是一个基于仿真的视觉-触觉数据合成平台，支持三种常用触觉传感器，通过大规模仿真数据训练视觉-触觉编码器，并在8个代表性触觉操作任务上进行评估，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作主要依赖视觉-语言-动作策略，但接触丰富的操作任务（如插入）仅靠视觉难以鲁棒完成。同时，物理世界中获取大规模可靠触觉数据成本高且困难，缺乏统一评估平台也限制了策略学习和系统分析。

Method: 提出UniVTAC仿真平台，支持三种常用视觉-触觉传感器，实现可扩展、可控的接触交互数据生成。基于该平台训练UniVTAC编码器，使用大规模仿真合成数据和设计的监督信号，为下游操作任务提供触觉中心的视觉-触觉表示。同时构建包含8个代表性视觉-触觉操作任务的基准测试。

Result: 实验结果显示，集成UniVTAC编码器在UniVTAC基准测试上平均成功率提升17.1%，真实世界机器人实验进一步显示任务成功率提升25%。

Conclusion: UniVTAC平台通过仿真合成大规模视觉-触觉数据，解决了物理数据获取困难的问题，其编码器显著提升了触觉驱动策略的性能，为视觉-触觉操作研究提供了统一的数据生成和评估框架。

Abstract: Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>


### [32] [VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model](https://arxiv.org/abs/2602.10098)
*Jingwen Sun,Wenyao Zhang,Zekun Qi,Shaojie Ren,Zezhi Liu,Hanxin Zhu,Guangzhong Sun,Xin Jin,Zhibo Chen*

Main category: cs.RO

TL;DR: VLA-JEPA是一个新的视觉-语言-动作预训练框架，通过泄漏自由状态预测学习鲁棒的动作相关状态转换表示，避免了传统方法的像素变化依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于互联网规模视频的VLA预训练方法存在明显缺陷：它们主要学习像素变化而非动作相关的状态转换，容易受到外观偏差、无关运动和信息泄漏的影响。

Method: 提出VLA-JEPA框架，核心是泄漏自由状态预测：目标编码器从未来帧产生潜在表示，而学生路径只看到当前观察。通过在潜在空间而非像素空间进行预测，学习对相机运动和无关背景变化鲁棒的动态抽象。

Result: 在LIBERO、LIBERO-Plus、SimplerEnv和真实世界操作任务上的实验表明，VLA-JEPA在泛化性和鲁棒性方面相比现有方法取得了持续提升。

Conclusion: VLA-JEPA提供了一个简单的两阶段训练方案（JEPA预训练+动作头微调），避免了传统潜在动作管道的多阶段复杂性，能够学习更鲁棒的动作相关状态转换表示。

Abstract: Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>


### [33] [Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2602.10101)
*Sizhe Yang,Linning Xu,Hao Li,Juncheng Mu,Jia Zeng,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Robo3R：一种用于机器人操作的实时3D重建模型，直接从RGB图像和机器人状态预测精确的度量尺度场景几何


<details>
  <summary>Details</summary>
Motivation: 3D空间感知对通用机器人操作至关重要，但现有深度传感器存在噪声和材料敏感性，现有重建模型缺乏物理交互所需的精度和度量一致性

Method: 前馈模型，联合推断尺度不变局部几何和相对相机位姿，通过学习的全局相似变换统一到规范机器人坐标系；使用掩码点头生成精细点云，基于关键点的PnP公式精炼相机外参和全局对齐

Result: 在Robo3R-4M合成数据集上训练，持续优于最先进的重建方法和深度传感器；在下游任务（模仿学习、仿真到真实迁移、抓取合成、无碰撞运动规划）中表现一致提升

Conclusion: Robo3R展示了作为机器人操作替代3D感知模块的潜力，能够提供精确、度量尺度的实时场景几何

Abstract: 3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>


### [34] [DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos](https://arxiv.org/abs/2602.10105)
*Juncheng Mu,Sizhe Yang,Yiming Bao,Hojin Bae,Tianming Wei,Linning Xu,Boyi Li,Huazhe Xu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: DexImit是一个自动化框架，可将单目人类操作视频转换为物理上合理的机器人数据，无需额外信息，用于解决双手灵巧操作中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 双手灵巧操作的数据稀缺严重限制了泛化能力，因为真实世界的数据收集成本高昂。人类操作视频作为操作知识的直接载体，具有扩展机器人学习的巨大潜力，但人类手与机器人灵巧手之间的本体差距使得直接从人类视频进行预训练极具挑战性。

Method: 采用四阶段生成流程：1）从任意视角重建具有近度量尺度的手-物交互；2）执行子任务分解和双手调度；3）合成与演示交互一致的机器人轨迹；4）为零样本真实世界部署进行全面的数据增强。

Result: DexImit能够基于人类视频（来自互联网或视频生成模型）生成大规模的机器人数据，能够处理多样化的操作任务，包括工具使用（如切苹果）、长时程任务（如制作饮料）和精细操作（如堆叠杯子）。

Conclusion: DexImit通过弥合人类手与机器人手之间的本体差距，释放了大规模人类操作视频数据的潜力，为机器人灵巧操作提供了可扩展的数据生成解决方案。

Abstract: Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>


### [35] [EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration](https://arxiv.org/abs/2602.10106)
*Modi Shi,Shijia Peng,Jin Chen,Haoran Jiang,Yinghui Li,Di Huang,Ping Luo,Hongyang Li,Li Chen*

Main category: cs.RO

TL;DR: EgoHumanoid是首个利用人类自我中心视角演示数据与少量机器人数据共同训练视觉-语言-动作策略的框架，使仿人机器人能够在多样真实环境中执行移动操作任务，性能比纯机器人基线提升51%。


<details>
  <summary>Details</summary>
Motivation: 人类演示数据具有丰富的环境多样性和自然扩展性，是机器人遥操作的理想替代方案。虽然这一范式已推动机械臂操控发展，但在更具挑战性、数据需求更大的仿人机器人移动操作领域潜力尚未充分挖掘。

Method: 开发了可扩展的人类数据采集便携系统，建立实用采集协议以提高可迁移性。核心是包含两个关键组件的人类-仿人机器人对齐流程：视图对齐减少相机高度和视角变化引起的视觉域差异；动作对齐将人类动作映射到统一的、运动学可行的仿人机器人控制动作空间。

Result: 广泛的真实世界实验表明，加入无机器人自我中心数据比纯机器人基线性能提升51%，特别是在未见环境中表现更优。分析揭示了哪些行为能有效迁移以及人类数据扩展的潜力。

Conclusion: EgoHumanoid框架成功证明了利用丰富人类演示数据与少量机器人数据共同训练仿人机器人移动操作策略的可行性，通过系统化的对齐流程有效弥合了人类与机器人之间的具身差距。

Abstract: Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>


### [36] [ST4VLA: Spatially Guided Training for Vision-Language-Action Models](https://arxiv.org/abs/2602.10109)
*Jinhui Ye,Fangjing Wang,Ning Gao,Junqiu Yu,Yangkun Zhu,Bin Wang,Jinyu Zhang,Weiyang Jin,Yanwei Fu,Feng Zheng,Yilun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: ST4VLA是一个双系统视觉-语言-动作框架，通过空间引导训练将动作学习与视觉语言模型中的空间先验对齐，显著提升了机器人任务性能


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态理解方面表现出色，但在扩展到具身任务时存在不足，需要将指令转化为低级运动动作

Method: 采用双阶段方法：1) 空间基础预训练，通过点、框和轨迹预测为VLM提供可迁移先验；2) 空间引导动作后训练，通过空间提示鼓励模型产生更丰富的空间先验来指导动作生成

Result: 在Google Robot上性能从66.1提升到84.6，在WidowX Robot上从54.7提升到73.2，在SimplerEnv上创下新的SOTA结果，并展现出对未见物体、改写指令的更强泛化能力

Conclusion: 空间引导训练是构建鲁棒、可泛化机器人学习的有前景方向，该方法在策略学习中保持空间基础，促进空间和动作目标的一致性优化

Abstract: Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>


### [37] [Learning Agile Quadrotor Flight in the Real World](https://arxiv.org/abs/2602.10111)
*Yunfan Ren,Zhiyuan Zhu,Jiaxu Xing,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出自適應框架，無需精確系統辨識或離線Sim2Real轉移，通過在線適應實現四旋翼在接近執行器飽和極限下的敏捷飛行


<details>
  <summary>Details</summary>
Motivation: 基於學習的控制器通常依賴大量模擬訓練和精確系統辨識，但固定策略仍易受外部氣動干擾和內部硬件退化等分佈外場景影響。為確保安全，控制器被迫採用保守安全邊界，限制了在非受控環境中的敏捷性。

Method: 提出自適應框架：1) 引入自適應時間縮放(ATS)主動探索平台物理極限；2) 使用在線殘差學習增強簡單名義模型；3) 基於學習的混合模型，提出真實世界錨定短時域反向傳播(RASH-BPTT)實現高效穩健的飛行中策略更新。

Result: 四旋翼能夠可靠地在接近執行器飽和極限下執行敏捷機動。系統在約100秒飛行時間內，將保守基礎策略的峰值速度從1.9 m/s提升到7.3 m/s。

Conclusion: 真實世界適應不僅能補償建模誤差，更是實現持續性能改進的實用機制，特別是在激進飛行狀態下。

Abstract: Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.

</details>


### [38] [Decoupled MPPI-Based Multi-Arm Motion Planning](https://arxiv.org/abs/2602.10114)
*Dan Evron,Elias Goldsztejn,Ronen I. Brafman*

Main category: cs.RO

TL;DR: MR-STORM：一种分布式多机器人运动规划算法，扩展了STORM算法以处理动态障碍物和多机器人协作，通过动态优先级方案实现高效协调


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的运动规划算法虽然能利用GPU实现高性能，但在控制多个机械臂时扩展性差，需要一种能有效处理多机器人协作的分布式解决方案

Method: 1. 扩展STORM算法以处理动态障碍物；2. 让每个机械臂计算自己的运动规划前缀并与其他机械臂共享，其他机械臂将其视为动态障碍物；3. 添加动态优先级方案

Result: MR-STORM在静态和动态障碍物环境下都展现出优于现有最先进算法的明显经验优势

Conclusion: MR-STORM成功解决了多机器人运动规划的扩展性问题，通过分布式处理和动态优先级机制实现了高效的多机器人协作

Abstract: Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>
