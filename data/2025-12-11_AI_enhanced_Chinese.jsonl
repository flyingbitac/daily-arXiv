{"id": "2512.09065", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09065", "abs": "https://arxiv.org/abs/2512.09065", "authors": ["Shivendra Agrawal", "Jake Brawer", "Ashutosh Naik", "Alessandro Roncone", "Bradley Hayes"], "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors", "comment": "8 pages", "summary": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.", "AI": {"tldr": "ShelfAware\u662f\u4e00\u4e2a\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u5728\u51c6\u9759\u6001\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u5168\u5c40\u5b9a\u4f4d\uff0c\u901a\u8fc7\u5c06\u573a\u666f\u8bed\u4e49\u4f5c\u4e3a\u7edf\u8ba1\u8bc1\u636e\u5904\u7406\uff0c\u7ed3\u5408\u6df1\u5ea6\u4f3c\u7136\u548c\u7c7b\u522b\u4e2d\u5fc3\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u5728\u4f4e\u6210\u672c\u89c6\u89c9\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5feb\u901f\u5b9a\u4f4d\u3002", "motivation": "\u8bb8\u591a\u5ba4\u5185\u5de5\u4f5c\u7a7a\u95f4\u662f\u51c6\u9759\u6001\u7684\uff1a\u5168\u5c40\u5e03\u5c40\u7a33\u5b9a\u4f46\u5c40\u90e8\u8bed\u4e49\u4e0d\u65ad\u53d8\u5316\uff0c\u4ea7\u751f\u91cd\u590d\u51e0\u4f55\u3001\u52a8\u6001\u6742\u4e71\u548c\u611f\u77e5\u566a\u58f0\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f1a\u7834\u574f\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u7cfb\u7edf\u3002", "method": "ShelfAware\u91c7\u7528\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5c06\u573a\u666f\u8bed\u4e49\u89c6\u4e3a\u5bf9\u8c61\u7c7b\u522b\u7684\u7edf\u8ba1\u8bc1\u636e\u800c\u975e\u56fa\u5b9a\u5730\u6807\u3002\u5b83\u878d\u5408\u6df1\u5ea6\u4f3c\u7136\u548c\u7c7b\u522b\u4e2d\u5fc3\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u8bed\u4e49\u89c6\u70b9\u5e93\u5728MCL\u5185\u6267\u884c\u9006\u8bed\u4e49\u63d0\u8bae\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u6709\u9488\u5bf9\u6027\u7684\u5047\u8bbe\u751f\u6210\u3002", "result": "\u5728\u8bed\u4e49\u5bc6\u96c6\u7684\u96f6\u552e\u73af\u5883\u4e2d\u8fdb\u884c100\u6b21\u5168\u5c40\u5b9a\u4f4d\u8bd5\u9a8c\uff0c\u6db5\u76d6\u56db\u79cd\u6761\u4ef6\uff08\u63a8\u8f66\u5b89\u88c5\u3001\u53ef\u7a7f\u6234\u3001\u52a8\u6001\u969c\u788d\u548c\u7a00\u758f\u8bed\u4e49\uff09\uff0cShelfAware\u5b9e\u73b096%\u6210\u529f\u7387\uff08vs. MCL 22%\u548cAMCL 10%\uff09\uff0c\u5e73\u5747\u6536\u655b\u65f6\u95f41.91\u79d2\uff0c\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u83b7\u5f97\u6700\u4f4e\u5e73\u79fbRMSE\uff0c80%\u6d4b\u8bd5\u5e8f\u5217\u4e2d\u4fdd\u6301\u7a33\u5b9a\u8ddf\u8e2a\uff0c\u5728\u6d88\u8d39\u7ea7\u7b14\u8bb0\u672c\u7535\u8111\u5e73\u53f0\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u901a\u8fc7\u5728\u7c7b\u522b\u7ea7\u522b\u5bf9\u8bed\u4e49\u8fdb\u884c\u5206\u5e03\u5efa\u6a21\u5e76\u5229\u7528\u9006\u63d0\u8bae\uff0cShelfAware\u89e3\u51b3\u4e86\u51c6\u9759\u6001\u9886\u57df\u4e2d\u5e38\u89c1\u7684\u51e0\u4f55\u6df7\u53e0\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u89c6\u89c9\u4f20\u611f\u5668\u548cVIO\uff0c\u53ef\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\u65e0\u5173\u7684\u79fb\u52a8\u673a\u5668\u4eba\u6784\u5efa\u6a21\u5757\uff0c\u652f\u6301\u521b\u5efa\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u968f\u65f6\u5f00\u59cb\u3001\u5171\u4eab\u63a7\u5236\u7684\u8f85\u52a9\u5bfc\u822a\u8bbe\u5907\u3002"}}
{"id": "2512.09086", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09086", "abs": "https://arxiv.org/abs/2512.09086", "authors": ["Xinyu Qi", "Zeyu Deng", "Shaun Alexander Macdonald", "Liying Li", "Chen Wang", "Muhammad Ali Imran", "Philip G. Zhao"], "title": "Inferring Operator Emotions from a Motion-Controlled Robotic Arm", "comment": null, "summary": "A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u8fdc\u7a0b\u63a7\u5236\u673a\u5668\u4eba\u8fd0\u52a8\u6570\u636e\uff0c\u53ef\u4ee5\u63a8\u65ad\u64cd\u4f5c\u8005\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u51c6\u786e\u7387\u8fbe\u523083.3%", "motivation": "\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u8005\u7684\u60c5\u7eea\u72b6\u6001\u4f1a\u663e\u8457\u5f71\u54cd\u673a\u5668\u4eba\u7684\u8fd0\u52a8\uff0c\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u540e\u679c\u3002\u5f53\u524d\u7684\u60c5\u7eea\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u751f\u7406\u4fe1\u53f7\u6216\u8eab\u4f53\u8bed\u8a00\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8fdc\u7a0b\u63a7\u5236\u573a\u666f\u4e2d\u5b58\u5728\u8bbe\u5907\u9650\u5236\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u8fdc\u7a0b\u63a7\u5236\u673a\u5668\u4eba\uff08\u975e\u4e3a\u60c5\u611f\u8868\u8fbe\u8bbe\u8ba1\uff09\u7684\u529f\u80fd\u6027\u8fd0\u52a8\u6765\u63a8\u65ad\u4eba\u7c7b\u64cd\u4f5c\u8005\u7684\u60c5\u7eea\u72b6\u6001\u3002\u7cfb\u7edf\u57fa\u4e8e\u64cd\u4f5c\u8005\u624b\u90e8\u8fd0\u52a8\u4ea7\u751f\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u6570\u636e\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u7cfb\u7edf\u5728\u8bc6\u522b\u7528\u6237\u901a\u8fc7\u673a\u5668\u4eba\u8fd0\u52a8\u8868\u8fbe\u7684\u60c5\u7eea\u72b6\u6001\u65b9\u9762\u8fbe\u5230\u4e8683.3%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5373\u4f7f\u662f\u672a\u8bbe\u8ba1\u7528\u4e8e\u60c5\u611f\u8868\u8fbe\u7684\u673a\u5668\u4eba\uff0c\u5176\u529f\u80fd\u6027\u8fd0\u52a8\u4e5f\u80fd\u6709\u6548\u53cd\u6620\u64cd\u4f5c\u8005\u7684\u60c5\u7eea\u72b6\u6001\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u5f53\u524d\u548c\u672a\u6765\u7684\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u53ca\u60c5\u611f\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.09101", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09101", "abs": "https://arxiv.org/abs/2512.09101", "authors": ["Lipeng Zhuang", "Shiyu Fan", "Florent P. Audonnet", "Yingdong Ru", "Gerardo Aragon Camarasa", "Paul Henderson"], "title": "Masked Generative Policy for Robotic Control", "comment": null, "summary": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.", "AI": {"tldr": "MGP\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u52a8\u4f5c\u8868\u793a\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u901a\u8fc7\u6761\u4ef6\u63a9\u7801\u53d8\u6362\u5668\u5e76\u884c\u751f\u6210\u6807\u8bb0\u5e76\u5feb\u901f\u4f18\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u6807\u8bb0\uff0c\u5728150\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u9884\u6d4b\u548c\u9c81\u68d2\u81ea\u9002\u5e94\u6267\u884c\u80fd\u529b\u7684\u6846\u67b6\u3002", "method": "\u5c06\u52a8\u4f5c\u8868\u793a\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u8bad\u7ec3\u6761\u4ef6\u63a9\u7801\u53d8\u6362\u5668\u5e76\u884c\u751f\u6210\u6807\u8bb0\u5e76\u5feb\u901f\u4f18\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u6807\u8bb0\uff1b\u63d0\u51faMGP-Short\uff08\u5e76\u884c\u63a9\u7801\u751f\u6210+\u57fa\u4e8e\u5206\u6570\u7684\u4f18\u5316\uff09\u548cMGP-Long\uff08\u5355\u6b21\u9884\u6d4b\u5b8c\u6574\u8f68\u8ff9+\u57fa\u4e8e\u65b0\u89c2\u6d4b\u52a8\u6001\u4f18\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u52a8\u4f5c\u6807\u8bb0\uff09\u4e24\u79cd\u91c7\u6837\u8303\u5f0f\u3002", "result": "\u5728Meta-World\u548cLIBERO\u57fa\u51c6\u7684150\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0cMGP\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6269\u6563\u548c\u81ea\u56de\u5f52\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff1a\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad89%\uff0c\u6bcf\u5e8f\u5217\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe35\u500d\uff1b\u5728\u52a8\u6001\u548c\u7f3a\u5931\u89c2\u6d4b\u73af\u5883\u4e2d\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad860%\uff1b\u89e3\u51b3\u4e86\u4e24\u4e2a\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u573a\u666f\u3002", "conclusion": "MGP\u6846\u67b6\u901a\u8fc7\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u8868\u793a\u548c\u6761\u4ef6\u63a9\u7801\u53d8\u6362\u5668\uff0c\u7ed3\u5408\u4e24\u79cd\u91c7\u6837\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u4e00\u81f4\u9884\u6d4b\u548c\u9c81\u68d2\u81ea\u9002\u5e94\u6267\u884c\uff0c\u5728\u590d\u6742\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09105", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09105", "abs": "https://arxiv.org/abs/2512.09105", "authors": ["Adi Manor", "Dan Cohen", "Ziv Keidar", "Avi Parush", "Hadas Erel"], "title": "Cognitive Trust in HRI: \"Pay Attention to Me and I'll Trust You Even if You are Wrong\"", "comment": "Confrence paper", "summary": "Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u673a\u5668\u4eba\u7684\u9ad8\u5ea6\u4e13\u6ce8\u529b\u53ef\u4ee5\u8865\u507f\u5176\u4f4e\u80fd\u529b\u8868\u73b0\uff0c\u5728\u5efa\u7acb\u8ba4\u77e5\u4fe1\u4efb\u4e2d\u5f62\u6210\u60c5\u611f\u8865\u507f\u673a\u5236", "motivation": "\u63a2\u7d22\u8ba4\u77e5\u4fe1\u4efb\u5f62\u6210\u4e2d\u673a\u5668\u4eba\u80fd\u529b\u4e0e\u4e13\u6ce8\u529b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u7814\u7a76\u662f\u5426\u5b58\u5728\u8865\u507f\u673a\u5236\uff0c\u5373\u4e00\u4e2a\u56e0\u7d20\u80fd\u5426\u5f25\u8865\u53e6\u4e00\u4e2a\u56e0\u7d20\u7684\u4e0d\u8db3", "method": "\u91c7\u75282x2\u5b9e\u9a8c\u8bbe\u8ba1\uff08\u80fd\u529b\uff1a\u9ad8/\u4f4e \u00d7 \u4e13\u6ce8\u529b\uff1a\u9ad8/\u4f4e\uff09\uff0c\u53c2\u4e0e\u8005\u4e0e\u673a\u5668\u72d7\u5408\u4f5c\u5b8c\u6210\u641c\u7d22\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e0d\u540c\u7ec4\u5408\u4e0b\u7684\u8ba4\u77e5\u4fe1\u4efb\u6c34\u5e73", "result": "\u9ad8\u5ea6\u4e13\u6ce8\u529b\u53ef\u4ee5\u8865\u507f\u4f4e\u80fd\u529b\uff1a\u4e0e\u9ad8\u5ea6\u4e13\u6ce8\u4f46\u80fd\u529b\u4f4e\u7684\u673a\u5668\u4eba\u5408\u4f5c\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u4e0e\u9ad8\u80fd\u529b\u673a\u5668\u4eba\u76f8\u5f53\u7684\u4fe1\u4efb\u6c34\u5e73\uff1b\u5f53\u673a\u5668\u4eba\u4e0d\u4e13\u6ce8\u65f6\uff0c\u4f4e\u80fd\u529b\u5bfc\u81f4\u8ba4\u77e5\u4fe1\u4efb\u663e\u8457\u4e0b\u964d", "conclusion": "\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u4fe1\u4efb\u5f62\u6210\u6bd4\u4f20\u7edf\u8ba4\u77e5\u66f4\u4e3a\u590d\u6742\uff0c\u6d89\u53ca\u901a\u5e38\u88ab\u5ffd\u89c6\u7684\u60c5\u611f\u8fc7\u7a0b\uff1b\u5b58\u5728\u60c5\u611f\u8865\u507f\u673a\u5236\uff0c\u9700\u8981\u5728\u4f20\u7edf\u57fa\u4e8e\u80fd\u529b\u7684\u8ba4\u77e5\u4fe1\u4efb\u6a21\u578b\u4e2d\u589e\u52a0\u8fd9\u4e00\u5c42\u9762\u7684\u8003\u91cf"}}
{"id": "2512.09111", "categories": ["cs.RO", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.09111", "abs": "https://arxiv.org/abs/2512.09111", "authors": ["Yuji Takubo", "Arpit Dwivedi", "Sukeerth Ramkumar", "Luis A. Pabon", "Daniele Gammelli", "Marco Pavone", "Simone D'Amico"], "title": "Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous", "comment": "28 pages, 12 figures. Submitted to AIAA SCITECH 2026", "summary": "Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.", "AI": {"tldr": "SAGES\u6846\u67b6\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u6ee1\u8db3\u975e\u51f8\u7ea6\u675f\u7684\u822a\u5929\u5668\u8f68\u8ff9\uff0c\u5b9e\u73b0\u8bed\u8a00\u6761\u4ef6\u5316\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u51cf\u5c11\u4e13\u5bb6\u8d1f\u62c5\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4e13\u5bb6\u8f93\u5165\uff08\u5982\u822a\u70b9\u3001\u7ea6\u675f\u3001\u4efb\u52a1\u65f6\u95f4\u7ebf\u7b49\uff09\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u4ea4\u4f1a\u4efb\u52a1\u4e2d\u7684\u64cd\u4f5c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51faSAGES\uff08\u8bed\u4e49\u81ea\u4e3b\u5236\u5bfc\u5f15\u64ce\uff09\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u8f6c\u6362\u4e3a\u53cd\u6620\u9ad8\u5c42\u610f\u56fe\u7684\u822a\u5929\u5668\u8f68\u8ff9\uff0c\u540c\u65f6\u5c0a\u91cd\u975e\u51f8\u7ea6\u675f\u3002", "result": "\u5728\u4e24\u4e2a\u573a\u666f\u4e2d\u9a8c\u8bc1\uff1a\u5177\u6709\u8fde\u7eed\u65f6\u95f4\u7ea6\u675f\u6267\u884c\u7684\u5bb9\u9519\u63a5\u8fd1\u64cd\u4f5c\u548c\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u5e73\u53f0\u3002SAGES\u53ef\u9760\u5730\u4ea7\u751f\u4e0e\u4eba\u7c7b\u547d\u4ee4\u4e00\u81f4\u7684\u8f68\u8ff9\uff0c\u5728\u4e0d\u540c\u884c\u4e3a\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u8d85\u8fc790%\u7684\u8bed\u4e49\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u8fd9\u662f\u8fc8\u5411\u8bed\u8a00\u6761\u4ef6\u5316\u3001\u7ea6\u675f\u611f\u77e5\u7684\u822a\u5929\u5668\u8f68\u8ff9\u751f\u6210\u7684\u521d\u6b65\u6b65\u9aa4\uff0c\u4f7f\u64cd\u4f5c\u5458\u80fd\u591f\u901a\u8fc7\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u4ea4\u4e92\u5f0f\u6307\u5bfc\u5b89\u5168\u6027\u548c\u884c\u4e3a\uff0c\u51cf\u5c11\u4e13\u5bb6\u8d1f\u62c5\u3002"}}
{"id": "2512.09283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09283", "abs": "https://arxiv.org/abs/2512.09283", "authors": ["Fan Wu", "Chenguang Yang", "Haibin Yang", "Shuo Wang", "Yanrui Xu", "Xing Zhou", "Meng Gao", "Yaoqi Xian", "Zhihong Zhu", "Shifeng Huang"], "title": "UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects", "comment": null, "summary": "Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.", "AI": {"tldr": "UPETrack\u662f\u4e00\u4e2a\u57fa\u4e8e\u5355\u5411\u4f4d\u7f6e\u4f30\u8ba1\u7684\u51e0\u4f55\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u8ddf\u8e2a\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff0c\u65e0\u9700\u7269\u7406\u5efa\u6a21\u3001\u865a\u62df\u4eff\u771f\u6216\u89c6\u89c9\u6807\u8bb0\uff0c\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u5728\u5de5\u4e1a\u88c5\u914d\u3001\u533b\u7597\u624b\u672f\u548c\u65e5\u5e38\u5e94\u7528\u4e2d\u9700\u8981\u5b9e\u65f6\u72b6\u6001\u8ddf\u8e2a\uff0c\u4f46\u5176\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u3001\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u9891\u7e41\u90e8\u5206\u906e\u6321\u7ed9\u9c81\u68d2\u5b9e\u65f6\u8ddf\u8e2a\u5e26\u6765\u4e86\u6839\u672c\u6027\u969c\u788d\u3002", "method": "UPETrack\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(1) \u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u7684\u53ef\u89c1\u6bb5\u8ddf\u8e2a\uff1b(2) \u4f7f\u7528\u63d0\u51fa\u7684\u5355\u5411\u4f4d\u7f6e\u4f30\u8ba1\u7b97\u6cd5\u8fdb\u884c\u906e\u6321\u533a\u57df\u9884\u6d4b\u3002UPE\u5229\u7528DLO\u5f62\u72b6\u7684\u51e0\u4f55\u8fde\u7eed\u6027\u548c\u65f6\u95f4\u6f14\u5316\u6a21\u5f0f\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u7ec4\u5408\u4f4d\u79fb\u9879\u3001\u8fd1\u7aef\u7ebf\u6027\u7ea6\u675f\u9879\u548c\u5386\u53f2\u66f2\u7387\u9879\u4e09\u4e2a\u4e3b\u8981\u673a\u5236\u63a8\u5bfc\u51fa\u95ed\u5f0f\u4f4d\u7f6e\u4f30\u8ba1\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUPETrack\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u90fd\u8d85\u8d8a\u4e86\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7b97\u6cd5\uff08TrackDLO\u548cCDCPD2\uff09\u3002", "conclusion": "UPETrack\u901a\u8fc7\u51e0\u4f55\u9a71\u52a8\u7684\u5355\u5411\u4f4d\u7f6e\u4f30\u8ba1\u6846\u67b6\uff0c\u4e3a\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u7684\u5b9e\u65f6\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u590d\u6742\u7684\u7269\u7406\u5efa\u6a21\u6216\u8fed\u4ee3\u4f18\u5316\u3002"}}
{"id": "2512.09297", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09297", "abs": "https://arxiv.org/abs/2512.09297", "authors": ["Huayi Zhou", "Kui Jia"], "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation", "comment": "under review", "summary": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.", "AI": {"tldr": "BiDemoSyn\u6846\u67b6\u4ece\u5355\u4e2a\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u5408\u6210\u6570\u5343\u4e2a\u63a5\u89e6\u4e30\u5bcc\u7684\u53cc\u624b\u64cd\u4f5c\u6f14\u793a\uff0c\u901a\u8fc7\u534f\u8c03\u5757\u5206\u89e3\u548c\u89c6\u89c9\u5f15\u5bfc\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u4e14\u7269\u7406\u53ef\u884c\u7684\u6570\u636e\u751f\u6210", "motivation": "\u5f53\u524d\u53cc\u624b\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u4e24\u96be\uff1a\u9065\u64cd\u4f5c\u63d0\u4f9b\u7269\u7406\u771f\u5b9e\u6570\u636e\u4f46\u52b3\u52a8\u5bc6\u96c6\uff0c\u4eff\u771f\u5408\u6210\u53ef\u6269\u5c55\u4f46\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u6269\u5c55\u53c8\u80fd\u4fdd\u6301\u7269\u7406\u771f\u5b9e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u53d8\u7684\u534f\u8c03\u5757\u548c\u53ef\u53d8\u7684\u7269\u4f53\u4f9d\u8d56\u8c03\u6574\uff0c\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u5bf9\u9f50\u548c\u8f7b\u91cf\u7ea7\u8f68\u8ff9\u4f18\u5316\uff0c\u4ece\u5355\u4e2a\u771f\u5b9e\u6f14\u793a\u5408\u6210\u6570\u5343\u4e2a\u7269\u7406\u53ef\u884c\u7684\u53cc\u624b\u64cd\u4f5c\u6f14\u793a\u3002", "result": "\u5728\u516d\u4e2a\u53cc\u81c2\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528BiDemoSyn\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u5bf9\u65b0\u7269\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BiDemoSyn\u5728\u6548\u7387\u548c\u771f\u5b9e\u4e16\u754c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u4e3a\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6a21\u4eff\u5b66\u4e60\u8def\u5f84\uff0c\u65e0\u9700\u5728\u7269\u7406\u771f\u5b9e\u6027\u4e0a\u59a5\u534f\u3002"}}
{"id": "2512.09310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09310", "abs": "https://arxiv.org/abs/2512.09310", "authors": ["Kwang Bin Lee", "Jiho Kang", "Sung-Hee Lee"], "title": "Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning", "comment": "8 pages, 4 figures", "summary": "Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u573a\u666f\u65e0\u5173\u7684\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u70b9\u5b9a\u4f4d\u3001\u53cc\u624b\u673a\u5668\u4eba\u5b50\u76ee\u6807\u89c4\u5212\u548c\u4ea4\u4e92\u70b9\u9a71\u52a8\u7684\u53cc\u624b\u673a\u5668\u4eba\u63d0\u793a\u4e09\u4e2a\u6a21\u5757\uff0c\u5b9e\u73b0\u4ece\u9ad8\u7ea7\u6307\u4ee4\u5230\u53ef\u6267\u884c\u53cc\u624b\u673a\u5668\u4eba\u884c\u4e3a\u7684\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u5668\u4e3b\u8981\u662f\u5355\u624b\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u65e0\u6cd5\u89e3\u51b3\u573a\u666f\u65e0\u5173\u8bbe\u7f6e\u4e2d\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a7a\u95f4\u3001\u51e0\u4f55\u548c\u534f\u8c03\u6311\u6218\u3002\u9700\u8981\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5c06\u9ad8\u7ea7\u6307\u4ee4\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u53cc\u624b\u673a\u5668\u4eba\u884c\u4e3a\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) \u89c6\u89c9\u70b9\u5b9a\u4f4d(VPG)\uff1a\u5206\u6790\u5355\u5f20\u573a\u666f\u56fe\u50cf\u68c0\u6d4b\u76f8\u5173\u7269\u4f53\u5e76\u751f\u6210\u4e16\u754c\u5bf9\u9f50\u7684\u4ea4\u4e92\u70b9\uff1b2) \u53cc\u624b\u673a\u5668\u4eba\u5b50\u76ee\u6807\u89c4\u5212\u5668(BSP)\uff1a\u57fa\u4e8e\u7a7a\u95f4\u90bb\u63a5\u548c\u8de8\u7269\u4f53\u53ef\u8fbe\u6027\u63a8\u7406\uff0c\u751f\u6210\u7d27\u51d1\u3001\u8fd0\u52a8\u4e2d\u7acb\u5316\u7684\u5b50\u76ee\u6807\uff1b3) \u4ea4\u4e92\u70b9\u9a71\u52a8\u7684\u53cc\u624b\u673a\u5668\u4eba\u63d0\u793a(IPBP)\uff1a\u5c06\u5b50\u76ee\u6807\u7ed1\u5b9a\u5230\u7ed3\u6784\u5316\u6280\u80fd\u5e93\uff0c\u5b9e\u4f8b\u5316\u6ee1\u8db3\u624b\u90e8\u72b6\u6001\u548c\u53ef\u4f9b\u6027\u7ea6\u675f\u7684\u540c\u6b65\u5355\u624b\u673a\u5668\u4eba\u6216\u53cc\u624b\u673a\u5668\u4eba\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u3001\u53ef\u884c\u4e14\u7d27\u51d1\u7684\u53cc\u624b\u673a\u5668\u4eba\u89c4\u5212\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u6742\u4e71\u573a\u666f\uff0c\u5c55\u793a\u4e86\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u9c81\u68d2\u573a\u666f\u65e0\u5173\u53ef\u4f9b\u6027\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u4f7f\u667a\u80fd\u4f53\u5728\u6742\u4e71\u3001\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u89c4\u5212\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u3001\u7269\u7406\u4e0a\u53ef\u884c\u4e14\u53ef\u5e76\u884c\u7684\u53cc\u624b\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7a7a\u95f4\u3001\u51e0\u4f55\u548c\u534f\u8c03\u6311\u6218\u3002"}}
{"id": "2512.09349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09349", "abs": "https://arxiv.org/abs/2512.09349", "authors": ["Lin Li", "Yuxin Cai", "Jianwu Fang", "Jianru Xue", "Chen Lv"], "title": "COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\\% in trained driving environments and by 50\\% in previously unseen environments, highlighting its strong generalization capability.", "AI": {"tldr": "COVLM-RL\uff1a\u7ed3\u5408\u5173\u952e\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u4e0eVLM\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7CoT\u63d0\u793a\u7b56\u7565\u751f\u6210\u8bed\u4e49\u51b3\u7b56\u5148\u9a8c\uff0c\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u8bad\u7ec3\u6548\u7387\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u65b0\u573a\u666f\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u9002\u5e94\u6027\u5f3a\u4f46\u6570\u636e\u6548\u7387\u4f4e\u4e14\u51b3\u7b56\u4e0d\u900f\u660e\u3002", "method": "\u63d0\u51faCOVLM-RL\u6846\u67b6\uff1a1) \u8bbe\u8ba1\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7b56\u7565\uff0c\u8ba9VLM\u5bf9\u5173\u952e\u4ea4\u901a\u5143\u7d20\u8fdb\u884c\u63a8\u7406\u5e76\u751f\u6210\u9ad8\u7ea7\u8bed\u4e49\u51b3\u7b56\u5148\u9a8c\uff1b2) \u5f15\u5165\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fddVLM\u7684\u8bed\u4e49\u89c4\u5212\u4e0eRL\u667a\u80fd\u4f53\u7684\u63a7\u5236\u8f93\u51fa\u5bf9\u9f50\uff1b3) \u5c06\u8bed\u4e49\u5148\u9a8c\u6ce8\u5165RL\u5faa\u73af\uff0c\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\u5e76\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u5728\u8bad\u7ec3\u8fc7\u7684\u9a7e\u9a76\u73af\u5883\u4e2d\u6210\u529f\u7387\u63d0\u534730%\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u6210\u529f\u7387\u63d0\u534750%\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "COVLM-RL\u901a\u8fc7\u6574\u5408VLM\u7684\u8bed\u4e49\u63a8\u7406\u4e0eRL\u7684\u9002\u5e94\u6027\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u7684\u6cdb\u5316\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09377", "abs": "https://arxiv.org/abs/2512.09377", "authors": ["Lidan Xu", "Dadong Fan", "Junhong Wang", "Wenshuo Li", "Hao Lu", "Jianzhong Qiao"], "title": "Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances", "comment": null, "summary": "Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\\mathbb{R}^3)^2\\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc1\u660e\u5728\u4ec5\u6709\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u53cc\u65e0\u4eba\u673a-\u6746\u7cfb\u7edf\u7684\u8f7d\u8377\u59ff\u6001\u5728\u591a\u6e90\u6270\u52a8\u4e0b\u662f\u53ef\u89c2\u6d4b\u7684\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6270\u52a8\u89c2\u6d4b\u5668\u7684\u8bef\u5dee\u72b6\u6001\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u72b6\u6001\u548c\u6270\u52a8\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u534f\u540c\u60ac\u540a\u7a7a\u4e2d\u8fd0\u8f93\u7cfb\u7edf\u5bf9\u591a\u6e90\u6270\u52a8\uff08\u5982\u7a7a\u6c14\u52a8\u529b\u6548\u5e94\u548c\u63a8\u529b\u4e0d\u786e\u5b9a\u6027\uff09\u9ad8\u5ea6\u654f\u611f\u3002\u4e3a\u5b9e\u73b0\u7cbe\u786e\u8f7d\u8377\u64cd\u7eb5\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u6d4b\u91cf\u7f06\u7ef3\u65b9\u5411\u6216\u8f7d\u8377\u59ff\u6001\uff0c\u8fd9\u589e\u52a0\u4e86\u7cfb\u7edf\u6210\u672c\u548c\u590d\u6742\u6027\u3002\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\uff0c\u5728\u591a\u6e90\u6270\u52a8\u4e0b\u8f7d\u8377\u59ff\u6001\u662f\u5426\u53ef\u89c2\u6d4b\uff1f", "method": "1. \u9488\u5bf9\u53cc\u65e0\u4eba\u673a-\u6746\u7cfb\u7edf\uff0c\u4f7f\u7528\u53ef\u89c2\u6d4b\u6027\u79e9\u5224\u636e\u8bc1\u660e\u5f53\u4ec5\u5b58\u5728\u4e24\u79cd\u6216\u66f4\u5c11\u7c7b\u578b\u7684\u96c6\u603b\u6270\u52a8\u65f6\uff0c\u6574\u4e2a\u7cfb\u7edf\u662f\u53ef\u89c2\u6d4b\u7684\uff1b2. \u9488\u5bf9\u6270\u52a8\u4ec5\u4f5c\u7528\u4e8e\u65e0\u4eba\u673a\u7684\u60c5\u51b5\uff0c\u5f00\u53d1\u590d\u5408\u6270\u52a8\u6ee4\u6ce2\u65b9\u6848\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6270\u52a8\u89c2\u6d4b\u5668\u7684\u8bef\u5dee\u72b6\u6001\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u5728\u6d41\u5f62$(mathbb{R}^3)^2times(TS^2)^3$\u4e0a\u6f14\u5316\u7684\u6574\u4e2a\u7cfb\u7edf\u7684\u72b6\u6001\u548c\u6270\u52a8\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u53ef\u4ee5\u5b8c\u5168\u4f30\u8ba1\u7cfb\u7edf\u7684\u72b6\u6001\u548c\u6270\u52a8\u3002\u8fd9\u662f\u9996\u6b21\u8bc1\u660e\u6b64\u7c7b\u7ed3\u8bba\uff0c\u4e3a\u901a\u8fc7\u6700\u5c0f\u5316\u4f20\u611f\u5668\u5957\u4ef6\u5b9e\u73b0\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6270\u52a8\u6761\u4ef6\u4e0b\uff0c\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u5373\u53ef\u5b9e\u73b0\u53cc\u65e0\u4eba\u673a-\u6746\u7cfb\u7edf\u7684\u5b8c\u5168\u72b6\u6001\u548c\u6270\u52a8\u4f30\u8ba1\uff0c\u4e3a\u51cf\u5c11\u4f20\u611f\u5668\u4f9d\u8d56\u3001\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u7cfb\u7edf\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2512.09406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09406", "abs": "https://arxiv.org/abs/2512.09406", "authors": ["Hai Ci", "Xiaokang Liu", "Pei Yang", "Yiren Song", "Mike Zheng Shou"], "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos", "comment": "13 pages, 6 figures", "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/", "AI": {"tldr": "\u63d0\u51fa\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u6846\u67b6\uff0c\u5c06\u666e\u901a\u4eba-\u7269\u4ea4\u4e92\u89c6\u9891\u8f6c\u6362\u4e3a\u8fd0\u52a8\u4e00\u81f4\u3001\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e", "motivation": "\u8ba9\u673a\u5668\u4eba\u901a\u8fc7\u89c2\u5bdf\u65e5\u5e38\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u64cd\u4f5c\u6280\u80fd\uff0c\u907f\u514d\u7e41\u7410\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u6280\u80fd\u83b7\u53d6", "method": "\u4f7f\u7528\u53ef\u8fc1\u79fb\u8868\u793a\u6865\u63a5\u5177\u8eab\u9e3f\u6c9f\uff1a\u901a\u8fc7\u4fee\u590d\u8bad\u7ec3\u89c6\u9891\u4e2d\u7684\u673a\u5668\u4eba\u624b\u81c2\u83b7\u5f97\u5e72\u51c0\u80cc\u666f\uff0c\u53e0\u52a0\u7b80\u5355\u89c6\u89c9\u7ebf\u7d22\uff08\u6807\u8bb0\u548c\u7bad\u5934\u8868\u793a\u5939\u722a\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\uff0c\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5c06\u673a\u5668\u4eba\u624b\u81c2\u91cd\u65b0\u63d2\u5165\u573a\u666f\uff1b\u5728\u6d4b\u8bd5\u65f6\u5bf9\u4eba\u7269\u89c6\u9891\u5e94\u7528\u76f8\u540c\u6d41\u7a0b\uff0c\u751f\u6210\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u7684\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u89c6\u9891\uff1b\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u5f0f\u5fae\u8c03SOTA\u89c6\u9891\u6269\u6563\u6a21\u578b\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5229\u7528\u5176\u4e30\u5bcc\u5148\u9a8c\u77e5\u8bc6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u3001\u66f4\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u4e3a\u4ece\u65e0\u6807\u7b7e\u4eba\u7c7b\u89c6\u9891\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411", "conclusion": "\u63d0\u51fa\u7684\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u65e5\u5e38\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.09410", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09410", "abs": "https://arxiv.org/abs/2512.09410", "authors": ["Jialin Ying", "Zhihao Li", "Zicheng Dong", "Guohua Wu", "Yihuan Liao"], "title": "Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation", "comment": "7 pages, 7 figures", "summary": "Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.", "AI": {"tldr": "PGF-MAPPO\uff1a\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u62d3\u6251\u89c4\u5212\u548c\u53cd\u5e94\u63a7\u5236\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8ffd\u9003\u95ee\u9898\uff0c\u901a\u8fc7A*\u52bf\u573a\u5956\u52b1\u5851\u5f62\u548c\u65b9\u5411\u6027\u524d\u6cbf\u5206\u914d\u63d0\u5347\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u73af\u5883\u8ffd\u9003\u4efb\u52a1\u4e2d\u9762\u4e34\u7a00\u758f\u5956\u52b1\u548c\u89c6\u91ce\u53d7\u9650\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\u3002", "method": "\u63d0\u51faPGF-MAPPO\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u62d3\u6251\u89c4\u5212\u4e0e\u53cd\u5e94\u63a7\u5236\u7ed3\u5408\u3002\u91c7\u7528A*\u52bf\u573a\u8fdb\u884c\u5bc6\u96c6\u5956\u52b1\u5851\u5f62\u89e3\u51b3\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff1b\u5f15\u5165\u65b9\u5411\u6027\u524d\u6cbf\u5206\u914d\uff08\u7ed3\u5408\u6700\u8fdc\u70b9\u91c7\u6837\u548c\u51e0\u4f55\u89d2\u5ea6\u6291\u5236\uff09\u5b9e\u73b0\u7a7a\u95f4\u5206\u6563\uff1b\u4f7f\u7528\u53c2\u6570\u5171\u4eab\u7684\u5206\u6563\u5f0f\u6279\u8bc4\u5668\u4fdd\u6301O(1)\u6a21\u578b\u590d\u6742\u5ea6\u3002", "result": "PGF-MAPPO\u5728\u8ffd\u6355\u66f4\u5feb\u9003\u9038\u8005\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6355\u83b7\u6548\u7387\u3002\u572810x10\u5730\u56fe\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u768420x20\u73af\u5883\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u548c\u5b66\u4e60\u7684\u57fa\u7840\u65b9\u6cd5\u3002", "conclusion": "PGF-MAPPO\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u548c\u524d\u6cbf\u5206\u914d\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u8ffd\u9003\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7fa4\u4f53\u5e94\u7528\u3002"}}
{"id": "2512.09411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09411", "abs": "https://arxiv.org/abs/2512.09411", "authors": ["Siting Zhu", "Yuxiang Huang", "Wenhua Wu", "Chaokang Jiang", "Yongbo Chen", "I-Ming Chen", "Hesheng Wang"], "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM", "comment": null, "summary": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.", "AI": {"tldr": "D\u00b2GSLAM\u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8868\u793a\u7684\u52a8\u6001SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u51c6\u786e\u7684\u52a8\u6001\u91cd\u5efa\u548c\u9c81\u68d2\u8ddf\u8e2a\uff0c\u901a\u8fc7\u51e0\u4f55\u63d0\u793a\u52a8\u6001\u5206\u79bb\u3001\u52a8\u9759\u590d\u5408\u8868\u793a\u3001\u6e10\u8fdb\u4f4d\u59ff\u4f18\u5316\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u635f\u5931\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6SLAM\u65b9\u6cd5\u5728\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002\u5927\u591a\u6570\u65b9\u6cd5\u76f4\u63a5\u79fb\u9664\u52a8\u6001\u7269\u4f53\uff0c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u7269\u4f53\u5305\u542b\u7684\u8fd0\u52a8\u4fe1\u606f\uff0c\u65e0\u6cd5\u540c\u65f6\u8fdb\u884c\u52a8\u6001\u91cd\u5efa\u548c\u8ddf\u8e2a\u3002", "method": "\u63d0\u51fa\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u51e0\u4f55\u63d0\u793a\u52a8\u6001\u5206\u79bb\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u8868\u793a\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u533a\u5206\u52a8\u9759\u5143\u7d20\uff1b2) \u52a8\u9759\u590d\u5408\u8868\u793a\uff0c\u6574\u5408\u9759\u60013D\u9ad8\u65af\u548c\u52a8\u60014D\u9ad8\u65af\uff1b3) \u6e10\u8fdb\u4f4d\u59ff\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u9759\u6001\u573a\u666f\u51e0\u4f55\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\u4fe1\u606f\uff1b4) \u8fd0\u52a8\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5229\u7528\u7269\u4f53\u8fd0\u52a8\u7684\u65f6\u5e8f\u8fde\u7eed\u6027\u3002", "result": "D\u00b2GSLAM\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u5efa\u56fe\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u540c\u65f6\u80fd\u591f\u8fdb\u884c\u51c6\u786e\u7684\u52a8\u6001\u5efa\u6a21\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2dSLAM\u7684\u6311\u6218\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u9759\u6001\u573a\u666f\u91cd\u5efa\u548c\u52a8\u6001\u7269\u4f53\u5efa\u6a21\uff0c\u4e3a\u52a8\u6001SLAM\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09431", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09431", "abs": "https://arxiv.org/abs/2512.09431", "authors": ["Quanyou Wang", "Mingzhang Zhu", "Ruochen Hou", "Kay Gillespie", "Alvin Zhu", "Shiqi Wang", "Yicheng Wang", "Gaberiel I. Fernandez", "Yeting Liu", "Colin Togashi", "Hyunwoo Nam", "Aditya Navghare", "Alex Xu", "Taoyuanmin Zhu", "Min Sung Ahn", "Arturo Flores Alvarez", "Justin Quan", "Ethan Hong", "Dennis W. Hong"], "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer", "comment": null, "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u8d62\u5f97RoboCup 2024\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u8d5b\u51a0\u519b\u7684ARTEMIS\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5305\u62ec\u5176\u8f7b\u91cf\u5316\u786c\u4ef6\u8bbe\u8ba1\u3001\u9ad8\u626d\u77e9\u51c6\u76f4\u9a71\u6267\u884c\u5668\u3001\u4e13\u7528\u8db3\u90e8\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u96c6\u6210\u7684\u611f\u77e5\u5b9a\u4f4d\u3001\u5bfc\u822a\u548c\u884c\u4e3a\u7ba1\u7406\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u9a71\u52a8\u3001\u4f20\u611f\u548c\u63a7\u5236\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u7ade\u6280\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53d1\u5c55\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002RoboCup\u4f5c\u4e3a\u5b8c\u5168\u81ea\u4e3b\u4eba\u5f62\u673a\u5668\u4eba\u7684\u56fd\u9645\u7ade\u8d5b\uff0c\u4e3a\u5b9e\u73b02050\u5e74\u4e0e\u4eba\u7c7b\u8db3\u7403\u8fd0\u52a8\u5458\u5bf9\u6297\u7684\u957f\u671f\u76ee\u6807\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u6311\u6218\u6027\u57fa\u51c6\u3002\u672c\u6587\u65e8\u5728\u5c55\u793a\u56e2\u961f\u8d62\u5f97RoboCup 2024\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u8db3\u7403\u8d5b\u51a0\u519b\u6240\u4f9d\u8d56\u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u521b\u65b0\u3002", "method": "\u786c\u4ef6\u65b9\u9762\uff1a\u91c7\u7528\u8f7b\u91cf\u5316\u7ed3\u6784\u7ec4\u4ef6\u3001\u9ad8\u626d\u77e9\u51c6\u76f4\u9a71\u6267\u884c\u5668\u548c\u4e13\u7528\u8db3\u90e8\u8bbe\u8ba1\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5f3a\u6709\u529b\u7684\u6b65\u6001\u5185\u8e22\u7403\u3002\u8f6f\u4ef6\u65b9\u9762\uff1a\u5f00\u53d1\u4e86\u96c6\u6210\u7684\u611f\u77e5\u548c\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u7acb\u4f53\u89c6\u89c9\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u57fa\u4e8e\u5730\u6807\u7684\u878d\u5408\u6280\u672f\uff0c\u53ef\u9760\u4f30\u8ba1\u7403\u3001\u7403\u95e8\u3001\u961f\u53cb\u548c\u5bf9\u624b\u4f4d\u7f6e\u3002\u4e2d\u5c42\u5bfc\u822a\u6808\u751f\u6210\u78b0\u649e\u611f\u77e5\u7684\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff0c\u96c6\u4e2d\u5f0f\u884c\u4e3a\u7ba1\u7406\u5668\u6839\u636e\u6e38\u620f\u72b6\u6001\u534f\u8c03\u9ad8\u7ea7\u51b3\u7b56\u3001\u89d2\u8272\u9009\u62e9\u548c\u8e22\u7403\u6267\u884c\u3002", "result": "\u8fd9\u4e9b\u5b50\u7cfb\u7edf\u7684\u65e0\u7f1d\u96c6\u6210\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u7cbe\u786e\u548c\u6218\u672f\u6709\u6548\u7684\u6e38\u620f\u73a9\u6cd5\uff0c\u5728\u771f\u5b9e\u6bd4\u8d5b\u7684\u52a8\u6001\u5bf9\u6297\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\u3002ARTEMIS\u673a\u5668\u4eba\u7cfb\u7edf\u6210\u529f\u8d62\u5f97\u4e862024\u5e74\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u8db3\u7403\u8d5b\u51a0\u519b\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86ARTEMIS\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u3001\u7cfb\u7edf\u67b6\u6784\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8fd9\u4e9b\u521b\u65b0\u4e3a\u8d62\u5f97RoboCup 2024\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u8db3\u7403\u8d5b\u51a0\u519b\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\uff0c\u63a8\u52a8\u4e86\u7ade\u6280\u4eba\u5f62\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.09447", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09447", "abs": "https://arxiv.org/abs/2512.09447", "authors": ["Jaehyun Kim", "Seungwon Choi", "Tae-Wan Kim"], "title": "Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments", "comment": "8 pages, 4 figures", "summary": "We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u622a\u65ad\u5e8f\u8d2f\u6982\u7387\u6bd4\u68c0\u9a8c(SPRT)\u7684\u6fc0\u5149\u96f7\u8fbe\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d2f\u79ef\u591a\u5e27\u63cf\u8ff0\u7b26\u76f8\u4f3c\u5ea6\u8bc1\u636e\u8fdb\u884c\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u65e8\u5728\u6291\u5236\u5ba4\u5185\u91cd\u590d\u7ed3\u6784\u73af\u5883\u4e2d\u7684\u8bef\u62a5\u3002", "motivation": "\u73b0\u6709\u6fc0\u5149\u96f7\u8fbe\u95ed\u73af\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u5e27\u63cf\u8ff0\u7b26\u6bd4\u8f83\u6216\u4f7f\u7528\u56fa\u5b9a\u9608\u503c\u914d\u5408\u540e\u671fICP\u9a8c\u8bc1\uff0c\u5728\u7ed3\u6784\u91cd\u590d\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u8bef\u62a5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6291\u5236\u8bef\u62a5\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u5c06\u6fc0\u5149\u96f7\u8fbe\u95ed\u73af\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u622a\u65ad\u5e8f\u8d2f\u6982\u7387\u6bd4\u68c0\u9a8c(SPRT)\uff0c\u901a\u8fc7\u7d2f\u79ef\u67e5\u8be2\u5e27\u4e0e\u5019\u9009\u5e27\u4e4b\u95f4\u77ed\u65f6\u5e8f\u5217\u7684\u63cf\u8ff0\u7b26\u76f8\u4f3c\u5ea6\u8bc1\u636e\uff0c\u6839\u636e\u7528\u6237\u6307\u5b9a\u7684I/II\u7c7b\u9519\u8bef\u76ee\u6807\u81ea\u9002\u5e94\u5730\u505a\u51fa\u63a5\u53d7/\u62d2\u7edd\u51b3\u7b56\u3002", "result": "\u5728\u4e94\u4e2a\u5e8f\u5217\u7684\u56fe\u4e66\u9986\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u56fa\u5b9a\u68c0\u7d22\u524d\u7aef\u548c\u591a\u79cd\u4ee3\u8868\u6027\u6fc0\u5149\u96f7\u8fbe\u5168\u5c40\u63cf\u8ff0\u7b26\u3002\u4e0e\u5355\u5e27\u548c\u542f\u53d1\u5f0f\u591a\u5e27\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e8f\u8d2f\u9a8c\u8bc1\u5668\u5728\u6240\u6709\u63cf\u8ff0\u7b26\u4e0a\u90fd\u4e00\u81f4\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u6df7\u6dc6\u95ed\u73af\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eSPRT\u7684\u591a\u5e27\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6291\u5236\u5ba4\u5185\u91cd\u590d\u7ed3\u6784\u73af\u5883\u4e2d\u7684\u8bef\u62a5\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u95ed\u73af\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.09462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09462", "abs": "https://arxiv.org/abs/2512.09462", "authors": ["Jayant Unde", "Takumi Inden", "Yuki Wakayama", "Jacinto Colan", "Yaonan Zhu", "Tadayoshi Aoyama", "Yasuhisa Hasegawa"], "title": "Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing", "comment": null, "summary": "In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u5e2e\u52a9\u8001\u5e74\u4eba\u6216\u504f\u762b\u60a3\u8005\u7a7f\u8131\u88e4\u5b50\u7684\u5939\u6301\u5668\u7cfb\u7edf\uff0c\u5e73\u8861\u67d4\u987a\u6027\u4e0e\u6293\u53d6\u529b\uff0c\u5b9e\u73b0\u5b89\u5168\u7cbe\u51c6\u64cd\u4f5c", "motivation": "\u4eba\u53e3\u8001\u9f84\u5316\u80cc\u666f\u4e0b\uff0c\u4fdd\u6301\u8001\u5e74\u4eba\u751f\u6d3b\u8d28\u91cf\u6210\u4e3a\u91cd\u8981\u5173\u5207\u3002\u5bf9\u4e8e\u8eab\u4f53\u80fd\u529b\u53d7\u635f\u7684\u8001\u5e74\u4eba\uff0c\u5982\u5395\u8f85\u52a9\u662f\u6700\u91cd\u8981\u7684\u95ee\u9898\u4e4b\u4e00\uff0c\u7279\u522b\u662f\u5e2e\u52a9\u8001\u5e74\u4eba\u6216\u504f\u762b\u60a3\u8005\u7a7f\u8131\u88e4\u5b50\u7684\u9700\u6c42", "method": "\u8bbe\u8ba1\u5f00\u53d1\u5939\u6301\u5668\u7cfb\u7edf\uff0c\u5e73\u8861\u67d4\u987a\u6027\u4e0e\u6293\u53d6\u529b\uff0c\u786e\u4fdd\u7cbe\u786e\u64cd\u4f5c\u540c\u65f6\u4fdd\u6301\u4e0e\u7528\u6237\u7684\u5b89\u5168\u67d4\u987a\u4ea4\u4e92\u3002\u5c06\u5939\u6301\u5668\u96c6\u6210\u5230\u5b9a\u5236\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u504f\u762b\u60a3\u8005\u63d0\u4f9b\u7a7f\u8131\u88e4\u5b50\u7684\u5168\u9762\u89e3\u51b3\u65b9\u6848", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u548c\u4e0e\u73b0\u6709\u7814\u7a76\u7684\u6bd4\u8f83\u8868\u660e\uff0c\u8be5\u5939\u6301\u5668\u80fd\u591f\u5728\u6709\u9650\u7a7a\u95f4\u5185\u6210\u529f\u8f85\u52a9\u7a7f\u8131\u88e4\u5b50\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u6210\u529f\u7387", "conclusion": "\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u63a8\u8fdb\u8f85\u52a9\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e2e\u52a9\u8001\u5e74\u4eba\u548c\u8eab\u4f53\u53d7\u635f\u8005\u4fdd\u6301\u72ec\u7acb\u6027\u5e76\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf"}}
{"id": "2512.09510", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09510", "abs": "https://arxiv.org/abs/2512.09510", "authors": ["Donato Caramia", "Florian T. Pokorny", "Giuseppe Triggiani", "Denis Ruffino", "David Naso", "Paolo Roberto Massenio"], "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics", "comment": null, "summary": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.", "AI": {"tldr": "ViTA-Seg\u662f\u4e00\u4e2a\u57fa\u4e8eVision Transformer\u7684\u7c7b\u65e0\u5173\u5b9e\u65f6amodal\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7bb1\u4f53\u6293\u53d6\u4e2d\u7684\u906e\u6321\u5904\u7406\uff0c\u5305\u542b\u5355\u5934\u548c\u53cc\u5934\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u5408\u6210\u6570\u636e\u96c6ViTA-SimData\u3002", "motivation": "\u673a\u5668\u4eba\u7bb1\u4f53\u6293\u53d6\u4e2d\u7684\u906e\u6321\u95ee\u9898\u4f1a\u5f71\u54cd\u51c6\u786e\u53ef\u9760\u7684\u6293\u53d6\u89c4\u5212\uff0c\u9700\u8981\u6062\u590d\u5305\u62ec\u9690\u85cf\u533a\u57df\u5728\u5185\u7684\u5b8c\u6574\u7269\u4f53\u63a9\u7801\u3002", "method": "\u63d0\u51faViTA-Seg\u6846\u67b6\uff0c\u5229\u7528\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u6062\u590d\u5b8c\u6574\u7269\u4f53\u63a9\u7801\uff0c\u5305\u542b\u4e24\u79cd\u67b6\u6784\uff1a\u5355\u5934\u7528\u4e8eamodal\u63a9\u7801\u9884\u6d4b\uff0c\u53cc\u5934\u7528\u4e8eamodal\u548c\u906e\u6321\u63a9\u7801\u9884\u6d4b\uff1b\u540c\u65f6\u63d0\u51faViTA-SimData\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u5728COOCA\u548cKINS\u4e24\u4e2aamodal\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViTA-Seg\u53cc\u5934\u67b6\u6784\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684amodal\u548c\u906e\u6321\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "ViTA-Seg\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4e3a\u5de5\u4e1a\u7bb1\u4f53\u6293\u53d6\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u906e\u6321\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09571", "abs": "https://arxiv.org/abs/2512.09571", "authors": ["Feng Yu", "Yu Hu", "Yang Su", "Yang Deng", "Linzuo Zhang", "Danping Zou"], "title": "Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing", "comment": "8 pages, 9 figures, Robotics and Automation Letters accept", "summary": "Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u65e0\u4eba\u673a\u7ade\u901f\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8f6f\u78b0\u649e\u8bad\u7ec3\u548c\u786c\u78b0\u649e\u7cbe\u70bc\u5e73\u8861\u901f\u5ea6\u4e0e\u907f\u969c\uff0c\u5b9e\u73b0\u654f\u6377\u98de\u884c\u548c\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u7ade\u901f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u56fa\u5b9a\u3001\u65e0\u969c\u788d\u7684\u8d5b\u9053\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u77e5\u3001\u6742\u4e71\u7684\u590d\u6742\u73af\u5883\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5e73\u8861\u7ade\u901f\u901f\u5ea6\u4e0e\u78b0\u649e\u907f\u514d\u7684\u77db\u76fe\uff1b\u53ef\u884c\u7a7a\u95f4\u6709\u9650\u5bfc\u81f4\u7b56\u7565\u63a2\u7d22\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff1b\u6df1\u5ea6\u56fe\u4e2d\u95e8\u4e0e\u969c\u788d\u7269\u7684\u611f\u77e5\u6a21\u7cca\u6027\uff08\u7279\u522b\u662f\u5f53\u95e8\u4f4d\u7f6e\u4ec5\u7c97\u7565\u6307\u5b9a\u65f6\uff09\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u521d\u59cb\u8f6f\u78b0\u649e\u8bad\u7ec3\u9636\u6bb5\uff0c\u4fdd\u6301\u9ad8\u901f\u98de\u884c\u7684\u7b56\u7565\u63a2\u7d22\uff1b2\uff09\u786c\u78b0\u649e\u7cbe\u70bc\u9636\u6bb5\uff0c\u5f3a\u5236\u9c81\u68d2\u7684\u969c\u788d\u7269\u907f\u514d\u3002\u91c7\u7528\u81ea\u9002\u5e94\u566a\u58f0\u589e\u5f3a\u8bfe\u7a0b\u5b66\u4e60\u548c\u975e\u5bf9\u79f0actor-critic\u67b6\u6784\uff0c\u9010\u6b65\u5c06\u7b56\u7565\u4f9d\u8d56\u4ece\u7279\u6743\u95e8\u72b6\u6001\u4fe1\u606f\u8f6c\u79fb\u5230\u57fa\u4e8e\u6df1\u5ea6\u7684\u89c6\u89c9\u8f93\u5165\u3002\u65bd\u52a0Lipschitz\u7ea6\u675f\u5e76\u96c6\u6210\u8d5b\u9053\u57fa\u5143\u751f\u6210\u5668\uff0c\u589e\u5f3a\u8fd0\u52a8\u7a33\u5b9a\u6027\u548c\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u8ba1\u7b97\u53d7\u9650\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u7cfb\u7edf\u5b9e\u73b0\u4e86\u654f\u6377\u98de\u884c\uff0c\u540c\u65f6\u5bf9\u95e8\u4f4d\u7f6e\u8bef\u5dee\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5f00\u53d1\u51fa\u80fd\u591f\u5728\u591a\u6837\u5316\u3001\u90e8\u5206\u672a\u77e5\u548c\u6742\u4e71\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u53ef\u6cdb\u5316\u65e0\u4eba\u673a\u7ade\u901f\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u7ade\u901f\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u901f\u5ea6\u4e0e\u5b89\u5168\uff0c\u5b9e\u73b0\u4e86\u8de8\u73af\u5883\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u65e0\u4eba\u673a\u7ade\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09619", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09619", "abs": "https://arxiv.org/abs/2512.09619", "authors": ["Minghao Guo", "Meng Cao", "Jiachen Tao", "Rongtao Xu", "Yan Yan", "Xiaodan Liang", "Ivan Laptev", "Xiaojun Chang"], "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models", "comment": null, "summary": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.", "AI": {"tldr": "GLaD\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5728\u9884\u8bad\u7ec3\u4e2d\u878d\u51653D\u51e0\u4f55\u5148\u9a8c\uff0c\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4ec5\u4f7f\u7528RGB\u4fe1\u606f\u7684\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56RGB\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u63a8\u7406\u548c\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u51e0\u4f55\u7ebf\u7d22\u3002\u51e0\u4f55\u4fe1\u606f\u5bf9\u4e8e\u673a\u5668\u4eba\u7406\u89e33D\u7a7a\u95f4\u548c\u6267\u884c\u7cbe\u786e\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165GLaD\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c063D\u51e0\u4f55\u5148\u9a8c\u878d\u5165\u9884\u8bad\u7ec3\u8fc7\u7a0b\u3002\u4e0d\u662f\u7b80\u5355\u5730\u5c06\u51e0\u4f55\u7279\u5f81\u84b8\u998f\u5230\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u800c\u662f\u5c06LLM\u4e2d\u5bf9\u5e94\u89c6\u89c9token\u7684\u9690\u85cf\u72b6\u6001\u4e0e\u51bb\u7ed3\u7684\u51e0\u4f55\u611f\u77e5\u89c6\u89c9\u53d8\u6362\u5668(VGGT)\u7684\u7279\u5f81\u5bf9\u9f50\uff0c\u786e\u4fdd\u51e0\u4f55\u7406\u89e3\u6df1\u5ea6\u96c6\u6210\u5230\u9a71\u52a8\u52a8\u4f5c\u9884\u6d4b\u7684\u591a\u6a21\u6001\u8868\u793a\u4e2d\u3002", "result": "\u5728Bridge\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cGLaD\u5728\u56db\u4e2aLIBERO\u4efb\u52a1\u5957\u4ef6\u4e2d\u5b9e\u73b0\u4e8694.1%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f7f\u7528\u76f8\u540c\u9884\u8bad\u7ec3\u6570\u636e\u7684UniVLA(92.5%)\u3002\u8fd9\u8868\u660e\u51e0\u4f55\u611f\u77e5\u9884\u8bad\u7ec3\u589e\u5f3a\u4e86\u7a7a\u95f4\u63a8\u7406\u548c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u51e0\u4f55\u611f\u77e5\u9884\u8bad\u7ec3\u80fd\u591f\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u7b56\u7565\u6cdb\u5316\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u663e\u5f0f\u6df1\u5ea6\u4f20\u611f\u5668\u62163D\u6807\u6ce8\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u65b9\u6cd5\u3002"}}
{"id": "2512.09656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09656", "abs": "https://arxiv.org/abs/2512.09656", "authors": ["Nicolas Marticorena", "Tobias Fischer", "Niko Suenderhauf"], "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat", "comment": "9 pages, 5 figures", "summary": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.", "AI": {"tldr": "ReMoSPLAT\uff1a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u548c\u4e8c\u6b21\u89c4\u5212\u7684\u53cd\u5e94\u5f0f\u79fb\u52a8\u673a\u68b0\u81c2\u63a7\u5236\u5668\uff0c\u80fd\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u8ddf\u8e2a\u540c\u65f6\u907f\u969c", "motivation": "\u53cd\u5e94\u5f0f\u63a7\u5236\u80fd\u4f18\u96c5\u534f\u8c03\u79fb\u52a8\u673a\u68b0\u81c2\u57fa\u5ea7\u548c\u624b\u81c2\u8fd0\u52a8\uff0c\u4f46\u5982\u4f55\u5728\u4e0d\u6d89\u53ca\u6602\u8d35\u89c4\u5212\u7684\u60c5\u51b5\u4e0b\u878d\u5165\u7cbe\u786e\u73af\u5883\u8868\u793a\u4ee5\u907f\u969c\u4ecd\u662f\u6311\u6218", "method": "\u63d0\u51faReMoSPLAT\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u516c\u5f0f\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u8fdb\u884c\u78b0\u649e\u907f\u514d\uff0c\u901a\u8fc7\u4f18\u5316\u516c\u5f0f\u4e2d\u7684\u989d\u5916\u7ea6\u675f\u548c\u6210\u672c\u5b9e\u73b0\u76ee\u6807", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u65b9\u6cd5\u53ef\u884c\u6027\uff0c\u6027\u80fd\u63a5\u8fd1\u4f9d\u8d56\u5b8c\u7f8e\u5730\u9762\u771f\u5b9e\u4fe1\u606f\u7684\u63a7\u5236\u5668\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u9ad8\u6548\u8ba1\u7b97\u673a\u5668\u4eba-\u969c\u788d\u7269\u8ddd\u79bb\u65b9\u6cd5\u7684\u6743\u8861", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u672b\u7aef\u59ff\u6001\u8ddf\u8e2a\u548c\u907f\u969c\uff0c\u5c55\u793a\u4e86\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u5728\u53cd\u5e94\u5f0f\u63a7\u5236\u4e2d\u7684\u5b9e\u7528\u6027"}}
{"id": "2512.09798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09798", "abs": "https://arxiv.org/abs/2512.09798", "authors": ["Misael Mamani", "Mariel Fernandez", "Grace Luna", "Steffani Limachi", "Leonel Apaza", "Carolina Montes-D\u00e1valos", "Marcelo Herrera", "Edwin Salcedo"], "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle", "comment": null, "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u592a\u9633\u80fd\u9a71\u52a8\u7684\u5168\u81ea\u4e3b\u65e0\u4eba\u6c34\u9762\u8247\uff0c\u914d\u5907\u65b0\u578b\u6ce8\u5c04\u5668\u91c7\u6837\u7cfb\u7edf\uff0c\u5355\u6b21\u4efb\u52a1\u53ef\u91c7\u96c672\u4e2a\u79bb\u6563\u6c34\u6837\uff0c\u7ed3\u5408ROS 2\u81ea\u4e3b\u5bfc\u822a\u6808\u548c\u6a21\u5757\u5316\u91c7\u6837\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u6c34\u8d28\u76d1\u6d4b\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u6c34\u9762\u8247\u91c7\u6837\u80fd\u529b\u6709\u9650\uff0c\u901a\u5e38\u53ea\u80fd\u91c7\u96c6\u5c11\u91cf\u6837\u672c\u6216\u4f9d\u8d56\u4ee3\u8868\u6027\u5dee\u7684\u5355\u70b9\u4f20\u611f\u5668\uff0c\u65e0\u6cd5\u6ee1\u8db3\u51c6\u786e\u6c34\u8d28\u8bc4\u4f30\u6240\u9700\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u8981\u6c42\u3002", "method": "\u5f00\u53d1\u592a\u9633\u80fd\u9a71\u52a8\u5168\u81ea\u4e3bUSV\uff0c\u91c7\u7528\u6ce8\u5c04\u5668\u91c7\u6837\u67b6\u6784\uff086\u00d712\u6a21\u5757\u5316\u7cfb\u7edf\uff09\uff0c\u96c6\u6210ROS 2\u81ea\u4e3b\u6808\uff08GPS-RTK\u5bfc\u822a\u3001LiDAR\u548c\u7acb\u4f53\u89c6\u89c9\u969c\u788d\u68c0\u6d4b\u3001Nav2\u4efb\u52a1\u89c4\u5212\uff09\u3001\u884c\u4e3a\u6811\u81ea\u4e3b\u67b6\u6784\u548c\u5206\u5e03\u5f0fmicro-ROS\u8282\u70b9\u63a7\u5236\u3002", "result": "\u5728\u73bb\u5229\u7ef4\u4e9a\u62c9\u5df4\u65afAchocalla\u6cfb\u6e56\u7684\u5b9e\u5730\u6d4b\u8bd5\u663e\u793a\uff1a\u822a\u70b9\u7cbe\u5ea6\u8fbe87%\uff0c\u7a33\u5b9a\u81ea\u4e3b\u5bfc\u822a\uff0c\u91c7\u96c6\u7684\u7269\u7406\u5316\u5b66\u53c2\u6570\uff08\u6e29\u5ea6\u3001pH\u3001\u7535\u5bfc\u7387\u3001\u603b\u6eb6\u89e3\u56fa\u4f53\uff09\u4e0e\u624b\u52a8\u53c2\u8003\u6570\u636e\u76f8\u5f53\uff0c\u91c7\u6837\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u8d85\u8fc7\u73b0\u6709USV\u91c7\u6837\u5668\u3002", "conclusion": "\u8be5\u5e73\u53f0\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u9ad8\u5206\u8fa8\u7387\u91c7\u6837\u548c\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\uff0c\u4e3a\u504f\u8fdc\u73af\u5883\u7684\u6c34\u751f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u8d28\u8bc4\u4f30\u7684\u7a7a\u95f4\u4ee3\u8868\u6027\u3002"}}
{"id": "2512.09833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09833", "abs": "https://arxiv.org/abs/2512.09833", "authors": ["Elias Krantz", "Ngai Nam Chan", "Gunnar Tibert", "Huina Mao", "Christer Fuglesang"], "title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration", "comment": "Presented at the International Conference on Space Robotics (iSpaRo) 2025. To appear in IEEE Xplore", "summary": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u901a\u4fe1\u6865\uff0c\u8fde\u63a5Basilisk\u822a\u5929\u5668\u52a8\u529b\u5b66\u4eff\u771f\u5668\u548cROS 2\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53cc\u5411\u6570\u636e\u4ea4\u6362\uff0c\u652f\u6301\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u7684\u65e0\u7f1d\u8fc7\u6e21", "motivation": "\u5c06\u9ad8\u4fdd\u771f\u822a\u5929\u5668\u4eff\u771f\u5668\u4e0e\u6a21\u5757\u5316\u673a\u5668\u4eba\u6846\u67b6\u96c6\u6210\u4ecd\u7136\u662f\u81ea\u4e3b\u6027\u5f00\u53d1\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u4e24\u8005\u4e4b\u95f4\u7684\u901a\u4fe1\u548c\u6570\u636e\u4ea4\u6362\u95ee\u9898", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u901a\u4fe1\u6865\uff0c\u65e0\u9700\u4fee\u6539Basilisk\u6838\u5fc3\u4ee3\u7801\uff0c\u53ef\u4e0eROS 2\u8282\u70b9\u65e0\u7f1d\u96c6\u6210\uff0c\u652f\u6301\u5b9e\u65f6\u53cc\u5411\u6570\u636e\u4ea4\u6362", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7f16\u961f\u98de\u884c\u573a\u666f\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5728\u4eff\u771f\u548cATMOS\u5e73\u9762\u5fae\u91cd\u529b\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5b9e\u73b0\u76f8\u540c\u90e8\u7f72", "conclusion": "\u8be5\u901a\u4fe1\u6865\u4e3a\u6a21\u5757\u5316\u822a\u5929\u5668\u81ea\u4e3b\u6027\u548c\u53ef\u91cd\u590d\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5feb\u901f\u5f00\u53d1\u3001\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u548c\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u7684\u65e0\u7f1d\u8fc7\u6e21"}}
{"id": "2512.09851", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09851", "abs": "https://arxiv.org/abs/2512.09851", "authors": ["Yuyang Li", "Yinghan Chen", "Zihang Zhao", "Puhao Li", "Tengyu Liu", "Siyuan Huang", "Yixin Zhu"], "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation", "comment": null, "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.", "AI": {"tldr": "TacThru\u4f20\u611f\u5668\u5b9e\u73b0\u540c\u65f6\u89c6\u89c9\u89e6\u89c9\u611f\u77e5\uff0cTacThru-UMI\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u4fe1\u53f7\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff0c\u57285\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\u8fbe\u523085.5%\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ea4\u66ff\u611f\u77e5\u548c\u7eaf\u89c6\u89c9\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u900f\u76ae\u4f20\u611f\u5668\u7f3a\u4e4f\u540c\u65f6\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\u4e14\u89e6\u89c9\u8ddf\u8e2a\u4e0d\u53ef\u9760\uff0c\u540c\u65f6\u5c06\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4fe1\u53f7\u96c6\u6210\u5230\u57fa\u4e8e\u5b66\u4e60\u7684\u64cd\u4f5c\u7ba1\u9053\u4e2d\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u63d0\u51faTacThru\u4f20\u611f\u5668\uff08\u5168\u900f\u660e\u5f39\u6027\u4f53\u3001\u6301\u7eed\u7167\u660e\u3001\u65b0\u578b\u5173\u952e\u7ebf\u6807\u8bb0\u3001\u9ad8\u6548\u8ddf\u8e2a\uff09\u548cTacThru-UMI\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff08\u57fa\u4e8eTransformer\u7684\u6269\u6563\u7b56\u7565\u96c6\u6210\u591a\u6a21\u6001\u4fe1\u53f7\uff09\u3002", "result": "\u57285\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cTacThru-UMI\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523085.5%\uff0c\u663e\u8457\u4f18\u4e8e\u4ea4\u66ff\u89e6\u89c9-\u89c6\u89c9\u611f\u77e5\uff0866.3%\uff09\u548c\u7eaf\u89c6\u89c9\uff0855.4%\uff09\u57fa\u7ebf\u3002\u5728\u8584\u8f6f\u7269\u4f53\u63a5\u89e6\u68c0\u6d4b\u548c\u9700\u8981\u591a\u6a21\u6001\u534f\u8c03\u7684\u7cbe\u786e\u64cd\u4f5c\u7b49\u5173\u952e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u540c\u65f6\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u73b0\u4ee3\u5b66\u4e60\u6846\u67b6\u7684\u7ed3\u5408\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2512.09903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09903", "abs": "https://arxiv.org/abs/2512.09903", "authors": ["Ryan Meegan", "Adam D'Souza", "Bryan Bo Cao", "Shubham Jain", "Kristin Dana"], "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos", "comment": null, "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.", "AI": {"tldr": "YOPO-Nav\uff1a\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u8f68\u8ff9\u7684\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\uff0c\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u7f16\u7801\u73af\u5883\uff0c\u901a\u8fc7\u5206\u5c42\u5b9a\u4f4d\u548c\u59ff\u6001\u7ec6\u5316\u5b9e\u73b0\u673a\u5668\u4eba\u5bfc\u822a", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5bfc\u822a\u4f9d\u8d56\u8be6\u7ec6\u5730\u56fe\u548c\u8def\u5f84\u89c4\u5212\uff0c\u6784\u5efa\u548c\u7ef4\u62a43D\u5730\u56fe\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5185\u5b58\u5bc6\u96c6\u3002\u672c\u6587\u5229\u7528\u63a2\u7d22\u89c6\u9891\u4f5c\u4e3a\u89c6\u89c9\u53c2\u8003\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u91cd\u8d70\u5df2\u63a2\u7d22\u8f68\u8ff9\u800c\u65e0\u9700\u4f9d\u8d56\u5ea6\u91cf\u5730\u56fe\u3002", "method": "\u63d0\u51faYOPO-Nav\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u7f16\u7801\u4e3a\u76f8\u4e92\u8fde\u63a5\u7684\u5c40\u90e83D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u7ec4\u6210\u7684\u7d27\u51d1\u7a7a\u95f4\u8868\u793a\u3002\u91c7\u7528\u5206\u5c42\u8bbe\u8ba1\uff1a\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6a21\u5757\u63d0\u4f9b\u7c97\u7565\u5b9a\u4f4d\uff0c\u5c40\u90e83DGS\u6a21\u578b\u7ec6\u5316\u76ee\u6807\u548c\u4e2d\u95f4\u59ff\u6001\u4ee5\u751f\u6210\u63a7\u5236\u52a8\u4f5c\u3002", "result": "\u5f15\u5165YOPO-Campus\u6570\u636e\u96c6\uff084\u5c0f\u65f6\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\uff0c\u8d85\u8fc76\u516c\u91cc\u4eba\u5de5\u9065\u63a7\u673a\u5668\u4eba\u8f68\u8ff9\uff09\u3002\u5728Clearpath Jackal\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\uff0cYOPO-Nav\u5728\u771f\u5b9e\u573a\u666f\u7684\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "YOPO-Nav\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u8f68\u8ff9\u7684\u9ad8\u6548\u89c6\u89c9\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf3D\u5730\u56fe\u7684\u6784\u5efa\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2512.09927", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09927", "abs": "https://arxiv.org/abs/2512.09927", "authors": ["Yifan Ye", "Jiaqi Ma", "Jun Cen", "Zhihe Lu"], "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "comment": "8 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}", "AI": {"tldr": "TEAM-VLA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u4ee4\u724c\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u548c\u5408\u5e76\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u4ee4\u724c\u6765\u52a0\u901f\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u901a\u5e38\u6709\u6570\u5341\u4ebf\u53c2\u6570\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u90e8\u7f72\u65f6\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u5ef6\u8fdf\u654f\u611f\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faTEAM-VLA\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u4ee4\u724c\u6269\u5c55\u673a\u5236\uff08\u5728\u6ce8\u610f\u529b\u9ad8\u4eae\u533a\u57df\u9644\u8fd1\u91c7\u6837\u989d\u5916\u4fe1\u606f\u4ee4\u724c\uff09\u548c\u9009\u62e9\u6027\u4ee4\u724c\u5408\u5e76\u673a\u5236\uff08\u5728\u6df1\u5c42\u8fdb\u884c\u52a8\u4f5c\u611f\u77e5\u6307\u5bfc\u7684\u4ee4\u724c\u5408\u5e76\uff09\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b8c\u6210\u6269\u5c55\u4e0e\u5408\u5e76\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTEAM-VLA\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u5b8c\u6574VLA\u6a21\u578b\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "TEAM-VLA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u4ee4\u724c\u538b\u7f29\u65b9\u6848\uff0c\u5728\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.09928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09928", "abs": "https://arxiv.org/abs/2512.09928", "authors": ["Minghui Lin", "Pengxiang Ding", "Shu Wang", "Zifeng Zhuang", "Yang Liu", "Xinyang Tong", "Wenxuan Song", "Shangke Lyu", "Siteng Huang", "Donglin Wang"], "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models", "comment": "Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA", "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.", "AI": {"tldr": "HiF-VLA\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u8fd0\u52a8\u4fe1\u606f\u8fdb\u884c\u53cc\u5411\u65f6\u5e8f\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u540e\u89c1\u3001\u6d1e\u89c1\u548c\u9884\u89c1\u673a\u5236\u63d0\u5347VLA\u6a21\u578b\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u80fd\u529b\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5927\u591a\u5047\u8bbe\u9a6c\u5c14\u53ef\u592b\u6027\uff0c\u4ec5\u4f9d\u8d56\u5f53\u524d\u89c2\u6d4b\uff0c\u5bfc\u81f4\u65f6\u5e8f\u77ed\u89c6\u95ee\u9898\uff0c\u5f71\u54cd\u957f\u65f6\u7a0b\u64cd\u4f5c\u7684\u8fde\u8d2f\u6027\u3002\u8fd0\u52a8\u4f5c\u4e3a\u66f4\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u65f6\u5e8f\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u80fd\u6355\u6349\u72b6\u6001\u95f4\u53d8\u5316\u5e76\u8fc7\u6ee4\u9759\u6001\u50cf\u7d20\u566a\u58f0\u3002", "method": "\u63d0\u51faHiF-VLA\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u8fd0\u52a8\u8fdb\u884c\u53cc\u5411\u65f6\u5e8f\u63a8\u7406\uff1a1\uff09\u901a\u8fc7\u540e\u89c1\u5148\u9a8c\u7f16\u7801\u8fc7\u53bb\u52a8\u6001\uff1b2\uff09\u901a\u8fc7\u9884\u89c1\u63a8\u7406\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\uff1b3\uff09\u901a\u8fc7\u540e\u89c1\u8c03\u5236\u7684\u8054\u5408\u4e13\u5bb6\u6574\u5408\u4e24\u8005\uff0c\u5b9e\u73b0\"\u8fb9\u884c\u52a8\u8fb9\u601d\u8003\"\u8303\u5f0f\u3002", "result": "\u5728LIBERO-Long\u548cCALVIN ABC-D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u53ef\u5ffd\u7565\u3002\u5728\u771f\u5b9e\u4e16\u754c\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u6709\u6548\u6027\u3002", "conclusion": "HiF-VLA\u901a\u8fc7\u5229\u7528\u8fd0\u52a8\u4f5c\u4e3a\u65f6\u5e8f\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u65f6\u5e8f\u77ed\u89c6\u95ee\u9898\uff0c\u4e3a\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53cc\u5411\u65f6\u5e8f\u63a8\u7406\u6846\u67b6\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
