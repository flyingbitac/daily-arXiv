{"id": "2602.21259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21259", "abs": "https://arxiv.org/abs/2602.21259", "authors": ["Ricardo B. Grando", "Victor A. Kich", "Alisson H. Kolling", "Junior C. D. Jesus", "Rodrigo S. Guerra", "Paulo L. J. Drews-Jr"], "title": "Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles", "comment": "Accepted to the Brazilian Conference on Robotics 2026", "summary": "Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles."}
{"id": "2602.21266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21266", "abs": "https://arxiv.org/abs/2602.21266", "authors": ["Mor Levenhar", "Itzik Klein"], "title": "Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints", "comment": "12 pages, 5 figuers", "summary": "Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models."}
{"id": "2602.21302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21302", "abs": "https://arxiv.org/abs/2602.21302", "authors": ["Krishna Suresh", "Chris Atkeson"], "title": "Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control", "comment": "Project website: https://flying-knots.github.io", "summary": "Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io"}
{"id": "2602.21316", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21316", "abs": "https://arxiv.org/abs/2602.21316", "authors": ["Milad Azizkhani", "Yue Chen"], "title": "Unified Complementarity-Based Contact Modeling and Planning for Soft Robots", "comment": "9 pages, 4 figures", "summary": "Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics."}
{"id": "2602.21331", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21331", "abs": "https://arxiv.org/abs/2602.21331", "authors": ["Nelson Chen", "William R. Johnson", "Rebecca Kramer-Bottiglio", "Kostas Bekris", "Mridul Aanjaneya"], "title": "CableRobotGraphSim: A Graph Neural Network for Modeling Partially Observable Cable-Driven Robot Dynamics", "comment": null, "summary": "General-purpose simulators have accelerated the development of robots. Traditional simulators based on first-principles, however, typically require full-state observability or depend on parameter search for system identification. This work presents \\texttt{CableRobotGraphSim}, a novel Graph Neural Network (GNN) model for cable-driven robots that aims to address shortcomings of prior simulation solutions. By representing cable-driven robots as graphs, with the rigid-bodies as nodes and the cables and contacts as edges, this model can quickly and accurately match the properties of other simulation models and real robots, while ingesting only partially observable inputs. Accompanying the GNN model is a sim-and-real co-training procedure that promotes generalization and robustness to noisy real data. This model is further integrated with a Model Predictive Path Integral (MPPI) controller for closed-loop navigation, which showcases the model's speed and accuracy."}
{"id": "2602.21366", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21366", "abs": "https://arxiv.org/abs/2602.21366", "authors": ["Y. Deemo Chen", "Arion Zimmermann", "Thomas A. Berrueta", "Soon-Jo Chung"], "title": "Environment-Aware Learning of Smooth GNSS Covariance Dynamics for Autonomous Racing", "comment": "8 pages, Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "Ensuring accurate and stable state estimation is a challenging task crucial to safety-critical domains such as high-speed autonomous racing, where measurement uncertainty must be both adaptive to the environment and temporally smooth for control. In this work, we develop a learning-based framework, LACE, capable of directly modeling the temporal dynamics of GNSS measurement covariance. We model the covariance evolution as an exponentially stable dynamical system where a deep neural network (DNN) learns to predict the system's process noise from environmental features through an attention mechanism. By using contraction-based stability and systematically imposing spectral constraints, we formally provide guarantees of exponential stability and smoothness for the resulting covariance dynamics. We validate our approach on an AV-24 autonomous racecar, demonstrating improved localization performance and smoother covariance estimates in challenging, GNSS-degraded environments. Our results highlight the promise of dynamically modeling the perceived uncertainty in state estimation problems that are tightly coupled with control sensitivity."}
{"id": "2602.21389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21389", "abs": "https://arxiv.org/abs/2602.21389", "authors": ["Zach J. Patterson", "Emily Sologuren", "Levi Cai", "Daniel Kim", "Alaa Maalouf", "Pascal Spino", "Daniela Rus"], "title": "Autonomous Sea Turtle Robot for Marine Fieldwork", "comment": "22 pages, 3 figures, 1 table, 5 supplementary figures, 1 supplementary table. Submitted for review", "summary": "Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems."}
{"id": "2602.21418", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21418", "abs": "https://arxiv.org/abs/2602.21418", "authors": ["Mohammadsaleh Razmi", "Iman Shojaei"], "title": "Event-Driven On-Sensor Locomotion Mode Recognition Using a Shank-Mounted IMU with Embedded Machine Learning for Exoskeleton Control", "comment": "10 pages, 6 figures. Sensor-level HAR using embedded IMU machine learning for wearable robotics", "summary": "This work presents a wearable human activity recognition (HAR) system that performs real-time inference directly inside a shank-mounted inertial measurement unit (IMU) to support low-latency control of a lower-limb exoskeleton. Unlike conventional approaches that continuously stream raw inertial data to a microcontroller for classification, the proposed system executes activity recognition at the sensor level using the embedded Machine Learning Core (MLC) of the STMicroelectronics LSM6DSV16X IMU, allowing the host microcontroller to remain in a low-power state and read only the recognized activity label from IMU registers. While the system generalizes to multiple human activities, this paper focuses on three representative locomotion modes - stance, level walking, and stair ascent - using data collected from adult participants. A lightweight decision-tree model was configured and deployed for on-sensor execution using ST MEMS Studio, enabling continuous operation without custom machine learning code on the microcontroller. During operation, the IMU asserts an interrupt when motion or a new classification is detected; the microcontroller wakes, reads the MLC output registers, and forwards the inferred mode to the exoskeleton controller. This interrupt-driven, on-sensor inference architecture reduces computation and communication overhead while preserving battery energy and improving robustness in distinguishing level walking from stair ascent for torque-assist control."}
{"id": "2602.21445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21445", "abs": "https://arxiv.org/abs/2602.21445", "authors": ["Haoxuan Wang", "Gengyu Zhang", "Yan Yan", "Ramana Rao Kompella", "Gaowen Liu"], "title": "VLA Knows Its Limits", "comment": "Project page at https://hatchetproject.github.io/autohorizon/", "summary": "Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models."}
{"id": "2602.21450", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.21450", "abs": "https://arxiv.org/abs/2602.21450", "authors": ["Felipe Bartelt", "Vinicius M. GonÃ§alves", "Luciano C. A. Pimenta"], "title": "Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups", "comment": null, "summary": "This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness."}
{"id": "2602.21531", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.21531", "abs": "https://arxiv.org/abs/2602.21531", "authors": ["Yue Yang", "Shuo Cheng", "Yu Fang", "Homanga Bharadhwaj", "Mingyu Ding", "Gedas Bertasius", "Daniel Szafir"], "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies", "comment": null, "summary": "General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/."}
{"id": "2602.21583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21583", "abs": "https://arxiv.org/abs/2602.21583", "authors": ["Wentao Zhang", "Zhaoqi Ma", "Jinjie Li", "Huayi Wang", "Haokun Liu", "Junichiro Sugihara", "Chen Chen", "Yicheng Chen", "Moju Zhao"], "title": "Learning Agile and Robust Omnidirectional Aerial Motion on Overactuated Tiltable-Quadrotors", "comment": null, "summary": "Tilt-rotor aerial robots enable omnidirectional maneuvering through thrust vectoring, but introduce significant control challenges due to the strong coupling between joint and rotor dynamics. While model-based controllers can achieve high motion accuracy under nominal conditions, their robustness and responsiveness often degrade in the presence of disturbances and modeling uncertainties. This work investigates reinforcement learning for omnidirectional aerial motion control on over-actuated tiltable quadrotors that prioritizes robustness and agility. We present a learning-based control framework that enables efficient acquisition of coordinated rotor-joint behaviors for reaching target poses in the $SE(3)$ space. To achieve reliable sim-to-real transfer while preserving motion accuracy, we integrate system identification with minimal and physically consistent domain randomization. Compared with a state-of-the-art NMPC controller, the proposed method achieves comparable six-degree-of-freedom pose tracking accuracy, while demonstrating superior robustness and generalization across diverse tasks, enabling zero-shot deployment on real hardware."}
{"id": "2602.21595", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21595", "abs": "https://arxiv.org/abs/2602.21595", "authors": ["Hyungmin Kim", "Hobeom Jeon", "Dohyung Kim", "Minsu Jang", "Jeahong Kim"], "title": "SPOC: Safety-Aware Planning Under Partial Observability And Physical Constraints", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Embodied Task Planning with large language models faces safety challenges in real-world environments, where partial observability and physical constraints must be respected. Existing benchmarks often overlook these critical factors, limiting their ability to evaluate both feasibility and safety. We introduce SPOC, a benchmark for safety-aware embodied task planning, which integrates strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation. Covering diverse household hazards such as fire, fluid, injury, object damage, and pollution, SPOC enables rigorous assessment through both state and constraint-based online metrics. Experiments with state-of-the-art LLMs reveal that current models struggle to ensure safety-aware planning, particularly under implicit constraints. Code and dataset are available at https://github.com/khm159/SPOC"}
{"id": "2602.21599", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21599", "abs": "https://arxiv.org/abs/2602.21599", "authors": ["Weisheng Xu", "Qiwei Wu", "Jiaxi Zhang", "Tan Jing", "Yangfan Li", "Yuetong Fang", "Jiaqi Xiong", "Kai Wu", "Rong Ou", "Renjing Xu"], "title": "Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control", "comment": null, "summary": "Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework."}
{"id": "2602.21612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21612", "abs": "https://arxiv.org/abs/2602.21612", "authors": ["Xuanqi Zeng", "Lingwei Zhang", "Linzhu Yue", "Zhitao Song", "Hongbo Zhang", "Tianlin Zhang", "Yun-Hui Liu"], "title": "Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization", "comment": "8 pages, 12 figures", "summary": "Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m."}
{"id": "2602.21622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21622", "abs": "https://arxiv.org/abs/2602.21622", "authors": ["Enyi Wang", "Wen Fan", "Dandan Zhang"], "title": "ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation", "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA 2026)", "summary": "Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios."}
{"id": "2602.21625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21625", "abs": "https://arxiv.org/abs/2602.21625", "authors": ["Lei Su", "Zhijie Peng", "Renyuan Ren", "Shengping Mao", "Juan Du", "Kaifeng Zhang", "Xuezhou Zhu"], "title": "Tacmap: Bridging the Tactile Sim-to-Real Gap via Geometry-Consistent Penetration Depth Map", "comment": "8 pages", "summary": "Vision-Based Tactile Sensors (VBTS) are essential for achieving dexterous robotic manipulation, yet the tactile sim-to-real gap remains a fundamental bottleneck. Current tactile simulations suffer from a persistent dilemma: simplified geometric projections lack physical authenticity, while high-fidelity Finite Element Methods (FEM) are too computationally prohibitive for large-scale reinforcement learning. In this work, we present Tacmap, a high-fidelity, computationally efficient tactile simulation framework anchored in volumetric penetration depth. Our key insight is to bridge the tactile sim-to-real gap by unifying both domains through a shared deform map representation. Specifically, we compute 3D intersection volumes as depth maps in simulation, while in the real world, we employ an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps. By aligning simulation and real-world in this unified geometric space, Tacmap minimizes domain shift while maintaining physical consistency. Quantitative evaluations across diverse contact scenarios demonstrate that Tacmap's deform maps closely mirror real-world measurements. Moreover, we validate the utility of Tacmap through an in-hand rotation task, where a policy trained exclusively in simulation achieves zero-shot transfer to a physical robot."}
{"id": "2602.21633", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21633", "abs": "https://arxiv.org/abs/2602.21633", "authors": ["Chenyv Liu", "Wentao Tan", "Lei Zhu", "Fengling Li", "Jingjing Li", "Guoli Yang", "Heng Tao Shen"], "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination", "comment": null, "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA."}
{"id": "2602.21644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21644", "abs": "https://arxiv.org/abs/2602.21644", "authors": ["Li Zhang", "Yu-An Liu", "Xijia Jiang", "Conghao Huang", "Danyang Li", "Yanyong Zhang"], "title": "DAGS-SLAM: Dynamic-Aware 3DGS SLAM via Spatiotemporal Motion Probability and Uncertainty-Aware Scheduling", "comment": null, "summary": "Mobile robots and IoT devices demand real-time localization and dense reconstruction under tight compute and energy budgets. While 3D Gaussian Splatting (3DGS) enables efficient dense SLAM, dynamic objects and occlusions still degrade tracking and mapping. Existing dynamic 3DGS-SLAM often relies on heavy optical flow and per-frame segmentation, which is costly for mobile deployment and brittle under challenging illumination. We present DAGS-SLAM, a dynamic-aware 3DGS-SLAM system that maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. DAGS-SLAM fuses lightweight YOLO instance priors with geometric cues to estimate and temporally update MP, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization. Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU, demonstrating a practical speed-accuracy tradeoff with reduced semantic invocations toward mobile deployment."}
{"id": "2602.21666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21666", "abs": "https://arxiv.org/abs/2602.21666", "authors": ["Luying Feng", "Yaochu Jin", "Hanze Hu", "Wei Chen"], "title": "Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits", "comment": null, "summary": "It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper."}
{"id": "2602.21670", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21670", "abs": "https://arxiv.org/abs/2602.21670", "authors": ["Tomoya Kawabe", "Rin Takano"], "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning", "comment": "Accepted to ICRA 2026. 8 pages, 2 figures", "summary": "Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate."}
{"id": "2602.21682", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21682", "abs": "https://arxiv.org/abs/2602.21682", "authors": ["Jishu Miao", "Han Chen", "Jiankun Zhai", "Qi Liu", "Tsubasa Hirakawa", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "title": "SunnyParking: Multi-Shot Trajectory Generation and Motion State Awareness for Human-like Parking", "comment": null, "summary": "Autonomous parking fundamentally differs from on-road driving due to its frequent direction changes and complex maneuvering requirements. However, existing End-to-End (E2E) planning methods often simplify the parking task into a geometric path regression problem, neglecting explicit modeling of the vehicle's kinematic state. This \"dimensionality deficiency\" easily leads to physically infeasible trajectories and deviates from real human driving behavior, particularly at critical gear-shift points in multi-shot parking scenarios. In this paper, we propose SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (e.g., forward/reverse). Additionally, we introduce a Fourier feature-based representation of target parking slots to overcome the resolution limitations of traditional bird's-eye view (BEV) approaches, enabling high-precision target interactions. Experimental results demonstrate that our framework generates more robust and human-like trajectories in complex multi-shot parking scenarios, while significantly improving gear-shift point localization accuracy compared to state-of-the-art methods. We open-source a new parking dataset of the CARLA simulator, specifically designed to evaluate full prediction capabilities under complex maneuvers."}
{"id": "2602.21684", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21684", "abs": "https://arxiv.org/abs/2602.21684", "authors": ["Xiaohan Lei", "Min Wang", "Wengang Zhou", "Xingyu Lu", "Houqiang Li"], "title": "Primary-Fine Decoupling for Action Generation in Robotic Imitation", "comment": "The Fourteenth International Conference on Learning Representations (ICLR), 2026", "summary": "Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation."}
{"id": "2602.21691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21691", "abs": "https://arxiv.org/abs/2602.21691", "authors": ["Yuting Zeng", "Manping Fan", "You Zhou", "Yongbin Yu", "Zhiwen Zheng", "Jingtao Zhang", "Liyong Ren", "Zhenglin Yang"], "title": "Trajectory Generation with Endpoint Regulation and Momentum-Aware Dynamics for Visually Impaired Scenarios", "comment": "9 pages, 7 figures", "summary": "Trajectory generation for visually impaired scenarios requires smooth and temporally consistent state in structured, low-speed dynamic environments. However, traditional jerk-based heuristic trajectory sampling with independent segment generation and conventional smoothness penalties often lead to unstable terminal behavior and state discontinuities under frequent regenerating. This paper proposes a trajectory generation approach that integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize the evolution of velocity and acceleration for segment consistency. Endpoint regulation is incorporated into trajectory sampling to stabilize terminal behavior, while a momentum-aware dynamics enforces consistent velocity and acceleration evolution across consecutive trajectory segments. Experimental results demonstrate reduced acceleration peaks and lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared with a baseline planner."}
{"id": "2602.21696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21696", "abs": "https://arxiv.org/abs/2602.21696", "authors": ["Xiaorui Wang", "Hongwu Wang", "Yue Fan", "Hao Cheng", "Feitian Zhang"], "title": "Dual-Regime Hybrid Aerodynamic Modeling of Winged Blimps With Neural Mixing", "comment": null, "summary": "Winged blimps operate across distinct aerodynamic regimes that cannot be adequately captured by a single model. At high speeds and small angles of attack, their dynamics exhibit strong coupling between lift and attitude, resembling fixed-wing aircraft behavior. At low speeds or large angles of attack, viscous effects and flow separation dominate, leading to drag-driven and damping-dominated dynamics. Accurately representing transitions between these regimes remains a fundamental challenge. This paper presents a hybrid aerodynamic modeling framework that integrates a fixed-wing Aerodynamic Coupling Model (ACM) and a Generalized Drag Model (GDM) using a learned neural network mixer with explicit physics-based regularization. The mixer enables smooth transitions between regimes while retaining explicit, physics-based aerodynamic representation. Model parameters are identified through a structured three-phase pipeline tailored for hybrid aerodynamic modeling. The proposed approach is validated on the RGBlimp platform through a large-scale experimental campaign comprising 1,320 real-world flight trajectories across 330 thruster and moving mass configurations, spanning a wide range of speeds and angles of attack. Experimental results demonstrate that the proposed hybrid model consistently outperforms single-model and predefined-mixer baselines, establishing a practical and robust aerodynamic modeling solution for winged blimps."}
{"id": "2602.21723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21723", "abs": "https://arxiv.org/abs/2602.21723", "authors": ["Yutang Lin", "Jieming Cui", "Yixuan Li", "Baoxiong Jia", "Yixin Zhu", "Siyuan Huang"], "title": "LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations", "comment": null, "summary": "Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments."}
{"id": "2602.21736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21736", "abs": "https://arxiv.org/abs/2602.21736", "authors": ["Hao Luo", "Ye Wang", "Wanpeng Zhang", "Haoqi Yuan", "Yicheng Feng", "Haiweng Xu", "Sipeng Zheng", "Zongqing Lu"], "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild", "comment": "CVPR2026", "summary": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data."}
{"id": "2602.21783", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21783", "abs": "https://arxiv.org/abs/2602.21783", "authors": ["Beatrice Luciani", "Alex van den Berg", "Matti Lang", "Alexandre L. Ratschat", "Laura Marchal-Crespo"], "title": "Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control", "comment": "14 pages, 5 figures, 3 tables", "summary": "Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy."}
{"id": "2602.21811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21811", "abs": "https://arxiv.org/abs/2602.21811", "authors": ["Qingtao Liu", "Zhengnan Sun", "Yu Cui", "Haoming Li", "Gaofeng Li", "Lin Shao", "Jiming Chen", "Qi Ye"], "title": "DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations", "comment": "Accepted by IEEE Transactions on Robotics (T-RO), 2026", "summary": "Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap."}
{"id": "2602.21816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21816", "abs": "https://arxiv.org/abs/2602.21816", "authors": ["Zhaowei Liang", "Song Wang", "Zhao Jin", "Shirui Wu", "Dan Wu"], "title": "Self-Curriculum Model-based Reinforcement Learning for Shape Control of Deformable Linear Objects", "comment": null, "summary": "Precise shape control of Deformable Linear Objects (DLOs) is crucial in robotic applications such as industrial and medical fields. However, existing methods face challenges in handling complex large deformation tasks, especially those involving opposite curvatures, and lack efficiency and precision. To address this, we propose a two-stage framework combining Reinforcement Learning (RL) and online visual servoing. In the large-deformation stage, a model-based reinforcement learning approach using an ensemble of dynamics models is introduced to significantly improve sample efficiency. Additionally, we design a self-curriculum goal generation mechanism that dynamically selects intermediate-difficulty goals with high diversity through imagined evaluations, thereby optimizing the policy learning process. In the small-deformation stage, a Jacobian-based visual servo controller is deployed to ensure high-precision convergence. Simulation results show that the proposed method enables efficient policy learning and significantly outperforms mainstream baselines in shape control success rate and precision. Furthermore, the framework effectively transfers the policy trained in simulation to real-world tasks with zero-shot adaptation. It successfully completes all 30 cases with diverse initial and target shapes across DLOs of different sizes and materials. The project website is available at: https://anonymous.4open.science/w/sc-mbrl-dlo-EB48/"}
{"id": "2602.21899", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.21899", "abs": "https://arxiv.org/abs/2602.21899", "authors": ["Arnau Romero", "Carmen Delgado", "Jana Baguer", "RaÃºl SuÃ¡rez", "Xavier Costa-PÃ©rez"], "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios", "comment": "arXiv admin note: substantial text overlap with arXiv:2403.09177", "summary": "Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilities of 5G/6G networks, we propose a novel SAR framework that includes Mission Planning and Mission Execution phases and that optimizes robot deployment. By considering parameters such as the exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time, our framework determines the minimum number of robots required and their optimal paths to ensure effective coverage and timely data backhaul over mobile networks. Our results demonstrate the trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Further, we quantify the impact of terrain elevation data on mission time and energy consumption, showing the benefits of incorporating real-world environmental factors that might also affect mobile signal propagation and connectivity into SAR planning. This framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations."}
{"id": "2602.21967", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21967", "abs": "https://arxiv.org/abs/2602.21967", "authors": ["Xiangqi Meng", "Pengxu Hou", "Zhenjun Zhao", "Javier Civera", "Daniel Cremers", "Hesheng Wang", "Haoang Li"], "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments", "comment": null, "summary": "In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance."}
{"id": "2602.21983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21983", "abs": "https://arxiv.org/abs/2602.21983", "authors": ["Jingchao Wei", "Jingkai Qin", "Yuxiao Cao", "Jingcheng Huang", "Xiangrui Zeng", "Min Li", "Zhouping Yin"], "title": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots", "comment": "submitted to AIM 2026", "summary": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions."}
{"id": "2602.22001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22001", "abs": "https://arxiv.org/abs/2602.22001", "authors": ["Freek Stulp", "Samuel Bustamante", "JoÃ£o SilvÃ©rio", "Alin Albu-SchÃ¤ffer", "Jeannette Bohg", "Shuran Song"], "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?", "comment": "12 pages, 4 figures", "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology."}
{"id": "2602.22006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22006", "abs": "https://arxiv.org/abs/2602.22006", "authors": ["Jiadong Lu", "Zhehan Li", "Tao Han", "Miao Xu", "Chao Xu", "Yanjun Cao"], "title": "Parallel Continuous-Time Relative Localization with Augmented Clamped Non-Uniform B-Splines", "comment": "26 pages, 23 figures", "summary": "Accurate relative localization is critical for multi-robot cooperation. In robot swarms, measurements from different robots arrive asynchronously and with clock time-offsets. Although Continuous-Time (CT) formulations have proved effective for handling asynchronous measurements in single-robot SLAM and calibration, extending CT methods to multi-robot settings faces great challenges to achieve high-accuracy, low-latency, and high-frequency performance. Especially, existing CT methods suffer from the inherent query-time delay of unclamped B-splines and high computational cost. This paper proposes CT-RIO, a novel Continuous-Time Relative-Inertial Odometry framework. We employ Clamped Non-Uniform B-splines (C-NUBS) to represent robot states for the first time, eliminating the query-time delay. We further augment C-NUBS with closed-form extension and shrinkage operations that preserve the spline shape, making it suitable for online estimation and enabling flexible knot management. This flexibility leads to the concept of knot-keyknot strategy, which supports spline extension at high-frequency while retaining sparse keyknots for adaptive relative-motion modeling. We then formulate a sliding-window relative localization problem that operates purely on relative kinematics and inter-robot constraints. To meet the demanding computation required at swarm scale, we decompose the tightly-coupled optimization into robot-wise sub-problems and solve them in parallel using incremental asynchronous block coordinate descent. Extensive experiments show that CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 s, and achieves RMSEs of 0.046 m and 1.8 Â°. It consistently outperforms state-of-the-art methods, with improvements of up to 60% under high-speed motion."}
{"id": "2602.22010", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22010", "abs": "https://arxiv.org/abs/2602.22010", "authors": ["Yue Su", "Sijin Chen", "Haixin Shi", "Mingyu Liu", "Zhengshen Zhang", "Ningyuan Huang", "Weiheng Zhong", "Zhengbang Zhu", "Yuxiao Liu", "Xihui Liu"], "title": "World Guidance: World Modeling in Condition Space for Action Generation", "comment": "Project Page: https://selen-suyue.github.io/WoGNet/", "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/"}
{"id": "2602.22056", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22056", "abs": "https://arxiv.org/abs/2602.22056", "authors": ["Edgar Welte", "Yitian Shi", "Rosa Wolf", "Maximillian Gilles", "Rania Rayyes"], "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation", "comment": "8 pages, 5 figures", "summary": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics."}
{"id": "2602.22088", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22088", "abs": "https://arxiv.org/abs/2602.22088", "authors": ["Hongjie Fang", "Shirun Tang", "Mingyu Mei", "Haoxiang Qin", "Zihao He", "Jingjing Chen", "Ying Feng", "Chenxi Wang", "Wanxi Liu", "Zaixing He", "Cewu Lu", "Shiquan Wang"], "title": "Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation", "comment": null, "summary": "Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/"}
{"id": "2602.22100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22100", "abs": "https://arxiv.org/abs/2602.22100", "authors": ["Andreas Kernbach", "Daniel Bargmann", "Werner Kraus", "Marco F. Huber"], "title": "Behavioral Cloning for Robotic Connector Assembly: An Empirical Study", "comment": "8 pages", "summary": "Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This work presents an empirical study investigating the suitability of behavioral cloning for learning an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. We compare several network architectures and other design choices using a dataset of up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses. The resulting system is then evaluated against five different connector geometries under varying connector poses, achieving an overall insertion success rate of over 90 %."}
{"id": "2602.22118", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22118", "abs": "https://arxiv.org/abs/2602.22118", "authors": ["Benjamin Bokser", "Daniel Gonzalez", "Surya Singh", "Aaron Preston", "Alex Bahner", "Annika WollschlÃ¤ger", "Arianna Ilvonen", "Asa Eckert-Erdheim", "Ashwin Khadke", "Bilal Hammoud", "Dean Molinaro", "Fabian Jenelten", "Henry Mayne", "Howie Choset", "Igor Bogoslavskyi", "Itic Tinman", "James Tigue", "Jan Preisig", "Kaiyu Zheng", "Kenny Sharma", "Kim Ang", "Laura Lee", "Liana Margolese", "Nicole Lin", "Oscar Frias", "Paul Drews", "Ravi Boggavarapu", "Rick Burnham", "Samuel Zapolsky", "Sangbae Kim", "Scott Biddlestone", "Sean Mayorga", "Shamel Fahmi", "Tyler McCollum", "Velin Dimitrov", "William Moyne", "Yu-Ming Chen", "Farbod Farshidian", "Marco Hutter", "David Perry", "Al Rizzi", "Gabe Nelson"], "title": "System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot", "comment": "19 Pages, 11 figures, 3 movies, 2 tables", "summary": "Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to synthesize a spatial linkage topology with a focus on vertical jump height and momentum-based balancing on a single wheel contact. Using a constrained Reinforcement Learning (RL) framework, we demonstrate zero-shot transfer of diverse athletic behaviors, including track-stands, jumps, wheelies, rear wheel hopping, and front flips. This 23.5 kg robot is capable of high speeds (8 m/s) and jumping on and over large obstacles (1 m tall, or 130% of the robot's nominal height)."}
{"id": "2602.22154", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22154", "abs": "https://arxiv.org/abs/2602.22154", "authors": ["Hossein B. Jond", "Veli BakÄ±rcÄ±oÄlu", "Logan E. Beaver", "Nejat TÃ¼kenmez", "Adel Akbarimajd", "Martin Saska"], "title": "Position-Based Flocking for Persistent Alignment without Velocity Sensing", "comment": null, "summary": "Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent collective motion over extended periods. Simulations with a collective of 50 agents demonstrate that the position-based flocking model attains faster and more sustained directional alignment and results in more compact formations than a velocity-alignment-based baseline. This position-based flocking model is particularly well-suited for real-world robotic swarms, where velocity measurements are unreliable, noisy, or unavailable. Experimental results using a team of nine real wheeled mobile robots are also presented."}
