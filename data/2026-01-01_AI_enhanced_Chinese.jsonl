{"id": "2512.23972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23972", "abs": "https://arxiv.org/abs/2512.23972", "authors": ["Liangtao Feng", "Zhenchang Liu", "Feng Zhang", "Xuefeng Ren"], "title": "SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration", "comment": null, "summary": "This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community.", "AI": {"tldr": "SHIELD\u662f\u4e00\u79cd\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u7403\u9762\u6295\u5f71\u6df7\u5408\u524d\u6cbf\u96c6\u6210\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8d28\u91cf\u4e0d\u4e00\u81f4\u3001\u4f20\u7edf\u524d\u6cbf\u65b9\u6cd5\u8ba1\u7b97\u8d1f\u62c5\u91cd\u4ee5\u53ca\u5f00\u653e\u533a\u57df\u98de\u884c\u5b89\u5168\u7b49\u95ee\u9898\u3002", "motivation": "\u6fc0\u5149\u96f7\u8fbe\u5728\u65e0\u4eba\u673a\u63a2\u7d22\u4e2d\u9762\u4e34\u51e0\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u89c2\u6d4b\u8d28\u91cf\u901a\u5e38\u4f4e\u4e8e\u6df1\u5ea6\u76f8\u673a\uff1b2\uff09\u57fa\u4e8e\u5df2\u77e5/\u672a\u77e5\u533a\u57df\u7684\u4f20\u7edf\u524d\u6cbf\u65b9\u6cd5\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u7279\u522b\u662f\u5904\u7406\u6fc0\u5149\u96f7\u8fbe\u5bbd\u89c6\u573a\u65f6\uff1b3\uff09\u65e0\u70b9\u4e91\u533a\u57df\u96be\u4ee5\u901a\u8fc7\u5149\u7ebf\u6295\u5c04\u5206\u7c7b\u4e3a\u81ea\u7531\u7a7a\u95f4\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u6fc0\u5149\u96f7\u8fbe\u5728\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u4e2d\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "SHIELD\u91c7\u7528\u4e09\u79cd\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u7ef4\u62a4\u89c2\u6d4b\u8d28\u91cf\u5360\u636e\u5730\u56fe\uff0c\u5728\u8be5\u5730\u56fe\u4e0a\u8fdb\u884c\u5149\u7ebf\u6295\u5c04\u4ee5\u89e3\u51b3\u63a2\u7d22\u4e2d\u70b9\u4e91\u8d28\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898\uff1b2\uff09\u4f7f\u7528\u6df7\u5408\u524d\u6cbf\u65b9\u6cd5\u5904\u7406\u8ba1\u7b97\u8d1f\u62c5\u548c\u70b9\u4e91\u8d28\u91cf\u9650\u5236\uff1b3\uff09\u63d0\u51fa\u5411\u5916\u7403\u9762\u6295\u5f71\u5149\u7ebf\u6295\u5c04\u7b56\u7565\uff0c\u5728\u5f00\u653e\u533a\u57df\u5171\u540c\u786e\u4fdd\u98de\u884c\u5b89\u5168\u548c\u63a2\u7d22\u6548\u7387\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u98de\u884c\u5b9e\u9a8c\u8bc1\u660e\u4e86SHIELD\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8d28\u91cf\u95ee\u9898\uff0c\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u5728\u5f00\u653e\u533a\u57df\u786e\u4fdd\u98de\u884c\u5b89\u5168\u5e76\u4fdd\u6301\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "SHIELD\u6210\u529f\u89e3\u51b3\u4e86\u6fc0\u5149\u96f7\u8fbe\u5728\u65e0\u4eba\u673a\u63a2\u7d22\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u81ea\u4e3b\u63a2\u7d22\u65b9\u6848\u3002\u4f5c\u8005\u627f\u8bfa\u5c06\u5f00\u6e90\u8be5\u5de5\u4f5c\u4ee5\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2512.24125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24125", "abs": "https://arxiv.org/abs/2512.24125", "authors": ["Yi Liu", "Sukai Wang", "Dafeng Wei", "Xiaowei Cai", "Linqing Zhong", "Jiange Yang", "Guanghui Ren", "Jinyu Zhang", "Maoqing Yao", "Chuankang Li", "Xindong He", "Liliang Chen", "Jianlan Luo"], "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training", "comment": null, "summary": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation.", "AI": {"tldr": "\u63d0\u51faERIQ\u57fa\u51c6\u548cFACT\u65b9\u6cd5\u6765\u89e3\u51b3VLA\u6a21\u578b\u4e2d\u6cdb\u5316\u4e0e\u7cbe\u786e\u63a7\u5236\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u6267\u884c\u8bc4\u4f30\uff0c\u5e76\u5f00\u53d1\u6d41\u5339\u914d\u52a8\u4f5c\u5206\u8bcd\u5668\u63d0\u5347\u8f68\u8ff9\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u5e7f\u6cdb\u6cdb\u5316\u548c\u9ad8\u7cbe\u5ea6\u52a8\u4f5c\u6267\u884c\uff0c\u4f46\u73b0\u6709VLA\u6a21\u578b\u96be\u4ee5\u5e73\u8861\u8fd9\u4e24\u8005\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u5347\u4e86\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5177\u8eab\u63a8\u7406\u5bfc\u81f4\u884c\u4e3a\u8106\u5f31\uff1b\u800c\u5f3a\u63a8\u7406\u80fd\u529b\u672c\u8eab\u53c8\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "method": "1. \u63d0\u51faERIQ\u57fa\u51c6\uff1a\u5305\u542b6K+\u95ee\u7b54\u5bf9\uff0c\u8986\u76d6\u56db\u4e2a\u63a8\u7406\u7ef4\u5ea6\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u6267\u884c\u6765\u7cfb\u7edf\u8bc4\u4f30\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff1b2. \u63d0\u51faFACT\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u52a8\u4f5c\u5206\u8bcd\u5668\uff0c\u5c06\u8fde\u7eed\u63a7\u5236\u8f6c\u6362\u4e3a\u79bb\u6563\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u8f68\u8ff9\u91cd\u5efa\uff1b3. \u5f00\u53d1GenieReasoner\uff1a\u5728\u7edf\u4e00\u7a7a\u95f4\u4e2d\u8054\u5408\u4f18\u5316\u63a8\u7406\u548c\u52a8\u4f5c\u3002", "result": "ERIQ\u57fa\u51c6\u63ed\u793a\u4e86\u5177\u8eab\u63a8\u7406\u80fd\u529b\u4e0e\u7aef\u5230\u7aefVLA\u6cdb\u5316\u4e4b\u95f4\u7684\u5f3a\u6b63\u76f8\u5173\u6027\u3002FACT\u65b9\u6cd5\u5728\u8f68\u8ff9\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002GenieReasoner\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u8fde\u7eed\u52a8\u4f5c\u548c\u5148\u524d\u79bb\u6563\u52a8\u4f5c\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ERIQ\u548cFACT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\u6765\u8bca\u65ad\u548c\u514b\u670d\u63a8\u7406-\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\uff0c\u63a8\u8fdb\u4e86\u9c81\u68d2\u3001\u901a\u7528\u673a\u5668\u4eba\u64cd\u7eb5\u7684\u53d1\u5c55\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u8bc4\u4f30\u548c\u8054\u5408\u4f18\u5316\uff0c\u4e3a\u5e73\u8861\u6cdb\u5316\u4e0e\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.24129", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24129", "abs": "https://arxiv.org/abs/2512.24129", "authors": ["Manuel Bied", "John Arockiasamy", "Andy Comeca", "Maximilian Schrapel", "Victoria Yang", "Alexey Rolich", "Barbara Bruno", "Maike Schwammberger", "Dieter Fiems", "Alexey Vinel"], "title": "ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving", "comment": null, "summary": "On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u4e4b\u95f4\u7684\u534f\u8c03\u8005\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u6280\u672f\u5b9e\u73b0\uff1a\u5148\u8fdb\u611f\u77e5\u3001\u8f66\u8f86\u901a\u4fe1\u3001\u793e\u4ea4\u4eba\u673a\u4ea4\u4e92\u548c\u5f62\u5f0f\u5316\u89c4\u8303\u3002", "motivation": "\u5728\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u7684\u8fc7\u7a0b\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u5171\u4eab\u9053\u8def\u7684\u6df7\u5408\u4ea4\u901a\u4e0d\u53ef\u907f\u514d\u3002\u5373\u4f7f\u6240\u6709\u8f66\u8f86\u90fd\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u884c\u4eba\u4ecd\u9700\u8fc7\u9a6c\u8def\uff0c\u56e0\u6b64\u9700\u8981\u534f\u8c03\u673a\u5236\u786e\u4fdd\u4ea4\u901a\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u534f\u8c03\u8005\uff0c\u6574\u5408\u56db\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u5148\u8fdb\u611f\u77e5\u6280\u672f\u8ba9\u673a\u5668\u4eba\u611f\u77e5\u73af\u5883\uff1b2) \u8f66\u8f86\u901a\u4fe1\u6280\u672f\u8ba9\u8054\u7f51\u8f66\u8f86\u5171\u4eab\u610f\u56fe\uff0c\u673a\u5668\u4eba\u53d1\u9001\u5f15\u5bfc\u6307\u4ee4\uff1b3) \u793e\u4ea4\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u8ba9\u673a\u5668\u4eba\u6709\u6548\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u548c\u9a7e\u9a76\u5458\u6c9f\u901a\uff1b4) \u5f62\u5f0f\u5316\u89c4\u8303\u6280\u672f\u8ba9\u673a\u5668\u4eba\u7406\u89e3\u4ea4\u901a\u89c4\u5219\u5e76\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u8bba\u6587\u6982\u8ff0\u4e86\u8fd9\u56db\u4e2a\u5173\u952e\u6280\u672f\uff0c\u5e76\u62a5\u544a\u4e86\u524d\u4e09\u4e2a\u6280\u672f\u7684\u521d\u6b65\u6982\u5ff5\u9a8c\u8bc1\u96c6\u6210\uff0c\u8bbe\u60f3\u793e\u4ea4\u673a\u5668\u4eba\u5728\u4e0e\u534f\u4f5c\u5f0f\u81ea\u52a8\u7535\u52a8\u81ea\u884c\u8f66\u573a\u666f\u4e2d\u4e3a\u884c\u4eba\u63d0\u4f9b\u5efa\u8bae\u3002", "conclusion": "\u793e\u4ea4\u673a\u5668\u4eba\u6709\u6f5c\u529b\u6210\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u4e4b\u95f4\u7684\u6709\u6548\u534f\u8c03\u8005\uff0c\u901a\u8fc7\u6574\u5408\u611f\u77e5\u3001\u901a\u4fe1\u3001\u4ea4\u4e92\u548c\u89c4\u8303\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2512.24210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24210", "abs": "https://arxiv.org/abs/2512.24210", "authors": ["Ruoshi Wen", "Guangzeng Chen", "Zhongren Cui", "Min Du", "Yang Gou", "Zhigang Han", "Liqun Huang", "Mingyu Lei", "Yunfei Li", "Zhuohang Li", "Wenlei Liu", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Yutao Ouyang", "Zeyu Ren", "Haixin Shi", "Wei Xu", "Haoxiang Zhang", "Jiajun Zhang", "Xiao Zhang", "Liwei Zheng", "Weiheng Zhong", "Yifei Zhou", "Zhengming Zhu", "Hang Li"], "title": "GR-Dexter Technical Report", "comment": null, "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.", "AI": {"tldr": "GR-Dexter\u662f\u4e00\u4e2a\u9762\u5411\u53cc\u624b\u673a\u5668\u4eba\u7075\u5de7\u624b\u64cd\u4f5c\u7684\u5168\u6808\u786c\u4ef6-\u6a21\u578b-\u6570\u636e\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u51d1\u768421\u81ea\u7531\u5ea6\u7075\u5de7\u624b\u8bbe\u8ba1\u3001\u76f4\u89c2\u7684\u53cc\u624b\u673a\u52a8\u7cfb\u7edf\u548c\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u7a0b\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u5939\u722a\u673a\u5668\u4eba\uff0c\u6269\u5c55\u5230\u5177\u6709\u9ad8\u81ea\u7531\u5ea6\u7684\u53cc\u624b\u673a\u5668\u4eba\u7075\u5de7\u624b\u9762\u4e34\u52a8\u4f5c\u7a7a\u95f4\u6269\u5927\u3001\u624b-\u7269\u4f53\u906e\u6321\u4e25\u91cd\u3001\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7b49\u6311\u6218", "method": "1) \u8bbe\u8ba1\u7d27\u51d1\u768421\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7075\u5de7\u624b\uff1b2) \u5f00\u53d1\u76f4\u89c2\u7684\u53cc\u624b\u673a\u52a8\u7cfb\u7edf\u7528\u4e8e\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\uff1b3) \u63d0\u51fa\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u52a8\u8f68\u8ff9\u3001\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u548c\u7cbe\u5fc3\u7b56\u5212\u7684\u8de8\u5177\u8eab\u6570\u636e\u96c6", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cGR-Dexter\u5728\u957f\u65f6\u7a0b\u65e5\u5e38\u64cd\u4f5c\u548c\u6cdb\u5316\u62fe\u653e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9886\u57df\u5185\u6027\u80fd\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u6307\u4ee4\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027", "conclusion": "GR-Dexter\u4e3a\u5b9e\u73b0\u901a\u7528\u7075\u5de7\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2512.24212", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24212", "abs": "https://arxiv.org/abs/2512.24212", "authors": ["Ming-Ming Yu", "Yi Chen", "B\u00f6rje F. Karlsson", "Wenjun Wu"], "title": "RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation", "comment": null, "summary": "Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.", "AI": {"tldr": "RANGER\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5355\u76ee\u76f8\u673a\uff0c\u65e0\u9700\u6df1\u5ea6\u548c\u59ff\u6001\u4fe1\u606f\uff0c\u5177\u6709\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u89c2\u5bdf\u65b0\u73af\u5883\u7684\u77ed\u89c6\u9891\u5373\u53ef\u63d0\u5347\u4efb\u52a1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u4e25\u91cd\u4f9d\u8d56\u6a21\u62df\u5668\u63d0\u4f9b\u7684\u7cbe\u786e\u6df1\u5ea6\u548c\u59ff\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff1b2\uff09\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\u3002", "method": "\u63d0\u51faRANGER\u6846\u67b6\uff0c\u57fa\u4e8e\u5f3a\u5927\u76843D\u57fa\u7840\u6a21\u578b\uff0c\u5305\u542b\u5173\u952e\u5e273D\u91cd\u5efa\u3001\u8bed\u4e49\u70b9\u4e91\u751f\u6210\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u63a2\u7d22\u4ef7\u503c\u4f30\u8ba1\u3001\u9ad8\u5c42\u81ea\u9002\u5e94\u8def\u5f84\u70b9\u9009\u62e9\u548c\u4f4e\u5c42\u52a8\u4f5c\u6267\u884c\u7b49\u7ec4\u4ef6\u3002", "result": "\u5728HM3D\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cRANGER\u5728\u5bfc\u822a\u6210\u529f\u7387\u548c\u63a2\u7d22\u6548\u7387\u65b9\u9762\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u9002\u5e94\u6027\uff0c\u4e14\u65e0\u9700\u73af\u5883\u7684\u5148\u9a8c3D\u5730\u56fe\u3002", "conclusion": "RANGER\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4ec5\u4f7f\u7528\u5355\u76ee\u76f8\u673a\u7684\u96f6\u6837\u672c\u8bed\u4e49\u5bfc\u822a\uff0c\u5177\u6709\u5f3a\u5927\u7684\u73af\u5883\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.24249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24249", "abs": "https://arxiv.org/abs/2512.24249", "authors": ["Fuqiang Gu", "Jiangshan Ai", "Xu Lu", "Xianlei Long", "Yan Li", "Tao Jiang", "Chao Chen", "Huidong Liu"], "title": "Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning for Accurate and Robust UAV Trajectory Tracking", "comment": "Accepted by IROS 2025 (2025 IEEE/RSJ International Conference on Intelligent Robots and Systems)", "summary": "Unmanned Aerial Vehicles (UAVs) play an important role in various applications, where precise trajectory tracking is crucial. However, conventional control algorithms for trajectory tracking often exhibit limited performance due to the underactuated, nonlinear, and highly coupled dynamics of quadrotor systems. To address these challenges, we propose HBO-PID, a novel control algorithm that integrates the Heteroscedastic Bayesian Optimization (HBO) framework with the classical PID controller to achieve accurate and robust trajectory tracking. By explicitly modeling input-dependent noise variance, the proposed method can better adapt to dynamic and complex environments, and therefore improve the accuracy and robustness of trajectory tracking. To accelerate the convergence of optimization, we adopt a two-stage optimization strategy that allow us to more efficiently find the optimal controller parameters. Through experiments in both simulation and real-world scenarios, we demonstrate that the proposed method significantly outperforms state-of-the-art (SOTA) methods. Compared to SOTA methods, it improves the position accuracy by 24.7% to 42.9%, and the angular accuracy by 40.9% to 78.4%.", "AI": {"tldr": "\u63d0\u51faHBO-PID\u63a7\u5236\u7b97\u6cd5\uff0c\u7ed3\u5408\u5f02\u65b9\u5dee\u8d1d\u53f6\u65af\u4f18\u5316\u4e0e\u7ecf\u5178PID\u63a7\u5236\u5668\uff0c\u63d0\u5347\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027", "motivation": "\u4f20\u7edf\u63a7\u5236\u7b97\u6cd5\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8f68\u8ff9\u8ddf\u8e2a\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u56e0\u4e3a\u56db\u65cb\u7ffc\u7cfb\u7edf\u5177\u6709\u6b20\u9a71\u52a8\u3001\u975e\u7ebf\u6027\u3001\u5f3a\u8026\u5408\u7684\u52a8\u6001\u7279\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6cd5", "method": "\u63d0\u51faHBO-PID\u7b97\u6cd5\uff0c\u5c06\u5f02\u65b9\u5dee\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u4e0e\u7ecf\u5178PID\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8f93\u5165\u4f9d\u8d56\u7684\u566a\u58f0\u65b9\u5dee\u6765\u9002\u5e94\u52a8\u6001\u590d\u6742\u73af\u5883\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u52a0\u901f\u6536\u655b", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4f4d\u7f6e\u7cbe\u5ea6\u63d0\u534724.7%\u81f342.9%\uff0c\u89d2\u5ea6\u7cbe\u5ea6\u63d0\u534740.9%\u81f378.4%", "conclusion": "HBO-PID\u7b97\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u89e3\u51b3\u6b20\u9a71\u52a8\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2512.24288", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24288", "abs": "https://arxiv.org/abs/2512.24288", "authors": ["Yinuo Zhao", "Huiqian Jin", "Lechun Jiang", "Xinyi Zhang", "Kun Wu", "Pei Ren", "Zhiyuan Xu", "Zhengping Che", "Lei Sun", "Dapeng Wu", "Chi Harold Liu", "Jian Tang"], "title": "Real-world Reinforcement Learning from Suboptimal Interventions", "comment": null, "summary": "Real-world reinforcement learning (RL) offers a promising approach to training precise and dexterous robotic manipulation policies in an online manner, enabling robots to learn from their own experience while gradually reducing human labor. However, prior real-world RL methods often assume that human interventions are optimal across the entire state space, overlooking the fact that even expert operators cannot consistently provide optimal actions in all states or completely avoid mistakes. Indiscriminately mixing intervention data with robot-collected data inherits the sample inefficiency of RL, while purely imitating intervention data can ultimately degrade the final performance achievable by RL. The question of how to leverage potentially suboptimal and noisy human interventions to accelerate learning without being constrained by them thus remains open. To address this challenge, we propose SiLRI, a state-wise Lagrangian reinforcement learning algorithm for real-world robot manipulation tasks. Specifically, we formulate the online manipulation problem as a constrained RL optimization, where the constraint bound at each state is determined by the uncertainty of human interventions. We then introduce a state-wise Lagrange multiplier and solve the problem via a min-max optimization, jointly optimizing the policy and the Lagrange multiplier to reach a saddle point. Built upon a human-as-copilot teleoperation system, our algorithm is evaluated through real-world experiments on diverse manipulation tasks. Experimental results show that SiLRI effectively exploits human suboptimal interventions, reducing the time required to reach a 90% success rate by at least 50% compared with the state-of-the-art RL method HIL-SERL, and achieving a 100% success rate on long-horizon manipulation tasks where other RL methods struggle to succeed. Project website: https://silri-rl.github.io/.", "AI": {"tldr": "SiLRI\u662f\u4e00\u79cd\u72b6\u6001\u7ea7\u62c9\u683c\u6717\u65e5\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5728\u7ebf\u673a\u5668\u4eba\u64cd\u4f5c\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675fRL\u4f18\u5316\uff0c\u6709\u6548\u5229\u7528\u6b21\u4f18\u4eba\u7c7b\u5e72\u9884\u52a0\u901f\u5b66\u4e60\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u771f\u5b9e\u4e16\u754cRL\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4eba\u7c7b\u5e72\u9884\u5728\u6574\u4e2a\u72b6\u6001\u7a7a\u95f4\u90fd\u662f\u6700\u4f18\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5373\u4f7f\u662f\u4e13\u5bb6\u64cd\u4f5c\u8005\u4e5f\u65e0\u6cd5\u5728\u6240\u6709\u72b6\u6001\u4e0b\u59cb\u7ec8\u63d0\u4f9b\u6700\u4f18\u52a8\u4f5c\u6216\u5b8c\u5168\u907f\u514d\u9519\u8bef\u3002\u76f2\u76ee\u6df7\u5408\u5e72\u9884\u6570\u636e\u4e0e\u673a\u5668\u4eba\u6536\u96c6\u6570\u636e\u4f1a\u7ee7\u627fRL\u7684\u6837\u672c\u4f4e\u6548\u6027\uff0c\u800c\u7eaf\u7cb9\u6a21\u4eff\u5e72\u9884\u6570\u636e\u6700\u7ec8\u4f1a\u9650\u5236RL\u53ef\u8fbe\u5230\u7684\u6027\u80fd\u3002\u5982\u4f55\u5229\u7528\u53ef\u80fd\u6b21\u4f18\u4e14\u6709\u566a\u58f0\u7684\u4eba\u7c7b\u5e72\u9884\u6765\u52a0\u901f\u5b66\u4e60\u800c\u4e0d\u53d7\u5176\u7ea6\u675f\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u63d0\u51faSiLRI\uff08\u72b6\u6001\u7ea7\u62c9\u683c\u6717\u65e5\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff09\uff0c\u5c06\u5728\u7ebf\u64cd\u4f5c\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675fRL\u4f18\u5316\uff0c\u5176\u4e2d\u6bcf\u4e2a\u72b6\u6001\u7684\u7ea6\u675f\u8fb9\u754c\u7531\u4eba\u7c7b\u5e72\u9884\u7684\u4e0d\u786e\u5b9a\u6027\u51b3\u5b9a\u3002\u5f15\u5165\u72b6\u6001\u7ea7\u62c9\u683c\u6717\u65e5\u4e58\u5b50\uff0c\u901a\u8fc7\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u8054\u5408\u4f18\u5316\u7b56\u7565\u548c\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u4ee5\u8fbe\u5230\u978d\u70b9\u3002\u8be5\u65b9\u6cd5\u5efa\u7acb\u5728\u4eba\u7c7b\u4f5c\u4e3a\u526f\u9a7e\u9a76\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u4e0a\u3002", "result": "\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cSiLRI\u80fd\u6709\u6548\u5229\u7528\u4eba\u7c7b\u6b21\u4f18\u5e72\u9884\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684RL\u65b9\u6cd5HIL-SERL\uff0c\u8fbe\u523090%\u6210\u529f\u7387\u6240\u9700\u65f6\u95f4\u81f3\u5c11\u51cf\u5c1150%\uff0c\u5e76\u5728\u5176\u4ed6RL\u65b9\u6cd5\u96be\u4ee5\u6210\u529f\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b9e\u73b0100%\u6210\u529f\u7387\u3002", "conclusion": "SiLRI\u901a\u8fc7\u72b6\u6001\u7ea7\u7ea6\u675f\u4f18\u5316\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5982\u4f55\u6709\u6548\u5229\u7528\u6b21\u4f18\u4eba\u7c7b\u5e72\u9884\u52a0\u901f\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301RL\u6700\u7ec8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2512.24310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24310", "abs": "https://arxiv.org/abs/2512.24310", "authors": ["TARS Robotics", "Yuhang Zheng", "Jichao Peng", "Weize Li", "Yupeng Zheng", "Xiang Li", "Yujie Jin", "Julong Wei", "Guanhua Zhang", "Ruiling Zheng", "Ming Cao", "Songen Gu", "Zhenhong Zou", "Kaige Li", "Ke Wu", "Mingmin Yang", "Jiahao Liu", "Pengfei Li", "Hengjie Si", "Feiyu Zhu", "Wang Fu", "Likun Wang", "Ruiwen Yao", "Jieru Zhao", "Yilun Chen", "Wenchao Din"], "title": "World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild", "comment": null, "summary": "Large-scale pre-training is fundamental for generalization in language and vision models, but data for dexterous hand manipulation remains limited in scale and diversity, hindering policy generalization. Limited scenario diversity, misaligned modalities, and insufficient benchmarking constrain current human manipulation datasets. To address these gaps, we introduce World In Your Hands (WiYH), a large-scale open-source ecosystem for human-centric manipulation learning. WiYH includes (1) the Oracle Suite, a wearable data collection kit with an auto-labeling pipeline for accurate motion capture; (2) the WiYH Dataset, featuring over 1,000 hours of multi-modal manipulation data across hundreds of skills in diverse real-world scenarios; and (3) extensive annotations and benchmarks supporting tasks from perception to action. Furthermore, experiments based on the WiYH ecosystem show that integrating WiYH's human-centric data significantly enhances the generalization and robustness of dexterous hand policies in tabletop manipulation tasks. We believe that World In Your Hands will bring new insights into human-centric data collection and policy learning to the community.", "AI": {"tldr": "WiYH\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u7a7f\u6234\u6570\u636e\u91c7\u96c6\u5957\u4ef6\u30011000+\u5c0f\u65f6\u591a\u6a21\u6001\u64cd\u4f5c\u6570\u636e\u96c6\u548c\u4e30\u5bcc\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u7075\u5de7\u624b\u64cd\u4f5c\u6570\u636e\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7075\u5de7\u624b\u64cd\u4f5c\u6570\u636e\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u591a\u6837\u6027\u4e0d\u8db3\u3001\u6a21\u6001\u4e0d\u5bf9\u9f50\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u4eba\u7c7b\u4e2d\u5fc3\u64cd\u4f5c\u6570\u636e\u96c6\u6765\u63a8\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faWorld In Your Hands (WiYH)\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u62ec\uff1a(1) Oracle Suite\u53ef\u7a7f\u6234\u6570\u636e\u91c7\u96c6\u5957\u4ef6\uff0c\u914d\u5907\u81ea\u52a8\u6807\u6ce8\u6d41\u6c34\u7ebf\u5b9e\u73b0\u7cbe\u786e\u8fd0\u52a8\u6355\u6349\uff1b(2) WiYH\u6570\u636e\u96c6\uff0c\u5305\u542b1000+\u5c0f\u65f6\u591a\u6a21\u6001\u64cd\u4f5c\u6570\u636e\uff0c\u6db5\u76d6\u6570\u767e\u79cd\u6280\u80fd\u548c\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f\uff1b(3) \u4e30\u5bcc\u7684\u6807\u6ce8\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u4ece\u611f\u77e5\u5230\u52a8\u4f5c\u7684\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6574\u5408WiYH\u7684\u4eba\u7c7b\u4e2d\u5fc3\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u7075\u5de7\u624b\u7b56\u7565\u5728\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u64cd\u4f5c\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "conclusion": "WiYH\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u6570\u636e\u91c7\u96c6\u548c\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\uff0c\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u7075\u5de7\u624b\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u3002"}}
{"id": "2512.24326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24326", "abs": "https://arxiv.org/abs/2512.24326", "authors": ["Camron Alexander Hirst", "Chris Reale", "Eric Frew"], "title": "3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS", "comment": null, "summary": "This paper presents the design, implementation, and flight test results of two novel 3D path-following guidance algorithms based on nonlinear model predictive control (MPC), with specific application to fixed-wing small uncrewed aircraft systems. To enable MPC, control-augmented modelling and system identification of the RAAVEN small uncrewed aircraft is presented. Two formulations of MPC are then showcased. The first schedules a static reference path rate over the MPC horizon, incentivizing a constant inertial speed. The second, with inspiration from model predictive contouring control, dynamically optimizes for the reference path rate over the controller horizon as the system operates. This allows for a weighted tradeoff between path progression and distance from path, two competing objectives in path-following guidance. Both controllers are formulated to operate over general smooth 3D arc-length parameterized curves. The MPC guidance algorithms are flown over several high-curvature test paths, with comparison to a baseline lookahead guidance law. The results showcase the real-world feasibility and superior performance of nonlinear MPC for 3D path-following guidance at ground speeds up to 36 meters per second.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u76843D\u8def\u5f84\u8ddf\u8e2a\u5236\u5bfc\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u56fa\u5b9a\u7ffc\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u98de\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u9ad8\u901f\u98de\u884c\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u56fa\u5b9a\u7ffc\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\u57283D\u8def\u5f84\u8ddf\u8e2a\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u9ad8\u901f\u98de\u884c\u4e2d\u5904\u7406\u9ad8\u66f2\u7387\u8def\u5f84\u7684\u5148\u8fdb\u5236\u5bfc\u7b97\u6cd5\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8def\u5f84\u8fdb\u5c55\u548c\u8def\u5f84\u8ddd\u79bb\u8fd9\u4e24\u4e2a\u7ade\u4e89\u76ee\u6807\u4e4b\u95f4\u96be\u4ee5\u5b9e\u73b0\u4f18\u5316\u5e73\u8861\u3002", "method": "\u9996\u5148\u5bf9RAAVEN\u5c0f\u578b\u65e0\u4eba\u673a\u8fdb\u884c\u63a7\u5236\u589e\u5f3a\u5efa\u6a21\u548c\u7cfb\u7edf\u8fa8\u8bc6\u4ee5\u652f\u6301MPC\u3002\u7136\u540e\u63d0\u51fa\u4e24\u79cdMPC\u7b97\u6cd5\uff1a\u7b2c\u4e00\u79cd\u5728MPC\u65f6\u57df\u5185\u8c03\u5ea6\u9759\u6001\u53c2\u8003\u8def\u5f84\u901f\u7387\uff0c\u6fc0\u52b1\u6052\u5b9a\u60ef\u6027\u901f\u5ea6\uff1b\u7b2c\u4e8c\u79cd\u53d7\u6a21\u578b\u9884\u6d4b\u8f6e\u5ed3\u63a7\u5236\u542f\u53d1\uff0c\u5728\u63a7\u5236\u5668\u65f6\u57df\u5185\u52a8\u6001\u4f18\u5316\u53c2\u8003\u8def\u5f84\u901f\u7387\uff0c\u5141\u8bb8\u5728\u8def\u5f84\u8fdb\u5c55\u548c\u8def\u5f84\u8ddd\u79bb\u4e4b\u95f4\u8fdb\u884c\u52a0\u6743\u6743\u8861\u3002\u4e24\u79cd\u63a7\u5236\u5668\u90fd\u8bbe\u8ba1\u4e3a\u5728\u4e00\u822c\u5149\u6ed13D\u5f27\u957f\u53c2\u6570\u5316\u66f2\u7ebf\u4e0a\u8fd0\u884c\u3002", "result": "\u5728\u591a\u4e2a\u9ad8\u66f2\u7387\u6d4b\u8bd5\u8def\u5f84\u4e0a\u8fdb\u884c\u4e86\u98de\u884c\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u57fa\u51c6\u524d\u77bb\u5236\u5bfc\u5f8b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u975e\u7ebf\u6027MPC\u57283D\u8def\u5f84\u8ddf\u8e2a\u5236\u5bfc\u4e2d\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5728\u5730\u9762\u901f\u5ea6\u9ad8\u8fbe36\u7c73/\u79d2\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e3a\u56fa\u5b9a\u7ffc\u5c0f\u578b\u65e0\u4eba\u673a\u76843D\u8def\u5f84\u8ddf\u8e2a\u5236\u5bfc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u66f2\u7387\u8def\u5f84\u5e76\u5728\u9ad8\u901f\u98de\u884c\u4e2d\u5b9e\u73b0\u8def\u5f84\u8fdb\u5c55\u548c\u8def\u5f84\u8ddd\u79bb\u4e4b\u95f4\u7684\u4f18\u5316\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2512.24384", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24384", "abs": "https://arxiv.org/abs/2512.24384", "authors": ["Yanlong Ma", "Nakul S. Joshi", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "Geometric Multi-Session Map Merging with Learned Local Descriptors", "comment": null, "summary": "Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.", "AI": {"tldr": "GMLD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u5c40\u90e8\u63cf\u8ff0\u7b26\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u4f1a\u8bdd\u70b9\u4e91\u5730\u56fe\u5408\u5e76\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u611f\u77e5\u7f16\u7801\u5668\u548c\u5e73\u9762\u51e0\u4f55\u53d8\u6362\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408\u56e0\u5b50\u56fe\u4f18\u5316\u5b9e\u73b0\u51c6\u786e\u7684\u5730\u56fe\u5bf9\u9f50\u3002", "motivation": "\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8fdb\u884c\u6269\u5c55\u81ea\u4e3b\u64cd\u4f5c\u9700\u8981\u591a\u4f1a\u8bdd\u5730\u56fe\u5408\u5e76\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u4f1a\u8bdd\u5730\u56fe\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u63d0\u53d6\u548c\u5168\u5c40\u4e00\u81f4\u6027\u4f18\u5316\u3002", "method": "\u63d0\u51faGMLD\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5173\u952e\u70b9\u611f\u77e5\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff1b2\uff09\u91c7\u7528\u5e73\u9762\u51e0\u4f55\u53d8\u6362\u5668\u589e\u5f3a\u51e0\u4f55\u7279\u5f81\uff1b3\uff09\u5728\u56e0\u5b50\u56fe\u4f18\u5316\u9636\u6bb5\u52a0\u5165\u8de8\u4f1a\u8bdd\u626b\u63cf\u5339\u914d\u6210\u672c\u56e0\u5b50\u4ee5\u63d0\u5347\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u5730\u56fe\u5408\u5e76\uff0c\u8bef\u5dee\u8f83\u4f4e\uff0c\u5b66\u4e60\u5230\u7684\u7279\u5f81\u5728\u95ed\u73af\u68c0\u6d4b\u548c\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GMLD\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u4f1a\u8bdd\u70b9\u4e91\u5730\u56fe\u5408\u5e76\u95ee\u9898\uff0c\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u5c40\u90e8\u63cf\u8ff0\u7b26\u548c\u5168\u5c40\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9c81\u68d2\u7684\u5730\u56fe\u5bf9\u9f50\u3002"}}
{"id": "2512.24402", "categories": ["cs.RO", "cs.AI", "cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24402", "abs": "https://arxiv.org/abs/2512.24402", "authors": ["Giovanni Lambertini", "Matteo Pini", "Eugenio Mascaro", "Francesco Moretti", "Ayoub Raji", "Marko Bertogna"], "title": "Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack", "comment": "Accepted to the 2026 IEEE/SICE International Symposium on System Integration (SII 2026)", "summary": "In this paper, we describe the automated simulation and reporting pipeline implemented for our autonomous racing stack, ur.autopilot. The backbone of the simulation is based on a high-fidelity model of the vehicle interfaced as a Functional Mockup Unit (FMU). The pipeline can execute the software stack and the simulation up to three times faster than real-time, locally or on GitHub for Continuous Integration/- Continuous Delivery (CI/CD). As the most important input of the pipeline, there is a set of running scenarios. Each scenario allows the initialization of the ego vehicle in different initial conditions (position and speed), as well as the initialization of any other configuration of the stack. This functionality is essential to validate efficiently critical modules, like the one responsible for high-speed overtaking maneuvers or localization, which are among the most challenging aspects of autonomous racing. Moreover, we describe how we implemented a fault injection module, capable of introducing sensor delays and perturbations as well as modifying outputs of any node of the stack. Finally, we describe the design of our automated reporting process, aimed at maximizing the effectiveness of the simulation analysis.", "AI": {"tldr": "\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u8f6f\u4ef6\u6808\u7684\u81ea\u52a8\u5316\u4eff\u771f\u4e0e\u62a5\u544a\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u4ee5\u6700\u9ad8\u4e09\u500d\u4e8e\u5b9e\u65f6\u901f\u5ea6\u6267\u884c\u4eff\u771f\uff0c\u652f\u6301\u672c\u5730\u548cCI/CD\u73af\u5883\uff0c\u5305\u542b\u573a\u666f\u521d\u59cb\u5316\u3001\u6545\u969c\u6ce8\u5165\u548c\u81ea\u52a8\u5316\u62a5\u544a\u529f\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u4eff\u771f\u548c\u62a5\u544a\u7cfb\u7edf\uff0c\u7528\u4e8e\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u8f6f\u4ef6\u6808\u4e2d\u7684\u5173\u952e\u6a21\u5757\uff0c\u7279\u522b\u662f\u9ad8\u901f\u8d85\u8f66\u548c\u5b9a\u4f4d\u7b49\u6700\u5177\u6311\u6218\u6027\u7684\u529f\u80fd\uff0c\u540c\u65f6\u652f\u6301\u6545\u969c\u6ce8\u5165\u548c\u6301\u7eed\u96c6\u6210/\u6301\u7eed\u4ea4\u4ed8\u3002", "method": "\u57fa\u4e8e\u8f66\u8f86\u9ad8\u4fdd\u771f\u6a21\u578b\u6784\u5efa\u4eff\u771f\u7cfb\u7edf\uff0c\u4f7f\u7528\u529f\u80fd\u6a21\u62df\u5355\u5143\u63a5\u53e3\uff0c\u5b9e\u73b0\u4e09\u500d\u4e8e\u5b9e\u65f6\u901f\u5ea6\u7684\u4eff\u771f\u6d41\u6c34\u7ebf\uff1b\u8bbe\u8ba1\u573a\u666f\u521d\u59cb\u5316\u7cfb\u7edf\u652f\u6301\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\uff1b\u5f00\u53d1\u6545\u969c\u6ce8\u5165\u6a21\u5757\u6a21\u62df\u4f20\u611f\u5668\u5ef6\u8fdf\u548c\u6270\u52a8\uff1b\u5efa\u7acb\u81ea\u52a8\u5316\u62a5\u544a\u6d41\u7a0b\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u80fd\u591f\u9ad8\u6548\u9a8c\u8bc1\u5173\u952e\u6a21\u5757\u7684\u81ea\u52a8\u5316\u4eff\u771f\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u672c\u5730\u548cGitHub CI/CD\u73af\u5883\uff0c\u5177\u5907\u573a\u666f\u521d\u59cb\u5316\u3001\u6545\u969c\u6ce8\u5165\u548c\u81ea\u52a8\u5316\u62a5\u544a\u529f\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u8f6f\u4ef6\u6808\u7684\u6d4b\u8bd5\u6548\u7387\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u4eff\u771f\u548c\u62a5\u544a\u6d41\u6c34\u7ebf\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u8f6f\u4ef6\u6808\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u901f\u8d85\u8f66\u548c\u5b9a\u4f4d\u7b49\u6311\u6218\u6027\u573a\u666f\u7684\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6545\u969c\u6ce8\u5165\u548c\u81ea\u52a8\u5316\u62a5\u544a\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5206\u6790\u6548\u7387\u3002"}}
{"id": "2512.24426", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24426", "abs": "https://arxiv.org/abs/2512.24426", "authors": ["Zhenghao \"Mark\" Peng", "Wenhao Ding", "Yurong You", "Yuxiao Chen", "Wenjie Luo", "Thomas Tian", "Yulong Cao", "Apoorva Sharma", "Danfei Xu", "Boris Ivanovic", "Boyi Li", "Bolei Zhou", "Yan Wang", "Marco Pavone"], "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning", "comment": null, "summary": "Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.", "AI": {"tldr": "CF-VLA\u662f\u4e00\u4e2a\u81ea\u53cd\u601d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u884c\u52a8\u524d\u7684\u81ea\u6211\u4fee\u6b63\uff0c\u63d0\u5347\u8f68\u8ff9\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u589e\u5f3a\u7684VLA\u6a21\u578b\u4e3b\u8981\u63cf\u8ff0\u611f\u77e5\u548c\u610f\u56fe\uff0c\u4f46\u5f88\u5c11\u8d28\u7591\u8ba1\u5212\u884c\u52a8\u7684\u5b89\u5168\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u6211\u53cd\u601d\u5e76\u4fee\u6b63\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faCF-VLA\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u65f6\u95f4\u5206\u6bb5\u7684\u5143\u52a8\u4f5c\u603b\u7ed3\u9a7e\u9a76\u610f\u56fe\uff0c\u7136\u540e\u57fa\u4e8e\u5143\u52a8\u4f5c\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u6a21\u62df\u6f5c\u5728\u7ed3\u679c\u3001\u8bc6\u522b\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u8f93\u51fa\u4fee\u6b63\u540e\u7684\u5143\u52a8\u4f5c\u6307\u5bfc\u6700\u7ec8\u8f68\u8ff9\u751f\u6210\u3002\u91c7\u7528rollout-filter-label\u6d41\u7a0b\u4ece\u57fa\u7840VLA\u7684rollout\u4e2d\u6316\u6398\u9ad8\u4ef7\u503c\u573a\u666f\u5e76\u6807\u6ce8\u53cd\u4e8b\u5b9e\u63a8\u7406\u8f68\u8ff9\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728\u5927\u89c4\u6a21\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1aCF-VLA\u5c06\u8f68\u8ff9\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe17.6%\uff0c\u5b89\u5168\u6307\u6807\u63d0\u534720.5%\uff0c\u5e76\u8868\u73b0\u51fa\u9002\u5e94\u6027\u601d\u7ef4\u2014\u2014\u4ec5\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u542f\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002", "conclusion": "CF-VLA\u5c06\u63a8\u7406\u8f68\u8ff9\u4ece\u4e00\u6b21\u6027\u63cf\u8ff0\u8f6c\u53d8\u4e3a\u56e0\u679c\u81ea\u6211\u4fee\u6b63\u4fe1\u53f7\uff0c\u5411\u80fd\u591f\"\u4e09\u601d\u800c\u540e\u884c\"\u7684\u81ea\u53cd\u601d\u81ea\u52a8\u9a7e\u9a76\u667a\u80fd\u4f53\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2512.24428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24428", "abs": "https://arxiv.org/abs/2512.24428", "authors": ["Qian Wang", "Omar Abdellall", "Tony Gao", "Xiatao Sun", "Daniel Rakita"], "title": "Subsecond 3D Mesh Generation for Robot Manipulation", "comment": "In submission", "summary": "3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u5355\u5f20RGB-D\u56fe\u50cf\u57281\u79d2\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u63a5\u5730\u76843D\u7f51\u683c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u901f\u5ea6\u6162\u548c\u7f3a\u4e4f\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "motivation": "3D\u7f51\u683c\u5728\u673a\u5668\u4eba\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u751f\u6210\u9ad8\u4fdd\u771f\u7f51\u683c\u901f\u5ea6\u8fc7\u6162\uff08\u901a\u5e38\u9700\u8981\u6570\u5341\u79d2\uff09\uff0c\u65e0\u6cd5\u5b9e\u65f6\u4f7f\u7528\uff1b2) \u7f51\u683c\u672c\u8eab\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u673a\u5668\u4eba\u9700\u6c42\uff0c\u9700\u8981\u6b63\u786e\u7684\u573a\u666f\u5206\u5272\u3001\u5c3a\u5ea6\u6821\u51c6\u548c\u59ff\u6001\u914d\u51c6\u7b49\u4e0a\u4e0b\u6587\u63a5\u5730\u5904\u7406\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u4e09\u4e2a\u4f18\u5316\u7ec4\u4ef6\uff1a1) \u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u5206\u5272\uff0c2) \u52a0\u901f\u7684\u57fa\u4e8e\u6269\u6563\u7684\u7f51\u683c\u751f\u6210\uff0c3) \u9c81\u68d2\u7684\u70b9\u4e91\u914d\u51c6\u3002\u6574\u4e2a\u6d41\u7a0b\u9488\u5bf9\u901f\u5ea6\u548c\u51c6\u786e\u6027\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u57281\u79d2\u5185\u4ece\u5355\u5f20RGB-D\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u63a5\u5730\u76843D\u7f51\u683c\u3002\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u8bc1\u660e\u7f51\u683c\u53ef\u4ee5\u4f5c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u548c\u89c4\u5212\u7684\u5b9e\u7528\u3001\u6309\u9700\u8868\u793a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e863D\u7f51\u683c\u751f\u6210\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4e0a\u4e0b\u6587\u63a5\u5730\u7684\u7f51\u683c\u751f\u6210\uff0c\u4f7f3D\u7f51\u683c\u6210\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u548c\u89c4\u5212\u7684\u5b9e\u7528\u8868\u793a\u5f62\u5f0f\uff0c\u6709\u671b\u63a8\u52a8\u5b9e\u65f6\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.24470", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24470", "abs": "https://arxiv.org/abs/2512.24470", "authors": ["Kim Alexander Christensen", "Andreas Gudahl Tufte", "Alexey Gusev", "Rohan Sinha", "Milan Ganai", "Ole Andreas Alsos", "Marco Pavoned", "Martin Steinert"], "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models", "comment": "17 pages without bibliography or appendix. The main paper has 16 figures", "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSemantic Lookout\u7cfb\u7edf\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u81ea\u4e3b\u8239\u8236\u63d0\u4f9b\u8bed\u4e49\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u68c0\u6d4b\u64cd\u4f5c\u8bbe\u8ba1\u57df\u5916\u7684\u5f02\u5e38\u60c5\u51b5\u5e76\u9009\u62e9\u5b89\u5168\u7684\u5907\u7528\u673a\u52a8\u65b9\u6848\uff0c\u6ee1\u8db3IMO MASS Code\u8349\u6848\u8981\u6c42\u3002", "motivation": "IMO MASS Code\u8349\u6848\u8981\u6c42\u81ea\u4e3b\u548c\u8fdc\u7a0b\u76d1\u7763\u7684\u6d77\u4e8b\u8239\u8236\u80fd\u591f\u68c0\u6d4b\u64cd\u4f5c\u8bbe\u8ba1\u57df\u7684\u504f\u79bb\uff0c\u8fdb\u5165\u9884\u5b9a\u4e49\u7684\u5907\u7528\u6a21\u5f0f\u5e76\u901a\u77e5\u64cd\u4f5c\u5458\u3002\u4f20\u7edf\u6d77\u4e8b\u81ea\u4e3b\u7cfb\u7edf\u5728\u5904\u7406\u4f9d\u8d56\u8bed\u4e49\u7406\u89e3\u7684\u5f02\u5e38\u60c5\u51b5\uff08\u5982\u6f5c\u6c34\u65d7\u3001\u706b\u707e\u7b49\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u5206\u5e03\u5916\u60c5\u51b5\u3002", "method": "\u63d0\u51faSemantic Lookout\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684\u5019\u9009\u7ea6\u675f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5907\u7528\u673a\u52a8\u9009\u62e9\u5668\u3002\u91c7\u7528\u5feb\u6162\u5f02\u5e38\u68c0\u6d4b\u7ba1\u9053\uff0c\u7ed3\u5408\u77ed\u65f6\u57df\u3001\u4eba\u7c7b\u53ef\u8986\u76d6\u7684\u5907\u7528\u673a\u52a8\u65b9\u6848\u3002\u7cfb\u7edf\u4ece\u6c34\u57df\u6709\u6548\u3001\u4e16\u754c\u951a\u5b9a\u7684\u8f68\u8ff9\u4e2d\u9009\u62e9\u4e00\u4e2a\u8c28\u614e\u52a8\u4f5c\uff08\u6216\u4fdd\u6301\u4f4d\u7f6e\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6301\u7eed\u7684\u4eba\u7c7b\u6743\u9650\u3002", "result": "\u572840\u4e2a\u6e2f\u53e3\u573a\u666f\u4e2d\u6d4b\u8bd5\uff1a10\u79d2\u5185\u6a21\u578b\u4fdd\u7559\u4e86\u6700\u5148\u8fdb\u6162\u901f\u6a21\u578b\u7684\u5927\u90e8\u5206\u611f\u77e5\u80fd\u529b\uff1b\u5907\u7528\u673a\u52a8\u9009\u62e9\u5668\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u51e0\u4f55\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5728\u706b\u707e\u573a\u666f\u4e2d\u589e\u52a0\u4e86\u5b89\u5168\u8ddd\u79bb\uff1b\u73b0\u573a\u8fd0\u884c\u9a8c\u8bc1\u4e86\u7aef\u5230\u7aef\u64cd\u4f5c\u53ef\u884c\u6027\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u8bed\u4e49\u5907\u7528\u673a\u52a8\u9009\u62e9\u5668\uff0c\u4e0eIMO MASS Code\u8349\u6848\u517c\u5bb9\uff0c\u5728\u5b9e\u9645\u5ef6\u8fdf\u9884\u7b97\u5185\u53ef\u884c\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u5173\u6ce8\u9886\u57df\u9002\u5e94\u7684\u6df7\u5408\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u8bed\u4e49\u4e0e\u591a\u4f20\u611f\u5668\u9e1f\u77b0\u611f\u77e5\u548c\u77ed\u65f6\u57df\u91cd\u65b0\u89c4\u5212\u76f8\u7ed3\u5408\u3002"}}
{"id": "2512.24550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24550", "abs": "https://arxiv.org/abs/2512.24550", "authors": ["Tomoya Yamanokuchi", "Alberto Bacchin", "Emilio Olivastri", "Ryotaro Arifuku", "Takamitsu Matsubara", "Emanuele Menegatti"], "title": "DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass", "comment": "48 pages", "summary": "In this work, we address the limitation of surface fitting-based grasp planning algorithm, which primarily focuses on geometric alignment between the gripper and object surface while overlooking the stability of contact point distribution, often resulting in unstable grasps due to inadequate contact configurations. To overcome this limitation, we propose a novel surface fitting algorithm that integrates contact stability while preserving geometric compatibility. Inspired by human grasping behavior, our method disentangles the grasp pose optimization into three sequential steps: (1) rotation optimization to align contact normals, (2) translation refinement to improve the alignment between the gripper frame origin and the object Center of Mass (CoM), and (3) gripper aperture adjustment to optimize contact point distribution. We validate our approach in simulation across 15 objects under both Known-shape (with clean CAD-derived dataset) and Observed-shape (with YCB object dataset) settings, including cross-platform grasp execution on three robot--gripper platforms. We further validate the method in real-world grasp experiments on a UR3e robot. Overall, DISF reduces CoM misalignment while maintaining geometric compatibility, translating into higher grasp success in both simulation and real-world execution compared to baselines. Additional videos and supplementary results are available on our project page: https://tomoya-yamanokuchi.github.io/disf-ras-project-page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u9762\u62df\u5408\u7b97\u6cd5DISF\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u517c\u5bb9\u6027\u7684\u540c\u65f6\u6574\u5408\u63a5\u89e6\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u4e09\u6b65\u4f18\u5316\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u5ffd\u7565\u63a5\u89e6\u70b9\u5206\u5e03\u800c\u5bfc\u81f4\u6293\u53d6\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8868\u9762\u62df\u5408\u7684\u6293\u53d6\u89c4\u5212\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u5939\u722a\u4e0e\u7269\u4f53\u8868\u9762\u7684\u51e0\u4f55\u5bf9\u9f50\uff0c\u4f46\u5ffd\u7565\u4e86\u63a5\u89e6\u70b9\u5206\u5e03\u7684\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u56e0\u63a5\u89e6\u914d\u7f6e\u4e0d\u8db3\u800c\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u6293\u53d6\u3002", "method": "\u53d7\u4eba\u7c7b\u6293\u53d6\u884c\u4e3a\u542f\u53d1\uff0c\u5c06\u6293\u53d6\u59ff\u6001\u4f18\u5316\u5206\u89e3\u4e3a\u4e09\u4e2a\u987a\u5e8f\u6b65\u9aa4\uff1a(1)\u65cb\u8f6c\u4f18\u5316\u4ee5\u5bf9\u9f50\u63a5\u89e6\u6cd5\u7ebf\uff0c(2)\u5e73\u79fb\u7ec6\u5316\u4ee5\u6539\u5584\u5939\u722a\u5750\u6807\u7cfb\u539f\u70b9\u4e0e\u7269\u4f53\u8d28\u5fc3\u7684\u5bf9\u9f50\uff0c(3)\u5939\u722a\u5f00\u5ea6\u8c03\u6574\u4ee5\u4f18\u5316\u63a5\u89e6\u70b9\u5206\u5e03\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5bf915\u4e2a\u7269\u4f53\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5305\u62ec\u5df2\u77e5\u5f62\u72b6\uff08\u5e72\u51c0CAD\u6570\u636e\u96c6\uff09\u548c\u89c2\u6d4b\u5f62\u72b6\uff08YCB\u7269\u4f53\u6570\u636e\u96c6\uff09\u8bbe\u7f6e\uff0c\u5e76\u5728\u4e09\u4e2a\u673a\u5668\u4eba-\u5939\u722a\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u8de8\u5e73\u53f0\u6293\u53d6\u6267\u884c\u3002\u5728UR3e\u673a\u5668\u4eba\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u6293\u53d6\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002DISF\u5728\u4fdd\u6301\u51e0\u4f55\u517c\u5bb9\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u8d28\u5fc3\u9519\u4f4d\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6267\u884c\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u66f4\u9ad8\u7684\u6293\u53d6\u6210\u529f\u7387\u3002", "conclusion": "DISF\u7b97\u6cd5\u901a\u8fc7\u6574\u5408\u63a5\u89e6\u7a33\u5b9a\u6027\u5230\u8868\u9762\u62df\u5408\u8fc7\u7a0b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u5ffd\u7565\u63a5\u89e6\u70b9\u5206\u5e03\u800c\u5bfc\u81f4\u7684\u6293\u53d6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u517c\u5bb9\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002"}}
{"id": "2512.24638", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24638", "abs": "https://arxiv.org/abs/2512.24638", "authors": ["Qingda Hu", "Ziheng Qiu", "Zijun Xu", "Kaizhao Zhang", "Xizhou Bu", "Zuolei Sun", "Bo Zhang", "Jieru Zhao", "Zhongxue Gan", "Wenchao Ding"], "title": "Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding", "comment": null, "summary": "State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/", "AI": {"tldr": "PAM\u662f\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u5de5\u4f5c\u8bb0\u5fc6\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u652f\u6301300\u5e27\u5386\u53f2\u7a97\u53e3\uff0c\u80fd\u5728\u4fdd\u630120Hz\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u5904\u7406\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u72b6\u6001\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u666e\u904d\u5b58\u5728\u72b6\u6001\u6a21\u7cca\u95ee\u9898\uff0c\u76f8\u540c\u89c2\u6d4b\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2a\u6709\u6548\u884c\u4e3a\u8f68\u8ff9\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5386\u53f2\u7a97\u53e3\u8fc7\u77ed\u65e0\u6cd5\u6709\u6548\u63d0\u53d6\u4fe1\u606f\uff0c\u8981\u4e48\u5386\u53f2\u7a97\u53e3\u8fc7\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u5f0f\uff0c\u5305\u542b\u5206\u5c42\u5e27\u7279\u5f81\u63d0\u53d6\u5668\uff08\u5206\u522b\u63d0\u53d6\u8fd0\u52a8\u57fa\u5143\u548c\u65f6\u5e8f\u6d88\u6b67\u7279\u5f81\uff09\u3001\u57fa\u4e8e\u8303\u56f4\u7279\u5b9a\u67e5\u8be2\u7684\u4e0a\u4e0b\u6587\u8def\u7531\u5668\uff08\u751f\u6210\u7d27\u51d1\u7684\u8de8\u5386\u53f2\u957f\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\uff09\uff0c\u4ee5\u53ca\u91cd\u5efa\u5386\u53f2\u4fe1\u606f\u7684\u8f85\u52a9\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u76847\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86PAM\u80fd\u540c\u65f6\u5904\u7406\u591a\u79cd\u72b6\u6001\u6a21\u7cca\u573a\u666f\uff0c\u652f\u6301\u7ea610\u79d2\u7684\u5386\u53f2\u7a97\u53e3\uff0c\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3\u548c20Hz\u4ee5\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "PAM\u901a\u8fc7\u81ea\u9002\u5e94\u5de5\u4f5c\u8bb0\u5fc6\u673a\u5236\uff0c\u4ee5\u6700\u5c0f\u989d\u5916\u8bad\u7ec3\u6210\u672c\u5b9e\u73b0\u4e86\u957f\u5386\u53f2\u7a97\u53e3\u7684\u9ad8\u6548\u5904\u7406\uff0c\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u72b6\u6001\u6a21\u7cca\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.24651", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24651", "abs": "https://arxiv.org/abs/2512.24651", "authors": ["Yury Kolomeytsev", "Dmitry Golembiovsky"], "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation", "comment": "22 pages, 4 figures", "summary": "Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.", "AI": {"tldr": "HMP-DRL\uff1a\u4e00\u79cd\u6df7\u5408\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u641c\u7d22\u5168\u5c40\u89c4\u5212\u5668\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5956\u52b1\u673a\u5236\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u56fe\u641c\u7d22\u89c4\u5212\u5668\u64c5\u957f\u957f\u8ddd\u79bb\u8def\u5f84\u89c4\u5212\u4f46\u7f3a\u4e4f\u53cd\u5e94\u6027\uff0c\u800c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u907f\u78b0\u65b9\u9762\u8868\u73b0\u826f\u597d\u4f46\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u5bfc\u81f4\u96be\u4ee5\u5230\u8fbe\u8fdc\u8ddd\u79bb\u76ee\u6807\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u63d0\u51faHMP-DRL\u6df7\u5408\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u56fe\u641c\u7d22\u5168\u5c40\u89c4\u5212\u5668\u751f\u6210\u8def\u5f84\uff1b2\uff09\u5c06\u8def\u5f84\u8f6c\u6362\u4e3a\u4e00\u7cfb\u5217\u68c0\u67e5\u70b9\uff0c\u7f16\u7801\u5230\u5c40\u90e8DRL\u7b56\u7565\u7684\u72b6\u6001\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\u4e2d\uff1b3\uff09\u91c7\u7528\u5b9e\u4f53\u611f\u77e5\u5956\u52b1\u7ed3\u6784\uff0c\u6839\u636e\u5468\u56f4\u667a\u80fd\u4f53\u7684\u8bed\u4e49\u7c7b\u578b\u52a8\u6001\u8c03\u6574\u5b89\u5168\u8fb9\u754c\u548c\u60e9\u7f5a\u3002", "result": "\u5728\u57fa\u4e8e\u771f\u5b9e\u5730\u56fe\u6570\u636e\u7684\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u6d4b\u8bd5\uff0cHMP-DRL\u5728\u673a\u5668\u4eba\u5bfc\u822a\u5173\u952e\u6307\u6807\uff08\u6210\u529f\u7387\u3001\u78b0\u649e\u7387\u3001\u5230\u8fbe\u76ee\u6807\u65f6\u95f4\uff09\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5305\u62ec\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u957f\u671f\u8def\u5f84\u6307\u5bfc\u4e0e\u8bed\u4e49\u611f\u77e5\u7684\u5c40\u90e8\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u80fd\u663e\u8457\u63d0\u5347\u590d\u6742\u4eba\u672c\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.24653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24653", "abs": "https://arxiv.org/abs/2512.24653", "authors": ["Chengkai Hou", "Kun Wu", "Jiaming Liu", "Zhengping Che", "Di Wu", "Fei Liao", "Guangrun Li", "Jingyang He", "Qiuxuan Feng", "Zhao Jin", "Chenyang Gu", "Zhuoyang Liu", "Nuowei Han", "Xiangju Mi", "Yaoxu Lv", "Yankai Fu", "Gaole Dai", "Langzhe Gu", "Tao Li", "Yuheng Zhang", "Yixue Zhang", "Xinhua Wang", "Shichao Fan", "Meng Li", "Zhen Zhao", "Ning Liu", "Zhiyuan Xu", "Pei Ren", "Junjie Ji", "Haonan Liu", "Kuan Cheng", "Shanghang Zhang", "Jian Tang"], "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence", "comment": null, "summary": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.", "AI": {"tldr": "RoboMIND 2.0\u662f\u4e00\u4e2a\u5305\u542b31\u4e07\u6761\u53cc\u81c2\u64cd\u4f5c\u8f68\u8ff9\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u6db5\u76d66\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u548c739\u4e2a\u590d\u6742\u4efb\u52a1\uff0c\u5305\u542b\u89e6\u89c9\u589e\u5f3a\u548c\u79fb\u52a8\u64cd\u4f5c\u6570\u636e\uff0c\u5e76\u914d\u67092\u4e07\u6761\u6a21\u62df\u8f68\u8ff9\u7684\u6570\u5b57\u5b6a\u751f\u3002MIND-2\u7cfb\u7edf\u91c7\u7528\u5206\u5c42\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5305\u542b\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6267\u884c\u5668\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u5728\u957f\u89c6\u91ce\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u64cd\u4f5c\u65b9\u9762\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86RoboMIND 2.0\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc731\u4e07\u6761\u53cc\u81c2\u64cd\u4f5c\u8f68\u8ff9\uff0c\u6db5\u76d66\u79cd\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u548c739\u4e2a\u590d\u6742\u4efb\u52a1\uff0c\u7279\u522b\u5305\u542b1.2\u4e07\u6761\u89e6\u89c9\u589e\u5f3a\u8f68\u8ff9\u548c2\u4e07\u6761\u79fb\u52a8\u64cd\u4f5c\u8f68\u8ff9\u3002\u540c\u65f6\u6784\u5efa\u4e86\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u73af\u5883\uff0c\u53d1\u5e03\u4e862\u4e07\u6761\u6a21\u62df\u8f68\u8ff9\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86MIND-2\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u5206\u5c42\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u5668\uff08MIND-2-VLM\uff09\u548c\u4f4e\u5c42\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6267\u884c\u5668\uff08MIND-2-VLA\uff09\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u89c4\u6a21\u8fdc\u8d85\u73b0\u6709\u6570\u636e\u96c6\uff0c\u7279\u522b\u9488\u5bf9\u63a5\u89e6\u4e30\u5bcc\u548c\u7a7a\u95f4\u6269\u5c55\u4efb\u52a1\u63d0\u4f9b\u4e86\u89e6\u89c9\u589e\u5f3a\u548c\u79fb\u52a8\u64cd\u4f5c\u6570\u636e\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5c06\u62bd\u8c61\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u5177\u4f53\u5b50\u76ee\u6807\uff0c\u5e76\u751f\u6210\u7cbe\u786e\u672c\u4f53\u611f\u77e5\u52a8\u4f5c\u7684\u5206\u5c42\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "RoboMIND 2.0\u6570\u636e\u96c6\u548cMIND-2\u7cfb\u7edf\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u6709\u671b\u63a8\u52a8\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u3001\u79fb\u52a8\u64cd\u4f5c\u548c\u957f\u89c6\u91ce\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2512.24657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24657", "abs": "https://arxiv.org/abs/2512.24657", "authors": ["Sungjae Min", "Hyungjoo Kim", "David Hyunchul Shim"], "title": "Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids", "comment": "Preprint", "summary": "Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u5316\u4eff\u4eba\u624b\uff0c\u91c7\u7528\u9c8d\u767b\u7ebf\u9a71\u52a8\u548c\u6eda\u52a8\u63a5\u89e6\u5173\u8282\u4f18\u5316\uff0c\u5b9e\u73b0\u5355\u7535\u673a\u63a7\u5236\u3001\u9ad8\u6293\u63e1\u529b\u548c\u4f4e\u8fdc\u7aef\u8d28\u91cf", "motivation": "\u7c7b\u4eba\u673a\u5668\u4eba\u9700\u8981\u5177\u5907\u9ad8\u6293\u63e1\u529b\u3001\u5feb\u901f\u9a71\u52a8\u3001\u591a\u81ea\u7531\u5ea6\u3001\u8f7b\u91cf\u5316\u4e14\u5c3a\u5bf8\u63a5\u8fd1\u4eba\u624b\u7684\u624b\u90e8\uff0c\u4f46\u73b0\u6709\u8bbe\u8ba1\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u77db\u76fe\u9700\u6c42\uff0c\u901a\u5e38\u5bfc\u81f4\u8f83\u91cd\u9a71\u52a8\u5668\u548c\u7b28\u91cd\u4f20\u52a8\u7cfb\u7edf", "method": "\u91c7\u7528\u9c8d\u767b\u7ebf\u9a71\u52a8\uff0c\u7ed3\u5408\u6eda\u52a8\u63a5\u89e6\u5173\u8282\u4f18\u5316\u4e0e\u62ee\u6297\u5f0f\u7ebf\u7f06\u9a71\u52a8\uff0c\u5b9e\u73b0\u5355\u7535\u673a\u63a7\u5236\u4e14\u7ebf\u7f06\u957f\u5ea6\u53d8\u5316\u6781\u5c0f\uff1b\u5c06\u9a71\u52a8\u6a21\u5757\u79fb\u81f3\u8eaf\u5e72\u4ee5\u51cf\u5c11\u8fdc\u7aef\u8d28\u91cf", "result": "\u624b\u90e8\u8fdc\u7aef\u8d28\u91cf\u4ec5236g\uff0c\u6307\u5c16\u529b\u8d85\u8fc718N\uff0c\u80fd\u4e3e\u8d77\u8d85\u8fc7\u81ea\u8eab\u8d28\u91cf100\u500d\u7684\u8d1f\u8f7d\uff1b\u901a\u8fc7Cutkosky\u5206\u7c7b\u6293\u63e1\u548c\u6270\u52a8\u4e0b\u7684\u8f68\u8ff9\u4e00\u81f4\u6027\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027", "conclusion": "\u8be5\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86\u4eff\u4eba\u624b\u5728\u8f7b\u91cf\u5316\u3001\u9ad8\u6293\u63e1\u529b\u3001\u5feb\u901f\u9a71\u52a8\u548c\u591a\u81ea\u7531\u5ea6\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u7c7b\u4eba\u673a\u5668\u4eba\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7075\u5de7\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.24673", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24673", "abs": "https://arxiv.org/abs/2512.24673", "authors": ["Yongsheng Zhao", "Lei Zhao", "Baoping Cheng", "Gongxin Yao", "Xuanzhang Wen", "Han Gao"], "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots", "comment": null, "summary": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.", "AI": {"tldr": "VLA-RAIL\u6846\u67b6\u901a\u8fc7\u5f02\u6b65\u63a8\u7406\u548c\u8f68\u8ff9\u5e73\u6ed1\u6280\u672f\uff0c\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6296\u52a8\u3001\u505c\u987f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u9ad8\u901f\u7684\u52a8\u4f5c\u6267\u884c\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u52a8\u4f5c\u6267\u884c\u4e2d\u5b58\u5728\u6296\u52a8\u3001\u505c\u6ede\u751a\u81f3\u6682\u505c\u7684\u95ee\u9898\uff0c\u8fd9\u4e0d\u4ec5\u9650\u5236\u4e86\u6267\u884c\u901f\u5ea6\uff0c\u8fd8\u964d\u4f4e\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u52a8\u4f5c\u5757\u878d\u5408\u7b56\u7565\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u4fdd\u8bc1\u5e73\u6ed1\u3001\u8fde\u7eed\u3001\u9ad8\u901f\u7684\u52a8\u4f5c\u6267\u884c\u3002", "method": "\u63d0\u51faVLA-RAIL\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u8f68\u8ff9\u5e73\u6ed1\u5668 - \u4f7f\u7528\u591a\u9879\u5f0f\u62df\u5408\u8fc7\u6ee4\u5355\u4e2a\u52a8\u4f5c\u5757\u8f68\u8ff9\u4e2d\u7684\u566a\u58f0\u548c\u6296\u52a8\uff1b2) \u5757\u878d\u5408\u5668 - \u65e0\u7f1d\u5bf9\u9f50\u5f53\u524d\u6267\u884c\u8f68\u8ff9\u548c\u65b0\u5230\u8fbe\u7684\u52a8\u4f5c\u5757\uff0c\u786e\u4fdd\u8fde\u7eed\u52a8\u4f5c\u5757\u4e4b\u95f4\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u8fde\u7eed\u6027\u3002\u6846\u67b6\u91c7\u7528\u5f02\u6b65\u63a8\u7406\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u65b9\u5f0f\u3002", "result": "\u5728\u52a8\u6001\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVLA-RAIL\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u52a8\u6296\u52a8\uff0c\u63d0\u9ad8\u4e86\u6267\u884c\u901f\u5ea6\uff0c\u5e76\u6539\u5584\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "VLA-RAIL\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5c06\u6210\u4e3aVLA\u6a21\u578b\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u80fd\u591f\u5b9e\u73b0\u5e73\u6ed1\u3001\u8fde\u7eed\u3001\u9ad8\u901f\u7684\u52a8\u4f5c\u6267\u884c\u3002"}}
{"id": "2512.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24680", "abs": "https://arxiv.org/abs/2512.24680", "authors": ["Kangjie Zhou", "Zhaoyang Li", "Han Gao", "Yao Su", "Hangxin Liu", "Junzhi Yu", "Chang Liu"], "title": "ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments", "comment": "17 pages, 12 figures, accepted to IEEE Transactions on Systems, Man, and Cybernetics: Systems", "summary": "Target search and tracking (SAT) is a fundamental problem for various robotic applications such as search and rescue and environmental exploration. This paper proposes an informative trajectory planning approach, namely ReSPIRe, for SAT in unknown cluttered environments under considerably inaccurate prior target information and limited sensing field of view. We first develop a novel sigma point-based approximation approach to fast and accurately estimate mutual information reward under non-Gaussian belief distributions, utilizing informative sampling in state and observation spaces to mitigate the computational intractability of integral calculation. To tackle significant uncertainty associated with inadequate prior target information, we propose the hierarchical particle structure in ReSPIRe, which not only extracts critical particles for global route guidance, but also adjusts the particle number adaptively for planning efficiency. Building upon the hierarchical structure, we develop the reusable belief tree search approach to build a policy tree for online trajectory planning under uncertainty, which reuses rollout evaluation to improve planning efficiency. Extensive simulations and real-world experiments demonstrate that ReSPIRe outperforms representative benchmark methods with smaller MI approximation error, higher search efficiency, and more stable tracking performance, while maintaining outstanding computational efficiency.", "AI": {"tldr": "ReSPIRe\u662f\u4e00\u79cd\u7528\u4e8e\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u76ee\u6807\u641c\u7d22\u4e0e\u8ddf\u8e2a\u7684\u4fe1\u606f\u5316\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7sigma\u70b9\u8fd1\u4f3c\u4f30\u8ba1\u4e92\u4fe1\u606f\u5956\u52b1\uff0c\u91c7\u7528\u5206\u5c42\u7c92\u5b50\u7ed3\u6784\u548c\u53ef\u91cd\u7528\u4fe1\u5ff5\u6811\u641c\u7d22\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u76ee\u6807\u641c\u7d22\u4e0e\u8ddf\u8e2a\u662f\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u4f46\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u9762\u4e34\u5148\u9a8c\u76ee\u6807\u4fe1\u606f\u4e0d\u51c6\u786e\u3001\u4f20\u611f\u5668\u89c6\u91ce\u6709\u9650\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7b49\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u3002", "method": "1) \u5f00\u53d1sigma\u70b9\u8fd1\u4f3c\u65b9\u6cd5\u5feb\u901f\u51c6\u786e\u4f30\u8ba1\u975e\u9ad8\u65af\u4fe1\u5ff5\u5206\u5e03\u4e0b\u7684\u4e92\u4fe1\u606f\u5956\u52b1\uff1b2) \u63d0\u51fa\u5206\u5c42\u7c92\u5b50\u7ed3\u6784\u63d0\u53d6\u5173\u952e\u7c92\u5b50\u8fdb\u884c\u5168\u5c40\u8def\u5f84\u5f15\u5bfc\uff0c\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u7c92\u5b50\u6570\u91cf\uff1b3) \u57fa\u4e8e\u5206\u5c42\u7ed3\u6784\u5f00\u53d1\u53ef\u91cd\u7528\u4fe1\u5ff5\u6811\u641c\u7d22\u65b9\u6cd5\uff0c\u91cd\u7528rollout\u8bc4\u4f30\u63d0\u9ad8\u5728\u7ebf\u8f68\u8ff9\u89c4\u5212\u6548\u7387\u3002", "result": "ReSPIRe\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u5c0f\u7684\u4e92\u4fe1\u606f\u8fd1\u4f3c\u8bef\u5dee\u3001\u66f4\u9ad8\u7684\u641c\u7d22\u6548\u7387\u548c\u66f4\u7a33\u5b9a\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u51fa\u8272\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "ReSPIRe\u901a\u8fc7\u521b\u65b0\u7684sigma\u70b9\u8fd1\u4f3c\u3001\u5206\u5c42\u7c92\u5b50\u7ed3\u6784\u548c\u53ef\u91cd\u7528\u4fe1\u5ff5\u6811\u641c\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u76ee\u6807\u641c\u7d22\u4e0e\u8ddf\u8e2a\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2512.24688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24688", "abs": "https://arxiv.org/abs/2512.24688", "authors": ["Zhehan Li", "Zheng Wang", "Jiadong Lu", "Qi Liu", "Zhiren Xun", "Yue Wang", "Fei Gao", "Chao Xu", "Yanjun Cao"], "title": "CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System", "comment": "21 pages, 23 figures, journal", "summary": "Relative localization is critical for cooperation in autonomous multi-robot systems. Existing approaches either rely on shared environmental features or inertial assumptions or suffer from non-line-of-sight degradation and outliers in complex environments. Robust and efficient fusion of inter-robot measurements such as bearings, distances, and inertials for tens of robots remains challenging. We present CREPES-X (Cooperative RElative Pose Estimation System with multiple eXtended features), a hierarchical relative localization framework that enhances speed, accuracy, and robustness under challenging conditions, without requiring any global information. CREPES-X starts with a compact hardware design: InfraRed (IR) LEDs, an IR camera, an ultra-wideband module, and an IMU housed in a cube no larger than 6cm on each side. Then CREPES-X implements a two-stage hierarchical estimator to meet different requirements, considering speed, accuracy, and robustness. First, we propose a single-frame relative estimator that provides instant relative poses for multi-robot setups through a closed-form solution and robust bearing outlier rejection. Then a multi-frame relative estimator is designed to offer accurate and robust relative states by exploring IMU pre-integration via robocentric relative kinematics with loosely- and tightly-coupled optimization. Extensive simulations and real-world experiments validate the effectiveness of CREPES-X, showing robustness to up to 90% bearing outliers, proving resilience in challenging conditions, and achieving RMSE of 0.073m and 1.817\u00b0 in real-world datasets.", "AI": {"tldr": "CREPES-X\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u51d1\u786c\u4ef6\u8bbe\u8ba1\u548c\u4e24\u9636\u6bb5\u4f30\u8ba1\u5668\u5b9e\u73b0\u9ad8\u901f\u3001\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5168\u5c40\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u5171\u4eab\u73af\u5883\u7279\u5f81\u6216\u60ef\u6027\u5047\u8bbe\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u6613\u53d7\u975e\u89c6\u8ddd\u5e72\u6270\u548c\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u96be\u4ee5\u9c81\u68d2\u9ad8\u6548\u5730\u878d\u5408\u591a\u79cd\u6d4b\u91cf\u6570\u636e\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u786c\u4ef6\u8bbe\u8ba1\uff08\u7ea2\u5916LED\u3001\u7ea2\u5916\u76f8\u673a\u3001\u8d85\u5bbd\u5e26\u6a21\u5757\u3001IMU\uff09\uff0c\u5b9e\u73b0\u4e24\u9636\u6bb5\u5206\u5c42\u4f30\u8ba1\u5668\uff1a\u5355\u5e27\u76f8\u5bf9\u4f30\u8ba1\u5668\u63d0\u4f9b\u5373\u65f6\u76f8\u5bf9\u4f4d\u59ff\uff0c\u591a\u5e27\u76f8\u5bf9\u4f30\u8ba1\u5668\u901a\u8fc7IMU\u9884\u79ef\u5206\u548c\u4f18\u5316\u63d0\u4f9b\u7cbe\u786e\u76f8\u5bf9\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u5bf9\u9ad8\u8fbe90%\u7684\u65b9\u4f4d\u5f02\u5e38\u503c\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.073\u7c73\u548c1.817\u00b0\u7684RMSE\u7cbe\u5ea6\u3002", "conclusion": "CREPES-X\u6846\u67b6\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u901f\u3001\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\uff0c\u65e0\u9700\u5168\u5c40\u4fe1\u606f\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2512.24698", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24698", "abs": "https://arxiv.org/abs/2512.24698", "authors": ["Dongyun Kang", "Min-Gyu Kim", "Tae-Gyu Song", "Hajun Kim", "Sehoon Ha", "Hae-Won Park"], "title": "Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer", "comment": "8 pages. Submitted to the IEEE for possible publication", "summary": "Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5ef6\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7b80\u5316\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u6a21\u578b\u540c\u4f26\u8f6c\u79fb\uff0c\u9ad8\u6548\u751f\u6210\u548c\u4f18\u5316\u817f\u5f0f\u673a\u5668\u4eba\u7684\u52a8\u6001\u8fd0\u52a8\u3002", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u751f\u6210\u4ecd\u5177\u6311\u6218\u6027\u3002\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u5956\u52b1\u8c03\u4f18\u6216\u9ad8\u8d28\u91cf\u6f14\u793a\uff0c\u800c\u7b80\u5316\u6a21\u578b\u5b58\u5728\u6a21\u578b\u5dee\u5f02\u95ee\u9898\uff0c\u96be\u4ee5\u8fc1\u79fb\u5230\u5168\u8eab\u52a8\u529b\u5b66\u73af\u5883\u3002", "method": "1. \u4f7f\u7528\u5355\u521a\u4f53\u6a21\u578b\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u7b80\u5316\u73af\u5883\u4e2d\u6355\u83b7\u6838\u5fc3\u8fd0\u52a8\u6a21\u5f0f\uff1b2. \u91c7\u7528\u5ef6\u7eed\u7b56\u7565\u9010\u6b65\u5c06\u7b56\u7565\u8fc1\u79fb\u5230\u5168\u8eab\u73af\u5883\uff1b3. \u901a\u8fc7\u8d28\u91cf\u60ef\u6027\u5728\u8eaf\u5e72\u548c\u817f\u90e8\u95f4\u7684\u6e10\u8fdb\u91cd\u5206\u5e03\uff0c\u5b9a\u4e49\u4ece\u5355\u521a\u4f53\u5230\u5168\u8eab\u6a21\u578b\u7684\u540c\u4f26\u8def\u5f84\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\uff0c\u8fc1\u79fb\u8fc7\u7a0b\u66f4\u7a33\u5b9a\u3002\u5728\u7ffb\u8f6c\u3001\u5899\u9762\u8f85\u52a9\u673a\u52a8\u7b49\u52a8\u6001\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u6210\u529f\u90e8\u7f72\u5230\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ef6\u7eed\u5b66\u4e60\u6846\u67b6\u80fd\u9ad8\u6548\u751f\u6210\u590d\u6742\u52a8\u6001\u884c\u4e3a\uff0c\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u6e10\u8fdb\u6a21\u578b\u540c\u4f26\u8f6c\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u6a21\u578b\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u7b80\u5316\u6a21\u578b\u5230\u5168\u8eab\u73af\u5883\u7684\u7a33\u5b9a\u7b56\u7565\u8fc1\u79fb\u3002"}}
{"id": "2512.24712", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24712", "abs": "https://arxiv.org/abs/2512.24712", "authors": ["Qian Cheng", "Weitao Zhou", "Cheng Jing", "Nanshan Deng", "Junze Wen", "Zhaoyang Liu", "Kun Jiang", "Diange Yang"], "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving", "comment": null, "summary": "Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment.This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.", "AI": {"tldr": "LSRE\u6846\u67b6\u5c06\u7a00\u758f\u91c7\u6837\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u8f6c\u6362\u4e3a\u5faa\u73af\u4e16\u754c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8bed\u4e49\u98ce\u9669\u8bc4\u4f30\uff0c\u65e0\u9700\u6bcf\u5e27VLM\u67e5\u8be2", "motivation": "\u73b0\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u9075\u5b88\u8d85\u8d8a\u6cd5\u5b9a\u4ea4\u901a\u89c4\u5219\u7684\u590d\u6742\u4eba\u7c7b\u793e\u4ea4\u89c4\u5219\uff0c\u8fd9\u4e9b\u8bed\u4e49\u7ea6\u675f\uff08\u5982\u8ba9\u884c\u7d27\u6025\u8f66\u8f86\u3001\u9075\u5b88\u4ea4\u8b66\u624b\u52bf\u7b49\uff09\u5bf9\u4eba\u7c7b\u76f4\u89c2\u4f46\u5bf9\u673a\u5668\u96be\u4ee5\u663e\u5f0f\u7f16\u7801\uff0c\u800c\u73b0\u6709VLM\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\u65e0\u6cd5\u5b9e\u65f6\u90e8\u7f72", "method": "\u63d0\u51faLSRE\uff08\u6f5c\u5728\u8bed\u4e49\u89c4\u5219\u7f16\u7801\uff09\u6846\u67b6\uff0c\u5c06\u7a00\u758f\u91c7\u6837\u7684VLM\u5224\u65ad\u8f6c\u6362\u4e3a\u5faa\u73af\u4e16\u754c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u5b9a\u4e49\u7684\u5b89\u5168\u8bed\u4e49\u7f16\u7801\u5230\u8f7b\u91cf\u7ea7\u6f5c\u5728\u5206\u7c7b\u5668\u4e2d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8bed\u4e49\u98ce\u9669\u8bc4\u4f30", "result": "\u5728CARLA\u4e2d\u7684\u516d\u4e2a\u8bed\u4e49\u6545\u969c\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0cLSRE\u8fbe\u5230\u4e0e\u5927\u578bVLM\u57fa\u7ebf\u76f8\u5f53\u7684\u8bed\u4e49\u98ce\u9669\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u65e9\u7684\u5371\u9669\u9884\u671f\uff0c\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5ef6\u8fdf\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u7f55\u89c1\u8bed\u4e49\u76f8\u4f3c\u6d4b\u8bd5\u6848\u4f8b", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u6f5c\u5728\u5206\u7c7b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u90e8\u7f72\u7684\u673a\u5236\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u590d\u6742\u7684\u8bed\u4e49\u7406\u89e3"}}
{"id": "2512.24766", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24766", "abs": "https://arxiv.org/abs/2512.24766", "authors": ["Karthik Dharmarajan", "Wenlong Huang", "Jiajun Wu", "Li Fei-Fei", "Ruohan Zhang"], "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "comment": "Project website: https://dream2flow.github.io/", "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.", "AI": {"tldr": "Dream2Flow\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc73D\u7269\u4f53\u6d41\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u8fde\u63a5\u8d77\u6765\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u7684\u5f00\u653e\u4e16\u754c\u7269\u4f53\u64cd\u4f5c\u3002", "motivation": "\u751f\u6210\u89c6\u9891\u6a21\u578b\u80fd\u591f\u96f6\u6837\u672c\u63a8\u7406\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u4f46\u96be\u4ee5\u5c06\u5176\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u6240\u9700\u7684\u4f4e\u7ea7\u52a8\u4f5c\u3002\u9700\u8981\u5f25\u5408\u89c6\u9891\u751f\u6210\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\"\u5177\u8eab\u9e3f\u6c9f\"\u3002", "method": "\u4f7f\u75283D\u7269\u4f53\u6d41\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff1a1\uff09\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u91cd\u5efa3D\u7269\u4f53\u8fd0\u52a8\uff1b2\uff09\u5c06\u64cd\u4f5c\u4efb\u52a1\u8f6c\u5316\u4e3a\u7269\u4f53\u8f68\u8ff9\u8ddf\u8e2a\u95ee\u9898\uff1b3\uff09\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u6216\u5f3a\u5316\u5b66\u4e60\u5c063D\u7269\u4f53\u6d41\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u4f4e\u7ea7\u6307\u4ee4\u3002", "result": "Dream2Flow\u80fd\u591f\u96f6\u6837\u672c\u6307\u5bfc\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u64cd\u4f5c\u591a\u79cd\u7c7b\u578b\u7269\u4f53\uff08\u521a\u6027\u3001\u94f0\u63a5\u3001\u53ef\u53d8\u5f62\u3001\u9897\u7c92\u72b6\uff09\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e863D\u7269\u4f53\u6d41\u4f5c\u4e3a\u901a\u7528\u63a5\u53e3\u7684\u6709\u6548\u6027\u3002", "conclusion": "3D\u7269\u4f53\u6d41\u662f\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u63a5\u53e3\uff0c\u80fd\u591f\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u9002\u914d\u5230\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u6307\u5bfc\u3002"}}
{"id": "2512.24845", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24845", "abs": "https://arxiv.org/abs/2512.24845", "authors": ["Qiuyi Gu", "Yuze Sheng", "Jincheng Yu", "Jiahao Tang", "Xiaolong Shan", "Zhaoyang Shen", "Tinghao Yi", "Xiaodan Liang", "Xinlei Chen", "Yu Wang"], "title": "ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation", "comment": null, "summary": "3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.", "AI": {"tldr": "ArtiSG\u6846\u67b6\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u6784\u5efa\u529f\u80fd\u60273D\u573a\u666f\u56fe\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5173\u8282\u7269\u4f53\u529f\u80fd\u4fe1\u606f\u7f3a\u5931\u3001\u89c6\u89c9\u6b67\u4e49\u548c\u7ec6\u7c92\u5ea6\u529f\u80fd\u5143\u7d20\u68c0\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u73b0\u67093D\u573a\u666f\u56fe\u7f3a\u4e4f\u7269\u7406\u64cd\u4f5c\u6240\u9700\u7684\u529f\u80fd\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5173\u8282\u7269\u4f53\u7684\u8fd0\u52a8\u673a\u5236\uff1b\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u6b67\u4e49\u3001\u4f9d\u8d56\u56fa\u5b9a\u6444\u50cf\u5934\u8bbe\u7f6e\u3001\u4ee5\u53ca\u65e0\u6cd5\u68c0\u6d4b\u7ec6\u7c92\u5ea6\u529f\u80fd\u5143\u7d20\uff08\u5982\u5c0f\u628a\u624b\uff09\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u4fbf\u643a\u5f0f\u8bbe\u5907\u6536\u96c6\u5173\u8282\u8fd0\u52a8\u6570\u636e\uff0c\u51c6\u786e\u4f30\u8ba16\u81ea\u7531\u5ea6\u5173\u8282\u8f68\u8ff9\u548c\u8f74\u7ebf\uff1b\u5c06\u8fd0\u52a8\u5b66\u5148\u9a8c\u6574\u5408\u5230\u5206\u5c42\u5f00\u653e\u8bcd\u6c47\u56fe\u4e2d\uff1b\u5229\u7528\u4ea4\u4e92\u6570\u636e\u53d1\u73b0\u89c6\u89c9\u611f\u77e5\u9057\u6f0f\u7684\u529f\u80fd\u5143\u7d20", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u529f\u80fd\u5143\u7d20\u53ec\u56de\u7387\u548c\u5173\u8282\u4f30\u8ba1\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b\u6784\u5efa\u7684\u56fe\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684\u529f\u80fd\u8bb0\u5fc6\uff0c\u6709\u6548\u6307\u5bfc\u673a\u5668\u4eba\u6267\u884c\u8bed\u8a00\u6307\u4ee4\u7684\u64cd\u63a7\u4efb\u52a1", "conclusion": "ArtiSG\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u529f\u80fd\u60273D\u573a\u666f\u56fe\uff0c\u89e3\u51b3\u4e86\u5173\u8282\u7269\u4f53\u529f\u80fd\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u8bed\u4e49\u7406\u89e3\u548c\u7269\u7406\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.24974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24974", "abs": "https://arxiv.org/abs/2512.24974", "authors": ["Yunxi Tang", "Tianqi Yang", "Jing Huang", "Xiangyu Chu", "Kwok Wai Samuel Au"], "title": "Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments", "comment": null, "summary": "Deformable linear objects (DLOs) manipulation presents significant challenges due to DLOs' inherent high-dimensional state space and complex deformation dynamics. The wide-populated obstacles in realistic workspaces further complicate DLO manipulation, necessitating efficient deformation planning and robust deformation tracking. In this work, we propose a novel framework for DLO manipulation in constrained environments. This framework combines hierarchical deformation planning with neural tracking, ensuring reliable performance in both global deformation synthesis and local deformation tracking. Specifically, the deformation planner begins by generating a spatial path set that inherently satisfies the homotopic constraints associated with DLO keypoint paths. Next, a path-set-guided optimization method is applied to synthesize an optimal temporal deformation sequence for the DLO. In manipulation execution, a neural model predictive control approach, leveraging a data-driven deformation model, is designed to accurately track the planned DLO deformation sequence. The effectiveness of the proposed framework is validated in extensive constrained DLO manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53d7\u9650\u73af\u5883\u4e2d\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u64cd\u4f5c\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u53d8\u5f62\u89c4\u5212\u548c\u795e\u7ecf\u8ddf\u8e2a\uff0c\u89e3\u51b3\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u53d8\u5f62\u52a8\u529b\u5b66\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u64cd\u4f5c\u9762\u4e34\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u3001\u590d\u6742\u53d8\u5f62\u52a8\u529b\u5b66\u4ee5\u53ca\u73b0\u5b9e\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u969c\u788d\u7269\u7b49\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u53d8\u5f62\u89c4\u5212\u548c\u9c81\u68d2\u7684\u53d8\u5f62\u8ddf\u8e2a\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u5206\u5c42\u53d8\u5f62\u89c4\u5212\u548c\u795e\u7ecf\u8ddf\u8e2a\uff1a1) \u53d8\u5f62\u89c4\u5212\u5668\u751f\u6210\u6ee1\u8db3\u540c\u4f26\u7ea6\u675f\u7684\u7a7a\u95f4\u8def\u5f84\u96c6\uff1b2) \u8def\u5f84\u96c6\u5f15\u5bfc\u7684\u4f18\u5316\u65b9\u6cd5\u5408\u6210\u6700\u4f18\u65f6\u95f4\u53d8\u5f62\u5e8f\u5217\uff1b3) \u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u53d8\u5f62\u6a21\u578b\u7684\u795e\u7ecf\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u8fdb\u884c\u7cbe\u786e\u8ddf\u8e2a\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u53d7\u9650DLO\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u5408\u5206\u5c42\u53d8\u5f62\u89c4\u5212\u548c\u795e\u7ecf\u8ddf\u8e2a\u7684\u6846\u67b6\u80fd\u591f\u53ef\u9760\u5730\u5728\u5168\u5c40\u53d8\u5f62\u5408\u6210\u548c\u5c40\u90e8\u53d8\u5f62\u8ddf\u8e2a\u4e2d\u5b9e\u73b0\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u7684\u6709\u6548\u64cd\u4f5c\u3002"}}
{"id": "2512.25072", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.25072", "abs": "https://arxiv.org/abs/2512.25072", "authors": ["Haozhi Qi", "Yen-Jen Wang", "Toru Lin", "Brent Yi", "Yi Ma", "Koushil Sreenath", "Jitendra Malik"], "title": "Coordinated Humanoid Manipulation with Choice Policies", "comment": "Code and Website: https://choice-policy.github.io/", "summary": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u6a21\u5757\u5316\u9065\u64cd\u4f5c\u754c\u9762\u4e0e\u53ef\u6269\u5c55\u5b66\u4e60\u6846\u67b6\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7Choice Policy\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u5934\u3001\u624b\u3001\u817f\u7684\u5168\u8eab\u534f\u8c03\u63a7\u5236\uff0c\u5728\u6d17\u7897\u673a\u88c5\u8f7d\u548c\u767d\u677f\u64e6\u62ed\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5b9e\u73b0\u5934\u3001\u624b\u3001\u817f\u7684\u7a33\u5065\u5168\u8eab\u534f\u8c03\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6536\u96c6\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u5e76\u5b66\u4e60\u591a\u6a21\u6001\u884c\u4e3a\u7684\u7cfb\u7edf\u3002", "method": "1. \u6a21\u5757\u5316\u9065\u64cd\u4f5c\u8bbe\u8ba1\uff1a\u5c06\u4eba\u5f62\u63a7\u5236\u5206\u89e3\u4e3a\u624b\u773c\u534f\u8c03\u3001\u6293\u53d6\u539f\u8bed\u3001\u624b\u81c2\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u548c\u8fd0\u52a8\u7b49\u76f4\u89c2\u5b50\u6a21\u5757\uff0c\u9ad8\u6548\u6536\u96c6\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u30022. Choice Policy\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff1a\u751f\u6210\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u5e76\u5b66\u4e60\u5bf9\u5176\u8bc4\u5206\uff0c\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\u548c\u6709\u6548\u5efa\u6a21\u591a\u6a21\u6001\u884c\u4e3a\u3002", "result": "\u5728\u6d17\u7897\u673a\u88c5\u8f7d\u548c\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\uff08\u767d\u677f\u64e6\u62ed\uff09\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002Choice Policy\u663e\u8457\u4f18\u4e8e\u6269\u6563\u7b56\u7565\u548c\u6807\u51c6\u884c\u4e3a\u514b\u9686\u3002\u5b9e\u9a8c\u8868\u660e\u624b\u773c\u534f\u8c03\u5bf9\u4e8e\u957f\u65f6\u57df\u4efb\u52a1\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u6570\u636e\u6536\u96c6\u548c\u4eba\u5f62\u673a\u5668\u4eba\u534f\u8c03\u64cd\u4f5c\u7684\u5b9e\u9645\u8def\u5f84\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
