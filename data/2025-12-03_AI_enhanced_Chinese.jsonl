{"id": "2512.02022", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02022", "abs": "https://arxiv.org/abs/2512.02022", "authors": ["Nan Lin", "Linrui Zhang", "Yuxuan Chen", "Zhenrui Chen", "Yujun Zhu", "Ruoxi Chen", "Peichen Wu", "Xiaoping Chen"], "title": "Reinforcement Learning for Robotic Safe Control with Force Sensing", "comment": null, "summary": "For the task with complicated manipulation in unstructured environments, traditional hand-coded methods are ineffective, while reinforcement learning can provide more general and useful policy. Although the reinforcement learning is able to obtain impressive results, its stability and reliability is hard to guarantee, which would cause the potential safety threats. Besides, the transfer from simulation to real world also will lead in unpredictable situations. To enhance the safety and reliability of robots, we introduce the force and haptic perception into reinforcement learning. Force and tactual sensation play key roles in robotic dynamic control and human-robot interaction. We demonstrate that the force-based reinforcement learning method can be more adaptive to environment, especially in sim-to-real transfer. Experimental results show in object pushing task, our strategy is safer and more efficient in both simulation and real world, thus it holds prospects for a wide variety of robotic applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u529b\u4e0e\u89e6\u89c9\u611f\u77e5\u878d\u5165\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u4e2d\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u65f6\uff0c\u4f20\u7edf\u624b\u5de5\u7f16\u7801\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u63d0\u4f9b\u66f4\u901a\u7528\u7684\u7b56\u7565\uff0c\u4f46\u5176\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u96be\u4ee5\u4fdd\u8bc1\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u4e14\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u4e5f\u4f1a\u5e26\u6765\u4e0d\u53ef\u9884\u6d4b\u7684\u60c5\u51b5\u3002", "method": "\u5c06\u529b\u548c\u89e6\u89c9\u611f\u77e5\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u529b\u4e0e\u89e6\u89c9\u611f\u77e5\u5728\u673a\u5668\u4eba\u52a8\u6001\u63a7\u5236\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5f00\u53d1\u57fa\u4e8e\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7269\u4f53\u63a8\u52a8\u4efb\u52a1\u4e2d\uff0c\u8be5\u7b56\u7565\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u90fd\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\uff0c\u7279\u522b\u662f\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u57fa\u4e8e\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u63d0\u5347\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u4e2d\u66f4\u5177\u9002\u5e94\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.02293", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02293", "abs": "https://arxiv.org/abs/2512.02293", "authors": ["Zihan Zhu", "Wei Zhang", "Norbert Haala", "Marc Pollefeys", "Daniel Barath"], "title": "VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM", "comment": "Project page: https://vigs-slam.github.io", "summary": "We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io", "AI": {"tldr": "VIGS-SLAM\u662f\u4e00\u4e2a\u89c6\u89c9-\u60ef\u60273D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u89c6\u89c9\u548c\u60ef\u6027\u4fe1\u606f\uff0c\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u4f4e\u7eb9\u7406\u548c\u66dd\u5149\u53d8\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u5b9e\u65f6\u8ddf\u8e2a\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u76843DGS-based SLAM\u65b9\u6cd5\u867d\u7136\u80fd\u5b9e\u73b0\u5bc6\u96c6\u548c\u903c\u771f\u7684\u5efa\u56fe\uff0c\u4f46\u7eaf\u89c6\u89c9\u8bbe\u8ba1\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u4f4e\u7eb9\u7406\u548c\u66dd\u5149\u53d8\u5316\u7b49\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u5f15\u5165\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6765\u589e\u5f3a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u89c6\u89c9-\u60ef\u60273D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff0c\u5728\u7edf\u4e00\u4f18\u5316\u6846\u67b6\u4e2d\u7d27\u5bc6\u8026\u5408\u89c6\u89c9\u548c\u60ef\u6027\u4fe1\u606f\uff0c\u8054\u5408\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u3001\u6df1\u5ea6\u548cIMU\u72b6\u6001\u3002\u7cfb\u7edf\u5305\u542b\u9c81\u68d2\u7684IMU\u521d\u59cb\u5316\u3001\u65f6\u53d8\u504f\u5dee\u5efa\u6a21\u4ee5\u53ca\u5177\u6709\u4e00\u81f4\u9ad8\u65af\u66f4\u65b0\u7684\u95ed\u73af\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eVIGS-SLAM\u5728\u9c81\u68d2\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "VIGS-SLAM\u901a\u8fc7\u89c6\u89c9-\u60ef\u6027\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86\u7eaf\u89c6\u89c9SLAM\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9e\u65f6\u8ddf\u8e2a\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u4e3a3DGS-based SLAM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02417", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02417", "abs": "https://arxiv.org/abs/2512.02417", "authors": ["Huiqian Li", "Wei Pan", "Haodong Zhang", "Jin Huang", "Zhihua Zhong"], "title": "Vehicle Dynamics Embedded World Models for Autonomous Driving", "comment": null, "summary": "World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVDD\u65b9\u6cd5\uff0c\u5c06\u81ea\u8f66\u52a8\u529b\u5b66\u4e0e\u73af\u5883\u52a8\u529b\u5b66\u89e3\u8026\u5efa\u6a21\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u5bf9\u4e0d\u540c\u8f66\u8f86\u53c2\u6570\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7PAD\u548cPAT\u7b56\u7565\u589e\u5f3a\u7b56\u7565\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u901a\u5e38\u5c06\u81ea\u8f66\u52a8\u529b\u5b66\u4e0e\u73af\u5883\u52a8\u529b\u5b66\u8054\u5408\u5b66\u4e60\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u4e14\u5bf9\u8f66\u8f86\u52a8\u529b\u5b66\u53d8\u5316\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u8f66\u8f86\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVDD\u65b9\u6cd5\uff0c\u5c06\u81ea\u8f66\u52a8\u529b\u5b66\u4e0e\u73af\u5883\u52a8\u529b\u5b66\u89e3\u8026\u5efa\u6a21\u3002\u5f15\u5165\u4e24\u79cd\u589e\u5f3a\u7b56\u7565\uff1a\u90e8\u7f72\u65f6\u7684\u7b56\u7565\u8c03\u6574(PAD)\u548c\u8bad\u7ec3\u65f6\u7684\u7b56\u7565\u589e\u5f3a(PAT)\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u6027\u80fd\u548c\u8f66\u8f86\u52a8\u529b\u5b66\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u5efa\u6a21\u81ea\u8f66\u52a8\u529b\u5b66\u4e0e\u73af\u5883\u52a8\u529b\u5b66\uff0cVDD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u8f66\u8f86\u53c2\u6570\uff0c\u7ed3\u5408PAD\u548cPAT\u7b56\u7565\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u7b56\u7565\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02535", "abs": "https://arxiv.org/abs/2512.02535", "authors": ["Jeric Lew", "Yuhong Cao", "Derek Ming Siang Tan", "Guillaume Sartoretti"], "title": "AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning", "comment": null, "summary": "Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as \"intent\" to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.", "AI": {"tldr": "AID\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u957f\u65f6\u8f68\u8ff9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b04\u500d\u52a0\u901f\u548c17%\u4fe1\u606f\u589e\u76ca\u63d0\u5347", "motivation": "\u5927\u89c4\u6a21\u6216\u65f6\u95f4\u5173\u952e\u573a\u666f\uff08\u5982\u73af\u5883\u76d1\u6d4b\u3001\u641c\u6551\uff09\u9700\u8981\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6709\u9650\u65f6\u95f4\u5185\u5b9e\u73b0\u5e7f\u6cdb\u8986\u76d6\u3002\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684MAIPP\u65b9\u6cd5\u4f7f\u7528\u81ea\u56de\u5f52\u610f\u56fe\u9884\u6d4b\u5668\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898", "method": "\u63d0\u51faAID\u6846\u67b6\uff1a1\uff09\u5bf9\u73b0\u6709MAIPP\u89c4\u5212\u5668\u751f\u6210\u7684\u8f68\u8ff9\u8fdb\u884c\u884c\u4e3a\u514b\u9686\uff1b2\uff09\u901a\u8fc7\u6269\u6563\u7b56\u7565\u7b56\u7565\u4f18\u5316\uff08DPPO\uff09\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7b56\u7565\u3002\u5229\u7528\u6269\u6563\u6a21\u578b\u4ee5\u975e\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u957f\u65f6\u8f68\u8ff9", "result": "AID\u5728\u8bad\u7ec3\u6765\u6e90\u7684MAIPP\u89c4\u5212\u5668\u57fa\u7840\u4e0a\u6301\u7eed\u6539\u8fdb\uff0c\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u6267\u884c\u52a0\u901f\u548c17%\u4fe1\u606f\u589e\u76ca\u63d0\u5347\uff0c\u5e76\u80fd\u6709\u6548\u6269\u5c55\u5230\u66f4\u591a\u667a\u80fd\u4f53", "conclusion": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u8868\u8fbe\u529b\u5f3a\u3001\u957f\u65f6\u7a0b\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684MAIPP\u6846\u67b6\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u534f\u8c03\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2512.02549", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02549", "abs": "https://arxiv.org/abs/2512.02549", "authors": ["Alessandro Ianniello", "Dave Murray-Rust", "Sara Muscolo", "Olger Siebinga", "Nicky Mol", "Denis Zatyagov", "Eva Verhoef", "Deborah Forster", "David Abbink"], "title": "Robotic capabilities framework: A boundary object and intermediate-level knowledge artifact for co-designing robotic processes", "comment": null, "summary": "As robots become more adaptable, responsive, and capable of interacting with humans, the design of effective human-robot collaboration becomes critical. Yet, this design process is typically led by monodisciplinary approaches, often overlooking interdisciplinary knowledge and the experiential knowledge of workers who will ultimately share tasks with these systems. To address this gap, we introduce the robotic capabilities framework, a vocabulary that enables transdisciplinary collaborations to meaningfully shape the future of work when robotic systems are integrated into the workplace. Rather than focusing on the internal workings of robots, the framework centers discussion on high-level capabilities, supporting dialogue around which elements of a task should remain human-led and which can be delegated to robots. We developed the framework through reflexive and iterative processes, and applied it in two distinct settings: by engaging roboticists in describing existing commercial robots using its vocabulary, and through a design activity with students working on robotics-related projects. The framework emerges as an intermediate-level knowledge artifact and a boundary object that bridges technical and experiential domains, guiding designers, empowering workers, and contributing to more just and collaborative futures of work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u80fd\u529b\u6846\u67b6\uff0c\u4f5c\u4e3a\u8de8\u5b66\u79d1\u534f\u4f5c\u7684\u8bcd\u6c47\u8868\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u7684\u672a\u6765\u5de5\u4f5c\u65b9\u5f0f\uff0c\u91cd\u70b9\u5173\u6ce8\u4efb\u52a1\u5206\u914d\u800c\u975e\u673a\u5668\u4eba\u5185\u90e8\u6280\u672f\u7ec6\u8282\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u53d8\u5f97\u66f4\u52a0\u9002\u5e94\u6027\u5f3a\u3001\u54cd\u5e94\u8fc5\u901f\u4e14\u80fd\u591f\u4e0e\u4eba\u7c7b\u4e92\u52a8\uff0c\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u8bbe\u8ba1\u8fc7\u7a0b\u901a\u5e38\u91c7\u7528\u5355\u5b66\u79d1\u65b9\u6cd5\uff0c\u5f80\u5f80\u5ffd\u89c6\u4e86\u8de8\u5b66\u79d1\u77e5\u8bc6\u548c\u6700\u7ec8\u4e0e\u8fd9\u4e9b\u7cfb\u7edf\u5171\u4eab\u4efb\u52a1\u7684\u5de5\u4eba\u7684\u7ecf\u9a8c\u77e5\u8bc6\u3002", "method": "\u5f00\u53d1\u4e86\u673a\u5668\u4eba\u80fd\u529b\u6846\u67b6\u4f5c\u4e3a\u8de8\u5b66\u79d1\u534f\u4f5c\u7684\u8bcd\u6c47\u8868\uff0c\u901a\u8fc7\u53cd\u601d\u6027\u548c\u8fed\u4ee3\u8fc7\u7a0b\u6784\u5efa\u3002\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u4e24\u4e2a\u4e0d\u540c\u573a\u666f\uff1a\u8ba9\u673a\u5668\u4eba\u4e13\u5bb6\u4f7f\u7528\u5176\u8bcd\u6c47\u63cf\u8ff0\u73b0\u6709\u5546\u4e1a\u673a\u5668\u4eba\uff0c\u4ee5\u53ca\u4e0e\u5b66\u751f\u8fdb\u884c\u673a\u5668\u4eba\u76f8\u5173\u9879\u76ee\u7684\u8bbe\u8ba1\u6d3b\u52a8\u3002", "result": "\u8be5\u6846\u67b6\u4f5c\u4e3a\u4e2d\u95f4\u5c42\u6b21\u7684\u77e5\u8bc6\u4ea7\u7269\u548c\u8fb9\u754c\u5bf9\u8c61\u51fa\u73b0\uff0c\u80fd\u591f\u8fde\u63a5\u6280\u672f\u548c\u7ecf\u9a8c\u9886\u57df\uff0c\u6307\u5bfc\u8bbe\u8ba1\u8005\uff0c\u8d4b\u80fd\u5de5\u4eba\uff0c\u5e76\u4fc3\u8fdb\u66f4\u516c\u6b63\u548c\u534f\u4f5c\u7684\u672a\u6765\u5de5\u4f5c\u65b9\u5f0f\u3002", "conclusion": "\u673a\u5668\u4eba\u80fd\u529b\u6846\u67b6\u4e3a\u8de8\u5b66\u79d1\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u901a\u8fc7\u5173\u6ce8\u9ad8\u7ea7\u80fd\u529b\u800c\u975e\u673a\u5668\u4eba\u5185\u90e8\u5de5\u4f5c\u539f\u7406\uff0c\u652f\u6301\u56f4\u7ed5\u4efb\u52a1\u5206\u914d\uff08\u54ea\u4e9b\u5e94\u7531\u4eba\u7c7b\u4e3b\u5bfc\uff0c\u54ea\u4e9b\u53ef\u59d4\u6258\u7ed9\u673a\u5668\u4eba\uff09\u7684\u5bf9\u8bdd\uff0c\u6709\u52a9\u4e8e\u5851\u9020\u66f4\u516c\u6b63\u548c\u534f\u4f5c\u7684\u4eba\u673a\u534f\u4f5c\u672a\u6765\u3002"}}
{"id": "2512.02609", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02609", "abs": "https://arxiv.org/abs/2512.02609", "authors": ["Shengkai Wu", "Jinrong Yang", "Wenqiu Luo", "Linfeng Gao", "Chaohui Shang", "Meiyu Zhi", "Mingshan Sun", "Fangping Yang", "Liangliang Ren", "Yong Zhao"], "title": "SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction", "comment": null, "summary": "Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.", "AI": {"tldr": "SAM2Grasp\u5229\u7528SAM2\u7684\u89c6\u89c9\u65f6\u5e8f\u8ddf\u8e2a\u80fd\u529b\uff0c\u901a\u8fc7\u63d0\u793a\u6761\u4ef6\u5316\u89e3\u51b3\u591a\u76ee\u6807\u6293\u53d6\u4e2d\u7684\u591a\u6a21\u6001\u95ee\u9898\uff0c\u4ec5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u52a8\u4f5c\u5934\u5b9e\u73b0\u9ad8\u6027\u80fd\u6293\u53d6", "motivation": "\u673a\u5668\u4eba\u6293\u53d6\u6a21\u4eff\u5b66\u4e60\u9762\u4e34\u591a\u6a21\u6001\u95ee\u9898\uff1a\u5f53\u573a\u666f\u4e2d\u5b58\u5728\u591a\u4e2a\u6709\u6548\u76ee\u6807\u65f6\uff0c\u5bf9\u4e0d\u540c\u7269\u4f53\u7684\u6f14\u793a\u4f1a\u4ea7\u751f\u51b2\u7a81\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4f1a\u5c06\u8fd9\u4e9b\u4e0d\u540c\u52a8\u4f5c\u5e73\u5747\u6210\u4e00\u4e2a\u65e0\u6548\u52a8\u4f5c", "method": "\u63d0\u51faSAM2Grasp\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684SAM2\u6a21\u578b\u63d0\u4f9b\u5f3a\u5927\u7684\u89c6\u89c9\u65f6\u5e8f\u8ddf\u8e2a\u80fd\u529b\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u52a8\u4f5c\u5934\u4e0e\u539f\u751f\u5206\u5272\u5934\u5e76\u884c\u5de5\u4f5c\u3002\u4ec5\u5728\u5c0f\u52a8\u4f5c\u5934\u4e0a\u4f7f\u7528SAM2\u9884\u8ba1\u7b97\u7684\u65f6\u5e8f\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u521d\u59cb\u63d0\u793a\uff08\u5982\u8fb9\u754c\u6846\uff09\u6307\u5b9a\u8981\u6293\u53d6\u7684\u5177\u4f53\u7269\u4f53\uff0c\u52a8\u4f5c\u5934\u4e3a\u8be5\u7269\u4f53\u9884\u6d4b\u552f\u4e00\u660e\u786e\u7684\u6293\u53d6\u8f68\u8ff9\uff0cSAM2\u7684\u65f6\u5e8f\u8ddf\u8e2a\u80fd\u529b\u81ea\u52a8\u7ef4\u6301\u5bf9\u9009\u5b9a\u7269\u4f53\u7684\u7a33\u5b9a\u8ddf\u8e2a", "result": "SAM2Grasp\u5728\u6742\u4e71\u591a\u76ee\u6807\u6293\u53d6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u6d88\u9664\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u7684\u6b67\u4e49", "conclusion": "\u901a\u8fc7\u5c06\u591a\u76ee\u6807\u6293\u53d6\u91cd\u65b0\u8868\u8ff0\u4e3a\u5355\u6a21\u6001\u63d0\u793a\u6761\u4ef6\u5316\u9884\u6d4b\u95ee\u9898\uff0cSAM2Grasp\u6210\u529f\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u591a\u6a21\u6001\u51b2\u7a81\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u65f6\u5e8f\u8ddf\u8e2a\u80fd\u529b\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u673a\u5668\u4eba\u6293\u53d6"}}
{"id": "2512.02729", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02729", "abs": "https://arxiv.org/abs/2512.02729", "authors": ["Yuhong Zhang", "Zihan Gao", "Shengpeng Li", "Ling-Hao Chen", "Kaisheng Liu", "Runqing Cheng", "Xiao Lin", "Junjia Liu", "Zhuoheng Li", "Jingyi Feng", "Ziyan He", "Jintian Lin", "Zheyan Huang", "Zhifang Liu", "Haoqian Wang"], "title": "RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning", "comment": "27 Pages, 21 figures", "summary": "We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.", "AI": {"tldr": "Robowheel\u662f\u4e00\u4e2a\u6570\u636e\u5f15\u64ce\uff0c\u53ef\u5c06\u4eba\u7c7b\u624b-\u7269\u4ea4\u4e92\u89c6\u9891\u8f6c\u6362\u4e3a\u8de8\u5f62\u6001\u673a\u5668\u4eba\u5b66\u4e60\u7684\u8bad\u7ec3\u76d1\u7763\u6570\u636e\uff0c\u901a\u8fc7\u91cd\u5efa\u3001\u7269\u7406\u7ea6\u675f\u4f18\u5316\u548c\u8de8\u5177\u8eab\u91cd\u5b9a\u5411\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5b66\u4e60\u4f9d\u8d56\u9065\u64cd\u4f5c\u6570\u636e\u6536\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u4e30\u5bcc\u7684\u4eba\u7c7b\u624b-\u7269\u4ea4\u4e92\u89c6\u9891\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u4e3a\u8de8\u5f62\u6001\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u65b9\u6848\u3002", "method": "1. \u4ece\u5355\u76eeRGB\u6216RGB-D\u8f93\u5165\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u624b-\u7269\u4ea4\u4e92\u91cd\u5efa\uff1b2. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5668\u5728\u63a5\u89e6\u548c\u7a7f\u900f\u7ea6\u675f\u4e0b\u7ec6\u5316\u76f8\u5bf9\u4f4d\u59ff\uff1b3. \u5c06\u91cd\u5efa\u7684\u63a5\u89e6\u4e30\u5bcc\u8f68\u8ff9\u91cd\u5b9a\u5411\u5230\u4e0d\u540c\u5177\u8eab\uff08\u673a\u68b0\u81c2\u3001\u7075\u5de7\u624b\u3001\u4eba\u5f62\u673a\u5668\u4eba\uff09\uff1b4. \u5728Isaac Sim\u4e2d\u6784\u5efa\u6a21\u62df\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u968f\u673a\u5316\u4e30\u5bcc\u6570\u636e\u5206\u5e03\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u7ba1\u9053\u751f\u6210\u7684\u8f68\u8ff9\u4e0e\u9065\u64cd\u4f5c\u6570\u636e\u540c\u6837\u7a33\u5b9a\uff0c\u5e76\u80fd\u5e26\u6765\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002\u9996\u6b21\u4e3a\"\u624b-\u7269\u4ea4\u4e92\u6a21\u6001\u53ef\u4f5c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u6709\u6548\u76d1\u7763\"\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc1\u636e\u3002\u76f8\u6bd4\u9065\u64cd\u4f5c\uff0cRobowheel\u66f4\u8f7b\u91cf\uff0c\u4ec5\u9700\u5355\u76ee\u76f8\u673a\u5373\u53ef\u63d0\u53d6\u901a\u7528\u7684\u3001\u5177\u8eab\u65e0\u5173\u7684\u8fd0\u52a8\u8868\u793a\u3002", "conclusion": "Robowheel\u5efa\u7acb\u4e86\u4ece\u89c6\u9891\u5230\u673a\u5668\u4eba\u8bad\u7ec3\u6570\u636e\u7684\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u8bc1\u660e\u4e86\u4eba\u7c7b\u624b-\u7269\u4ea4\u4e92\u89c6\u9891\u4f5c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u76d1\u7763\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8de8\u5f62\u6001\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002"}}
{"id": "2512.02777", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.02777", "abs": "https://arxiv.org/abs/2512.02777", "authors": ["Heye Huang", "Yibin Yang", "Mingfeng Fan", "Haoran Wang", "Xiaocong Zhao", "Jianqiang Wang"], "title": "CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy", "comment": "25 pages, 6 figures", "summary": "Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.", "AI": {"tldr": "CogDrive\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u9a71\u52a8\u7684\u591a\u6a21\u6001\u9884\u6d4b\u4e0e\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6a21\u6001\u63a8\u7406\u4e0e\u5b89\u5168\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u4ea4\u901a\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7f55\u89c1\u4f46\u5b89\u5168\u5173\u952e\u7684\u884c\u4e3a\uff0c\u800c\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u5728\u590d\u6742\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7edf\u4e00\u7406\u89e3\u591a\u6a21\u6001\u4ea4\u4e92\u5e76\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u52a8\u6001\u89c4\u5212\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u9a71\u52a8\u7684\u591a\u6a21\u6001\u9884\u6d4b\u4e0e\u89c4\u5212\u6846\u67b6\uff1a\u9884\u6d4b\u6a21\u5757\u57fa\u4e8e\u62d3\u6251\u8fd0\u52a8\u8bed\u4e49\u548c\u6700\u8fd1\u90bb\u5173\u7cfb\u7f16\u7801\u5efa\u7acb\u4ea4\u4e92\u6a21\u5f0f\u7684\u8ba4\u77e5\u8868\u793a\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u6a21\u6001\u635f\u5931\u548c\u591a\u6a21\u6001\u9ad8\u65af\u89e3\u7801\uff1b\u89c4\u5212\u6a21\u5757\u5f15\u5165\u5e94\u6025\u54cd\u5e94\u6982\u5ff5\uff0c\u4f18\u5316\u5b89\u5168\u7a33\u5b9a\u8f68\u8ff9\uff0c\u5305\u542b\u77ed\u671f\u4e00\u81f4\u5206\u652f\u548c\u957f\u671f\u5206\u652f\u3002", "result": "\u5728Argoverse2\u548cINTERACTION\u6570\u636e\u96c6\u4e0a\uff0cCogDrive\u5728\u8f68\u8ff9\u7cbe\u5ea6\u548c\u6f0f\u68c0\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b\u95ed\u73af\u4eff\u771f\u9a8c\u8bc1\u4e86\u5728\u6c47\u5165\u548c\u4ea4\u53c9\u53e3\u573a\u666f\u4e2d\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u591a\u6a21\u6001\u9884\u6d4b\u4e0e\u5b89\u5168\u5bfc\u5411\u89c4\u5212\uff0cCogDrive\u4e3a\u590d\u6742\u4ea4\u901a\u4e2d\u7684\u5b89\u5168\u81ea\u4e3b\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u8303\u5f0f\u3002"}}
{"id": "2512.02787", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02787", "abs": "https://arxiv.org/abs/2512.02787", "authors": ["Xianchao Zeng", "Xinyu Zhou", "Youcheng Li", "Jiayou Shi", "Tianle Li", "Liangming Chen", "Lei Ren", "Yong-Lu Li"], "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/", "AI": {"tldr": "ViFailback\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u5931\u8d25\u8bca\u65ad\u548c\u7ea0\u6b63\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u89c6\u89c9\u7b26\u53f7\u589e\u5f3a\u6807\u6ce8\u6548\u7387\uff0c\u5e76\u5f00\u53d1\u4e86ViFailback-8B\u6a21\u578b\u6765\u63d0\u5347\u5931\u8d25\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5931\u8d25\u8bca\u65ad\u548c\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002\u73b0\u6709\u7684\u5931\u8d25\u6570\u636e\u96c6\u5927\u591a\u662f\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7a0b\u5e8f\u5316\u751f\u6210\u7684\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86ViFailback\u6846\u67b6\uff0c\u5229\u7528\u663e\u5f0f\u89c6\u89c9\u7b26\u53f7\u589e\u5f3a\u6807\u6ce8\u6548\u7387\uff1b\u53d1\u5e03\u4e86\u5305\u542b58,126\u4e2a\u89c6\u89c9\u95ee\u7b54\u5bf9\u548c5,202\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u8f68\u8ff9\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b\u5efa\u7acb\u4e86\u5305\u542b11\u4e2a\u7ec6\u7c92\u5ea6VQA\u4efb\u52a1\u7684ViFailback-Bench\u57fa\u51c6\uff1b\u5f00\u53d1\u4e86ViFailback-8B\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff1b\u5c06\u8be5\u6a21\u578b\u4e0eVLA\u6a21\u578b\u96c6\u6210\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b9e\u9a8c\u3002", "result": "ViFailback-8B\u5728ViFailback-Bench\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u80fd\u591f\u751f\u6210\u7528\u4e8e\u7ea0\u6b63\u52a8\u4f5c\u6307\u5bfc\u7684\u89c6\u89c9\u7b26\u53f7\u3002\u901a\u8fc7\u5c06ViFailback-8B\u4e0eVLA\u6a21\u578b\u96c6\u6210\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u5e2e\u52a9VLA\u6a21\u578b\u4ece\u5931\u8d25\u4e2d\u6062\u590d\u7684\u80fd\u529b\u3002", "conclusion": "ViFailback\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u5931\u8d25\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e13\u7528\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5931\u8d25\u8bca\u65ad\u548c\u7ea0\u6b63\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5931\u8d25\u6062\u590d\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02834", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02834", "abs": "https://arxiv.org/abs/2512.02834", "authors": ["Siyuan Yang", "Yang Zhang", "Haoran He", "Ling Pan", "Xiu Li", "Chenjia Bai", "Xuelong Li"], "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach", "comment": "The first two authors contributed equally. Yang Zhang leads the whole project", "summary": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTACO\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u8f7b\u91cf\u7ea7\u4f2a\u8ba1\u6570\u4f30\u8ba1\u5668\u89e3\u51b3VLA\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\u7684\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u52a8\u4f5c\u6267\u884c\u7684\u6210\u529f\u7387\u3002", "motivation": "VLA\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u6a21\u5f0f\uff0c\u800c\u5fae\u8c03\u6570\u636e\u96c6\u5f80\u5f80\u5305\u542b\u8fd0\u52a8\u5b66\u4e0a\u4e0d\u7406\u60f3\u6216\u5197\u4f59\u7684\u52a8\u4f5c\u6a21\u5f0f\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u51fa\u73b0\u4e0d\u7a33\u5b9a\u6027\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u9884\u8bad\u7ec3VLA\u6a21\u578b\u5728\u76d1\u7763\u5fae\u8c03\u540e\uff0c\u5728\u4e0d\u540c\u91c7\u6837\u566a\u58f0\u4e0b\u5b58\u5728\u5173\u952e\u7684\u63a8\u7406\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51faTACO\u6846\u67b6\uff0c\u91c7\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4f2a\u8ba1\u6570\u4f30\u8ba1\u5668\u4f5c\u4e3a\u52a8\u4f5c\u5757\u7684\u9ad8\u4fdd\u771f\u9a8c\u8bc1\u5668\u3002VLA\u6a21\u578b\u96c6\u6210TACO\u540e\uff0c\u53ef\u4ee5\u4ece\u6240\u6709\u91c7\u6837\u7684\u52a8\u4f5c\u5757\u4e2d\u9009\u62e9\u5177\u6709\u6700\u5927\u4f2a\u8ba1\u6570\u7684\u52a8\u4f5c\u6267\u884c\uff0c\u4ece\u800c\u9632\u6b62\u5206\u5e03\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301VLA\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u4eff\u771f\u57fa\u51c6\uff08RoboTwin2.0\u3001Robotwin\u3001LIBERO\u3001SimplerEnv\uff09\u548c\u53cc\u81c2\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u6210\u529f\u7387\u3002", "conclusion": "TACO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\u7684\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7ea6\u675f\u9632\u6b62\u5206\u5e03\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u5f3a\u5316\u5b66\u4e60\u66f4\u65b0\u5177\u6709\u663e\u8457\u7684\u8ba1\u7b97\u4f18\u52bf\u3002"}}
{"id": "2512.02844", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02844", "abs": "https://arxiv.org/abs/2512.02844", "authors": ["Xinzheng Wu", "Junyi Chen", "Naiting Zhong", "Yong Shen"], "title": "VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion", "comment": "25 pages, 9 figures", "summary": "The safe deployment of autonomous driving systems (ADSs) relies on comprehensive testing and evaluation. However, safety-critical scenarios that can effectively expose system vulnerabilities are extremely sparse in the real world. Existing scenario generation methods face challenges in efficiently constructing long-tail scenarios that ensure fidelity, criticality, and interactivity, while particularly lacking real-time dynamic response capabilities to the vehicle under test (VUT). To address these challenges, this paper proposes a safety-critical testing scenario generation framework that integrates the high-level semantic understanding capabilities of Vision Language Models (VLMs) with the fine-grained generation capabilities of adaptive guided diffusion models. The framework establishes a three-layer hierarchical architecture comprising a strategic layer for VLM-directed scenario generation objective determination, a tactical layer for guidance function formulation, and an operational layer for guided diffusion execution. We first establish a high-quality fundamental diffusion model that learns the data distribution of real driving scenarios. Next, we design an adaptive guided diffusion method that enables real-time, precise control of background vehicles (BVs) in closed-loop simulation. The VLM is then incorporated to autonomously generate scenario generation objectives and guidance functions through deep scenario understanding and risk reasoning, ultimately guiding the diffusion model to achieve VLM-directed scenario generation. Experimental results demonstrate that the proposed method can efficiently generate realistic, diverse, and highly interactive safety-critical testing scenarios. Furthermore, case studies validate the adaptability and VLM-directed generation performance of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u9002\u5e94\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u5173\u952e\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u6240\u9700\u7684\u957f\u5c3e\u3001\u9ad8\u4fdd\u771f\u3001\u4ea4\u4e92\u5f0f\u5371\u9669\u573a\u666f\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u9700\u8981\u5168\u9762\u6d4b\u8bd5\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u80fd\u6709\u6548\u66b4\u9732\u7cfb\u7edf\u6f0f\u6d1e\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u6781\u5176\u7a00\u5c11\u3002\u73b0\u6709\u573a\u666f\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u6784\u5efa\u540c\u65f6\u4fdd\u8bc1\u4fdd\u771f\u5ea6\u3001\u5173\u952e\u6027\u548c\u4ea4\u4e92\u6027\u7684\u957f\u5c3e\u573a\u666f\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u88ab\u6d4b\u8f66\u8f86\u7684\u5b9e\u65f6\u52a8\u6001\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u5206\u5c42\u67b6\u6784\uff1a\u6218\u7565\u5c42\uff08VLM\u786e\u5b9a\u573a\u666f\u751f\u6210\u76ee\u6807\uff09\u3001\u6218\u672f\u5c42\uff08\u5236\u5b9a\u5f15\u5bfc\u51fd\u6570\uff09\u3001\u64cd\u4f5c\u5c42\uff08\u6267\u884c\u5f15\u5bfc\u6269\u6563\uff09\u3002\u9996\u5148\u5efa\u7acb\u5b66\u4e60\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u6570\u636e\u5206\u5e03\u7684\u57fa\u7840\u6269\u6563\u6a21\u578b\uff0c\u7136\u540e\u8bbe\u8ba1\u81ea\u9002\u5e94\u5f15\u5bfc\u6269\u6563\u65b9\u6cd5\u5b9e\u73b0\u95ed\u73af\u4eff\u771f\u4e2d\u80cc\u666f\u8f66\u8f86\u7684\u5b9e\u65f6\u7cbe\u786e\u63a7\u5236\uff0c\u6700\u540e\u96c6\u6210VLM\u901a\u8fc7\u6df1\u5ea6\u573a\u666f\u7406\u89e3\u548c\u98ce\u9669\u63a8\u7406\u81ea\u4e3b\u751f\u6210\u573a\u666f\u76ee\u6807\u548c\u5f15\u5bfc\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u9ad8\u5ea6\u4ea4\u4e92\u7684\u5b89\u5168\u5173\u952e\u6d4b\u8bd5\u573a\u666f\u3002\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9002\u5e94\u6027\u548cVLM\u5f15\u5bfc\u751f\u6210\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86VLM\u7684\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u81ea\u9002\u5e94\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u751f\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u4fdd\u771f\u3001\u5173\u952e\u3001\u4ea4\u4e92\u5f0f\u957f\u5c3e\u573a\u666f\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02851", "abs": "https://arxiv.org/abs/2512.02851", "authors": ["Iana Zhura", "Sausar Karaf", "Faryal Batool", "Nipun Dhananjaya Weerakkodi Mudalige", "Valerii Serpiva", "Ali Alridha Abdulkarim", "Aleksey Fedoseev", "Didar Seyidov", "Amjad Hajira", "Dzmitry Tsetserukou"], "title": "SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots", "comment": "This work has been submitted for publication and is currently under review", "summary": "Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100\\% navigation success and 0.09 s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.", "AI": {"tldr": "SwarmDiffusion\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u6269\u6563\u6a21\u578b\uff0c\u4ece\u5355\u5f20RGB\u56fe\u50cf\u8054\u5408\u9884\u6d4b\u53ef\u901a\u884c\u6027\u5e76\u751f\u6210\u53ef\u884c\u8f68\u8ff9\uff0c\u65e0\u9700\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u6216\u5916\u90e8\u89c4\u5212\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u63d0\u793a\uff0c\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u4ec5\u8f93\u51fa\u53ef\u901a\u884c\u6027\u5730\u56fe\uff0c\u9700\u8981\u5916\u90e8\u89c4\u5212\u5668\u751f\u6210\u8f68\u8ff9\uff0c\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51fa\u65e0\u89c4\u5212\u5668\u7684\u8f68\u8ff9\u6784\u5efa\u6d41\u7a0b\uff1a\u968f\u673a\u5316\u822a\u70b9\u91c7\u6837\u3001\u8d1d\u585e\u5c14\u66f2\u7ebf\u5e73\u6ed1\uff0c\u4ee5\u53ca\u5f3a\u5236\u8fde\u901a\u6027\u3001\u5b89\u5168\u6027\u3001\u65b9\u5411\u6027\u548c\u8def\u5f84\u7ec6\u5ea6\u7684\u6b63\u5219\u5316\u3002\u5229\u7528VLM\u76d1\u7763\u65e0\u9700\u63d0\u793a\u5de5\u7a0b\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a0\u5165\u7d27\u51d1\u7684\u673a\u5668\u4eba\u72b6\u6001\u6761\u4ef6\u3002", "result": "\u5728\u5ba4\u5185\u73af\u5883\u548c\u4e24\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff08\u56db\u8db3\u548c\u7a7a\u4e2d\uff09\u4e0a\u5b9e\u73b080-100%\u5bfc\u822a\u6210\u529f\u7387\uff0c0.09\u79d2\u63a8\u7406\u65f6\u95f4\uff0c\u4ec5\u9700500\u4e2a\u989d\u5916\u89c6\u89c9\u6837\u672c\u5373\u53ef\u9002\u5e94\u65b0\u673a\u5668\u4eba\u3002\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u4e2d\u53ef\u9760\u6cdb\u5316\u5230\u672a\u89c1\u73af\u5883\u3002", "conclusion": "SwarmDiffusion\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u65e0\u9700\u63d0\u793a\u7684\u7edf\u4e00\u53ef\u901a\u884c\u6027\u63a8\u7406\u548c\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u8f6c\u79fb\uff0c\u5b66\u4e60\u7a33\u5b9a\u8fd0\u52a8\u5148\u9a8c\u800c\u65e0\u9700\u6f14\u793a\u3002"}}
{"id": "2512.02902", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02902", "abs": "https://arxiv.org/abs/2512.02902", "authors": ["Weiqi Li", "Quande Zhang", "Ruifeng Zhai", "Liang Lin", "Guangrun Wang"], "title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling", "comment": null, "summary": "Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.", "AI": {"tldr": "VLA\u6a21\u578b\u5728\u5206\u5e03\u5185\u8868\u73b0\u826f\u597d\u4f46\u9762\u5bf9\u65b0\u89c6\u89d2\u548c\u89c6\u89c9\u6270\u52a8\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u95ee\u9898\u4e3b\u8981\u6e90\u4e8e\u7a7a\u95f4\u5efa\u6a21\u800c\u975e\u7269\u7406\u5efa\u6a21\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e00\u6b21\u6027\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u66f4\u65b0\u91cd\u65b0\u6821\u51c6\u89c6\u89c9\u8868\u793a\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5206\u5e03\u5185\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u65b0\u76f8\u673a\u89c6\u89d2\u548c\u89c6\u89c9\u6270\u52a8\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u8106\u5f31\u6027\u4e3b\u8981\u6e90\u4e8e\u7a7a\u95f4\u5efa\u6a21\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u800c\u975e\u7269\u7406\u5efa\u6a21\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u8868\u793a\u7684\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e00\u6b21\u6027\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u66f4\u65b0\u91cd\u65b0\u6821\u51c6\u89c6\u89c9\u8868\u793a\u3002\u5177\u4f53\u5305\u62ec\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u7279\u5f81\u4ee4\u724c\u8c03\u5236\uff1a\u5bf9\u89c6\u89c9\u4ee4\u724c\u5e94\u7528\u5168\u5c40\u4eff\u5c04\u53d8\u6362\uff1b2) \u7279\u5f81\u7ebf\u6027\u9002\u5e94\uff1a\u5728ViT\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u4f4e\u79e9\u66f4\u65b0\u3002", "result": "FTM\u65b9\u6cd5\u4ec5\u75284K\u53c2\u6570\u5c31\u5c06Libero\u89c6\u89d2\u51c6\u786e\u7387\u4ece48.5%\u63d0\u5347\u523087.1%\u3002FLA\u65b9\u6cd5\u75284.7M\u53c2\u6570\u8fbe\u523090.8%\u7684\u6210\u529f\u7387\uff0c\u4e0eLoRA\u89c4\u6a21\u5fae\u8c03\u76f8\u5f53\u4f46\u6210\u672c\u66f4\u4f4e\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u9884\u8bad\u7ec3VLA\u6a21\u578b\u5177\u6709\u5927\u91cf\u672a\u5f00\u53d1\u7684\u9c81\u68d2\u6027\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9884\u8bad\u7ec3VLA\u6a21\u578b\u5177\u6709\u663e\u8457\u7684\u672a\u5f00\u53d1\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u6700\u5c0f\u89c6\u89c9\u9002\u5e94\u5c31\u8db3\u4ee5\u6062\u590d\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u3002\u8f7b\u91cf\u7ea7\u89c6\u89c9\u9002\u5e94\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u7a7a\u95f4\u5efa\u6a21\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u65b0\u89c6\u89d2\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2512.02951", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02951", "abs": "https://arxiv.org/abs/2512.02951", "authors": ["Nicholas Baiata", "Nilanjan Chakraborty"], "title": "Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger", "comment": null, "summary": "Task-space control of robotic fingers is a critical enabler of dexterous manipulation, as manipulation objectives are most naturally specified in terms of fingertip motions and applied forces rather than individual joint angles. While task-space planning and control have been extensively studied for larger, arm-scale manipulators, demonstrations of precise task-space trajectory tracking in compact, multi-DoF robotic fingers remain scarce. In this paper, we present the physical prototyping and experimental characterization of a three-degree-of-freedom, linkage-driven, series-parallel robotic finger with analytic forward kinematics and a closed-form Jacobian. A resolved motion rate control (RMRC) scheme is implemented to achieve closed-loop task-space trajectory tracking. We experimentally evaluate the fingertip tracking performance across a variety of trajectories, including straight lines, circles, and more complex curves, and report millimeter-level accuracy. To the best of our knowledge, this work provides one of the first systematic experimental demonstrations of precise task-space trajectory tracking in a linkage-driven robotic finger, thereby establishing a benchmark for future designs aimed at dexterous in-hand manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u81ea\u7531\u5ea6\u8fde\u6746\u9a71\u52a8\u4e32\u5e76\u8054\u673a\u5668\u4eba\u624b\u6307\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\u7684\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u8ddf\u8e2a\u63a7\u5236\u3002", "motivation": "\u4efb\u52a1\u7a7a\u95f4\u63a7\u5236\u5bf9\u4e8e\u7075\u5de7\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5927\u578b\u673a\u68b0\u81c2\u4e0a\uff0c\u5bf9\u4e8e\u7d27\u51d1\u578b\u591a\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u624b\u6307\u7684\u7cbe\u786e\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\u8ddf\u8e2a\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u8bbe\u8ba1\u4e86\u5177\u6709\u89e3\u6790\u6b63\u5411\u8fd0\u52a8\u5b66\u548c\u95ed\u5f0f\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u4e09\u81ea\u7531\u5ea6\u8fde\u6746\u9a71\u52a8\u4e32\u5e76\u8054\u673a\u5668\u4eba\u624b\u6307\u539f\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u57fa\u4e8e\u89e3\u6790\u8fd0\u52a8\u901f\u7387\u63a7\u5236\uff08RMRC\uff09\u7684\u95ed\u73af\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u624b\u6307\u5728\u76f4\u7ebf\u3001\u5706\u5f62\u548c\u590d\u6742\u66f2\u7ebf\u7b49\u591a\u79cd\u8f68\u8ff9\u4e0a\u90fd\u80fd\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7684\u6307\u5c16\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8fde\u6746\u9a71\u52a8\u673a\u5668\u4eba\u624b\u6307\u7684\u7cbe\u786e\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e3a\u672a\u6765\u7075\u5de7\u624b\u5185\u64cd\u4f5c\u8bbe\u8ba1\u5efa\u7acb\u4e86\u57fa\u51c6\u3002"}}
