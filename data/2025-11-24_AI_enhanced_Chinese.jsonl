{"id": "2511.16844", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16844", "abs": "https://arxiv.org/abs/2511.16844", "authors": ["Disha Kamale", "Xi Yu", "Cristian-Ioan Vasile"], "title": "A*-based Temporal Logic Path Planning with User Preferences on Relaxed Task Satisfaction", "comment": null, "summary": "In this work, we consider the problem of planning for temporal logic tasks in large robot environments. When full task compliance is unattainable, we aim to achieve the best possible task satisfaction by integrating user preferences for relaxation into the planning process. Utilizing the automata-based representations for temporal logic goals and user preferences, we propose an A*-based planning framework. This approach effectively tackles large-scale problems while generating near-optimal high-level trajectories. To facilitate this, we propose a simple, efficient heuristic that allows for planning over large robot environments in a fraction of time and search memory as compared to uninformed search algorithms. We present extensive case studies to demonstrate the scalability, runtime analysis as well as empirical bounds on the suboptimality of the proposed heuristic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eA*\u7684\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5927\u578b\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5904\u7406\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u504f\u597d\u6765\u4f18\u5316\u4efb\u52a1\u6ee1\u610f\u5ea6\u3002", "motivation": "\u5728\u5927\u578b\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0c\u5f53\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u7528\u6237\u504f\u597d\u6765\u83b7\u5f97\u6700\u4f73\u4efb\u52a1\u6ee1\u610f\u5ea6\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u65f6\u5e8f\u903b\u8f91\u76ee\u6807\u548c\u7528\u6237\u504f\u597d\u7684\u81ea\u52a8\u673a\u8868\u793a\uff0c\u63d0\u51fa\u57fa\u4e8eA*\u7684\u89c4\u5212\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u53ef\u91c7\u7eb3\u542f\u53d1\u5f0f\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u751f\u6210\u63a5\u8fd1\u6700\u4f18\u7684\u9ad8\u5c42\u8f68\u8ff9\uff0c\u76f8\u6bd4\u65e0\u4fe1\u606f\u641c\u7d22\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u641c\u7d22\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u89c4\u5212\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u673a\u5668\u4eba\u73af\u5883\u4e2d\u9ad8\u6548\u5b9e\u73b0\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\u7684\u4f18\u5316\u89c4\u5212\u3002"}}
{"id": "2511.16898", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.16898", "abs": "https://arxiv.org/abs/2511.16898", "authors": ["Ariel Slepyan", "Laura Xing", "Rudy Zhang", "Nitish Thakor"], "title": "Single-Pixel Tactile Skin via Compressive Sampling", "comment": "24 pages, 6 main figures, 6 supplemental figures", "summary": "Development of large-area, high-speed electronic skins is a grand challenge for robotics, prosthetics, and human-machine interfaces, but is fundamentally limited by wiring complexity and data bottlenecks. Here, we introduce Single-Pixel Tactile Skin (SPTS), a paradigm that uses compressive sampling to reconstruct rich tactile information from an entire sensor array via a single output channel. This is achieved through a direct circuit-level implementation where each sensing element, equipped with a miniature microcontroller, contributes a dynamically weighted analog signal to a global sum, performing distributed compressed sensing in hardware. Our flexible, daisy-chainable design simplifies wiring to a few input lines and one output, and significantly reduces measurement requirements compared to raster scanning methods. We demonstrate the system's performance by achieving object classification at an effective 3500 FPS and by capturing transient dynamics, resolving an 8 ms projectile impact into 23 frames. A key feature is the support for adaptive reconstruction, where sensing fidelity scales with measurement time. This allows for rapid contact localization using as little as 7% of total data, followed by progressive refinement to a high-fidelity image - a capability critical for responsive robotic systems. This work offers an efficient pathway towards large-scale tactile intelligence for robotics and human-machine interfaces.", "AI": {"tldr": "SPTS\u662f\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u76ae\u80a4\u7cfb\u7edf\uff0c\u901a\u8fc7\u538b\u7f29\u91c7\u6837\u6280\u672f\u4ece\u5355\u4e2a\u8f93\u51fa\u901a\u9053\u91cd\u5efa\u6574\u4e2a\u4f20\u611f\u5668\u9635\u5217\u7684\u4e30\u5bcc\u89e6\u89c9\u4fe1\u606f\uff0c\u663e\u8457\u7b80\u5316\u5e03\u7ebf\u5e76\u63d0\u9ad8\u6570\u636e\u91c7\u96c6\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u9762\u79ef\u9ad8\u901f\u7535\u5b50\u76ae\u80a4\u5728\u673a\u5668\u4eba\u3001\u5047\u80a2\u548c\u4eba\u673a\u754c\u9762\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u5e03\u7ebf\u590d\u6742\u6027\u548c\u6570\u636e\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528\u76f4\u63a5\u7535\u8def\u7ea7\u5b9e\u73b0\uff0c\u6bcf\u4e2a\u4f20\u611f\u5143\u4ef6\u914d\u5907\u5fae\u578b\u5fae\u63a7\u5236\u5668\uff0c\u5411\u5168\u5c40\u603b\u548c\u8d21\u732e\u52a8\u6001\u52a0\u6743\u7684\u6a21\u62df\u4fe1\u53f7\uff0c\u5728\u786c\u4ef6\u4e2d\u6267\u884c\u5206\u5e03\u5f0f\u538b\u7f29\u611f\u77e5\u3002", "result": "\u5b9e\u73b0\u4e863500 FPS\u7684\u6709\u6548\u7269\u4f53\u5206\u7c7b\uff0c\u80fd\u591f\u6355\u6349\u77ac\u6001\u52a8\u529b\u5b66\uff08\u59828ms\u5f39\u4e38\u649e\u51fb\u89e3\u6790\u4e3a23\u5e27\uff09\uff0c\u652f\u6301\u81ea\u9002\u5e94\u91cd\u5efa\uff0c\u4ec5\u97007%\u6570\u636e\u5373\u53ef\u5feb\u901f\u5b9a\u4f4d\u63a5\u89e6\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u548c\u4eba\u673a\u754c\u9762\u7684\u5927\u89c4\u6a21\u89e6\u89c9\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u7b80\u5316\u5e03\u7ebf\u548c\u51cf\u5c11\u6d4b\u91cf\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e6\u89c9\u4f20\u611f\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.16911", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.16911", "abs": "https://arxiv.org/abs/2511.16911", "authors": ["Yendo Hu", "Yiliang Wu", "Weican Chen"], "title": "Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization", "comment": "12 pages, 13 figures, and 2 tables", "summary": "In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6539\u8fdb\u591a\u673a\u5668\u4eba\u7f16\u961f\u907f\u969c\u7b97\u6cd5\u548c\u589e\u5f3a\u4eba\u5de5\u52bf\u573a\u6cd5\u7684\u6df7\u5408\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u7f16\u961f\u907f\u969c\u4e2d\u7684\u8def\u5f84\u5197\u4f59\u3001\u822a\u5411\u7a81\u53d8\u548c\u78b0\u649e\u98ce\u9669\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u6cd5\u5728\u591a\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u5bb9\u6613\u5bfc\u81f4\u98de\u884c\u8def\u5f84\u5197\u4f59\u3001\u822a\u5411\u9891\u7e41\u7a81\u53d8\uff0c\u4e14\u5728\u907f\u969c\u8fc7\u7a0b\u4e2d\u6781\u6613\u53d1\u751f\u65e0\u4eba\u673a\u95f4\u78b0\u649e\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684MRF IAPF\u7b97\u6cd5\u548c\u589e\u5f3a\u7684\u5355\u65e0\u4eba\u673a\u8def\u5f84\u4f18\u5316APF\uff0c\u6574\u5408\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u529b\uff08\u969c\u788d\u7269\u65a5\u529b\u3001\u65e0\u4eba\u673a\u95f4\u4f5c\u7528\u529b\u3001\u76ee\u6807\u5f15\u529b\uff09\uff0c\u5e76\u5f15\u5165\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u548c\u8f85\u52a9\u5b50\u76ee\u6807\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfAPF\u7f16\u961f\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u4f18\u5316\u548c\u822a\u5411\u7a33\u5b9a\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u80fd\u6709\u6548\u907f\u969c\u5e76\u5feb\u901f\u6062\u590d\u7f16\u961f\u6784\u578b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u9759\u6001\u672a\u77e5\u969c\u788d\u7269\u73af\u5883\u4e2d\u5177\u6709\u9002\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.16949", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16949", "abs": "https://arxiv.org/abs/2511.16949", "authors": ["Junseo Kim", "Guido Dumont", "Xinyu Gao", "Gang Chen", "Holger Caesar", "Javier Alonso-Mora"], "title": "MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots", "comment": null, "summary": "Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86MobileOcc\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5728\u62e5\u6324\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5360\u636e\u611f\u77e5\uff0c\u5305\u542b\u9759\u6001\u7269\u4f53\u5360\u636e\u6807\u6ce8\u548c\u4e13\u95e8\u7528\u4e8e\u4eba\u7c7b\u5360\u636e\u5efa\u6a21\u7684\u7f51\u683c\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u5bc6\u96c63D\u8bed\u4e49\u5360\u636e\u611f\u77e5\u5728\u79fb\u52a8\u673a\u5668\u4eba\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u884c\u4eba\u5bc6\u96c6\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u9759\u6001\u7269\u4f53\u5360\u636e\u6807\u6ce8\u7684\u6807\u6ce8\u6d41\u6c34\u7ebf\uff0c\u5e76\u63d0\u51fa\u65b0\u9896\u7684\u7f51\u683c\u4f18\u5316\u6846\u67b6\uff0c\u4ece2D\u56fe\u50cf\u91cd\u5efa\u53ef\u53d8\u5f62\u4eba\u4f53\u51e0\u4f55\uff0c\u7136\u540e\u4f7f\u7528LiDAR\u70b9\u4e91\u6570\u636e\u8fdb\u884c\u7cbe\u5316\u548c\u4f18\u5316\u3002", "result": "\u5efa\u7acb\u4e86\u5360\u636e\u9884\u6d4b\u548c\u884c\u4eba\u901f\u5ea6\u9884\u6d4b\u4e24\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u4f7f\u7528\u5355\u76ee\u3001\u7acb\u4f53\u548c\u5168\u666f\u5360\u636e\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u57283D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6807\u6ce8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u5728\u62e5\u6324\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5360\u636e\u611f\u77e5\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2511.17001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17001", "abs": "https://arxiv.org/abs/2511.17001", "authors": ["Sicheng Xie", "Lingchen Meng", "Zhiying Du", "Shuyuan Tu", "Haidong Cao", "Jiaqi Leng", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Stable Offline Hand-Eye Calibration for any Robot with Just One Mark", "comment": null, "summary": "Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information ({\\ie}, camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. In this paper, we present CalibAll, a simple yet effective method that \\textbf{requires only a single mark} and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks, yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms. It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.", "AI": {"tldr": "CalibAll\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u76f8\u673a\u5916\u53c2\u4f30\u8ba1\u65b9\u6cd5\uff0c\u53ea\u9700\u5728\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4e0a\u6807\u6ce8\u5355\u4e2a\u6807\u8bb0\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7cbe\u7684\u6821\u51c6\u6d41\u7a0b\u5b9e\u73b0\u65e0\u8bad\u7ec3\u3001\u7a33\u5b9a\u4e14\u51c6\u786e\u7684\u76f8\u673a\u5916\u53c2\u4f30\u8ba1\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u76f8\u673a\u5916\u53c2\u4fe1\u606f\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u6709\u76ca\uff0c\u4f46\u901a\u5e38\u96be\u4ee5\u83b7\u53d6\uff0c\u73b0\u6709\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u5c40\u90e8\u6700\u5c0f\u503c\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5728\u672b\u7aef\u6267\u884c\u5668\u4e0a\u6807\u6ce8\u5355\u4e2a\u6807\u8bb0\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5bf9\u5e94\u80fd\u529b\u81ea\u52a8\u5b9a\u4f4d\u6807\u8bb0\uff0c\u7ed3\u5408\u70b9\u8ddf\u8e2a\u548c3D\u672b\u7aef\u8f68\u8ff9\u901a\u8fc7\u65f6\u5e8fPnP\u83b7\u5f97\u7c97\u5916\u53c2\uff0c\u518d\u901a\u8fc7\u6e32\u67d3\u4f18\u5316\u5bf9\u9f50\u6e32\u67d3\u548c\u771f\u5b9e\u63a9\u7801\u8fdb\u884c\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u548c\u901a\u7528\u6709\u6548\u6027\uff0c\u5e76\u80fd\u751f\u6210\u6df1\u5ea6\u56fe\u3001\u94fe\u63a5\u63a9\u7801\u548c\u672b\u7aef2D\u8f68\u8ff9\u7b49\u6709\u7528\u8f85\u52a9\u6807\u6ce8\u3002", "conclusion": "CalibAll\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u76f8\u673a\u5916\u53c2\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u5e76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8f85\u52a9\u4fe1\u606f\u3002"}}
{"id": "2511.17013", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17013", "abs": "https://arxiv.org/abs/2511.17013", "authors": ["Yiwen Ying", "Hanjing Ye", "Senzi Luo", "Luyao Liu", "Yu Zhan", "Li He", "Hong Zhang"], "title": "MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints", "comment": "6 pages, 9 figures, accepted at IEEE ROBIO 2025", "summary": "Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u5e27\u70b9\u7ea6\u675f\u7684\u4e3b\u52a8\u7aef\u5230\u7aef\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6a21\u5757\u9884\u6d4b\u79fb\u52a8\u969c\u788d\u7269\u7684\u672a\u6765\u8def\u5f84\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4e3b\u52a8\u89c4\u907f\u6f5c\u5728\u5371\u9669\uff0c\u5728\u672a\u77e5\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u5bfc\u822a\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u9759\u6001\u73af\u5883\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u5355\u5e27\u89c2\u6d4b\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u591a\u5e27\u70b9\u7ea6\u675f\u6846\u67b6\uff0c\u5305\u62ec\u5f53\u524d\u5e27\u548c\u9884\u6d4b\u6a21\u5757\u9884\u6d4b\u7684\u672a\u6765\u5e27\uff0c\u7ed3\u5408\u4e13\u95e8\u6a21\u5757\u57fa\u4e8e\u591a\u5e27\u89c2\u6d4b\u9884\u6d4b\u79fb\u52a8\u969c\u788d\u7269\u7684\u672a\u6765\u8def\u5f84\uff0c\u5b9e\u73b0\u4e3b\u52a8\u7aef\u5230\u7aef\u5bfc\u822a\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u672a\u77e5\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u5e27\u70b9\u7ea6\u675f\u6846\u67b6\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u4e3b\u52a8\u9884\u6d4b\u548c\u89c4\u907f\u79fb\u52a8\u969c\u788d\u7269\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5065\u548c\u9ad8\u6548\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2511.17079", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17079", "abs": "https://arxiv.org/abs/2511.17079", "authors": ["Yijie Zhu", "Rui Shao", "Ziyang Liu", "Jie He", "Jizhihui Liu", "Jiuru Wang", "Zitong Yu"], "title": "H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation", "comment": "Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/H-GAR", "summary": "Unified video and action prediction models hold great potential for robotic manipulation, as future observations offer contextual cues for planning, while actions reveal how interactions shape the environment. However, most existing approaches treat observation and action generation in a monolithic and goal-agnostic manner, often leading to semantically misaligned predictions and incoherent behaviors. To this end, we propose H-GAR, a Hierarchical interaction framework via Goal-driven observation-Action Refinement.To anchor prediction to the task objective, H-GAR first produces a goal observation and a coarse action sketch that outline a high-level route toward the goal. To enable explicit interaction between observation and action under the guidance of the goal observation for more coherent decision-making, we devise two synergistic modules. (1) Goal-Conditioned Observation Synthesizer (GOS) synthesizes intermediate observations based on the coarse-grained actions and the predicted goal observation. (2) Interaction-Aware Action Refiner (IAAR) refines coarse actions into fine-grained, goal-consistent actions by leveraging feedback from the intermediate observations and a Historical Action Memory Bank that encodes prior actions to ensure temporal consistency. By integrating goal grounding with explicit action-observation interaction in a coarse-to-fine manner, H-GAR enables more accurate manipulation. Extensive experiments on both simulation and real-world robotic manipulation tasks demonstrate that H-GAR achieves state-of-the-art performance.", "AI": {"tldr": "H-GAR\u662f\u4e00\u4e2a\u5206\u5c42\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u9a71\u52a8\u7684\u89c2\u5bdf-\u52a8\u4f5c\u7cbe\u70bc\u6765\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u5b83\u9996\u5148\u751f\u6210\u76ee\u6807\u89c2\u5bdf\u548c\u7c97\u7565\u52a8\u4f5c\u8349\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u4e24\u4e2a\u534f\u540c\u6a21\u5757\u8fdb\u884c\u89c2\u5bdf\u5408\u6210\u548c\u52a8\u4f5c\u7cbe\u70bc\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u89c2\u5bdf\u548c\u52a8\u4f5c\u751f\u6210\u89c6\u4e3a\u6574\u4f53\u4e14\u76ee\u6807\u4e0d\u53ef\u77e5\u7684\u65b9\u5f0f\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0d\u5bf9\u9f50\u7684\u9884\u6d4b\u548c\u4e0d\u8fde\u8d2f\u7684\u884c\u4e3a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u9884\u6d4b\u951a\u5b9a\u5230\u4efb\u52a1\u76ee\u6807\u5e76\u5b9e\u73b0\u89c2\u5bdf\u4e0e\u52a8\u4f5c\u663e\u5f0f\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "H-GAR\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff1a1\uff09\u9996\u5148\u751f\u6210\u76ee\u6807\u89c2\u5bdf\u548c\u7c97\u7565\u52a8\u4f5c\u8349\u56fe\uff1b2\uff09\u76ee\u6807\u6761\u4ef6\u89c2\u5bdf\u5408\u6210\u5668\u57fa\u4e8e\u7c97\u7565\u52a8\u4f5c\u548c\u9884\u6d4b\u76ee\u6807\u5408\u6210\u4e2d\u95f4\u89c2\u5bdf\uff1b3\uff09\u4ea4\u4e92\u611f\u77e5\u52a8\u4f5c\u7cbe\u70bc\u5668\u5229\u7528\u4e2d\u95f4\u89c2\u5bdf\u53cd\u9988\u548c\u5386\u53f2\u52a8\u4f5c\u8bb0\u5fc6\u5e93\u7cbe\u70bc\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cH-GAR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u76ee\u6807\u951a\u5b9a\u4e0e\u663e\u5f0f\u52a8\u4f5c-\u89c2\u5bdf\u4ea4\u4e92\u4ee5\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u96c6\u6210\uff0cH-GAR\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2511.17097", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17097", "abs": "https://arxiv.org/abs/2511.17097", "authors": ["Shuo Wang", "Yucheng Wang", "Guoxin Lian", "Yongcai Wang", "Maiyue Chen", "Kaihui Wang", "Bo Zhang", "Zhizhong Su", "Yutian Zhou", "Wanting Li", "Deying Li", "Zhaoxin Fan"], "title": "Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.", "AI": {"tldr": "Progress-Think\u63d0\u51fa\u8bed\u4e49\u8fdb\u5ea6\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u6307\u4ee4\u98ce\u683c\u7684\u8fdb\u5ea6\u6765\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6027\u80fd\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\u5b9e\u73b0\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u7684\u8fdb\u5ea6\u611f\u77e5\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u4e13\u6ce8\u4e8e\u76f4\u63a5\u52a8\u4f5c\u9884\u6d4b\uff0c\u65e9\u671f\u8fdb\u5ea6\u65b9\u6cd5\u9884\u6d4b\u6570\u503c\u6210\u5c31\uff0c\u90fd\u5ffd\u89c6\u4e86\u89c2\u5bdf\u5e8f\u5217\u548c\u6307\u4ee4\u5e8f\u5217\u7684\u5355\u8c03\u5171\u8fdb\u7279\u6027\uff0c\u9700\u8981\u66f4\u4e00\u81f4\u7684\u5bfc\u822a\u8fdb\u5c55\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u81ea\u5bf9\u9f50\u8fdb\u5ea6\u9884\u8bad\u7ec3\u901a\u8fc7\u89c6\u89c9\u5386\u53f2\u548c\u6307\u4ee4\u524d\u7f00\u7684\u5fae\u5206\u5bf9\u9f50\u542f\u52a8\u63a8\u7406\u6a21\u5757\uff1b2)\u8fdb\u5ea6\u5f15\u5bfc\u7b56\u7565\u9884\u8bad\u7ec3\u5c06\u5b66\u4e60\u5230\u7684\u8fdb\u5ea6\u72b6\u6001\u6ce8\u5165\u5bfc\u822a\u4e0a\u4e0b\u6587\uff1b3)\u8fdb\u5ea6\u7b56\u7565\u534f\u540c\u5fae\u8c03\u4f7f\u7528\u5b9a\u5236\u7684\u8fdb\u5ea6\u611f\u77e5\u5f3a\u5316\u76ee\u6807\u8054\u5408\u4f18\u5316\u4e24\u4e2a\u6a21\u5757\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u8bc1\u660e\u8bed\u4e49\u8fdb\u5ea6\u80fd\u4ea7\u751f\u66f4\u4e00\u81f4\u7684\u5bfc\u822a\u8fdb\u5c55\u8868\u793a\u3002", "conclusion": "\u8bed\u4e49\u8fdb\u5ea6\u63a8\u7406\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u5bfc\u822a\uff0c\u901a\u8fc7\u8fdb\u5ea6\u611f\u77e5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.17166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17166", "abs": "https://arxiv.org/abs/2511.17166", "authors": ["Tim Lakemann", "Daniel Bonilla Licea", "Viktor Walter", "Martin Saska"], "title": "Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers", "comment": null, "summary": "Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e3b\u52a8\u6807\u8bb0\u5728\u73af\u5883\u4e2d\u7684\u53cd\u5c04\u8fdb\u884c\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u673a\u5668\u4eba\u5c3a\u5bf8\u6216\u6807\u8bb0\u914d\u7f6e\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u5ba4\u5185\u5916\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u8303\u56f4\uff08\u8d85\u8fc730\u7c73\uff09\u548c\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73af\u5883\u4e2d\u4e3b\u52a8\u6807\u8bb0\u7684\u53cd\u5c04\u662f\u673a\u8f7d\u89c6\u89c9\u76f8\u5bf9\u5b9a\u4f4d\u4e2d\u5e38\u89c1\u7684\u6a21\u7cca\u6e90\uff0c\u901a\u5e38\u88ab\u89c6\u4e3a\u5e72\u6270\u56e0\u7d20\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u901a\u5e38\u4e0d\u9700\u8981\u7684\u53cd\u5c04\u6765\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u76f8\u5bf9\u5b9a\u4f4d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cd\u5c04\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u65e0\u9700\u673a\u5668\u4eba\u5c3a\u5bf8\u6216\u9884\u5b9a\u4e49\u6807\u8bb0\u914d\u7f6e\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u72ec\u7acb\u4e8e\u8868\u9762\u7279\u6027\uff0c\u7279\u522b\u8003\u8651\u975e\u5e73\u9762\u8868\u9762\uff08\u5c24\u5176\u662f\u52a8\u6001\u6c34\u9762\uff09\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u5ba4\u5185\u5916\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u53cd\u5c04\u5b9a\u4f4d\u7cfb\u7edf\u5728\u65e0\u9700\u56e2\u961f\u6210\u5458\u5c3a\u5bf8\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u53ef\u9760\u8fd0\u884c\uff0c\u6709\u6548\u8303\u56f4\u8d85\u8fc730\u7c73\uff0c\u7cbe\u5ea6\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u53cd\u5c04\u5b9a\u4f4d\u65b9\u6cd5\u4e3a\u5f02\u6784\u5fae\u578b\u7a7a\u4e2d\u7fa4\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u6d77\u6d0b\u90e8\u7f72\u7b49\u52a8\u6001\u6c34\u9762\u573a\u666f\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u89c6\u9891\u548c\u6e90\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2511.17178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17178", "abs": "https://arxiv.org/abs/2511.17178", "authors": ["Kento Kawaharazuka", "Yoshiki Obinata", "Naoaki Kanazawa", "Haoyu Jia", "Kei Okada"], "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models", "comment": null, "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u57fa\u4e8e\u9ed1\u76d2\u4f18\u5316\u7684\u673a\u5668\u4eba\u8eab\u4f53\u8bbe\u8ba1\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9ed1\u76d2\u4f18\u5316\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5e76\u884c\u4f7f\u7528LLMs\u8fdb\u884c\u91c7\u6837\uff0c\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u6570\u503c\u4f18\u5316\u867d\u7136\u5feb\u901f\u4f46\u4e0d\u9002\u7528\u4e8e\u590d\u6742\u7ed3\u6784\u6216\u79bb\u6563\u503c\u60c5\u51b5\uff0c\u800c\u9ed1\u76d2\u4f18\u5316\u91c7\u6837\u6548\u7387\u4f4e\u4e14\u9700\u8981\u5927\u91cf\u8fed\u4ee3\u624d\u80fd\u83b7\u5f97\u826f\u597d\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u673a\u5668\u4eba\u8bbe\u8ba1\u4f18\u5316\u7684\u6548\u7387\u3002", "method": "\u5728\u57fa\u4e8e\u9ed1\u76d2\u4f18\u5316\u7684\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u884c\u4f7f\u7528LLMs\u8fdb\u884c\u91c7\u6837\uff0c\u4e3aLLMs\u63d0\u4f9b\u95ee\u9898\u8bbe\u7f6e\u548c\u5e7f\u6cdb\u53cd\u9988\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u63a2\u7d22\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u8eab\u4f53\u8bbe\u8ba1\u4f18\u5316\u7684\u6548\u7387\uff0c\u4f46\u5b58\u5728\u7279\u5b9a\u7684\u7279\u5f81\u548c\u5c40\u9650\u6027\u9700\u8981\u8ba8\u8bba\u3002"}}
{"id": "2511.17237", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17237", "abs": "https://arxiv.org/abs/2511.17237", "authors": ["Alessio Saccuti", "Riccardo Monica", "Jacopo Aleotti"], "title": "A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde", "comment": null, "summary": "In this paper a novel ROS2 driver for UR robot manipulators is presented, based on the ur_rtde C++ library. The proposed driver aims to be a flexible solution, adaptable to a wide range of applications. The driver exposes the high-level commands of Universal Robots URScripts, and custom commands can be added using a plugin system. Several commands have been implemented, including motion execution along a waypoint-based path. The driver is published as open source.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eur_rtde C++\u5e93\u7684\u65b0\u578bROS2\u9a71\u52a8\u7a0b\u5e8f\uff0c\u7528\u4e8eUR\u673a\u5668\u4eba\u673a\u68b0\u81c2\uff0c\u65e8\u5728\u63d0\u4f9b\u7075\u6d3b\u4e14\u53ef\u9002\u5e94\u591a\u79cd\u5e94\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u66b4\u9732Universal Robots URScripts\u9ad8\u7ea7\u547d\u4ee4\u7684\u7075\u6d3bROS2\u9a71\u52a8\u7a0b\u5e8f\uff0c\u901a\u8fc7\u63d2\u4ef6\u7cfb\u7edf\u652f\u6301\u81ea\u5b9a\u4e49\u547d\u4ee4\uff0c\u4ee5\u9002\u5e94\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u5e94\u7528\u9700\u6c42\u3002", "method": "\u57fa\u4e8eur_rtde C++\u5e93\u6784\u5efaROS2\u9a71\u52a8\u7a0b\u5e8f\uff0c\u5b9e\u73b0URScripts\u7684\u9ad8\u7ea7\u547d\u4ee4\u63a5\u53e3\uff0c\u5e76\u8bbe\u8ba1\u63d2\u4ef6\u7cfb\u7edf\u6765\u6269\u5c55\u81ea\u5b9a\u4e49\u547d\u4ee4\u529f\u80fd\uff0c\u5305\u62ec\u57fa\u4e8e\u8def\u5f84\u70b9\u7684\u8fd0\u52a8\u6267\u884c\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u5b9e\u73b0\u4e86\u591a\u79cd\u547d\u4ee4\u529f\u80fd\uff0c\u5305\u62ec\u8def\u5f84\u70b9\u8fd0\u52a8\u6267\u884c\uff0c\u9a71\u52a8\u7a0b\u5e8f\u5df2\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u53d1\u5e03\u3002", "conclusion": "\u8be5ROS2\u9a71\u52a8\u7a0b\u5e8f\u4e3aUR\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u53d1\u5e03\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u793e\u533a\u7684\u534f\u4f5c\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2511.17266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17266", "abs": "https://arxiv.org/abs/2511.17266", "authors": ["Leone Costi", "Dario Izzo"], "title": "Simulation of Active Soft Nets for Capture of Space Debris", "comment": null, "summary": "In this work, we propose a simulator, based on the open-source physics engine MuJoCo, for the design and control of soft robotic nets for the autonomous removal of space debris. The proposed simulator includes net dynamics, contact between the net and the debris, self-contact of the net, orbital mechanics, and a controller that can actuate thrusters on the four satellites at the corners of the net. It showcases the case of capturing Envisat, a large ESA satellite that remains in orbit as space debris following the end of its mission. This work investigates different mechanical models, which can be used to simulate the net dynamics, simulating various degrees of compliance, and different control strategies to achieve the capture of the debris, depending on the relative position of the net and the target. Unlike previous works on this topic, we do not assume that the net has been previously ballistically thrown toward the target, and we start from a relatively static configuration. The results show that a more compliant net achieves higher performance when attempting the capture of Envisat. Moreover, when paired with a sliding mode controller, soft nets are able to achieve successful capture in 100% of the tested cases, whilst also showcasing a higher effective area at contact and a higher number of contact points between net and Envisat.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMuJoCo\u7269\u7406\u5f15\u64ce\u7684\u8f6f\u673a\u5668\u4eba\u7f51\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u7a7a\u95f4\u788e\u7247\u81ea\u4e3b\u6e05\u9664\u7684\u8bbe\u8ba1\u4e0e\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u6355\u83b7Envisat\u536b\u661f\u7684\u6848\u4f8b\uff0c\u53d1\u73b0\u66f4\u67d4\u987a\u7684\u7f51\u5728\u6355\u83b7\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5f00\u53d1\u7528\u4e8e\u7a7a\u95f4\u788e\u7247\u6e05\u9664\u7684\u8f6f\u673a\u5668\u4eba\u7f51\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5047\u8bbe\u7f51\u5df2\u88ab\u5f39\u5c04\u6295\u63b7\u7684\u9650\u5236\uff0c\u4ece\u76f8\u5bf9\u9759\u6001\u914d\u7f6e\u5f00\u59cb\u7814\u7a76\u4e0d\u540c\u673a\u68b0\u6a21\u578b\u548c\u63a7\u5236\u7b56\u7565\u5bf9\u6355\u83b7\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8eMuJoCo\u7269\u7406\u5f15\u64ce\u6784\u5efa\u6a21\u62df\u5668\uff0c\u5305\u542b\u7f51\u52a8\u529b\u5b66\u3001\u7f51\u4e0e\u788e\u7247\u63a5\u89e6\u3001\u7f51\u81ea\u63a5\u89e6\u3001\u8f68\u9053\u529b\u5b66\u548c\u63a7\u5236\u5668\uff0c\u7814\u7a76\u4e0d\u540c\u67d4\u987a\u5ea6\u7684\u673a\u68b0\u6a21\u578b\u548c\u4e0d\u540c\u63a7\u5236\u7b56\u7565\u3002", "result": "\u66f4\u67d4\u987a\u7684\u7f51\u5728\u6355\u83b7Envisat\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u5f53\u4e0e\u6ed1\u6a21\u63a7\u5236\u5668\u914d\u5bf9\u65f6\uff0c\u8f6f\u7f51\u5728100%\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u6210\u529f\u6355\u83b7\uff0c\u663e\u793a\u51fa\u66f4\u5927\u7684\u6709\u6548\u63a5\u89e6\u9762\u79ef\u548c\u66f4\u591a\u63a5\u89e6\u70b9\u3002", "conclusion": "\u8f6f\u673a\u5668\u4eba\u7f51\u5728\u7a7a\u95f4\u788e\u7247\u6355\u83b7\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5f53\u91c7\u7528\u9002\u5f53\u7684\u63a7\u5236\u7b56\u7565\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u7684\u6355\u83b7\u6548\u679c\u3002"}}
{"id": "2511.17276", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17276", "abs": "https://arxiv.org/abs/2511.17276", "authors": ["Julien Merand", "Boris Meden", "Mathieu Grossard"], "title": "Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data", "comment": null, "summary": "This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u4ec5\u4ece\u591a\u6307\u6293\u624b\u7684\u70b9\u4e91\u6570\u636e\u4e2d\u9ad8\u6548\u786e\u5b9a\u5173\u8282\u914d\u7f6e\uff0c\u65e0\u9700\u4f20\u7edf\u9006\u8fd0\u52a8\u5b66\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u6570\u5b66\u8ba1\u7b97\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u624b\u90e8\u8fd0\u52a8\u5b66\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4ee5\u5173\u952e\u7ed3\u6784\u5143\u7d20\u7684\u70b9\u4e91\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u91cd\u5efa\u76f8\u5e94\u7684\u5173\u8282\u914d\u7f6e\u3002", "result": "\u5728MultiDex\u6293\u53d6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5904\u7406\u65f6\u95f40.05\u6beb\u79d2\uff0c\u51c6\u786e\u7387\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728AI\u9a71\u52a8\u7684\u6293\u53d6\u89c4\u5212\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u5173\u8282\u914d\u7f6e\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17299", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17299", "abs": "https://arxiv.org/abs/2511.17299", "authors": ["Tom\u00e1\u0161 Musil", "Mat\u011bj Petrl\u00edk", "Martin Saska"], "title": "MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning", "comment": "8 pages, 9 figures, submitted to RA-L", "summary": "Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u89c6\u89c9\u7684\u81ea\u4e3b\u63a2\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u5bc6\u96c6\u8ddd\u79bb\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\u5b89\u5168\u8986\u76d6\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u5ba4\u5185\u59163D\u73af\u5883\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u7a00\u758f\u5355\u76eeSLAM\u524d\u7aef\u7684\u7279\u6027\uff0c\u5728\u6620\u5c04\u548c\u89c4\u5212\u4e2d\u89e3\u51b3\u7a00\u758f\u6df1\u5ea6\u6570\u636e\u3001\u81ea\u7531\u7a7a\u95f4\u95f4\u9699\u548c\u5927\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4ec5\u914d\u5907\u5355\u76ee\u76f8\u673a\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u76ee\u524d\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u7684\u5173\u952e\u80fd\u529b\u3002", "method": "\u6620\u5c04\u6a21\u5757\u901a\u8fc7\u5728\u7eb9\u7406\u7a00\u758f\u533a\u57df\u8fc7\u91c7\u6837\u81ea\u7531\u7a7a\u95f4\u5e76\u8ddf\u8e2a\u969c\u788d\u7269\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u6765\u89e3\u51b3\u7a00\u758f\u6df1\u5ea6\u6570\u636e\u95ee\u9898\uff1b\u89c4\u5212\u6a21\u5757\u901a\u8fc7\u5feb\u901f\u91cd\u89c4\u5212\u548c\u611f\u77e5\u611f\u77e5\u7684\u822a\u5411\u63a7\u5236\u6765\u5904\u7406\u589e\u52a0\u7684\u81ea\u7531\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5305\u62ec\u6d88\u878d\u7814\u7a76\u3002\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u4e16\u754c\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u4e2d\u5b9e\u73b03D\u5355\u76ee\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5f53\u8003\u8651\u89c6\u5dee\u8981\u6c42\u548c\u7eb9\u7406\u7f3a\u5931\u8868\u9762\u7684\u53ef\u80fd\u6027\u65f6\uff0c\u57fa\u4e8e\u8fb9\u754c\u7684\u63a2\u7d22\u5728\u7a00\u758f\u5355\u76ee\u6df1\u5ea6\u6570\u636e\u4e0b\u662f\u53ef\u884c\u7684\u3002\u8be5\u65b9\u6cd5\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2511.17318", "categories": ["cs.RO", "cs.AI", "cs.CE", "cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.17318", "abs": "https://arxiv.org/abs/2511.17318", "authors": ["Mikael Lundb\u00e4ck", "Erik Wallin", "Carola H\u00e4ggstr\u00f6m", "Mattias Nystr\u00f6m", "Andreas Gr\u00f6nlund", "Mats Richardson", "Petrus J\u00f6nsson", "William Arnvik", "Lucas Hedstr\u00f6m", "Arvid F\u00e4lldin", "Martin Servin"], "title": "FORWARD: Dataset of a forwarder operating in rough terrain", "comment": "25 pages, 22 figures", "summary": "We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.", "AI": {"tldr": "FORWARD\u662f\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e86\u745e\u5178\u4e2d\u90e8\u4e24\u4e2a\u91c7\u4f10\u573a\u5730\u7684\u4f10\u6728\u96c6\u6750\u673a\u5728\u5d0e\u5c96\u5730\u5f62\u4e2d\u7684\u4f5c\u4e1a\u6570\u636e\uff0c\u5305\u542bRTK-GNSS\u3001360\u5ea6\u6444\u50cf\u5934\u3001\u632f\u52a8\u4f20\u611f\u5668\u7b49\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u7528\u4e8e\u5f00\u53d1\u68ee\u6797\u673a\u68b0\u7684\u4ea4\u901a\u80fd\u529b\u3001\u611f\u77e5\u548c\u81ea\u4e3b\u63a7\u5236\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u7528\u4e8e\u68ee\u6797\u673a\u68b0\u4ea4\u901a\u80fd\u529b\u3001\u611f\u77e5\u548c\u81ea\u4e3b\u63a7\u5236\u7684\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u8003\u8651\u6548\u7387\u3001\u6cb9\u8017\u3001\u5b89\u5168\u6027\u548c\u73af\u5883\u5f71\u54cd\uff0c\u540c\u65f6\u63a2\u7d22\u6797\u4e1a\u673a\u68b0\u6a21\u62df\u5668\u7684\u81ea\u52a8\u751f\u6210\u548c\u6821\u51c6\u3002", "method": "\u4f7f\u7528\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u5927\u578bKomatsu\u4f10\u6728\u96c6\u6750\u673a\u6536\u96c6\u6570\u636e\uff0c\u5305\u62ecRTK-GNSS\uff08\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff09\u3001360\u5ea6\u6444\u50cf\u5934\u3001\u64cd\u4f5c\u5458\u632f\u52a8\u4f20\u611f\u5668\u3001CAN\u603b\u7ebf\u4fe1\u53f7\u8bb0\u5f55\u548c\u591a\u4e2aIMU\uff0c\u5728\u6fc0\u5149\u626b\u63cf\u7684\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u533a\u57df\u8fdb\u884c\u4f5c\u4e1a\u3002", "result": "\u6536\u96c6\u4e86\u7ea618\u5c0f\u65f6\u5e38\u89c4\u6728\u6750\u91c7\u8fd0\u5de5\u4f5c\u7684\u6570\u636e\uff0c\u5305\u542b5Hz\u7684\u4e8b\u4ef6\u65f6\u95f4\u65e5\u5fd7\u3001\u751f\u4ea7\u65e5\u5fd7\u6587\u4ef6\u3001\u89c6\u9891\u6750\u6599\u548c\u5730\u5f62\u6570\u636e\uff0c\u5e76\u6807\u6ce8\u4e86\u5de5\u4f5c\u5143\u7d20\uff0c\u8fd8\u5305\u62ec\u5728\u68ee\u6797\u9053\u8def\u548c\u5730\u5f62\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u573a\u666f\u89c4\u8303\u3002", "conclusion": "\u8be5\u5f00\u653e\u6570\u636e\u96c6\u4e3a\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u3001\u4eff\u771f\u548c\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\u5f00\u53d1\u68ee\u6797\u673a\u68b0\u7684\u4ea4\u901a\u80fd\u529b\u3001\u611f\u77e5\u548c\u81ea\u4e3b\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u652f\u6301\u6a21\u62df\u5668\u81ea\u52a8\u751f\u6210\u548c\u81ea\u52a8\u5316\u573a\u666f\u63cf\u8ff0\u7684\u63a2\u7d22\u3002"}}
{"id": "2511.17335", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17335", "abs": "https://arxiv.org/abs/2511.17335", "authors": ["Chiori Hori", "Yoshiki Masuyama", "Siddarth Jain", "Radu Corcodel", "Devesh Jha", "Diego Romeres", "Jonathan Le Roux"], "title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM", "comment": "Accepted to ASRU 2025", "summary": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u4e0a\u4e0b\u6587Q-former\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u9891\u4e2d\u7684\u5de6\u53f3\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u6761\u4ef6\u5316\u65b9\u6cd5\u76f4\u63a5\u5c06\u6587\u672c\u5d4c\u5165\u8f93\u5165\u5230LLM\u89e3\u7801\u5668\u4e2d\uff0c\u4ee5\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u52a8\u4f5c\u786e\u8ba4\u548c\u52a8\u4f5c\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u591a\u6a21\u6001\u53d8\u6362\u5668\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7247\u6bb5\u7ea7\u5904\u7406\uff0c\u672a\u80fd\u5229\u7528\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u52a8\u4f5c\u5728\u6574\u4e2a\u89c6\u9891\u4e2d\u76f8\u4e92\u4f9d\u8d56\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u597d\u5730\u6574\u5408\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u957f\u4e0a\u4e0b\u6587Q-former\u65b9\u6cd5\uff0c\u6574\u5408\u89c6\u9891\u4e2d\u7684\u5de6\u53f3\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff1b\u540c\u65f6\u63d0\u51fa\u6587\u672c\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u5d4c\u5165\u8f93\u5165\u5230LLM\u89e3\u7801\u5668\u4e2d\uff0c\u4ee5\u51cf\u8f7bQ-former\u4e2d\u4fe1\u606f\u7684\u9ad8\u5ea6\u62bd\u8c61\u5316\u95ee\u9898\u3002", "result": "\u5728YouCook2\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u786e\u8ba4\u751f\u6210\u7684\u51c6\u786e\u6027\u662f\u52a8\u4f5c\u89c4\u5212\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\uff1b\u957f\u4e0a\u4e0b\u6587Q-former\u901a\u8fc7\u6574\u5408VideoLLaMA3\u6539\u5584\u4e86\u786e\u8ba4\u548c\u52a8\u4f5c\u89c4\u5212\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587Q-former\u80fd\u591f\u6709\u6548\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4e2d\u52a8\u4f5c\u786e\u8ba4\u548c\u52a8\u4f5c\u89c4\u5212\u7684\u6027\u80fd\uff0c\u786e\u8ba4\u751f\u6210\u7684\u51c6\u786e\u6027\u5bf9\u6574\u4f53\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.17366", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17366", "abs": "https://arxiv.org/abs/2511.17366", "authors": ["Yankai Fu", "Ning Chen", "Junkai Zhao", "Shaozhe Shan", "Guocai Yao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model", "comment": null, "summary": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.", "AI": {"tldr": "METIS\u662f\u4e00\u4e2a\u7528\u4e8e\u7075\u5de7\u64cd\u4f5c\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6e90\u81ea\u6211\u4e2d\u5fc3\u6570\u636e\u96c6\u6784\u5efaEgoAtlas\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u8fd0\u52a8\u611f\u77e5\u52a8\u6001\u8868\u793a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u9ad8\u5e73\u5747\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u5927\u89c4\u6a21\u52a8\u4f5c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5229\u7528\u4eba\u7c7b\u6570\u636e\u7684\u4e30\u5bcc\u5148\u9a8c\u77e5\u8bc6\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u573a\u666f\u9650\u5236\u548c\u89c6\u89c9\u5dee\u5f02\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u6784\u5efaEgoAtlas\u6570\u636e\u96c6\u6574\u5408\u591a\u6e90\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u6570\u636e\uff0c\u63d0\u53d6\u8fd0\u52a8\u611f\u77e5\u52a8\u6001\u4f5c\u4e3a\u7d27\u51d1\u79bb\u6563\u5316\u8fd0\u52a8\u8868\u793a\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u96c6\u6210\u63a8\u7406\u548c\u52a8\u4f5c\u6267\u884c\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u9ad8\u5e73\u5747\u6210\u529f\u7387\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5206\u5e03\u5916\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "METIS\u662f\u8fc8\u5411\u7075\u5de7\u64cd\u4f5c\u901a\u7528\u6a21\u578b\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\uff0c\u9a8c\u8bc1\u4e86\u5229\u7528\u4eba\u7c7b\u6570\u636e\u6784\u5efa\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.17373", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17373", "abs": "https://arxiv.org/abs/2511.17373", "authors": ["Yixuan Pan", "Ruoyi Qiao", "Li Chen", "Kashyap Chitta", "Liang Pan", "Haoguang Mai", "Qingwen Bu", "Hao Zhao", "Cunyuan Zheng", "Ping Luo", "Hongyang Li"], "title": "Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data", "comment": null, "summary": "Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other. In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions. We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.", "AI": {"tldr": "AMS\u6846\u67b6\u9996\u6b21\u5c06\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u548c\u6781\u7aef\u5e73\u8861\u7ef4\u62a4\u7edf\u4e00\u5728\u5355\u4e00\u7b56\u7565\u4e2d\uff0c\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u6355\u6349\u6570\u636e\u548c\u5408\u6210\u5e73\u8861\u52a8\u4f5c\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u65b9\u6848\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u654f\u6377\u6027\u4e0e\u7a33\u5b9a\u6027\u7684\u7edf\u4e00\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u9700\u8981\u5728\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u6267\u884c\u5e7f\u6cdb\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4e13\u6ce8\u4e8e\u654f\u6377\u52a8\u6001\u6280\u80fd\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u7a33\u5b9a\u6027\u5173\u952e\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u6765\u540c\u65f6\u5b9e\u73b0\u8fd9\u4e24\u79cd\u80fd\u529b\u3002", "method": "\u5229\u7528\u5f02\u6784\u6570\u636e\u6e90\uff1a\u4eba\u7c7b\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\u63d0\u4f9b\u654f\u6377\u884c\u4e3a\uff0c\u7269\u7406\u7ea6\u675f\u7684\u5408\u6210\u5e73\u8861\u52a8\u4f5c\u6355\u83b7\u7a33\u5b9a\u914d\u7f6e\uff1b\u8bbe\u8ba1\u6df7\u5408\u5956\u52b1\u65b9\u6848\uff0c\u5728\u6240\u6709\u6570\u636e\u4e0a\u5e94\u7528\u901a\u7528\u8ddf\u8e2a\u76ee\u6807\uff0c\u4ec5\u5728\u5408\u6210\u52a8\u4f5c\u4e2d\u6ce8\u5165\u5e73\u8861\u7279\u5b9a\u5148\u9a8c\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff0c\u5305\u62ec\u6027\u80fd\u9a71\u52a8\u91c7\u6837\u548c\u52a8\u4f5c\u7279\u5b9a\u5956\u52b1\u5851\u9020\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9eUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u4e00\u7b56\u7565\u80fd\u591f\u6267\u884c\u821e\u8e48\u3001\u8dd1\u6b65\u7b49\u654f\u6377\u6280\u80fd\uff0c\u540c\u65f6\u4e5f\u80fd\u96f6\u6837\u672c\u6267\u884c\u53f6\u95ee\u8e72\u7b49\u6781\u7aef\u5e73\u8861\u52a8\u4f5c\u3002", "conclusion": "AMS\u4f5c\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u63a7\u5236\u8303\u5f0f\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u654f\u6377\u6027\u548c\u7a33\u5b9a\u6027\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2511.17375", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.17375", "abs": "https://arxiv.org/abs/2511.17375", "authors": ["Benjamin R. Toaz", "Quentin Goss", "John Thompson", "Seta Bo\u011fosyan", "Shaunak D. Bopardikar", "Mustafa \u0130lhan Akba\u015f", "Metin G\u00f6ka\u015fan"], "title": "Vector Cost Behavioral Planning for Autonomous Robotic Systems with Contemporary Validation Strategies", "comment": "Technical report associated with submission to Journal of Intelligent & Robotic Systems, currently under review", "summary": "The vector cost bimatrix game is a method for multi-objective decision making that enables autonomous robotic systems to optimize for multiple goals at once while avoiding worst-case scenarios in neglected objectives. We expand this approach to arbitrary numbers of objectives and compare its performance to scalar weighted sum methods during competitive motion planning. Explainable Artificial Intelligence (XAI) software is used to aid in the analysis of high dimensional decision-making data. State-space Exploration of Multidimensional Boundaries using Adherence Strategies (SEMBAS) is applied to explore performance modes in the parameter space as a sensitivity study for the baseline and proposed frameworks. While some works have explored aspects of game theoretic planning and intelligent systems validation separately, we combine each of these into a novel and comprehensive simulation pipeline. This integration demonstrates a dramatic improvement of the vector cost method over scalarization and offers an interpretable and generalizable framework for robotic behavioral planning. Code available at https://github.com/toazbenj/race_simulation. The video companion to this work is available at https://tinyurl.com/vectorcostvideo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5411\u91cf\u6210\u672c\u53cc\u77e9\u9635\u535a\u5f08\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u76ee\u6807\u51b3\u7b56\uff0c\u4f7f\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76ee\u6807\uff0c\u5e76\u5728\u7ade\u4e89\u6027\u8fd0\u52a8\u89c4\u5212\u4e2d\u907f\u514d\u6700\u574f\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u5230\u4efb\u610f\u6570\u91cf\u7684\u76ee\u6807\uff0c\u5e76\u901a\u8fc7XAI\u548cSEMBAS\u5206\u6790\u9ad8\u7ef4\u51b3\u7b56\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5411\u91cf\u6210\u672c\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6807\u91cf\u52a0\u6743\u548c\u65b9\u6cd5\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u76ee\u6807\u51b3\u7b56\u65b9\u6cd5\u5982\u6807\u91cf\u52a0\u6743\u548c\u6cd5\u5728\u7ade\u4e89\u6027\u8fd0\u52a8\u89c4\u5212\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u4e2a\u76ee\u6807\u7684\u6743\u8861\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76ee\u6807\u5e76\u907f\u514d\u6700\u574f\u60c5\u51b5\u7684\u6846\u67b6\uff0c\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u89c4\u5212\u63d0\u4f9b\u53ef\u89e3\u91ca\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5411\u91cf\u6210\u672c\u53cc\u77e9\u9635\u535a\u5f08\u65b9\u6cd5\uff0c\u6269\u5c55\u5230\u4efb\u610f\u6570\u91cf\u7684\u76ee\u6807\u3002\u4f7f\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd(XAI)\u5206\u6790\u9ad8\u7ef4\u51b3\u7b56\u6570\u636e\uff0c\u5e94\u7528\u72b6\u6001\u7a7a\u95f4\u591a\u7ef4\u8fb9\u754c\u63a2\u7d22(SEMBAS)\u8fdb\u884c\u53c2\u6570\u7a7a\u95f4\u654f\u611f\u6027\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u5305\u542b\u535a\u5f08\u8bba\u89c4\u5212\u548c\u667a\u80fd\u7cfb\u7edf\u9a8c\u8bc1\u7684\u5b8c\u6574\u4eff\u771f\u6d41\u6c34\u7ebf\u3002", "result": "\u5411\u91cf\u6210\u672c\u65b9\u6cd5\u76f8\u6bd4\u6807\u91cf\u52a0\u6743\u548c\u65b9\u6cd5\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u884c\u4e3a\u89c4\u5212\u6846\u67b6\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5411\u91cf\u6210\u672c\u53cc\u77e9\u9635\u535a\u5f08\u65b9\u6cd5\u4e3a\u591a\u76ee\u6807\u51b3\u7b56\u63d0\u4f9b\u4e86\u4f18\u4e8e\u4f20\u7edf\u6807\u91cf\u65b9\u6cd5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408XAI\u548cSEMBAS\u5206\u6790\u5de5\u5177\uff0c\u5f62\u6210\u4e86\u53ef\u89e3\u91ca\u4e14\u901a\u7528\u7684\u673a\u5668\u4eba\u884c\u4e3a\u89c4\u5212\u6846\u67b6\uff0c\u4ee3\u7801\u548c\u89c6\u9891\u8d44\u6599\u5df2\u516c\u5f00\u3002"}}
{"id": "2511.17384", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17384", "abs": "https://arxiv.org/abs/2511.17384", "authors": ["Yifan Li", "Lichi Li", "Anh Dao", "Xinyu Zhou", "Yicheng Qiao", "Zheda Mai", "Daeun Lee", "Zichen Chen", "Zhen Tan", "Mohit Bansal", "Yu Kong"], "title": "IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation", "comment": null, "summary": "While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the \"collision rate\" and \"warning rate\" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.", "AI": {"tldr": "IndustryNav\u662f\u9996\u4e2a\u52a8\u6001\u5de5\u4e1a\u5bfc\u822a\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u4ed3\u5e93\u73af\u5883\u4e2d\u7684\u4e3b\u52a8\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8def\u5f84\u89c4\u5212\u3001\u78b0\u649e\u907f\u514d\u548c\u4e3b\u52a8\u63a2\u7d22\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u5bb6\u5ead\u73af\u5883\uff0c\u65e0\u6cd5\u8bc4\u4f30\u52a8\u6001\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6574\u4f53\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6355\u6349\u52a8\u6001\u590d\u6742\u6027\u7684\u57fa\u51c6\u3002", "method": "\u4f7f\u752812\u4e2a\u624b\u52a8\u521b\u5efa\u7684\u9ad8\u4fdd\u771fUnity\u4ed3\u5e93\u573a\u666f\uff0c\u7ed3\u5408\u52a8\u6001\u7269\u4f53\u548c\u4eba\u5458\u79fb\u52a8\uff0c\u91c7\u7528PointGoal\u5bfc\u822a\u7ba1\u9053\u5c06\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u4e0e\u5168\u5c40\u91cc\u7a0b\u8ba1\u7ed3\u5408\uff0c\u8bc4\u4f30\u5c40\u90e8-\u5168\u5c40\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5bf99\u4e2a\u6700\u5148\u8fdbVLLM\u7684\u5168\u9762\u7814\u7a76\u8868\u660e\uff0c\u95ed\u6e90\u6a21\u578b\u5177\u6709\u6301\u7eed\u4f18\u52bf\uff0c\u4f46\u6240\u6709\u667a\u80fd\u4f53\u5728\u7a33\u5065\u8def\u5f84\u89c4\u5212\u3001\u78b0\u649e\u907f\u514d\u548c\u4e3b\u52a8\u63a2\u7d22\u65b9\u9762\u90fd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u5177\u8eab\u7814\u7a76\u9700\u8981\u8d85\u8d8a\u88ab\u52a8\u611f\u77e5\uff0c\u8f6c\u5411\u9700\u8981\u7a33\u5b9a\u89c4\u5212\u3001\u4e3b\u52a8\u63a2\u7d22\u548c\u5728\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u5b89\u5168\u884c\u4e3a\u7684\u4efb\u52a1\u3002"}}
{"id": "2511.17401", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17401", "abs": "https://arxiv.org/abs/2511.17401", "authors": ["Xiaoshan Zhou", "Carol C. Menassa", "Vineet R. Kamat"], "title": "Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment", "comment": "37 pages, 9 figures, and 7 tables", "summary": "Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u7406\u7684\u8111\u673a\u63a5\u53e3\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u8fde\u7eed\u8ffd\u8e2a\u8fd0\u52a8\u63a7\u5236\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5c06\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u4e8672%\u3002", "motivation": "\u5f53\u524d\u8111\u673a\u63a5\u53e3\u8fd0\u52a8\u63a7\u5236\u7cfb\u7edf\u5927\u591a\u9650\u4e8e\u79bb\u6563\u547d\u4ee4\uff0c\u65e0\u6cd5\u652f\u6301\u7528\u6237\u5b9e\u65f6\u81ea\u7531\u8c03\u6574\u901f\u5ea6\u548c\u65b9\u5411\u7684\u8fde\u7eed\u8ffd\u8e2a\u63a7\u5236\uff0c\u800c\u8fd9\u79cd\u81ea\u7136\u7684\u79fb\u52a8\u63a7\u5236\u5bf9\u4e8e\u8f6e\u6905\u7528\u6237\u5728\u590d\u6742\u516c\u5171\u7a7a\u95f4\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u8111\u542f\u53d1\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\uff0c\u89e3\u7801\u57fa\u4e8e\u52a0\u901f\u5ea6\u7684\u8fd0\u52a8\u8868\u5f81\u4e2d\u7684\u4f53\u73b0\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u81ea\u52a8\u76f8\u5173\u6027\u786e\u5b9a\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u91c7\u7528\u6301\u7eed\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u5728\u57fa\u4e8e\u8fd0\u52a8\u60f3\u8c61\u7684\u76ee\u6807\u8ffd\u8e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f1a\u8bdd\u7d2f\u79ef\u8fc1\u79fb\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52\u548cEEGNet\u65b9\u6cd5\uff0c\u5c06\u9884\u6d4b\u901f\u5ea6\u4e0e\u771f\u5b9e\u901f\u5ea6\u4e4b\u95f4\u7684\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u4e8672%\u3002", "conclusion": "\u7406\u8bba\u4e0a\u652f\u6301\u4e86\u4f53\u73b0\u8ba4\u77e5\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u5927\u8111\u5185\u5728\u8fd0\u52a8\u63a7\u5236\u52a8\u529b\u5b66\u7684\u4f53\u73b0\u548c\u9884\u6d4b\u6027\u8d28\uff1b\u5b9e\u8df5\u4e0a\u4e3a\u66f4\u7a33\u5b9a\u548c\u76f4\u89c2\u7684\u8111\u673a\u63a5\u53e3\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2511.17411", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17411", "abs": "https://arxiv.org/abs/2511.17411", "authors": ["Nikolay Nikolov", "Giuliano Albanese", "Sombit Dey", "Aleksandar Yanev", "Luc Van Gool", "Jan-Nico Zaech", "Danda Pani Paudel"], "title": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding", "comment": null, "summary": "Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $\u03c0_0$-FAST and $\u03c0_{0.5}$, while it uses 20$\\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.", "AI": {"tldr": "SPEAR-1\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u5f3a\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76843D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5728\u51cf\u5c1120\u500d\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5927\u591a\u57fa\u4e8e\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u7f3a\u4e4f3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u800c\u76f4\u63a5\u6536\u96c6\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u63d0\u51faSPEAR-VLM\uff0c\u901a\u8fc7\u4e3a\u6613\u4e8e\u6536\u96c6\u7684\u975e\u673a\u5668\u4eba\u56fe\u50cf\u6570\u636e\u6dfb\u52a03D\u6ce8\u91ca\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3VLM\u76843D\u7406\u89e3\u80fd\u529b\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efaSPEAR-1\u6a21\u578b\uff0c\u6574\u5408\u57fa\u4e8e3D\u611f\u77e5\u7684\u8bed\u8a00\u6307\u4ee4\u63a7\u5236\u3002", "result": "\u572824\u4e2aOpen X-Embodiment\u6570\u636e\u96c6\u7684\u7ea64500\u4e07\u5e27\u6570\u636e\u4e0a\u8bad\u7ec3\uff0cSPEAR-1\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u03c0\u2080-FAST\u548c\u03c0\u2080.\u2085\u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u51cf\u5c11\u4e8620\u500d\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u89e3\u9501\u4e86\u65b0\u7684VLM\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u5177\u8eab\u63a7\u5236\u7684\u53ef\u9760\u6027\uff0c\u8d85\u8d8a\u4e86\u4ec5\u4f7f\u7528\u673a\u5668\u4eba\u6570\u636e\u6240\u80fd\u8fbe\u5230\u7684\u6c34\u5e73\u3002"}}
{"id": "2511.17441", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17441", "abs": "https://arxiv.org/abs/2511.17441", "authors": ["Shihan Wu", "Xuecheng Liu", "Shaoxuan Xie", "Pengwei Wang", "Xinghang Li", "Bowen Yang", "Zhe Li", "Kai Zhu", "Hongyu Wu", "Yiheng Liu", "Zhaoye Long", "Yue Wang", "Chong Liu", "Dihan Wang", "Ziqiang Ni", "Xiang Yang", "You Liu", "Ruoxuan Feng", "Runtian Xu", "Lei Zhang", "Denghang Huang", "Chenghao Jin", "Anlan Yin", "Xinlong Wang", "Zhenguo Sun", "Junkai Zhao", "Mengfei Du", "Mingyu Cao", "Xiansheng Chen", "Hongyang Cheng", "Xiaojie Zhang", "Yankai Fu", "Ning Chen", "Cheng Chi", "Sixiang Chen", "Huaihai Lyu", "Xiaoshuai Hao", "Yankai Fu", "Yequan Wang", "Bo Lei", "Dong Liu", "Xi Yang", "Yance Jiao", "Tengfei Pan", "Yunyan Zhang", "Songjing Wang", "Ziqian Zhang", "Xu Liu", "Ji Zhang", "Caowei Meng", "Zhizheng Zhang", "Jiyang Gao", "Song Wang", "Xiaokun Leng", "Zhiqiang Xie", "Zhenzhen Zhou", "Peng Huang", "Wu Yang", "Yandong Guo", "Yichao Zhu", "Suibing Zheng", "Hao Cheng", "Xinmin Ding", "Yang Yue", "Huanqian Wang", "Chi Chen", "Jingrui Pang", "YuXi Qian", "Haoran Geng", "Lianli Gao", "Haiyuan Li", "Bin Fang", "Gao Huang", "Yaodong Yang", "Hao Dong", "He Wang", "Hang Zhao", "Yadong Mu", "Di Hu", "Hao Zhao", "Tiejun Huang", "Shanghang Zhang", "Yonghua Lin", "Zhongyuan Wang", "Guocai Yao"], "title": "RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation", "comment": null, "summary": "Bimanual manipulation is essential for achieving human-like dexterity in robots, but the large-scale and diverse bimanual robot datasets remain scarce due to hardware heterogeneity across robotic platforms. To address the challenge, we present RoboCOIN, a comprehensive multi-embodiment bimanual manipulation dataset with over 180,000 demonstrations collected from 15 distinct robotic platforms. The dataset covers 16 scenarios, including residential, commercial, and working environments, with 421 tasks systematically organized by bimanual coordination patterns and object properties. Our key innovation is a hierarchical capability pyramid that provides multi-level annotations, spanning trajectory-level concepts, segment-level subtasks, and frame-level kinematics. We further develop CoRobot, a comprehensive processing framework featuring Robot Trajectory Markup Language (RTML) for quality assessment, automated annotation generation, and unified multi-embodiment management. Extensive experiments demonstrate the reliability and effectiveness of RoboCOIN in multi-embodiment bimanual learning, with significant performance improvements across various model architectures and robotic platforms. The complete dataset and framework are open-sourced and publicly available for further research purposes. Project website: https://FlagOpen.github.io/RoboCOIN/.", "AI": {"tldr": "RoboCOIN\u662f\u4e00\u4e2a\u5305\u542b18\u4e07\u6b21\u6f14\u793a\u7684\u591a\u5e73\u53f0\u53cc\u81c2\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u6db5\u76d615\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u548c16\u79cd\u573a\u666f\uff0c\u901a\u8fc7\u5206\u5c42\u80fd\u529b\u91d1\u5b57\u5854\u63d0\u4f9b\u591a\u7ea7\u6807\u6ce8\uff0c\u5e76\u5f00\u53d1\u4e86CoRobot\u5904\u7406\u6846\u67b6\u7528\u4e8e\u8d28\u91cf\u8bc4\u4f30\u548c\u7edf\u4e00\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u53cc\u81c2\u64cd\u4f5c\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u673a\u5668\u4eba\u5e73\u53f0\u5f02\u6784\u6027\u7684\u6311\u6218\uff0c\u4e3a\u673a\u5668\u4eba\u5b9e\u73b0\u7c7b\u4eba\u7075\u5de7\u6027\u63d0\u4f9b\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u652f\u6301\u3002", "method": "\u6784\u5efa\u5305\u542b15\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u300116\u79cd\u573a\u666f\u3001421\u4e2a\u4efb\u52a1\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5206\u5c42\u80fd\u529b\u91d1\u5b57\u5854\u8fdb\u884c\u591a\u7ea7\u6807\u6ce8\uff0c\u5f00\u53d1\u57fa\u4e8eRTML\u7684CoRobot\u6846\u67b6\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u548c\u7edf\u4e00\u7ba1\u7406\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc718\u4e07\u6b21\u6f14\u793a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90fd\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "RoboCOIN\u4e3a\u591a\u5e73\u53f0\u53cc\u81c2\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u96c6\u548c\u6846\u67b6\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2511.17496", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17496", "abs": "https://arxiv.org/abs/2511.17496", "authors": ["Zhiyu Huang", "Zewei Zhou", "Tianhui Cai", "Yun Zhang", "Jiaqi Ma"], "title": "MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments", "comment": null, "summary": "Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86Masked Denoising Generation (MDG)\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u5efa\u6a21\uff0c\u901a\u8fc7\u566a\u58f0\u63a9\u7801\u91cd\u6784\u65f6\u7a7a\u5f20\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u63a7\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u53d7\u9650\u4e8e\u8fed\u4ee3\u91c7\u6837\u3001\u987a\u5e8f\u89e3\u7801\u6216\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\uff0c\u5f71\u54cd\u6548\u7387\u548c\u91cd\u7528\u6027\u3002\u9700\u8981\u7edf\u4e00\u7684\u751f\u6210\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "MDG\u5c06\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u5efa\u6a21\u91cd\u6784\u4e3a\u72ec\u7acb\u566a\u58f0\u65f6\u7a7a\u5f20\u91cf\u7684\u91cd\u5efa\uff0c\u5e94\u7528\u8fde\u7eed\u3001\u6bcf\u667a\u80fd\u4f53\u6bcf\u65f6\u95f4\u6b65\u7684\u566a\u58f0\u63a9\u7801\uff0c\u5b9e\u73b0\u5c40\u90e8\u53bb\u566a\u548c\u53ef\u63a7\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u5728Waymo Sim Agents\u548cnuPlan Planning\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u95ed\u73af\u6027\u80fd\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u5f00\u73af\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u751f\u6210\u3002", "conclusion": "MDG\u4f5c\u4e3a\u7b80\u5355\u800c\u901a\u7528\u7684\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u5efa\u6a21\u8303\u5f0f\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u5f00\u73af\u9884\u6d4b\u3001\u95ed\u73af\u4eff\u771f\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u6761\u4ef6\u751f\u6210\u7b49\u4efb\u52a1\u3002"}}
{"id": "2511.17497", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17497", "abs": "https://arxiv.org/abs/2511.17497", "authors": ["Yuezhan Tao", "Dexter Ong", "Fernando Cladera", "Jason Hughes", "Camillo J. Taylor", "Pratik Chaudhari", "Vijay Kumar"], "title": "HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation", "comment": null, "summary": "We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.", "AI": {"tldr": "HALO\u7cfb\u7edf\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5355\u76ee\u76f8\u673a\u3001GPS\u548cIMU\u7684\u5b9e\u65f6\u9ad8\u7a7a\u822a\u62cd\u5ea6\u91cf-\u8bed\u4e49\u5efa\u56fe\u4e0e\u63a2\u7d22\uff0c\u80fd\u5728\u5927\u578b\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u5bc6\u96c63D\u91cd\u5efa\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89c4\u5212\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5728\u8fdc\u8ddd\u79bb\u4e0b\u4f7f\u7528\u89c6\u89c9\u8fdb\u884c\u5b9e\u65f6\u5bc6\u96c63D\u91cd\u5efa\uff0c\u4ee5\u53ca\u5728\u5927\u578b\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u573a\u666f\u51e0\u4f55\u548c\u8bed\u4e49\u7684\u5efa\u56fe\u4e0e\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u5355\u76ee\u76f8\u673a\u3001GPS\u548cIMU\u7ec4\u5408\uff0c\u5f00\u53d1\u4e86\u540d\u4e3aHALO\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5b9e\u65f6\u8fdb\u884c\u5ea6\u91cf-\u8bed\u4e49\u5efa\u56fe\uff0c\u5e76\u57fa\u4e8e\u6b64\u4fe1\u606f\u89c4\u5212\u4fe1\u606f\u4e30\u5bcc\u7684\u8def\u5f84\u6765\u5b8c\u6210\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u7684\u591a\u4efb\u52a1\u4efb\u52a1\u3002", "result": "\u5728\u9ad8\u8fbe78,000\u5e73\u65b9\u7c73\u7684\u5927\u89c4\u6a21\u73af\u5883\u4eff\u771f\u8bc4\u4f30\u4e2d\uff0cHALO\u4ee5\u66f4\u5c11\u7684\u63a2\u7d22\u65f6\u95f4\u5b8c\u6210\u4efb\u52a1\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u63a2\u7d22\u57fa\u7ebf\uff0c\u884c\u8fdb\u8ddd\u79bb\u7684\u7ade\u4e89\u6bd4\u63d0\u9ad8\u4e8668%\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u6240\u6709\u6a21\u5757\u90fd\u80fd\u5728\u673a\u5668\u4eba\u4e0a\u8fd0\u884c\uff0c\u572840\u7c73\u9ad8\u5ea6\u4e0b\u652f\u6301\u8986\u76d624,600\u5e73\u65b9\u7c73\u533a\u57df\u7684\u6709\u6548\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "HALO\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u7a7a\u822a\u62cd\u5ea6\u91cf-\u8bed\u4e49\u5efa\u56fe\u4e0e\u63a2\u7d22\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u5927\u578b\u6237\u5916\u73af\u5883\u7684\u81ea\u4e3b\u63a2\u7d22\u548c\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17502", "abs": "https://arxiv.org/abs/2511.17502", "authors": ["Jun Cen", "Siteng Huang", "Yuqian Yuan", "Hangjie Yuan", "Chaohui Yu", "Yuming Jiang", "Jiayan Guo", "Kehan Li", "Hao Luo", "Fan Wang", "Xin Li", "Deli Zhao", "Hao Chen"], "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model", "comment": null, "summary": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.", "AI": {"tldr": "RynnVLA-002\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u73af\u5883\u52a8\u6001\u548c\u52a8\u4f5c\u89c4\u5212\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u8054\u5408\u5b66\u4e60\u73af\u5883\u52a8\u6001\u548c\u52a8\u4f5c\u89c4\u5212\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\u7684\u76f8\u4e92\u589e\u5f3a\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u548c\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u4e16\u754c\u6a21\u578b\u5229\u7528\u52a8\u4f5c\u548c\u89c6\u89c9\u8f93\u5165\u9884\u6d4b\u672a\u6765\u56fe\u50cf\u72b6\u6001\uff0c\u5b66\u4e60\u73af\u5883\u7269\u7406\u7279\u6027\uff1bVLA\u6a21\u578b\u4ece\u56fe\u50cf\u89c2\u5bdf\u751f\u6210\u540e\u7eed\u52a8\u4f5c\uff0c\u589e\u5f3a\u89c6\u89c9\u7406\u89e3\u5e76\u652f\u6301\u4e16\u754c\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5728LIBERO\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523097.4%\u7684\u6210\u529f\u7387\uff08\u65e0\u9700\u9884\u8bad\u7ec3\uff09\uff0c\u5728\u771f\u5b9e\u4e16\u754cLeRobot\u5b9e\u9a8c\u4e2d\uff0c\u96c6\u6210\u7684\u4e16\u754c\u6a21\u578b\u5c06\u6574\u4f53\u6210\u529f\u7387\u63d0\u5347\u4e8650%\u3002", "conclusion": "RynnVLA-002\u8d85\u8d8a\u4e86\u5355\u72ec\u7684VLA\u548c\u4e16\u754c\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u76f8\u4e92\u589e\u5f3a\u6548\u679c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
