{"id": "2602.18569", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18569", "abs": "https://arxiv.org/abs/2602.18569", "authors": ["Jos\u00e9e Mallah", "Zakii Javed", "Zafer Azak", "Thomas Stone", "Luigi G. Occhipinti"], "title": "Design and Biomechanical Evaluation of a Lightweight Low-Complexity Soft Bilateral Ankle Exoskeleton", "comment": null, "summary": "Many people could benefit from exoskeleton assistance during gait, for either medical or nonmedical purposes. But exoskeletons bring added mass and structure, which in turn require compensating for. In this work, we present a lightweight, low-complexity, soft bilateral ankle exoskeleton for plantarflexion assistance, with a shoe attachment design that can be mounted on top of any pair of shoes. Experimental tests show no significant difference in lower limb kinematics and kinetics when wearing the exoskeleton in zero-torque mode relative to not wearing an exoskeleton, showing that our device does not obstruct healthy gait, and proving it as a compliant and comfortable device, promising to provide effective assistance. Hence, a control system was developed, and additional tests are underway.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u8f7b\u91cf\u3001\u4f4e\u590d\u6742\u5ea6\u3001\u67d4\u8f6f\u7684\u53cc\u4fa7\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\uff0c\u7528\u4e8e\u8dd6\u5c48\u8f85\u52a9\uff0c\u53ef\u5b89\u88c5\u5728\u4efb\u610f\u978b\u4e0a\uff0c\u4e0d\u5f71\u54cd\u5065\u5eb7\u6b65\u6001", "motivation": "\u8bb8\u591a\u4eba\uff08\u65e0\u8bba\u662f\u533b\u7597\u8fd8\u662f\u975e\u533b\u7597\u76ee\u7684\uff09\u90fd\u80fd\u4ece\u6b65\u6001\u5916\u9aa8\u9abc\u8f85\u52a9\u4e2d\u53d7\u76ca\uff0c\u4f46\u4f20\u7edf\u5916\u9aa8\u9abc\u5b58\u5728\u91cd\u91cf\u5927\u3001\u7ed3\u6784\u590d\u6742\u7684\u95ee\u9898\uff0c\u9700\u8981\u8865\u507f\u8fd9\u4e9b\u7f3a\u70b9", "method": "\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u67d4\u8f6f\u53cc\u4fa7\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\uff0c\u91c7\u7528\u978b\u9762\u9644\u7740\u8bbe\u8ba1\uff0c\u53ef\u5b89\u88c5\u5728\u4efb\u610f\u978b\u4e0a\uff1b\u5f00\u53d1\u4e86\u63a7\u5236\u7cfb\u7edf\uff0c\u5e76\u5728\u96f6\u626d\u77e9\u6a21\u5f0f\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5", "result": "\u5b9e\u9a8c\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u96f6\u626d\u77e9\u6a21\u5f0f\u4e0b\u7a7f\u6234\u5916\u9aa8\u9abc\u4e0e\u4e0d\u7a7f\u6234\u5916\u9aa8\u9abc\u76f8\u6bd4\uff0c\u4e0b\u80a2\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u8bc1\u660e\u8bbe\u5907\u4e0d\u963b\u788d\u5065\u5eb7\u6b65\u6001\uff0c\u662f\u987a\u5e94\u6027\u826f\u597d\u4e14\u8212\u9002\u7684\u8bbe\u5907", "conclusion": "\u8be5\u5916\u9aa8\u9abc\u8bbe\u5907\u5177\u6709\u63d0\u4f9b\u6709\u6548\u8f85\u52a9\u7684\u6f5c\u529b\uff0c\u63a7\u5236\u7cfb\u7edf\u5df2\u5f00\u53d1\u5b8c\u6210\uff0c\u6b63\u5728\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u6d4b\u8bd5"}}
{"id": "2602.18603", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18603", "abs": "https://arxiv.org/abs/2602.18603", "authors": ["Anjiabei Wang", "Shuangge Wang", "Tesca Fitzgerald"], "title": "Enhancing Goal Inference via Correction Timing", "comment": "25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)", "summary": "Corrections offer a natural modality for people to provide feedback to a robot, by (i) intervening in the robot's behavior when they believe the robot is failing (or will fail) the task objectives and (ii) modifying the robot's behavior to successfully fulfill the task. Each correction offers information on what the robot should and should not do, where the corrected behavior is more aligned with task objectives than the original behavior. Most prior work on learning from corrections involves interpreting a correction as a new demonstration (consisting of the modified robot behavior), or a preference (for the modified trajectory compared to the robot's original behavior). However, this overlooks one essential element of the correction feedback, which is the human's decision to intervene in the robot's behavior in the first place. This decision can be influenced by multiple factors including the robot's task progress, alignment with human expectations, dynamics, motion legibility, and optimality. In this work, we investigate whether the timing of this decision can offer a useful signal for inferring these task-relevant influences. In particular, we investigate three potential applications for this learning signal: (1) identifying features of a robot's motion that may prompt people to correct it, (2) quickly inferring the final goal of a human's correction based on the timing and initial direction of their correction motion, and (3) learning more precise constraints for task objectives. Our results indicate that correction timing results in improved learning for the first two of these applications. Overall, our work provides new insights on the value of correction timing as a signal for robot learning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u7ea0\u6b63\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u7ea0\u6b63\u65f6\u673a\u4f5c\u4e3a\u5b66\u4e60\u4fe1\u53f7\u7684\u4ef7\u503c", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u7ea0\u6b63\u89c6\u4e3a\u65b0\u6f14\u793a\u6216\u504f\u597d\uff0c\u4f46\u5ffd\u7565\u4e86\u4eba\u7c7b\u51b3\u5b9a\u5e72\u9884\u673a\u5668\u4eba\u884c\u4e3a\u7684\u65f6\u673a\u8fd9\u4e00\u5173\u952e\u4fe1\u53f7\uff0c\u8be5\u65f6\u673a\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd", "method": "\u7814\u7a76\u7ea0\u6b63\u65f6\u673a\u4f5c\u4e3a\u5b66\u4e60\u4fe1\u53f7\uff0c\u63a2\u7d22\u5176\u5728\u4e09\u4e2a\u6f5c\u5728\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\uff1a\u8bc6\u522b\u89e6\u53d1\u7ea0\u6b63\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7279\u5f81\u3001\u5feb\u901f\u63a8\u65ad\u7ea0\u6b63\u76ee\u6807\u3001\u5b66\u4e60\u66f4\u7cbe\u786e\u7684\u4efb\u52a1\u7ea6\u675f", "result": "\u7ea0\u6b63\u65f6\u673a\u5728\u524d\u4e24\u4e2a\u5e94\u7528\u4e2d\uff08\u8bc6\u522b\u89e6\u53d1\u7279\u5f81\u548c\u5feb\u901f\u63a8\u65ad\u76ee\u6807\uff09\u663e\u8457\u6539\u5584\u4e86\u5b66\u4e60\u6548\u679c", "conclusion": "\u7ea0\u6b63\u65f6\u673a\u4f5c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u4fe1\u53f7\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2602.18606", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18606", "abs": "https://arxiv.org/abs/2602.18606", "authors": ["Rwik Rana", "Jesse Quattrociocchi", "Dongmyeong Lee", "Christian Ellis", "Amanda Adkins", "Adam Uccello", "Garrett Warnell", "Joydeep Biswas"], "title": "OVerSeeC: Open-Vocabulary Costmap Generation from Satellite Images and Natural Language", "comment": "Website : https://amrl.cs.utexas.edu/overseec/", "summary": "Aerial imagery provides essential global context for autonomous navigation, enabling route planning at scales inaccessible to onboard sensing. We address the problem of generating global costmaps for long-range planning directly from satellite imagery when entities and mission-specific traversal rules are expressed in natural language at test time. This setting is challenging since mission requirements vary, terrain entities may be unknown at deployment, and user prompts often encode compositional traversal logic. Existing approaches relying on fixed ontologies and static cost mappings cannot accommodate such flexibility. While foundation models excel at language interpretation and open-vocabulary perception, no single model can simultaneously parse nuanced mission directives, locate arbitrary entities in large-scale imagery, and synthesize them into an executable cost function for planners. We therefore propose OVerSeeC, a zero-shot modular framework that decomposes the problem into Interpret-Locate-Synthesize: (i) an LLM extracts entities and ranked preferences, (ii) an open-vocabulary segmentation pipeline identifies these entities from high-resolution imagery, and (iii) the LLM uses the user's natural language preferences and masks to synthesize executable costmap code. Empirically, OVerSeeC handles novel entities, respects ranked and compositional preferences, and produces routes consistent with human-drawn trajectories across diverse regions, demonstrating robustness to distribution shifts. This shows that modular composition of foundation models enables open-vocabulary, preference-aligned costmap generation for scalable, mission-adaptive global planning.", "AI": {"tldr": "OVerSeeC\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u89e3\u91ca-\u5b9a\u4f4d-\u5408\u6210\u4e09\u4e2a\u6b65\u9aa4\uff0c\u76f4\u63a5\u4ece\u536b\u661f\u56fe\u50cf\u751f\u6210\u7528\u4e8e\u957f\u8ddd\u79bb\u89c4\u5212\u7684\u5168\u5c40\u6210\u672c\u56fe\uff0c\u80fd\u591f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u5b9e\u4f53\u548c\u4efb\u52a1\u7279\u5b9a\u7a7f\u8d8a\u89c4\u5219\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u672c\u4f53\u548c\u9759\u6001\u6210\u672c\u6620\u5c04\uff0c\u65e0\u6cd5\u9002\u5e94\u4efb\u52a1\u9700\u6c42\u53d8\u5316\u3001\u90e8\u7f72\u65f6\u672a\u77e5\u5730\u5f62\u5b9e\u4f53\u4ee5\u53ca\u7528\u6237\u63d0\u793a\u4e2d\u7f16\u7801\u7684\u7ec4\u5408\u7a7f\u8d8a\u903b\u8f91\u3002\u867d\u7136\u57fa\u7840\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u548c\u5f00\u653e\u8bcd\u6c47\u611f\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u80fd\u540c\u65f6\u89e3\u6790\u7ec6\u5fae\u7684\u4efb\u52a1\u6307\u4ee4\u3001\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u4efb\u610f\u5b9e\u4f53\uff0c\u5e76\u5c06\u5b83\u4eec\u5408\u6210\u4e3a\u89c4\u5212\u5668\u53ef\u6267\u884c\u7684\u6210\u672c\u51fd\u6570\u3002", "method": "\u63d0\u51faOVerSeeC\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u6a21\u5757\uff1a1) LLM\u63d0\u53d6\u5b9e\u4f53\u548c\u6392\u5e8f\u504f\u597d\uff1b2) \u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7ba1\u9053\u4ece\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u8bc6\u522b\u8fd9\u4e9b\u5b9e\u4f53\uff1b3) LLM\u4f7f\u7528\u7528\u6237\u7684\u81ea\u7136\u8bed\u8a00\u504f\u597d\u548c\u63a9\u7801\u5408\u6210\u53ef\u6267\u884c\u7684\u6210\u672c\u56fe\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOVerSeeC\u80fd\u591f\u5904\u7406\u65b0\u9896\u5b9e\u4f53\u3001\u5c0a\u91cd\u6392\u5e8f\u548c\u7ec4\u5408\u504f\u597d\uff0c\u5e76\u5728\u4e0d\u540c\u533a\u57df\u4ea7\u751f\u4e0e\u4eba\u5de5\u7ed8\u5236\u8f68\u8ff9\u4e00\u81f4\u7684\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7684\u6a21\u5757\u5316\u7ec4\u5408\u80fd\u591f\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u3001\u504f\u597d\u5bf9\u9f50\u7684\u6210\u672c\u56fe\u751f\u6210\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u5168\u5c40\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18622", "abs": "https://arxiv.org/abs/2602.18622", "authors": ["Antonio Lopez", "Jack Muirhead", "Carlo Pinciroli"], "title": "FORMICA: Decision-Focused Learning for Communication-Free Multi-Robot Task Allocation", "comment": "13 pages, 2 figures, ANTS 2026", "summary": "Most multi-robot task allocation methods rely on communication to resolve conflicts and reach consistent assignments. In environments with limited bandwidth, degraded infrastructure, or adversarial interference, existing approaches degrade sharply. We introduce a learning-based framework that achieves high-quality task allocation without any robot-to-robot communication. The key idea is that robots coordinate implicitly by predicting teammates' bids: if each robot can anticipate competition for a task, it can adjust its choices accordingly. Our method predicts bid distributions to correct systematic errors in analytical mean-field approximations. While analytical predictions assume idealized conditions (uniform distributions, known bid functions), our learned approach adapts to task clustering and spatial heterogeneity. Inspired by Smart Predict-then-Optimize (SPO), we train predictors end-to-end to minimize Task Allocation Regret rather than prediction error. To scale to large swarms, we develop a mean-field approximation where each robot predicts the distribution of competing bids rather than individual bids, reducing complexity from $O(NT)$ to $O(T)$. We call our approach FORMICA: Field-Oriented Regret-Minimizing Implicit Coordination Algorithm. Experiments show FORMICA substantially outperforms a natural analytical baseline. In scenarios with 16 robots and 64 tasks, our approach improves system reward by 17% and approaches the optimal MILP solution. When deployed on larger scenarios (256 robots, 4096 tasks), the same model improves performance by 7%, demonstrating strong generalization. Training requires only 21 seconds on a laptop, enabling rapid adaptation to new environments.", "AI": {"tldr": "FORMICA\uff1a\u4e00\u79cd\u65e0\u9700\u673a\u5668\u4eba\u95f4\u901a\u4fe1\u7684\u5b66\u4e60\u578b\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u961f\u53cb\u51fa\u4ef7\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\uff0c\u5728\u6709\u9650\u5e26\u5bbd\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\u4f9d\u8d56\u901a\u4fe1\u89e3\u51b3\u51b2\u7a81\uff0c\u4f46\u5728\u5e26\u5bbd\u53d7\u9650\u3001\u57fa\u7840\u8bbe\u65bd\u9000\u5316\u6216\u5bf9\u6297\u5e72\u6270\u73af\u5883\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9700\u8981\u65e0\u9700\u901a\u4fe1\u7684\u534f\u8c03\u65b9\u6848", "method": "\u63d0\u51faFORMICA\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u961f\u53cb\u51fa\u4ef7\u5206\u5e03\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u6700\u5c0f\u5316\u4efb\u52a1\u5206\u914d\u9057\u61be\u800c\u975e\u9884\u6d4b\u8bef\u5dee\uff0c\u4f7f\u7528\u5747\u503c\u573a\u8fd1\u4f3c\u5c06\u590d\u6742\u5ea6\u4eceO(NT)\u964d\u81f3O(T)", "result": "\u572816\u673a\u5668\u4eba64\u4efb\u52a1\u573a\u666f\u4e2d\u63d0\u5347\u7cfb\u7edf\u5956\u52b117%\uff0c\u63a5\u8fd1\u6700\u4f18MILP\u89e3\uff1b\u5728256\u673a\u5668\u4eba4096\u4efb\u52a1\u573a\u666f\u4e2d\u63d0\u53477%\uff0c\u8bad\u7ec3\u4ec5\u970021\u79d2\uff0c\u5c55\u73b0\u826f\u597d\u6cdb\u5316\u80fd\u529b", "conclusion": "FORMICA\u5728\u65e0\u9700\u901a\u4fe1\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4efb\u52a1\u5206\u914d\uff0c\u901a\u8fc7\u9884\u6d4b\u51fa\u4ef7\u5206\u5e03\u7ea0\u6b63\u5206\u6790\u5747\u503c\u573a\u8fd1\u4f3c\u7684\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u9002\u5e94\u4efb\u52a1\u805a\u7c7b\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2602.18638", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18638", "abs": "https://arxiv.org/abs/2602.18638", "authors": ["Jaeeun Kim", "Junhee Lim", "Yu She"], "title": "Soft Surfaced Vision-Based Tactile Sensing for Bipedal Robot Applications", "comment": "8 pages, 11 figures, RoboSoft 2026. For the supplementary video, please visit: https://youtu.be/ceJiy9q_2Aw", "summary": "Legged locomotion benefits from embodied sensing, where perception emerges from the physical interaction between body and environment. We present a soft-surfaced, vision-based tactile foot sensor that endows a bipedal robot with a skin-like deformable layer that captures contact deformations optically, turning foot-ground interactions into rich haptic signals. From a contact image stream, our method estimates contact pose (position and orientation), visualizes shear, computes center of pressure (CoP), classifies terrain, and detects geometric features of the contact patch. We validate these capabilities on a tilting platform and in visually obscured conditions, showing that foot-borne tactile feedback improves balance control and terrain awareness beyond proprioception alone. These findings suggest that integrating tactile perception into legged robot feet improves stability, adaptability, and environmental awareness, offering a promising direction toward more compliant and intelligent locomotion systems. For the supplementary video, please visit: https://youtu.be/ceJiy9q_2Aw", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u7684\u8f6f\u8868\u9762\u89c6\u89c9\u89e6\u89c9\u811a\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u5149\u5b66\u6355\u6349\u63a5\u89e6\u53d8\u5f62\uff0c\u5c06\u8db3\u5730\u4ea4\u4e92\u8f6c\u5316\u4e3a\u4e30\u5bcc\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u5347\u673a\u5668\u4eba\u7684\u5e73\u8861\u63a7\u5236\u548c\u5730\u5f62\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u817f\u5f0f\u8fd0\u52a8\u53d7\u76ca\u4e8e\u5177\u8eab\u611f\u77e5\uff0c\u5176\u4e2d\u611f\u77e5\u6e90\u4e8e\u8eab\u4f53\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u7269\u7406\u4ea4\u4e92\u3002\u73b0\u6709\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\uff0c\u7f3a\u4e4f\u5bf9\u8db3\u5730\u63a5\u89e6\u7684\u4e30\u5bcc\u89e6\u89c9\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5e73\u8861\u63a7\u5236\u548c\u5730\u5f62\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u8f6f\u8868\u9762\u89c6\u89c9\u89e6\u89c9\u811a\u4f20\u611f\u5668\uff0c\u5728\u53cc\u8db3\u673a\u5668\u4eba\u811a\u90e8\u5b89\u88c5\u76ae\u80a4\u72b6\u53ef\u53d8\u5f62\u5c42\uff0c\u901a\u8fc7\u5149\u5b66\u6355\u6349\u63a5\u89e6\u53d8\u5f62\u3002\u4ece\u63a5\u89e6\u56fe\u50cf\u6d41\u4e2d\u4f30\u8ba1\u63a5\u89e6\u59ff\u6001\uff08\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\u3001\u53ef\u89c6\u5316\u526a\u5207\u529b\u3001\u8ba1\u7b97\u538b\u529b\u4e2d\u5fc3\u3001\u5206\u7c7b\u5730\u5f62\u5e76\u68c0\u6d4b\u63a5\u89e6\u6591\u5757\u7684\u51e0\u4f55\u7279\u5f81\u3002", "result": "\u5728\u503e\u659c\u5e73\u53f0\u548c\u89c6\u89c9\u906e\u6321\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u4f20\u611f\u5668\u80fd\u529b\uff0c\u663e\u793a\u8db3\u90e8\u89e6\u89c9\u53cd\u9988\u76f8\u6bd4\u4ec5\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u80fd\u663e\u8457\u6539\u5584\u5e73\u8861\u63a7\u5236\u548c\u5730\u5f62\u611f\u77e5\u3002\u89e6\u89c9\u611f\u77e5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u6027\u3001\u9002\u5e94\u6027\u548c\u73af\u5883\u610f\u8bc6\u3002", "conclusion": "\u5c06\u89e6\u89c9\u611f\u77e5\u96c6\u6210\u5230\u817f\u5f0f\u673a\u5668\u4eba\u811a\u90e8\u53ef\u4ee5\u6539\u5584\u7a33\u5b9a\u6027\u3001\u9002\u5e94\u6027\u548c\u73af\u5883\u610f\u8bc6\uff0c\u4e3a\u5b9e\u73b0\u66f4\u67d4\u987a\u548c\u667a\u80fd\u7684\u8fd0\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.18661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18661", "abs": "https://arxiv.org/abs/2602.18661", "authors": ["Saitarun Nadipineni", "Keshav Pandiyan", "Kaspar Althoefer", "Shinichi Hirai", "Thilina Dulantha Lalitharatne"], "title": "Robotic Fruits with Tunable Stiffness and Sensing: Towards a Methodology for Developing Realistic Physical Twins of Fruits", "comment": "6 pages, 5 figures, 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft) 2026", "summary": "The global agri-food sector faces increasing challenges from labour shortages, high consumer demand, and supply-chain disruptions, resulting in substantial losses of unharvested produce. Robotic harvesting has emerged as a promising alternative; however, evaluating and training soft grippers for delicate fruits remains difficult due to the highly variable mechanical properties of natural produce. This makes it difficult to establish reliable benchmarks or data-driven control strategies. Existing testing practices rely on large quantities of real fruit to capture this variability, leading to inefficiency, higher costs, and waste. The methodology presented in this work aims to address these limitations by developing tunable soft physical twins that emulate the stiffness characteristics of real fruits at different ripeness levels. A fiber-reinforced pneumatic physical twin of a kiwi fruit was designed and fabricated to replicate the stiffness at different ripeness levels. Experimental results show that the stiffness of the physical twin can be tuned accurately over multiple trials (97.35 - 99.43% accuracy). Gripping tasks with a commercial robotic gripper showed that sensor feedback from the physical twin can reflect the applied gripping forces. Finally, a stress test was performed over 50 cycles showed reliable maintenance of desired stiffness (0.56 - 1.10% error). This work shows promise that robotic physical twins could adjust their stiffness to resemble that of real fruits. This can provide a sustainable, controllable platform for benchmarking and training robotic grippers.", "AI": {"tldr": "\u5f00\u53d1\u53ef\u8c03\u8c10\u7684\u8f6f\u7269\u7406\u5b6a\u751f\u4f53\u6765\u6a21\u62df\u4e0d\u540c\u6210\u719f\u5ea6\u6c34\u679c\u7684\u521a\u5ea6\u7279\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u6293\u53d6\u5668\u63d0\u4f9b\u53ef\u6301\u7eed\u3001\u53ef\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0", "motivation": "\u519c\u4e1a\u98df\u54c1\u884c\u4e1a\u9762\u4e34\u52b3\u52a8\u529b\u77ed\u7f3a\u3001\u4f9b\u5e94\u94fe\u4e2d\u65ad\u7b49\u95ee\u9898\uff0c\u673a\u5668\u4eba\u91c7\u6458\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5929\u7136\u6c34\u679c\u673a\u68b0\u7279\u6027\u9ad8\u5ea6\u53ef\u53d8\uff0c\u8bc4\u4f30\u548c\u8bad\u7ec3\u8f6f\u6293\u53d6\u5668\u5bf9\u6613\u635f\u6c34\u679c\u7684\u6293\u53d6\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u771f\u5b9e\u6c34\u679c\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u6210\u672c\u9ad8\u6602\u548c\u6d6a\u8d39\u3002", "method": "\u5f00\u53d1\u53ef\u8c03\u8c10\u7684\u8f6f\u7269\u7406\u5b6a\u751f\u4f53\uff0c\u6a21\u62df\u771f\u5b9e\u6c34\u679c\u5728\u4e0d\u540c\u6210\u719f\u5ea6\u4e0b\u7684\u521a\u5ea6\u7279\u6027\u3002\u8bbe\u8ba1\u5e76\u5236\u9020\u4e86\u7315\u7334\u6843\u7684\u7ea4\u7ef4\u589e\u5f3a\u6c14\u52a8\u7269\u7406\u5b6a\u751f\u4f53\uff0c\u80fd\u591f\u590d\u5236\u4e0d\u540c\u6210\u719f\u5ea6\u6c34\u5e73\u7684\u521a\u5ea6\u3002", "result": "\u7269\u7406\u5b6a\u751f\u4f53\u7684\u521a\u5ea6\u53ef\u4ee5\u5728\u591a\u6b21\u8bd5\u9a8c\u4e2d\u7cbe\u786e\u8c03\u8c10\uff0897.35-99.43%\u51c6\u786e\u5ea6\uff09\u3002\u4f7f\u7528\u5546\u4e1a\u673a\u5668\u4eba\u6293\u53d6\u5668\u8fdb\u884c\u7684\u6293\u53d6\u4efb\u52a1\u663e\u793a\uff0c\u7269\u7406\u5b6a\u751f\u4f53\u7684\u4f20\u611f\u5668\u53cd\u9988\u80fd\u591f\u53cd\u6620\u65bd\u52a0\u7684\u6293\u53d6\u529b\u3002\u7ecf\u8fc750\u6b21\u5faa\u73af\u7684\u5e94\u529b\u6d4b\u8bd5\u663e\u793a\uff0c\u6240\u9700\u521a\u5ea6\u80fd\u591f\u53ef\u9760\u7ef4\u6301\uff080.56-1.10%\u8bef\u5dee\uff09\u3002", "conclusion": "\u673a\u5668\u4eba\u7269\u7406\u5b6a\u751f\u4f53\u80fd\u591f\u8c03\u6574\u5176\u521a\u5ea6\u4ee5\u6a21\u62df\u771f\u5b9e\u6c34\u679c\uff0c\u4e3a\u673a\u5668\u4eba\u6293\u53d6\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u3001\u53ef\u63a7\u7684\u5e73\u53f0\uff0c\u6709\u671b\u89e3\u51b3\u519c\u4e1a\u673a\u5668\u4eba\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.18663", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18663", "abs": "https://arxiv.org/abs/2602.18663", "authors": ["Harry Robertshaw", "Nikola Fischer", "Lennart Karstensen", "Benjamin Jackson", "Xingyu Chen", "S. M. Hadi Sadati", "Christos Bergeles", "Alejandro Granados", "Thomas C Booth"], "title": "Toward AI Autonomous Navigation for Mechanical Thrombectomy using Hierarchical Modular Multi-agent Reinforcement Learning (HM-MARL)", "comment": "Published in IEEE Robotics and Automation Letters", "summary": "Mechanical thrombectomy (MT) is typically the optimal treatment for acute ischemic stroke involving large vessel occlusions, but access is limited due to geographic and logistical barriers. Reinforcement learning (RL) shows promise in autonomous endovascular navigation, but generalization across 'long' navigation tasks remains challenging. We propose a Hierarchical Modular Multi-Agent Reinforcement Learning (HM-MARL) framework for autonomous two-device navigation in vitro, enabling efficient and generalizable navigation. HM-MARL was developed to autonomously navigate a guide catheter and guidewire from the femoral artery to the internal carotid artery (ICA). A modular multi-agent approach was used to decompose the complex navigation task into specialized subtasks, each trained using Soft Actor-Critic RL. The framework was validated in both in silico and in vitro testbeds to assess generalization and real-world feasibility. In silico, a single-vasculature model achieved 92-100% success rates on individual anatomies, while a multi-vasculature model achieved 56-80% across multiple patient anatomies. In vitro, both HM-MARL models successfully navigated 100% of trials from the femoral artery to the right common carotid artery and 80% to the right ICA but failed on the left-side vessel superhuman challenge due to the anatomy and catheter type used in navigation. This study presents the first demonstration of in vitro autonomous navigation in MT vasculature. While HM-MARL enables generalization across anatomies, the simulation-to-real transition introduces challenges. Future work will refine RL strategies using world models and validate performance on unseen in vitro data, advancing autonomous MT towards clinical translation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u53cc\u8bbe\u5907\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u5728\u4f53\u5916\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u4ece\u80a1\u52a8\u8109\u5230\u9888\u5185\u52a8\u8109\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u5c55\u793a\u4e86\u5728\u673a\u68b0\u53d6\u6813\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u673a\u68b0\u53d6\u6813\u662f\u6cbb\u7597\u5927\u8840\u7ba1\u95ed\u585e\u6027\u6025\u6027\u7f3a\u8840\u6027\u8111\u5352\u4e2d\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u4f46\u53d7\u5730\u7406\u548c\u540e\u52e4\u969c\u788d\u9650\u5236\uff0c\u53ef\u53ca\u6027\u6709\u9650\u3002\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\"\u957f\"\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u53cc\u8bbe\u5907\u5bfc\u822a\u4efb\u52a1\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u4f7f\u7528Soft Actor-Critic\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002\u6846\u67b6\u5728\u4f53\u5916\u548c\u8ba1\u7b97\u673a\u6a21\u62df\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8ba1\u7b97\u673a\u6a21\u62df\u4e2d\uff0c\u5355\u4e00\u8840\u7ba1\u6a21\u578b\u5728\u4e2a\u4f53\u89e3\u5256\u7ed3\u6784\u4e0a\u8fbe\u523092-100%\u6210\u529f\u7387\uff0c\u591a\u8840\u7ba1\u6a21\u578b\u5728\u591a\u4e2a\u60a3\u8005\u89e3\u5256\u7ed3\u6784\u4e0a\u8fbe\u523056-80%\u6210\u529f\u7387\u3002\u4f53\u5916\u5b9e\u9a8c\u4e2d\uff0c\u4e24\u79cdHM-MARL\u6a21\u578b\u4ece\u80a1\u52a8\u8109\u5230\u53f3\u9888\u603b\u52a8\u8109\u7684\u6210\u529f\u7387\u4e3a100%\uff0c\u5230\u53f3\u9888\u5185\u52a8\u8109\u4e3a80%\uff0c\u4f46\u5728\u5de6\u4fa7\u8840\u7ba1\u7684\u8d85\u4eba\u6311\u6218\u4e2d\u5931\u8d25\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86\u673a\u68b0\u53d6\u6813\u8840\u7ba1\u5185\u4f53\u5916\u81ea\u4e3b\u5bfc\u822a\u3002\u867d\u7136HM-MARL\u80fd\u591f\u5b9e\u73b0\u8de8\u89e3\u5256\u7ed3\u6784\u7684\u6cdb\u5316\uff0c\u4f46\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\u5e26\u6765\u4e86\u6311\u6218\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u4f53\u5916\u6570\u636e\u4e0a\u9a8c\u8bc1\u6027\u80fd\uff0c\u63a8\u52a8\u81ea\u4e3b\u673a\u68b0\u53d6\u6813\u5411\u4e34\u5e8a\u8f6c\u5316\u3002"}}
{"id": "2602.18688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18688", "abs": "https://arxiv.org/abs/2602.18688", "authors": ["Shipeng Liu", "J. Diego Caporale", "Yifeng Zhang", "Xingjue Liao", "William Hoganson", "Wilson Hu", "Shivangi Misra", "Neha Peddinti", "Rachel Holladay", "Ethan Fulcher", "Akshay Ram Panyam", "Andrik Puentes", "Jordan M. Bretzfelder", "Michael Zanetti", "Uland Wong", "Daniel E. Koditschek", "Mark Yim", "Douglas Jerolmack", "Cynthia Sung", "Feifei Qian"], "title": "Scout-Rover cooperation: online terrain strength mapping and traversal risk estimation for planetary-analog explorations", "comment": "8 figures", "summary": "Robot-aided exploration of planetary surfaces is essential for understanding geologic processes, yet many scientifically valuable regions, such as Martian dunes and lunar craters, remain hazardous due to loose, deformable regolith. We present a scout-rover cooperation framework that expands safe access to such terrain using a hybrid team of legged and wheeled robots. In our approach, a high-mobility legged robot serves as a mobile scout, using proprioceptive leg-terrain interactions to estimate regolith strength during locomotion and construct spatially resolved terrain maps. These maps are integrated with rover locomotion models to estimate traversal risk and inform path planning.\n  We validate the framework through analogue missions at the NASA Ames Lunar Simulant Testbed and the White Sands Dune Field. Experiments demonstrate (1) online terrain strength mapping from legged locomotion and (2) rover-specific traversal-risk estimation enabling safe navigation to scientific targets. Results show that scout-generated terrain maps reliably capture spatial variability and predict mobility failure modes, allowing risk-aware path planning that avoids hazardous regions. By combining embodied terrain sensing with heterogeneous rover cooperation, this framework enhances operational robustness and expands the reachable science workspace in deformable planetary environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u817f\u5f0f\u4fa6\u5bdf\u673a\u5668\u4eba\u4e0e\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u817f\u5f0f\u673a\u5668\u4eba\u7684\u672c\u4f53\u611f\u77e5\u817f-\u5730\u5f62\u4ea4\u4e92\u5728\u7ebf\u4f30\u8ba1\u677e\u8f6f\u571f\u58e4\u5f3a\u5ea6\uff0c\u6784\u5efa\u5730\u5f62\u56fe\u5e76\u8bc4\u4f30\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u901a\u884c\u98ce\u9669\uff0c\u5b9e\u73b0\u5371\u9669\u53d8\u5f62\u5730\u5f62\u4e0b\u7684\u5b89\u5168\u5bfc\u822a\u3002", "motivation": "\u884c\u661f\u8868\u9762\u79d1\u5b66\u63a2\u6d4b\u9700\u8981\u8fdb\u5165\u677e\u6563\u3001\u53ef\u53d8\u5f62\u8868\u571f\u533a\u57df\uff08\u5982\u706b\u661f\u6c99\u4e18\u3001\u6708\u7403\u9668\u77f3\u5751\uff09\uff0c\u4f46\u8fd9\u4e9b\u5730\u5f62\u5bf9\u4f20\u7edf\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u5b58\u5728\u9ad8\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5730\u5f62\u529b\u5b66\u7279\u6027\u7684\u5b9e\u65f6\u611f\u77e5\u80fd\u529b\uff0c\u9650\u5236\u4e86\u79d1\u5b66\u63a2\u6d4b\u8303\u56f4\u3002", "method": "\u91c7\u7528\u817f\u5f0f\u673a\u5668\u4eba\u4f5c\u4e3a\u79fb\u52a8\u4fa6\u5bdf\u5458\uff0c\u5229\u7528\u5176\u817f-\u5730\u5f62\u4ea4\u4e92\u7684\u672c\u4f53\u611f\u77e5\u6570\u636e\u5728\u7ebf\u4f30\u8ba1\u8868\u571f\u5f3a\u5ea6\uff1b\u6784\u5efa\u7a7a\u95f4\u5206\u8fa8\u7387\u5730\u5f62\u56fe\uff1b\u7ed3\u5408\u6f2b\u6e38\u8f66\u8fd0\u52a8\u6a21\u578b\u8bc4\u4f30\u901a\u884c\u98ce\u9669\uff1b\u8fdb\u884c\u98ce\u9669\u611f\u77e5\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5728NASA\u827e\u59c6\u65af\u6708\u7403\u6a21\u62df\u8bd5\u9a8c\u573a\u548c\u767d\u6c99\u6c99\u4e18\u573a\u7684\u6a21\u62df\u4efb\u52a1\u9a8c\u8bc1\u4e2d\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\uff1a(1) \u817f\u5f0f\u8fd0\u52a8\u5728\u7ebf\u5730\u5f62\u5f3a\u5ea6\u6d4b\u7ed8\uff1b(2) \u6f2b\u6e38\u8f66\u7279\u5b9a\u901a\u884c\u98ce\u9669\u8bc4\u4f30\uff1b(3) \u98ce\u9669\u611f\u77e5\u8def\u5f84\u89c4\u5212\u907f\u514d\u5371\u9669\u533a\u57df\u3002\u4fa6\u5bdf\u751f\u6210\u7684\u5730\u5f62\u56fe\u80fd\u53ef\u9760\u6355\u6349\u7a7a\u95f4\u53d8\u5f02\u5e76\u9884\u6d4b\u79fb\u52a8\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5177\u8eab\u5730\u5f62\u611f\u77e5\u4e0e\u5f02\u6784\u6f2b\u6e38\u8f66\u534f\u540c\u76f8\u7ed3\u5408\uff0c\u8be5\u6846\u67b6\u589e\u5f3a\u4e86\u64cd\u4f5c\u9c81\u68d2\u6027\uff0c\u6269\u5c55\u4e86\u53ef\u53d8\u5f62\u884c\u661f\u73af\u5883\u4e2d\u53ef\u5230\u8fbe\u7684\u79d1\u5b66\u5de5\u4f5c\u7a7a\u95f4\uff0c\u4e3a\u5371\u9669\u5730\u5f62\u63a2\u6d4b\u63d0\u4f9b\u4e86\u5b89\u5168\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18707", "abs": "https://arxiv.org/abs/2602.18707", "authors": ["Haotian He", "Ning Guo", "Siqi Shi", "Qipeng Liu", "Wenzhao Lian"], "title": "CLASH: Collision Learning via Augmented Sim-to-real Hybridization to Bridge the Reality Gap", "comment": null, "summary": "The sim-to-real gap, particularly in the inaccurate modeling of contact-rich dynamics like collisions, remains a primary obstacle to deploying robot policies trained in simulation. Conventional physics engines often trade accuracy for computational speed, leading to discrepancies that prevent direct policy transfer. To address this, we introduce Collision Learning via Augmented Sim-to-real Hybridization (CLASH), a data-efficient framework that creates a high-fidelity hybrid simulator by learning a surrogate collision model from a minimal set of real-world data. In CLASH, a base model is first distilled from an imperfect simulator (MuJoCo) to capture general physical priors; this model is then fine-tuned with a remarkably small number of real-world interactions (as few as 10 samples) to correct for the simulator's inherent inaccuracies. The resulting hybrid simulator not only achieves higher predictive accuracy but also reduces collision computation time by nearly 50\\%. We demonstrate that policies obtained with our hybrid simulator transfer more robustly to the real world, doubling the success rate in sequential pushing tasks with reinforecement learning and significantly increase the task performance with model-based control.", "AI": {"tldr": "CLASH\u6846\u67b6\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5b66\u4e60\u78b0\u649e\u6a21\u578b\uff0c\u521b\u5efa\u9ad8\u4fdd\u771f\u6df7\u5408\u4eff\u771f\u5668\uff0c\u663e\u8457\u7f29\u5c0fsim-to-real\u5dee\u8ddd\uff0c\u63d0\u5347\u7b56\u7565\u8fc1\u79fb\u6210\u529f\u7387", "motivation": "\u4f20\u7edf\u7269\u7406\u5f15\u64ce\u5728\u6a21\u62df\u63a5\u89e6\u4e30\u5bcc\u7684\u52a8\u529b\u5b66\uff08\u5982\u78b0\u649e\uff09\u65f6\uff0c\u5f80\u5f80\u4e3a\u4e86\u8ba1\u7b97\u901f\u5ea6\u800c\u727a\u7272\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u963b\u788d\u4e86\u4eff\u771f\u8bad\u7ec3\u7b56\u7565\u7684\u76f4\u63a5\u90e8\u7f72", "method": "\u63d0\u51faCLASH\u6846\u67b6\uff1a\u9996\u5148\u4ece\u4e0d\u5b8c\u7f8e\u7684\u4eff\u771f\u5668\uff08MuJoCo\uff09\u4e2d\u84b8\u998f\u51fa\u57fa\u7840\u6a21\u578b\u4ee5\u83b7\u53d6\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff0c\u7136\u540e\u7528\u6781\u5c11\u91cf\u7684\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\uff08\u4ec5\u970010\u4e2a\u6837\u672c\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u7ea0\u6b63\u4eff\u771f\u5668\u7684\u56fa\u6709\u8bef\u5dee\uff0c\u4ece\u800c\u521b\u5efa\u9ad8\u4fdd\u771f\u6df7\u5408\u4eff\u771f\u5668", "result": "\u6df7\u5408\u4eff\u771f\u5668\u4e0d\u4ec5\u9884\u6d4b\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u8fd8\u5c06\u78b0\u649e\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u8fd150%\uff1b\u4f7f\u7528\u8be5\u4eff\u771f\u5668\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fc1\u79fb\u66f4\u7a33\u5065\uff0c\u5728\u987a\u5e8f\u63a8\u52a8\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6210\u529f\u7387\u7ffb\u500d\uff0c\u57fa\u4e8e\u6a21\u578b\u63a7\u5236\u7684\u4efb\u52a1\u6027\u80fd\u4e5f\u663e\u8457\u63d0\u5347", "conclusion": "CLASH\u6846\u67b6\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u65b9\u5f0f\u5b66\u4e60\u78b0\u649e\u6a21\u578b\uff0c\u6709\u6548\u7f29\u5c0fsim-to-real\u5dee\u8ddd\uff0c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7a33\u5065\u8fc1\u79fb\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.18716", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18716", "abs": "https://arxiv.org/abs/2602.18716", "authors": ["Hoseong Jung", "Sungil Son", "Daesol Cho", "Jonghae Park", "Changhyun Choi", "H. Jin Kim"], "title": "Temporal Action Representation Learning for Tactical Resource Control and Subsequent Maneuver Generation", "comment": "ICRA 2026, 8 pages", "summary": "Autonomous robotic systems should reason about resource control and its impact on subsequent maneuvers, especially when operating with limited energy budgets or restricted sensing. Learning-based control is effective in handling complex dynamics and represents the problem as a hybrid action space unifying discrete resource usage and continuous maneuvers. However, prior works on hybrid action space have not sufficiently captured the causal dependencies between resource usage and maneuvers. They have also overlooked the multi-modal nature of tactical decisions, both of which are critical in fast-evolving scenarios. In this paper, we propose TART, a Temporal Action Representation learning framework for Tactical resource control and subsequent maneuver generation. TART leverages contrastive learning based on a mutual information objective, designed to capture inherent temporal dependencies in resource-maneuver interactions. These learned representations are quantized into discrete codebook entries that condition the policy, capturing recurring tactical patterns and enabling multi-modal and temporally coherent behaviors. We evaluate TART in two domains where resource deployment is critical: (i) a maze navigation task where a limited budget of discrete actions provides enhanced mobility, and (ii) a high-fidelity air combat simulator in which an F-16 agent operates weapons and defensive systems in coordination with flight maneuvers. Across both domains, TART consistently outperforms hybrid-action baselines, demonstrating its effectiveness in leveraging limited resources and producing context-aware subsequent maneuvers.", "AI": {"tldr": "TART\u662f\u4e00\u4e2a\u7528\u4e8e\u6218\u672f\u8d44\u6e90\u63a7\u5236\u548c\u540e\u7eed\u673a\u52a8\u751f\u6210\u7684\u65f6\u5e8f\u52a8\u4f5c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e92\u4fe1\u606f\u76ee\u6807\u6355\u6349\u8d44\u6e90-\u673a\u52a8\u4ea4\u4e92\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u8ff7\u5bab\u5bfc\u822a\u548c\u7a7a\u6218\u6a21\u62df\u4e2d\u4f18\u4e8e\u73b0\u6709\u6df7\u5408\u52a8\u4f5c\u57fa\u7ebf\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6709\u9650\u80fd\u91cf\u9884\u7b97\u6216\u53d7\u9650\u611f\u77e5\u6761\u4ef6\u4e0b\u9700\u8981\u63a8\u7406\u8d44\u6e90\u63a7\u5236\u53ca\u5176\u5bf9\u540e\u7eed\u673a\u52a8\u7684\u5f71\u54cd\u3002\u73b0\u6709\u7684\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u8d44\u6e90\u4f7f\u7528\u4e0e\u673a\u52a8\u4e4b\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e5f\u5ffd\u89c6\u4e86\u6218\u672f\u51b3\u7b56\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u800c\u8fd9\u4e9b\u5728\u5feb\u901f\u6f14\u53d8\u7684\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTART\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u4e92\u4fe1\u606f\u76ee\u6807\u7684\u5bf9\u6bd4\u5b66\u4e60\u6765\u6355\u6349\u8d44\u6e90-\u673a\u52a8\u4ea4\u4e92\u4e2d\u7684\u56fa\u6709\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\u3002\u5b66\u4e60\u5230\u7684\u8868\u793a\u88ab\u91cf\u5316\u4e3a\u79bb\u6563\u7801\u672c\u6761\u76ee\uff0c\u7528\u4e8e\u6761\u4ef6\u5316\u7b56\u7565\uff0c\u4ece\u800c\u6355\u6349\u91cd\u590d\u51fa\u73b0\u7684\u6218\u672f\u6a21\u5f0f\u5e76\u5b9e\u73b0\u591a\u6a21\u6001\u548c\u65f6\u5e8f\u4e00\u81f4\u7684\u884c\u4e3a\u3002", "result": "\u5728\u4e24\u4e2a\u5173\u952e\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a(1)\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\uff0c\u5176\u4e2d\u6709\u9650\u7684\u79bb\u6563\u52a8\u4f5c\u9884\u7b97\u63d0\u4f9b\u589e\u5f3a\u7684\u79fb\u52a8\u80fd\u529b\uff1b(2)\u9ad8\u4fdd\u771f\u7a7a\u6218\u6a21\u62df\u5668\uff0cF-16\u667a\u80fd\u4f53\u534f\u8c03\u4f7f\u7528\u6b66\u5668\u548c\u9632\u5fa1\u7cfb\u7edf\u4e0e\u98de\u884c\u673a\u52a8\u3002\u5728\u4e24\u4e2a\u9886\u57df\u4e2d\uff0cTART\u59cb\u7ec8\u4f18\u4e8e\u6df7\u5408\u52a8\u4f5c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TART\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u6709\u9650\u8d44\u6e90\u5e76\u4ea7\u751f\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u540e\u7eed\u673a\u52a8\uff0c\u5728\u9700\u8981\u6218\u672f\u8d44\u6e90\u63a7\u5236\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u6f14\u53d8\u7684\u52a8\u6001\u73af\u5883\u4e2d\u3002"}}
{"id": "2602.18742", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18742", "abs": "https://arxiv.org/abs/2602.18742", "authors": ["Seungku Kim", "Suhyeok Jang", "Byungjun Yoon", "Dongyoung Kim", "John Won", "Jinwoo Shin"], "title": "RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning", "comment": "20 pages; 6 figures; Project page is available at https://seungkukim.github.io/robocurate/", "summary": "Synthetic data generated by video generative models has shown promise for robot learning as a scalable pipeline, but it often suffers from inconsistent action quality due to imperfectly generated videos. Recently, vision-language models (VLMs) have been leveraged to validate video quality, but they have limitations in distinguishing physically accurate videos and, even then, cannot directly evaluate the generated actions themselves. To tackle this issue, we introduce RoboCurate, a novel synthetic robot data generation framework that evaluates and filters the quality of annotated actions by comparing them with simulation replay. Specifically, RoboCurate replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance. We observe RoboCurate's generated data yield substantial relative improvements in success rates compared to using real data only, achieving +70.1% on GR-1 Tabletop (300 demos), +16.1% on DexMimicGen in the pre-training setup, and +179.9% in the challenging real-world ALLEX humanoid dexterous manipulation setting.", "AI": {"tldr": "RoboCurate\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u91cd\u653e\u9a8c\u8bc1\u751f\u6210\u89c6\u9891\u4e2d\u7684\u52a8\u4f5c\u8d28\u91cf\uff0c\u5e76\u5229\u7528\u56fe\u50cf\u7f16\u8f91\u548c\u89c6\u9891\u8f6c\u6362\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5b58\u5728\u52a8\u4f5c\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9a8c\u8bc1\u89c6\u9891\u8d28\u91cf\u65f6\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u7269\u7406\u51c6\u786e\u6027\u548c\u52a8\u4f5c\u672c\u8eab\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faRoboCurate\u6846\u67b6\uff1a1\uff09\u5728\u6a21\u62df\u5668\u4e2d\u91cd\u653e\u9884\u6d4b\u52a8\u4f5c\uff0c\u901a\u8fc7\u6bd4\u8f83\u6a21\u62df\u8f68\u8ff9\u4e0e\u751f\u6210\u89c6\u9891\u7684\u8fd0\u52a8\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u52a8\u4f5c\u8d28\u91cf\uff1b2\uff09\u4f7f\u7528\u56fe\u50cf\u5230\u56fe\u50cf\u7f16\u8f91\u89e3\u9501\u8d85\u51fa\u53ef\u7528\u6570\u636e\u96c6\u7684\u89c2\u5bdf\u591a\u6837\u6027\uff1b3\uff09\u5e94\u7528\u52a8\u4f5c\u4fdd\u6301\u7684\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u6765\u589e\u5f3a\u5916\u89c2\u3002", "result": "\u76f8\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\uff0cRoboCurate\u751f\u6210\u7684\u6570\u636e\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1aGR-1 Tabletop\uff08300\u6f14\u793a\uff09+70.1%\uff0cDexMimicGen\u9884\u8bad\u7ec3\u8bbe\u7f6e+16.1%\uff0cALLEX\u4eba\u5f62\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c+179.9%\u3002", "conclusion": "RoboCurate\u901a\u8fc7\u6a21\u62df\u91cd\u653e\u9a8c\u8bc1\u52a8\u4f5c\u8d28\u91cf\u5e76\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5408\u6210\u673a\u5668\u4eba\u6570\u636e\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2602.18803", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18803", "abs": "https://arxiv.org/abs/2602.18803", "authors": ["Finn Lukas Busch", "Matti Vahs", "Quantao Yang", "Jes\u00fas Gerardo Ortega Peimbert", "Yixi Cai", "Jana Tumova", "Olov Andersson"], "title": "Learning to Localize Reference Trajectories in Image-Space for Visual Navigation", "comment": null, "summary": "We present LoTIS, a model for visual navigation that provides robot-agnostic image-space guidance by localizing a reference RGB trajectory in the robot's current view, without requiring camera calibration, poses, or robot-specific training. Instead of predicting actions tied to specific robots, we predict the image-space coordinates of the reference trajectory as they would appear in the robot's current view. This creates robot-agnostic visual guidance that easily integrates with local planning. Consequently, our model's predictions provide guidance zero-shot across diverse embodiments. By decoupling perception from action and learning to localize trajectory points rather than imitate behavioral priors, we enable a cross-trajectory training strategy for robustness to viewpoint and camera changes. We outperform state-of-the-art methods by 20-50 percentage points in success rate on conventional forward navigation, achieving 94-98% success rate across diverse sim and real environments. Furthermore, we achieve over 5x improvements on challenging tasks where baselines fail, such as backward traversal. The system is straightforward to use: we show how even a video from a phone camera directly enables different robots to navigate to any point on the trajectory. Videos, demo, and code are available at https://finnbusch.com/lotis.", "AI": {"tldr": "LoTIS\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u65e0\u5173\u7684\u89c6\u89c9\u5bfc\u822a\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u53c2\u8003RGB\u8f68\u8ff9\u5b9a\u4f4d\u5230\u673a\u5668\u4eba\u5f53\u524d\u89c6\u56fe\u4e2d\uff0c\u63d0\u4f9b\u56fe\u50cf\u7a7a\u95f4\u5f15\u5bfc\uff0c\u65e0\u9700\u76f8\u673a\u6807\u5b9a\u3001\u4f4d\u59ff\u6216\u673a\u5668\u4eba\u7279\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u4e0e\u7279\u5b9a\u673a\u5668\u4eba\u7ed1\u5b9a\uff0c\u9700\u8981\u76f8\u673a\u6807\u5b9a\u3001\u4f4d\u59ff\u4fe1\u606f\u6216\u673a\u5668\u4eba\u7279\u5b9a\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u90e8\u7f72\u4fbf\u5229\u6027\u3002", "method": "\u9884\u6d4b\u53c2\u8003\u8f68\u8ff9\u5728\u673a\u5668\u4eba\u5f53\u524d\u89c6\u56fe\u4e2d\u7684\u56fe\u50cf\u7a7a\u95f4\u5750\u6807\uff0c\u800c\u975e\u7279\u5b9a\u673a\u5668\u4eba\u7684\u52a8\u4f5c\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u52a8\u4f5c\u7684\u89e3\u8026\u3002\u91c7\u7528\u8de8\u8f68\u8ff9\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u5bf9\u89c6\u89d2\u548c\u76f8\u673a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4f20\u7edf\u524d\u5411\u5bfc\u822a\u4e2d\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u534720-50\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8fbe\u523094-98%\u7684\u6210\u529f\u7387\u3002\u5728\u53cd\u5411\u904d\u5386\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u5b9e\u73b05\u500d\u4ee5\u4e0a\u6539\u8fdb\u3002", "conclusion": "LoTIS\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u673a\u5668\u4eba\u65e0\u5173\u89c6\u89c9\u5bfc\u822a\u65b9\u6848\uff0c\u4ec5\u9700\u624b\u673a\u89c6\u9891\u5373\u53ef\u8ba9\u4e0d\u540c\u673a\u5668\u4eba\u6cbf\u8f68\u8ff9\u5bfc\u822a\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8de8\u673a\u5668\u4eba\u90e8\u7f72\u3002"}}
{"id": "2602.18813", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18813", "abs": "https://arxiv.org/abs/2602.18813", "authors": ["Tommoro Robotics", ":", "Jesoon Kang", "Taegeon Park", "Jisu An", "Soo Min Kimm", "Jaejoon Kim", "Jinu Pahk", "Byungju Kim", "Junseok Lee", "Namheon Baek", "Sungwan Ha", "Hojun Baek", "Eduardo Ayerve Cruz", "Wontae Kim", "Junghyeon Choi", "Yousuk Lee", "Joonmo Han", "Sunghyun Cho", "Sunghyun Kwon", "Soyoung Lee", "Jun Ki Lee", "Seung-Joon Yi", "Byoung-Tak Zhang", "Theo Taeyeong Kim"], "title": "Habilis-$\u03b2$: A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model", "comment": null, "summary": "We introduce Habilis-$\u03b2$, a fast-motion and long-lasting on-device vision-language-action (VLA) model designed for real-world deployment. Current VLA evaluation remains largely confined to single-trial success rates under curated resets, which fails to capture the fast-motion and long-lasting capabilities essential for practical operation. To address this, we introduce the Productivity-Reliability Plane (PRP), which evaluates performance through Tasks per Hour (TPH) and Mean Time Between Intervention (MTBI) under a continuous-run protocol that demands both high-speed execution and sustained robustness. Habilis-$\u03b2$ achieves high performance by integrating language-free pre-training on large-scale play data for robust interaction priors with post-training on cyclic task demonstrations that capture state drift across consecutive task iterations. The system further employs ESPADA for phase-adaptive motion shaping to accelerate free-space transit, utilizes rectified-flow distillation to enable high-frequency control on edge devices, and incorporates classifier-free guidance (CFG) as a deployment-time knob to dynamically balance instruction adherence and learned interaction priors. In 1-hour continuous-run evaluations, Habilis-$\u03b2$ achieves strong performance under the PRP metrics, compared to $\u03c0_{0.5}$ in both simulation and real-world environments. In simulation, Habilis-$\u03b2$ achieves 572.6 TPH and 39.2 s MTBI (vs. 120.5 TPH and 30.5 s for $\u03c0_{0.5}$), while in a real-world humanoid logistics workflow it achieves 124 TPH and 137.4 s MTBI (vs. 19 TPH and 46.1 s for $\u03c0_{0.5}$). Finally, Habilis-$\u03b2$ achieves the highest reported performance on the standard RoboTwin 2.0 leaderboard across representative tasks, validating its effectiveness in complex manipulation scenarios.", "AI": {"tldr": "Habilis-\u03b2\u662f\u4e00\u4e2a\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u5feb\u901f\u8fd0\u52a8\u3001\u957f\u65f6\u8fd0\u884c\u7684\u8bbe\u5907\u7aef\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u4ea7\u529b-\u53ef\u9760\u6027\u5e73\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u8fde\u7eed\u8fd0\u884c\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524dVLA\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6b21\u8bd5\u9a8c\u6210\u529f\u7387\uff0c\u65e0\u6cd5\u6355\u6349\u5b9e\u9645\u5e94\u7528\u4e2d\u6240\u9700\u7684\u5feb\u901f\u8fd0\u52a8\u548c\u957f\u65f6\u8fd0\u884c\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u53cd\u6620\u771f\u5b9e\u90e8\u7f72\u9700\u6c42\u3002", "method": "\u6574\u5408\u65e0\u8bed\u8a00\u9884\u8bad\u7ec3\u83b7\u53d6\u4ea4\u4e92\u5148\u9a8c\uff0c\u540e\u8bad\u7ec3\u6355\u6349\u72b6\u6001\u6f02\u79fb\uff1b\u91c7\u7528ESPADA\u8fdb\u884c\u76f8\u4f4d\u81ea\u9002\u5e94\u8fd0\u52a8\u6574\u5f62\u52a0\u901f\u81ea\u7531\u7a7a\u95f4\u79fb\u52a8\uff0c\u4f7f\u7528\u6574\u6d41\u6d41\u84b8\u998f\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u9ad8\u9891\u63a7\u5236\uff0c\u90e8\u7f72\u65f6\u901a\u8fc7CFG\u52a8\u6001\u5e73\u8861\u6307\u4ee4\u9075\u5faa\u548c\u4ea4\u4e92\u5148\u9a8c\u3002", "result": "\u57281\u5c0f\u65f6\u8fde\u7eed\u8fd0\u884c\u8bc4\u4f30\u4e2d\uff0cHabilis-\u03b2\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fbe\u5230572.6 TPH\u548c39.2\u79d2MTBI\uff0c\u771f\u5b9e\u4e16\u754c\u7269\u6d41\u5de5\u4f5c\u6d41\u4e2d\u8fbe\u5230124 TPH\u548c137.4\u79d2MTBI\uff0c\u663e\u8457\u4f18\u4e8e\u03c00.5\u57fa\u7ebf\uff0c\u5e76\u5728RoboTwin 2.0\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u6700\u9ad8\u6027\u80fd\u3002", "conclusion": "Habilis-\u03b2\u901a\u8fc7\u521b\u65b0\u7684\u8bc4\u4f30\u6846\u67b6PRP\u548c\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5728\u5feb\u901f\u8fd0\u52a8\u548c\u957f\u65f6\u8fd0\u884c\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u771f\u5b9e\u4e16\u754cVLA\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18814", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18814", "abs": "https://arxiv.org/abs/2602.18814", "authors": ["Nicola Cigarini", "Giulia Michieletto", "Angelo Cenedese"], "title": "RotorSuite: A MATLAB/Simulink Toolbox for Tilt Multi-Rotor UAV Modeling", "comment": null, "summary": "In recent years, aerial platforms have evolved from passive flying sensors into versatile, contact-aware robotic systems, leading to rapid advances in platform design. Standard coplanar and collinear quadrotors have been complemented by modern tilted and tilting multi-rotor platforms with enhanced maneuverability. To properly analyze, control, and validate the performance of these emerging platforms, an accurate modeling step is required; however, this can be time-consuming, user-dependent and error-prone. To address this issue, we propose a MATLAB/Simulink toolbox for modeling and simulating the dynamics of a broad class of multi-rotor platforms through both an analytical and physics-based approaches. The toolbox, named RotorSuite, is provided with comprehensive documentation and example use cases, representing a valuable tool for didactic, research, and industrial development purposes.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aRotorSuite\u7684MATLAB/Simulink\u5de5\u5177\u7bb1\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u6a21\u62df\u5404\u7c7b\u591a\u65cb\u7ffc\u5e73\u53f0\u7684\u52a8\u529b\u5b66\u7279\u6027", "motivation": "\u968f\u7740\u7a7a\u4e2d\u5e73\u53f0\u4ece\u88ab\u52a8\u98de\u884c\u4f20\u611f\u5668\u53d1\u5c55\u4e3a\u63a5\u89e6\u611f\u77e5\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5e73\u53f0\u8bbe\u8ba1\u5feb\u901f\u8fdb\u6b65\uff0c\u4f46\u51c6\u786e\u5efa\u6a21\u8fd9\u4e9b\u65b0\u5174\u5e73\u53f0\u7684\u8fc7\u7a0b\u8017\u65f6\u3001\u4f9d\u8d56\u7528\u6237\u4e14\u5bb9\u6613\u51fa\u9519", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aMATLAB/Simulink\u5de5\u5177\u7bb1\uff0c\u901a\u8fc7\u89e3\u6790\u548c\u57fa\u4e8e\u7269\u7406\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u6a21\u62df\u5404\u7c7b\u591a\u65cb\u7ffc\u5e73\u53f0\u7684\u52a8\u529b\u5b66\u7279\u6027", "result": "\u5f00\u53d1\u4e86\u540d\u4e3aRotorSuite\u7684\u5de5\u5177\u7bb1\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u6587\u6863\u548c\u793a\u4f8b\u7528\u4f8b\uff0c\u9002\u7528\u4e8e\u6559\u5b66\u3001\u7814\u7a76\u548c\u5de5\u4e1a\u5f00\u53d1", "conclusion": "RotorSuite\u5de5\u5177\u7bb1\u4e3a\u591a\u65cb\u7ffc\u5e73\u53f0\u7684\u5efa\u6a21\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u7b80\u5316\u4e86\u5206\u6790\u3001\u63a7\u5236\u548c\u9a8c\u8bc1\u8fc7\u7a0b"}}
{"id": "2602.18835", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18835", "abs": "https://arxiv.org/abs/2602.18835", "authors": ["Moniesha Thilakarathna", "Xing Wang", "Min Wang", "David Hinwood", "Shuangzhe Liu", "Damith Herath"], "title": "GRAB: A Systematic Real-World Grasping Benchmark for Robotic Food Waste Sorting", "comment": "23 pages, 12 Figures, 3 Tables, submitted to Advanced Intelligent Systems Journal and under review", "summary": "Food waste management is critical for sustainability, yet inorganic contaminants hinder recycling potential. Robotic automation presents a compelling approach to this challenge by accelerating the sorting process through automated contaminant removal. Still, the diverse and unpredictable nature of contaminants creates major challenges for robotic grasping. Benchmarking frameworks are critical for evaluating challenges from various perspectives. However, existing protocols rely on limited simulation datasets, prioritise simple metrics such as success rate, and overlook key object and environment-related pre-grasp conditions. This paper introduces GRAB, a comprehensive Grasping Real-World Article Benchmarking framework that addresses this gap by integrating diverse deformable objects, advanced grasp-pose-estimation vision, and, importantly, pre-grasp conditions, establishing a set of critical graspability metrics. It systematically compares industrial grasping modalities through an in-depth experimental evaluation involving 1,750 food contaminant grasp attempts across four high-fidelity scenes. This large-scale evaluation provides an extensive assessment of grasp performance for food waste sorting, offering a level of depth that has rarely been explored in previous studies. The results reveal distinct gripper strengths and limitations, with object quality emerging as the dominant performance factor in cluttered environments, while vision quality and clutter levels play moderate roles. These findings highlight essential design considerations and reinforce the necessity of developing multimodal gripper technologies capable of robust cross-category performance for effective robotic food waste sorting.", "AI": {"tldr": "GRAB\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6837\u5316\u53d8\u5f62\u7269\u4f53\u3001\u5148\u8fdb\u89c6\u89c9\u6293\u53d6\u59ff\u6001\u4f30\u8ba1\u548c\u6293\u53d6\u524d\u6761\u4ef6\uff0c\u4e3a\u98df\u54c1\u5e9f\u7269\u5206\u7c7b\u4e2d\u7684\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "motivation": "\u98df\u54c1\u5e9f\u7269\u7ba1\u7406\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e0\u673a\u6c61\u67d3\u7269\u963b\u788d\u4e86\u56de\u6536\u6f5c\u529b\u3002\u673a\u5668\u4eba\u81ea\u52a8\u5316\u901a\u8fc7\u81ea\u52a8\u6c61\u67d3\u7269\u53bb\u9664\u52a0\u901f\u5206\u7c7b\u8fc7\u7a0b\uff0c\u4f46\u6c61\u67d3\u7269\u7684\u591a\u6837\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u7ed9\u673a\u5668\u4eba\u6293\u53d6\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u4f9d\u8d56\u6709\u9650\u7684\u6a21\u62df\u6570\u636e\u96c6\uff0c\u4f18\u5148\u8003\u8651\u7b80\u5355\u7684\u6210\u529f\u7387\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u5173\u952e\u7684\u7269\u4f53\u548c\u73af\u5883\u76f8\u5173\u7684\u6293\u53d6\u524d\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86GRAB\uff08Grasping Real-World Article Benchmarking\uff09\u6846\u67b6\uff0c\u6574\u5408\u4e86\u591a\u6837\u5316\u53d8\u5f62\u7269\u4f53\u3001\u5148\u8fdb\u7684\u6293\u53d6\u59ff\u6001\u4f30\u8ba1\u89c6\u89c9\u7cfb\u7edf\uff0c\u4ee5\u53ca\u5173\u952e\u7684\u6293\u53d6\u524d\u6761\u4ef6\u3002\u901a\u8fc7\u56db\u4e2a\u9ad8\u4fdd\u771f\u573a\u666f\u4e2d\u76841,750\u6b21\u98df\u54c1\u6c61\u67d3\u7269\u6293\u53d6\u5c1d\u8bd5\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5de5\u4e1a\u6293\u53d6\u6a21\u5f0f\u3002", "result": "\u5927\u89c4\u6a21\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540c\u6293\u53d6\u5668\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002\u5728\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u7269\u4f53\u8d28\u91cf\u6210\u4e3a\u4e3b\u5bfc\u6027\u80fd\u56e0\u7d20\uff0c\u800c\u89c6\u89c9\u8d28\u91cf\u548c\u6742\u4e71\u7a0b\u5ea6\u8d77\u4e2d\u7b49\u4f5c\u7528\u3002\u7269\u4f53\u8d28\u91cf\u5bf9\u6293\u53d6\u6027\u80fd\u7684\u5f71\u54cd\u6bd4\u89c6\u89c9\u8d28\u91cf\u6216\u6742\u4e71\u7a0b\u5ea6\u66f4\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5173\u952e\u7684\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u8de8\u7c7b\u522b\u6027\u80fd\u7684\u591a\u6a21\u6001\u6293\u53d6\u5668\u6280\u672f\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u673a\u5668\u4eba\u98df\u54c1\u5e9f\u7269\u5206\u7c7b\u3002"}}
{"id": "2602.18850", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18850", "abs": "https://arxiv.org/abs/2602.18850", "authors": ["J. E. Dom\u00ednguez-Vidal", "Alberto Sanfeliu"], "title": "When the Inference Meets the Explicitness or Why Multimodality Can Make Us Forget About the Perfect Predictor", "comment": "Original version submitted to the International Journal of Social Robotics. Final version available on the SORO website", "summary": "Although in the literature it is common to find predictors and inference systems that try to predict human intentions, the uncertainty of these models due to the randomness of human behavior has led some authors to start advocating the use of communication systems that explicitly elicit human intention. In this work, it is analyzed the use of four different communication systems with a human-robot collaborative object transportation task as experimental testbed: two intention predictors (one based on force prediction and another with an enhanced velocity prediction algorithm) and two explicit communication methods (a button interface and a voice-command recognition system). These systems were integrated into IVO, a custom mobile social robot equipped with force sensor to detect the force exchange between both agents and LiDAR to detect the environment. The collaborative task required transporting an object over a 5-7 meter distance with obstacles in the middle, demanding rapid decisions and precise physical coordination. 75 volunteers perform a total of 255 executions divided into three groups, testing inference systems in the first round, communication systems in the second, and the combined strategies in the third. The results show that, 1) once sufficient performance is achieved, the human no longer notices and positively assesses technical improvements; 2) the human prefers systems that are more natural to them even though they have higher failure rates; and 3) the preferred option is the right combination of both systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u4eba\u673a\u534f\u4f5c\u610f\u56fe\u6c9f\u901a\u7cfb\u7edf\uff1a\u4e24\u79cd\u610f\u56fe\u9884\u6d4b\u6a21\u578b\uff08\u57fa\u4e8e\u529b\u9884\u6d4b\u548c\u589e\u5f3a\u901f\u5ea6\u9884\u6d4b\uff09\u548c\u4e24\u79cd\u663e\u5f0f\u6c9f\u901a\u65b9\u6cd5\uff08\u6309\u94ae\u754c\u9762\u548c\u8bed\u97f3\u547d\u4ee4\uff09\uff0c\u5728\u79fb\u52a8\u793e\u4ea4\u673a\u5668\u4ebaIVO\u4e0a\u8fdb\u884c\u7269\u4f53\u642c\u8fd0\u534f\u4f5c\u4efb\u52a1\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u7cfb\u7edf\u8fbe\u5230\u8db3\u591f\u6027\u80fd\u540e\uff0c\u4eba\u7c7b\u4e0d\u518d\u6ce8\u610f\u5230\u6280\u672f\u6539\u8fdb\uff1b\u4eba\u4eec\u66f4\u559c\u6b22\u66f4\u81ea\u7136\u7684\u7cfb\u7edf\uff08\u5373\u4f7f\u6545\u969c\u7387\u66f4\u9ad8\uff09\uff1b\u6700\u4f73\u65b9\u6848\u662f\u9884\u6d4b\u4e0e\u663e\u5f0f\u6c9f\u901a\u7684\u7ed3\u5408\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u884c\u4e3a\u7684\u968f\u673a\u6027\u5bfc\u81f4\u610f\u56fe\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u4e00\u4e9b\u7814\u7a76\u8005\u5f00\u59cb\u5021\u5bfc\u4f7f\u7528\u663e\u5f0f\u6c9f\u901a\u7cfb\u7edf\u6765\u660e\u786e\u83b7\u53d6\u4eba\u7c7b\u610f\u56fe\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u610f\u56fe\u9884\u6d4b\u7cfb\u7edf\u548c\u663e\u5f0f\u6c9f\u901a\u7cfb\u7edf\u5728\u4eba\u673a\u534f\u4f5c\u7269\u4f53\u642c\u8fd0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u54ea\u79cd\u6c9f\u901a\u65b9\u5f0f\u66f4\u6709\u6548\u3002", "method": "\u7814\u7a76\u4f7f\u7528IVO\u79fb\u52a8\u793e\u4ea4\u673a\u5668\u4eba\uff08\u914d\u5907\u529b\u4f20\u611f\u5668\u548cLiDAR\uff09\uff0c\u57285-7\u7c73\u8ddd\u79bb\u7684\u969c\u788d\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u642c\u8fd0\u534f\u4f5c\u4efb\u52a1\u3002\u6bd4\u8f83\u56db\u79cd\u7cfb\u7edf\uff1a1\uff09\u57fa\u4e8e\u529b\u9884\u6d4b\u7684\u610f\u56fe\u9884\u6d4b\u5668\uff1b2\uff09\u589e\u5f3a\u901f\u5ea6\u9884\u6d4b\u7b97\u6cd5\uff1b3\uff09\u6309\u94ae\u754c\u9762\uff1b4\uff09\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u7cfb\u7edf\u300275\u540d\u5fd7\u613f\u8005\u5b8c\u6210255\u6b21\u5b9e\u9a8c\uff0c\u5206\u4e3a\u4e09\u7ec4\uff1a\u7b2c\u4e00\u8f6e\u6d4b\u8bd5\u63a8\u7406\u7cfb\u7edf\uff0c\u7b2c\u4e8c\u8f6e\u6d4b\u8bd5\u6c9f\u901a\u7cfb\u7edf\uff0c\u7b2c\u4e09\u8f6e\u6d4b\u8bd5\u7ec4\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u53d1\u73b0\uff1a1\uff09\u4e00\u65e6\u7cfb\u7edf\u8fbe\u5230\u8db3\u591f\u6027\u80fd\uff0c\u4eba\u7c7b\u4e0d\u518d\u6ce8\u610f\u5230\u6280\u672f\u6539\u8fdb\u5e76\u7ed9\u4e88\u79ef\u6781\u8bc4\u4ef7\uff1b2\uff09\u4eba\u7c7b\u66f4\u559c\u6b22\u66f4\u81ea\u7136\u7684\u7cfb\u7edf\uff08\u5982\u8bed\u97f3\u547d\u4ee4\uff09\uff0c\u5373\u4f7f\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6545\u969c\u7387\u66f4\u9ad8\uff1b3\uff09\u6700\u53d7\u9752\u7750\u7684\u65b9\u6848\u662f\u9884\u6d4b\u7cfb\u7edf\u4e0e\u663e\u5f0f\u6c9f\u901a\u7cfb\u7edf\u7684\u6070\u5f53\u7ec4\u5408\u3002", "conclusion": "\u5728\u4eba\u673a\u534f\u4f5c\u610f\u56fe\u6c9f\u901a\u4e2d\uff0c\u5355\u7eaf\u7684\u6280\u672f\u6539\u8fdb\u5728\u8fbe\u5230\u4e00\u5b9a\u9608\u503c\u540e\u4e0d\u518d\u88ab\u7528\u6237\u611f\u77e5\u3002\u7528\u6237\u504f\u597d\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5373\u4f7f\u8fd9\u610f\u5473\u7740\u66f4\u9ad8\u7684\u6545\u969c\u7387\u3002\u6700\u4f18\u7684\u4eba\u673a\u534f\u4f5c\u6c9f\u901a\u7b56\u7565\u5e94\u8be5\u662f\u9884\u6d4b\u6027\u7cfb\u7edf\u4e0e\u663e\u5f0f\u6c9f\u901a\u7cfb\u7edf\u7684\u667a\u80fd\u7ed3\u5408\uff0c\u65e2\u80fd\u5229\u7528\u9884\u6d4b\u63d0\u9ad8\u6548\u7387\uff0c\u53c8\u80fd\u901a\u8fc7\u663e\u5f0f\u6c9f\u901a\u786e\u4fdd\u610f\u56fe\u7684\u51c6\u786e\u4f20\u8fbe\u3002"}}
{"id": "2602.18862", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18862", "abs": "https://arxiv.org/abs/2602.18862", "authors": ["Yifei Yuan", "Ghaith Androwis", "Xianlian Zhou"], "title": "Gait Asymmetry from Unilateral Weakness and Improvement With Ankle Assistance: a Reinforcement Learning based Simulation Study", "comment": null, "summary": "Unilateral muscle weakness often leads to asymmetric gait, disrupting interlimb coordination and stance timing. This study presents a reinforcement learning (RL) based musculoskeletal simulation framework to (1) quantify how progressive unilateral muscle weakness affects gait symmetry and (2) evaluate whether ankle exoskeleton assistance can improve gait symmetry under impaired conditions. The overarching goal is to establish a simulation- and learning-based workflow that supports early controller development prior to patient experiments. Asymmetric gait was induced by reducing right-leg muscle strength to 75%, 50%, and 25% of baseline. Gait asymmetry was quantified using toe-off timing, peak contact forces, and joint-level symmetry metrics. Increasing weakness produced progressively larger temporal and kinematic asymmetry, most pronounced at the ankle. Ankle range of motion symmetry degraded from near-symmetric behavior at 100% strength (symmetry index, SI = +6.4%; correlation r=0.974) to severe asymmetry at 25% strength (SI = -47.1%, r=0.889), accompanied by a load shift toward the unimpaired limb. At 50% strength, ankle exoskeleton assistance improved kinematic symmetry relative to the unassisted impaired condition, reducing the magnitude of ankle SI from 25.8% to 18.5% and increasing ankle correlation from r=0.948 to 0.966, although peak loading remained biased toward the unimpaired side. Overall, this framework supports controlled evaluation of impairment severity and assistive strategies, and provides a basis for future validation in human experiments.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6a21\u62df\u6846\u67b6\u7814\u7a76\u5355\u4fa7\u808c\u8089\u65e0\u529b\u5bf9\u6b65\u6001\u5bf9\u79f0\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\u8f85\u52a9\u7684\u6539\u5584\u6548\u679c", "motivation": "\u5355\u4fa7\u808c\u8089\u65e0\u529b\u4f1a\u5bfc\u81f4\u4e0d\u5bf9\u79f0\u6b65\u6001\uff0c\u7834\u574f\u80a2\u4f53\u95f4\u534f\u8c03\u548c\u7ad9\u7acb\u65f6\u95f4\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u6e10\u8fdb\u6027\u5355\u4fa7\u808c\u8089\u65e0\u529b\u5bf9\u6b65\u6001\u5bf9\u79f0\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\u8f85\u52a9\u5728\u53d7\u635f\u6761\u4ef6\u4e0b\u7684\u6539\u5584\u6548\u679c\uff0c\u4e3a\u60a3\u8005\u5b9e\u9a8c\u524d\u7684\u65e9\u671f\u63a7\u5236\u5668\u5f00\u53d1\u5efa\u7acb\u4eff\u771f\u548c\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u808c\u8089\u9aa8\u9abc\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53f3\u817f\u808c\u8089\u529b\u91cf\u51cf\u5c11\u5230\u57fa\u7ebf\u6c34\u5e73\u768475%\u300150%\u548c25%\u6765\u8bf1\u5bfc\u4e0d\u5bf9\u79f0\u6b65\u6001\u3002\u4f7f\u7528\u79bb\u5730\u65f6\u95f4\u3001\u5cf0\u503c\u63a5\u89e6\u529b\u548c\u5173\u8282\u7ea7\u5bf9\u79f0\u6027\u6307\u6807\u91cf\u5316\u6b65\u6001\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u8bc4\u4f30\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\u8f85\u52a9\u7684\u6548\u679c", "result": "\u968f\u7740\u808c\u8089\u65e0\u529b\u7684\u589e\u52a0\uff0c\u65f6\u95f4\u548c\u8fd0\u52a8\u5b66\u4e0d\u5bf9\u79f0\u6027\u9010\u6e10\u589e\u5927\uff0c\u8e1d\u5173\u8282\u6700\u4e3a\u660e\u663e\u3002\u8e1d\u5173\u8282\u6d3b\u52a8\u8303\u56f4\u5bf9\u79f0\u6027\u4ece100%\u529b\u91cf\u65f6\u7684\u63a5\u8fd1\u5bf9\u79f0\uff08SI=+6.4%\uff0cr=0.974\uff09\u6076\u5316\u523025%\u529b\u91cf\u65f6\u7684\u4e25\u91cd\u4e0d\u5bf9\u79f0\uff08SI=-47.1%\uff0cr=0.889\uff09\uff0c\u540c\u65f6\u8d1f\u8377\u5411\u672a\u53d7\u635f\u80a2\u4f53\u8f6c\u79fb\u3002\u572850%\u529b\u91cf\u65f6\uff0c\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\u8f85\u52a9\u6539\u5584\u4e86\u8fd0\u52a8\u5b66\u5bf9\u79f0\u6027\uff0c\u5c06\u8e1d\u5173\u8282SI\u4ece25.8%\u964d\u4f4e\u523018.5%\uff0c\u76f8\u5173\u6027\u4ecer=0.948\u63d0\u9ad8\u52300.966\uff0c\u5c3d\u7ba1\u5cf0\u503c\u8d1f\u8377\u4ecd\u504f\u5411\u672a\u53d7\u635f\u4fa7", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u5bf9\u635f\u4f24\u4e25\u91cd\u7a0b\u5ea6\u548c\u8f85\u52a9\u7b56\u7565\u7684\u53d7\u63a7\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u5728\u4eba\u4f53\u5b9e\u9a8c\u4e2d\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u4eff\u771f\u5728\u6b65\u6001\u5eb7\u590d\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2602.18872", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18872", "abs": "https://arxiv.org/abs/2602.18872", "authors": ["Tatiana Berlenko", "Kirill Krinkin"], "title": "Equivalence and Divergence of Bayesian Log-Odds and Dempster's Combination Rule for 2D Occupancy Grids", "comment": "29 pages, 6 figures, 6 tables. Includes complete proofs, ablation studies, and supplementary statistical analysis", "summary": "We introduce a pignistic-transform-based methodology for fair comparison of Bayesian log-odds and Dempster's combination rule in occupancy grid mapping, matching per-observation decision probabilities to isolate the fusion rule from sensor parameterization. Under BetP matching across simulation, two real lidar datasets, and downstream path planning, Bayesian fusion is consistently favored (15/15 directional consistency, p = 3.1e-5) with small absolute differences (0.001-0.022). Under normalized plausibility matching, the direction reverses, confirming the result is matching-criterion-specific. The methodology is reusable for any future Bayesian/belief function comparison.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8epignistic\u53d8\u6362\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u516c\u5e73\u6bd4\u8f83\u8d1d\u53f6\u65af\u5bf9\u6570\u51e0\u7387\u4e0eDempster\u7ec4\u5408\u89c4\u5219\u5728\u5360\u636e\u6805\u683c\u5730\u56fe\u6784\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5339\u914d\u6bcf\u4e2a\u89c2\u6d4b\u7684\u51b3\u7b56\u6982\u7387\u6765\u9694\u79bb\u878d\u5408\u89c4\u5219\u4e0e\u4f20\u611f\u5668\u53c2\u6570\u5316\u7684\u5f71\u54cd\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u516c\u5e73\u7684\u65b9\u6cd5\u6765\u6bd4\u8f83\u8d1d\u53f6\u65af\u878d\u5408\u548cDempster\u7ec4\u5408\u89c4\u5219\u5728\u5360\u636e\u6805\u683c\u5730\u56fe\u6784\u5efa\u4e2d\u7684\u6027\u80fd\uff0c\u907f\u514d\u4f20\u611f\u5668\u53c2\u6570\u5316\u5bf9\u6bd4\u8f83\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u57fa\u4e8epignistic\u53d8\u6362\u7684\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u5339\u914d\u6bcf\u4e2a\u89c2\u6d4b\u7684\u51b3\u7b56\u6982\u7387\u6765\u9694\u79bb\u878d\u5408\u89c4\u5219\u4e0e\u4f20\u611f\u5668\u53c2\u6570\u5316\u7684\u5f71\u54cd\u3002\u5728BetP\u5339\u914d\u548c\u5f52\u4e00\u5316\u4f3c\u771f\u6027\u5339\u914d\u4e24\u79cd\u6761\u4ef6\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728BetP\u5339\u914d\u6761\u4ef6\u4e0b\uff0c\u8d1d\u53f6\u65af\u878d\u5408\u5728\u6a21\u62df\u3001\u4e24\u4e2a\u771f\u5b9e\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u96c6\u4ee5\u53ca\u4e0b\u6e38\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u66f4\u4f18\uff0815/15\u65b9\u5411\u4e00\u81f4\u6027\uff0cp = 3.1e-5\uff09\uff0c\u7edd\u5bf9\u5dee\u5f02\u8f83\u5c0f\uff080.001-0.022\uff09\u3002\u5728\u5f52\u4e00\u5316\u4f3c\u771f\u6027\u5339\u914d\u6761\u4ef6\u4e0b\uff0c\u7ed3\u679c\u65b9\u5411\u53cd\u8f6c\uff0c\u8868\u660e\u7ed3\u679c\u4f9d\u8d56\u4e8e\u5339\u914d\u51c6\u5219\u7684\u9009\u62e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bba\u4e3a\u672a\u6765\u4efb\u4f55\u8d1d\u53f6\u65af/\u4fe1\u5ff5\u51fd\u6570\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u6846\u67b6\uff0c\u540c\u65f6\u8868\u660e\u6bd4\u8f83\u7ed3\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6240\u9009\u62e9\u7684\u5339\u914d\u51c6\u5219\u3002"}}
{"id": "2602.18951", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18951", "abs": "https://arxiv.org/abs/2602.18951", "authors": ["Azizollah Taheri", "Derya Aksaray"], "title": "Temporal-Logic-Aware Frontier-Based Exploration", "comment": "8 pages, 7 figures", "summary": "This paper addresses the problem of temporal logic motion planning for an autonomous robot operating in an unknown environment. The objective is to enable the robot to satisfy a syntactically co-safe Linear Temporal Logic (scLTL) specification when the exact locations of the desired labels are not known a priori. We introduce a new type of automaton state, referred to as commit states. These states capture intermediate task progress resulting from actions whose consequences are irreversible. In other words, certain future paths to satisfaction become not feasible after taking those actions that lead to the commit states. By leveraging commit states, we propose a sound and complete frontier-based exploration algorithm that strategically guides the robot to make progress toward the task while preserving all possible ways of satisfying it. The efficacy of the proposed method is validated through simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u627f\u8bfa\u72b6\u6001\u7684\u672a\u77e5\u73af\u5883\u4e0b\u673a\u5668\u4eba\u65f6\u6001\u903b\u8f91\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u6807\u7b7e\u4f4d\u7f6e\u672a\u77e5\u7684scLTL\u89c4\u8303\u4efb\u52a1", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u6267\u884c\u65f6\u6001\u903b\u8f91\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u5f53\u671f\u671b\u6807\u7b7e\u7684\u786e\u5207\u4f4d\u7f6e\u4e8b\u5148\u672a\u77e5\u65f6\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u63a2\u7d22\u5e76\u5b8c\u6210\u4efb\u52a1\u7684\u65b9\u6cd5", "method": "\u5f15\u5165\u627f\u8bfa\u72b6\u6001\u6982\u5ff5\uff0c\u6355\u6349\u4e0d\u53ef\u9006\u52a8\u4f5c\u5e26\u6765\u7684\u4e2d\u95f4\u4efb\u52a1\u8fdb\u5c55\uff1b\u63d0\u51fa\u57fa\u4e8e\u524d\u6cbf\u7684\u63a2\u7d22\u7b97\u6cd5\uff0c\u5728\u5411\u4efb\u52a1\u8fdb\u5c55\u7684\u540c\u65f6\u4fdd\u7559\u6240\u6709\u53ef\u80fd\u7684\u6ee1\u8db3\u65b9\u5f0f", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u53ef\u9760\u4e14\u5b8c\u5907\u7684\uff0c\u80fd\u591f\u6218\u7565\u6027\u5730\u5f15\u5bfc\u673a\u5668\u4eba\u5411\u4efb\u52a1\u8fdb\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u6240\u6709\u53ef\u80fd\u7684\u4efb\u52a1\u6ee1\u8db3\u8def\u5f84", "conclusion": "\u901a\u8fc7\u627f\u8bfa\u72b6\u6001\u548c\u524d\u6cbf\u63a2\u7d22\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672a\u77e5\u73af\u5883\u4e0b\u65f6\u6001\u903b\u8f91\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2602.18967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18967", "abs": "https://arxiv.org/abs/2602.18967", "authors": ["Felix Verstraete", "Lan Wei", "Wen Fan", "Dandan Zhang"], "title": "TactEx: An Explainable Multimodal Robotic Interaction Framework for Human-Like Touch and Hardness Estimation", "comment": "Accepted by 2026 ICRA", "summary": "Accurate perception of object hardness is essential for safe and dexterous contact-rich robotic manipulation. Here, we present TactEx, an explainable multimodal robotic interaction framework that unifies vision, touch, and language for human-like hardness estimation and interactive guidance. We evaluate TactEx on fruit-ripeness assessment, a representative task that requires both tactile sensing and contextual understanding. The system fuses GelSight-Mini tactile streams with RGB observations and language prompts. A ResNet50+LSTM model estimates hardness from sequential tactile data, while a cross-modal alignment module combines visual cues with guidance from a large language model (LLM). This explainable multimodal interface allows users to distinguish ripeness levels with statistically significant class separation (p < 0.01 for all fruit pairs). For touch placement, we compare YOLO with Grounded-SAM (GSAM) and find GSAM to be more robust for fine-grained segmentation and contact-site selection. A lightweight LLM parses user instructions and produces grounded natural-language explanations linked to the tactile outputs. In end-to-end evaluations, TactEx attains 90% task success on simple user queries and generalises to novel tasks without large-scale tuning. These results highlight the promise of combining pretrained visual and tactile models with language grounding to advance explainable, human-like touch perception and decision-making in robotics.", "AI": {"tldr": "TactEx\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u4ea4\u4e92\u6846\u67b6\uff0c\u878d\u5408\u89c6\u89c9\u3001\u89e6\u89c9\u548c\u8bed\u8a00\u8fdb\u884c\u7c7b\u4eba\u786c\u5ea6\u4f30\u8ba1\u548c\u4ea4\u4e92\u6307\u5bfc\uff0c\u5728\u6c34\u679c\u6210\u719f\u5ea6\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u51c6\u786e\u611f\u77e5\u7269\u4f53\u786c\u5ea6\u5bf9\u4e8e\u5b89\u5168\u548c\u7075\u5de7\u7684\u63a5\u89e6\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7ed3\u5408\u89e6\u89c9\u611f\u77e5\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "\u7cfb\u7edf\u878d\u5408GelSight-Mini\u89e6\u89c9\u6d41\u3001RGB\u89c2\u5bdf\u548c\u8bed\u8a00\u63d0\u793a\uff0c\u4f7f\u7528ResNet50+LSTM\u6a21\u578b\u4ece\u5e8f\u5217\u89e6\u89c9\u6570\u636e\u4f30\u8ba1\u786c\u5ea6\uff0c\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\uff0c\u6bd4\u8f83YOLO\u548cGrounded-SAM\u8fdb\u884c\u63a5\u89e6\u70b9\u9009\u62e9\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u533a\u5206\u4e0d\u540c\u6210\u719f\u5ea6\u6c34\u5e73\uff08\u6240\u6709\u6c34\u679c\u5bf9\u7684p<0.01\uff09\uff0c\u5728\u7b80\u5355\u7528\u6237\u67e5\u8be2\u4e0a\u8fbe\u523090%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u800c\u65e0\u9700\u5927\u89c4\u6a21\u8c03\u4f18\u3002", "conclusion": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u89e6\u89c9\u6a21\u578b\u4e0e\u8bed\u8a00\u57fa\u7840\u5316\uff0c\u6709\u671b\u63a8\u8fdb\u673a\u5668\u4eba\u4e2d\u53ef\u89e3\u91ca\u7684\u3001\u7c7b\u4eba\u7684\u89e6\u89c9\u611f\u77e5\u548c\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2602.18976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18976", "abs": "https://arxiv.org/abs/2602.18976", "authors": ["Pongporn Supa", "Alex Dunnett", "Feng Xiao", "Rui Wu", "Mirko Kovac", "Basaran Bahadir Kocer"], "title": "Bumper Drone: Elastic Morphology Design for Aerial Physical Interaction", "comment": "Accepted to the 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft) 2026", "summary": "Aerial robots are evolving from avoiding obstacles to exploiting the environmental contact interactions for navigation, exploration and manipulation. A key challenge in such aerial physical interactions lies in handling uncertain contact forces on unknown targets, which typically demand accurate sensing and active control. We present a drone platform with elastic horns that enables touch-and-go manoeuvres - a self-regulated, consecutive bumping motion that allows the drone to maintain proximity to a wall without relying on active obstacle avoidance. It leverages environmental interaction as a form of embodied control, where low-level stabilisation and near-obstacle navigation emerge from the passive dynamic responses of the drone-obstacle system that resembles a mass-spring-damper system. Experiments show that the elastic horn can absorb impact energy while maintaining vehicle stability, reducing pitch oscillations by 38% compared to the rigid horn configuration. The lower horn arrangement was found to reduce pitch oscillations by approximately 54%. In addition to intermittent contact, the platform equipped with elastic horns also demonstrates stable, sustained contact with static objects, relying on a standard attitude PID controller.", "AI": {"tldr": "\u65e0\u4eba\u673a\u5e73\u53f0\u914d\u5907\u5f39\u6027\u89e6\u89d2\uff0c\u901a\u8fc7\"\u89e6\u78b0\u5373\u8d70\"\u673a\u52a8\u5b9e\u73b0\u88ab\u52a8\u73af\u5883\u4ea4\u4e92\u5bfc\u822a\uff0c\u65e0\u9700\u4e3b\u52a8\u907f\u969c\u63a7\u5236", "motivation": "\u7a7a\u4e2d\u673a\u5668\u4eba\u9700\u8981\u4ece\u907f\u514d\u969c\u788d\u7269\u53d1\u5c55\u5230\u5229\u7528\u73af\u5883\u63a5\u89e6\u4ea4\u4e92\u8fdb\u884c\u5bfc\u822a\u3001\u63a2\u7d22\u548c\u64cd\u4f5c\u3002\u5173\u952e\u6311\u6218\u5728\u4e8e\u5904\u7406\u672a\u77e5\u76ee\u6807\u4e0a\u7684\u4e0d\u786e\u5b9a\u63a5\u89e6\u529b\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u7cbe\u786e\u4f20\u611f\u548c\u4e3b\u52a8\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u914d\u5907\u5f39\u6027\u89e6\u89d2\u7684\u65e0\u4eba\u673a\u5e73\u53f0\uff0c\u5b9e\u73b0\"\u89e6\u78b0\u5373\u8d70\"\u673a\u52a8\u3002\u5229\u7528\u73af\u5883\u4ea4\u4e92\u4f5c\u4e3a\u4f53\u73b0\u63a7\u5236\u5f62\u5f0f\uff0c\u901a\u8fc7\u65e0\u4eba\u673a-\u969c\u788d\u7269\u7cfb\u7edf\u7684\u88ab\u52a8\u52a8\u6001\u54cd\u5e94\uff08\u7c7b\u4f3c\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\uff09\u5b9e\u73b0\u4f4e\u7ea7\u7a33\u5b9a\u548c\u8fd1\u969c\u788d\u7269\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f39\u6027\u89e6\u89d2\u80fd\u5438\u6536\u51b2\u51fb\u80fd\u91cf\u5e76\u4fdd\u6301\u98de\u884c\u5668\u7a33\u5b9a\u6027\uff0c\u4e0e\u521a\u6027\u89e6\u89d2\u914d\u7f6e\u76f8\u6bd4\u51cf\u5c1138%\u7684\u4fef\u4ef0\u632f\u8361\u3002\u4e0b\u90e8\u89e6\u89d2\u5e03\u7f6e\u8fdb\u4e00\u6b65\u51cf\u5c11\u7ea654%\u7684\u4fef\u4ef0\u632f\u8361\u3002\u5e73\u53f0\u8fd8\u80fd\u4e0e\u9759\u6001\u7269\u4f53\u4fdd\u6301\u7a33\u5b9a\u6301\u7eed\u63a5\u89e6\uff0c\u4ec5\u4f9d\u8d56\u6807\u51c6\u59ff\u6001PID\u63a7\u5236\u5668\u3002", "conclusion": "\u5f39\u6027\u89e6\u89d2\u8bbe\u8ba1\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u901a\u8fc7\u88ab\u52a8\u52a8\u6001\u54cd\u5e94\u5b9e\u73b0\u73af\u5883\u4ea4\u4e92\u5bfc\u822a\uff0c\u51cf\u5c11\u5bf9\u4e3b\u52a8\u907f\u969c\u63a7\u5236\u7684\u4f9d\u8d56\uff0c\u4e3a\u7a7a\u4e2d\u7269\u7406\u4ea4\u4e92\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.18991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18991", "abs": "https://arxiv.org/abs/2602.18991", "authors": ["Ruohan Zhang", "Mohammad Amin Mirzaee", "Wenzhen Yuan"], "title": "FruitTouch: A Perceptive Gripper for Gentle and Scalable Fruit Harvesting", "comment": "8 pages, 7 figures", "summary": "The automation of fruit harvesting has gained increasing significance in response to rising labor shortages. A sensorized gripper is a key component of this process, which must be compact enough for confined spaces, able to stably grasp diverse fruits, and provide reliable feedback on fruit conditions for efficient harvesting. To address this need, we propose FruitTouch, a compact gripper that integrates high-resolution, vision-based tactile sensing through an optimized optical design. This configuration accommodates a wide range of fruit sizes while maintaining low cost and mechanical simplicity. Tactile images captured by an embedded camera provide rich information for real-time force estimation, slip detection, and softness prediction. We validate the gripper in real-world fruit harvesting experiments, demonstrating robust grasp stability and effective damage prevention.", "AI": {"tldr": "FruitTouch\u662f\u4e00\u79cd\u7d27\u51d1\u578b\u5939\u722a\uff0c\u96c6\u6210\u4e86\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\uff0c\u7528\u4e8e\u6c34\u679c\u81ea\u52a8\u91c7\u6458\uff0c\u80fd\u591f\u7a33\u5b9a\u6293\u53d6\u591a\u79cd\u6c34\u679c\u5e76\u5b9e\u65f6\u76d1\u6d4b\u529b\u3001\u6ed1\u52a8\u548c\u8f6f\u5ea6\u3002", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u63a8\u52a8\u6c34\u679c\u91c7\u6458\u81ea\u52a8\u5316\u9700\u6c42\uff0c\u9700\u8981\u7d27\u51d1\u3001\u80fd\u7a33\u5b9a\u6293\u53d6\u591a\u79cd\u6c34\u679c\u5e76\u63d0\u4f9b\u53ef\u9760\u53cd\u9988\u7684\u4f20\u611f\u5668\u5316\u5939\u722a\u3002", "method": "\u63d0\u51faFruitTouch\u7d27\u51d1\u5939\u722a\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u5149\u5b66\u8bbe\u8ba1\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\uff0c\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u6355\u6349\u89e6\u89c9\u56fe\u50cf\u3002", "result": "\u89e6\u89c9\u56fe\u50cf\u63d0\u4f9b\u4e30\u5bcc\u4fe1\u606f\u7528\u4e8e\u5b9e\u65f6\u529b\u4f30\u8ba1\u3001\u6ed1\u52a8\u68c0\u6d4b\u548c\u8f6f\u5ea6\u9884\u6d4b\uff0c\u5728\u771f\u5b9e\u6c34\u679c\u91c7\u6458\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7a33\u5b9a\u7684\u6293\u53d6\u548c\u6709\u6548\u7684\u635f\u4f24\u9884\u9632\u3002", "conclusion": "FruitTouch\u89e3\u51b3\u4e86\u6c34\u679c\u91c7\u6458\u81ea\u52a8\u5316\u7684\u5173\u952e\u9700\u6c42\uff0c\u5c55\u793a\u4e86\u7d27\u51d1\u8bbe\u8ba1\u4e0b\u7a33\u5b9a\u6293\u53d6\u548c\u5b9e\u65f6\u76d1\u6d4b\u7684\u80fd\u529b\u3002"}}
{"id": "2602.19038", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19038", "abs": "https://arxiv.org/abs/2602.19038", "authors": ["Claire Liang", "Franziska Babel", "Hannah Pelikan", "Sydney Thompson", "Xiang Zhi Tan"], "title": "A Checklist for Deploying Robots in Public: Articulating Tacit Knowledge in the HRI Community", "comment": null, "summary": "Many of the challenges encountered in in-the-wild public deployments of robots remain undocumented despite sharing many common pitfalls. This creates a high barrier of entry and results in repetition of avoidable mistakes. To articulate the tacit knowledge in the HRI community, this paper presents a guideline in the form of a checklist to support researchers in preparing for robot deployments in public. Drawing on their own experience with public robot deployments, the research team collected essential topics to consider in public HRI research. These topics are represented as modular flip cards in a hierarchical table, structured into deployment phases and important domains. We interviewed six interdisciplinary researchers with expertise in public HRI and show how including community input refines the checklist. We further show the checklist in action in context of real public studies. Finally, we contribute the checklist as an open-source, customizable community resource that both collects joint expertise for continual evolution and is usable as a list, set of cards, and an interactive web tool.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u516c\u5171\u673a\u5668\u4eba\u90e8\u7f72\u7684\u68c0\u67e5\u6e05\u5355\u6307\u5357\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5361\u7247\u5f62\u5f0f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u907f\u514d\u5e38\u89c1\u9519\u8bef\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u793e\u533a\u8d44\u6e90\u6301\u7eed\u6f14\u8fdb\u3002", "motivation": "\u516c\u5171\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u5b58\u5728\u8bb8\u591a\u672a\u8bb0\u5f55\u7684\u6311\u6218\u548c\u5e38\u89c1\u9677\u9631\uff0c\u5bfc\u81f4\u8fdb\u5165\u95e8\u69db\u9ad8\u4e14\u91cd\u590d\u72af\u9519\u3002\u4e3a\u4e86\u5206\u4eab\u4eba\u673a\u4ea4\u4e92\u793e\u533a\u7684\u9690\u6027\u77e5\u8bc6\uff0c\u9700\u8981\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u6307\u5bfc\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u7814\u7a76\u56e2\u961f\u81ea\u8eab\u516c\u5171\u673a\u5668\u4eba\u90e8\u7f72\u7ecf\u9a8c\u6536\u96c6\u5173\u952e\u4e3b\u9898\uff0c\u6784\u5efa\u6a21\u5757\u5316\u7ffb\u8f6c\u5361\u7247\u7684\u5206\u5c42\u8868\u683c\u7ed3\u6784\uff0c\u6309\u90e8\u7f72\u9636\u6bb5\u548c\u91cd\u8981\u9886\u57df\u7ec4\u7ec7\u3002\u91c7\u8bbf\u516d\u4f4d\u8de8\u5b66\u79d1\u4e13\u5bb6\u5b8c\u5584\u6e05\u5355\uff0c\u5e76\u5728\u771f\u5b9e\u516c\u5171\u7814\u7a76\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5f00\u53d1\u51fa\u5b9e\u7528\u7684\u68c0\u67e5\u6e05\u5355\u5de5\u5177\uff0c\u5305\u542b\u793e\u533a\u8f93\u5165\u7684\u7cbe\u70bc\u5185\u5bb9\uff0c\u5728\u5b9e\u9645\u516c\u5171\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002\u63d0\u4f9b\u5f00\u6e90\u3001\u53ef\u5b9a\u5236\u7684\u793e\u533a\u8d44\u6e90\uff0c\u652f\u6301\u5217\u8868\u3001\u5361\u7247\u548c\u4ea4\u4e92\u5f0f\u7f51\u9875\u5de5\u5177\u591a\u79cd\u4f7f\u7528\u5f62\u5f0f\u3002", "conclusion": "\u8be5\u68c0\u67e5\u6e05\u5355\u6210\u529f\u6536\u96c6\u5e76\u7cfb\u7edf\u5316\u4e86\u516c\u5171HRI\u7814\u7a76\u7684\u96c6\u4f53\u4e13\u4e1a\u77e5\u8bc6\uff0c\u964d\u4f4e\u4e86\u90e8\u7f72\u95e8\u69db\uff0c\u907f\u514d\u4e86\u91cd\u590d\u9519\u8bef\uff0c\u5e76\u4f5c\u4e3a\u53ef\u6f14\u8fdb\u7684\u793e\u533a\u8d44\u6e90\u652f\u6301\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2602.19062", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19062", "abs": "https://arxiv.org/abs/2602.19062", "authors": ["Jia Song", "Ce Hao", "Jiangcheng Su"], "title": "Path planning for unmanned surface vehicle based on predictive artificial potential field. International Journal of Advanced Robotic Systems", "comment": null, "summary": "Path planning for high-speed unmanned surface vehicles requires more complex solutions to reduce sailing time and save energy. This article proposes a new predictive artificial potential field that incorporates time information and predictive potential to plan smoother paths. It explores the principles of the artificial potential field, considering vehicle dynamics and local minimum reachability. The study first analyzes the most advanced traditional artificial potential field and its drawbacks in global and local path planning. It then introduces three modifications to the predictive artificial potential field-angle limit, velocity adjustment, and predictive potential to enhance the feasibility and flatness of the generated path. A comparison between the traditional and predictive artificial potential fields demonstrates that the latter successfully restricts the maximum turning angle, shortens sailing time, and intelligently avoids obstacles. Simulation results further verify that the predictive artificial potential field addresses the concave local minimum problem and improves reachability in special scenarios, ultimately generating a more efficient path that reduces sailing time and conserves energy for unmanned surface vehicles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u548c\u9884\u6d4b\u52bf\u80fd\u7684\u65b0\u578b\u9884\u6d4b\u4eba\u5de5\u52bf\u573a\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u901f\u65e0\u4eba\u6c34\u9762\u8247\u7684\u8def\u5f84\u89c4\u5212\uff0c\u65e8\u5728\u51cf\u5c11\u822a\u884c\u65f6\u95f4\u548c\u8282\u7ea6\u80fd\u6e90\u3002", "motivation": "\u9ad8\u901f\u65e0\u4eba\u6c34\u9762\u8247\u7684\u8def\u5f84\u89c4\u5212\u9700\u8981\u66f4\u590d\u6742\u7684\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u822a\u884c\u65f6\u95f4\u548c\u8282\u7ea6\u80fd\u6e90\u3002\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u65b9\u6cd5\u5728\u5168\u5c40\u548c\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u5c40\u90e8\u6700\u5c0f\u503c\u53ef\u8fbe\u6027\u65b9\u9762\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u6700\u5148\u8fdb\u7684\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u53ca\u5176\u7f3a\u70b9\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u9884\u6d4b\u4eba\u5de5\u52bf\u573a\u7684\u4e09\u4e2a\u6539\u8fdb\uff1a\u89d2\u5ea6\u9650\u5236\u3001\u901f\u5ea6\u8c03\u6574\u548c\u9884\u6d4b\u52bf\u80fd\u3002\u8fd9\u4e9b\u6539\u8fdb\u8003\u8651\u4e86\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u5c40\u90e8\u6700\u5c0f\u503c\u53ef\u8fbe\u6027\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u8def\u5f84\u7684\u53ef\u884c\u6027\u548c\u5e73\u6ed1\u6027\u3002", "result": "\u4e0e\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u76f8\u6bd4\uff0c\u9884\u6d4b\u4eba\u5de5\u52bf\u573a\u6210\u529f\u9650\u5236\u4e86\u6700\u5927\u8f6c\u5f2f\u89d2\u5ea6\uff0c\u7f29\u77ed\u4e86\u822a\u884c\u65f6\u95f4\uff0c\u5e76\u80fd\u667a\u80fd\u907f\u969c\u3002\u4eff\u771f\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u51f9\u5f62\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\uff0c\u5728\u7279\u6b8a\u573a\u666f\u4e2d\u63d0\u9ad8\u4e86\u53ef\u8fbe\u6027\u3002", "conclusion": "\u9884\u6d4b\u4eba\u5de5\u52bf\u573a\u65b9\u6cd5\u4e3a\u9ad8\u901f\u65e0\u4eba\u6c34\u9762\u8247\u751f\u6210\u66f4\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u51cf\u5c11\u4e86\u822a\u884c\u65f6\u95f4\u5e76\u8282\u7ea6\u4e86\u80fd\u6e90\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.19077", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.19077", "abs": "https://arxiv.org/abs/2602.19077", "authors": ["Yi Jin", "Chang Liu", "Roger D. Quinn", "Robert J. Wood", "C. Chase Cao"], "title": "Design, Locomotion, and Control of Amphibious Robots: Recent Advances", "comment": null, "summary": "Amphibious robots, operating seamlessly across land and water, are advancing applications in conservation, disaster response, and defense. Their performance depends on locomotion mechanisms, actuation technologies, and sensor-control integration. This review highlights recent progress in these areas, examining movement strategies, material-based actuators, and control systems for autonomy and adaptability. Challenges and opportunities are outlined to guide future research toward more efficient, resilient, and multifunctional amphibious robots.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u56de\u987e\u4e86\u4e24\u6816\u673a\u5668\u4eba\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u8fd0\u52a8\u673a\u5236\u3001\u9a71\u52a8\u6280\u672f\u548c\u4f20\u611f\u63a7\u5236\u96c6\u6210\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e24\u6816\u673a\u5668\u4eba\u80fd\u591f\u5728\u9646\u5730\u548c\u6c34\u57df\u65e0\u7f1d\u64cd\u4f5c\uff0c\u5728\u73af\u5883\u4fdd\u62a4\u3001\u707e\u5bb3\u54cd\u5e94\u548c\u56fd\u9632\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002\u7136\u800c\uff0c\u5176\u6027\u80fd\u53d7\u5230\u8fd0\u52a8\u673a\u5236\u3001\u9a71\u52a8\u6280\u672f\u548c\u4f20\u611f\u63a7\u5236\u96c6\u6210\u7b49\u56e0\u7d20\u7684\u9650\u5236\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u6765\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e24\u6816\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u7b56\u7565\u3001\u6750\u6599\u57fa\u9a71\u52a8\u5668\u3001\u63a7\u5236\u7cfb\u7edf\u7b49\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u6574\u5408\u73b0\u6709\u7814\u7a76\u6210\u679c\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u4e24\u6816\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u673a\u5236\u3001\u9a71\u52a8\u6280\u672f\u548c\u63a7\u5236\u96c6\u6210\u65b9\u9762\u7684\u4e3b\u8981\u8fdb\u5c55\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u7684\u6280\u672f\u6311\u6218\u548c\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u4e24\u6816\u673a\u5668\u4eba\u7814\u7a76\u9700\u8981\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u97e7\u6027\u548c\u591a\u529f\u80fd\u7684\u65b9\u5411\u53d1\u5c55\uff0c\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6280\u672f\u6311\u6218\u548c\u53d1\u5c55\u673a\u9047\u6307\u5bfc\u3002"}}
{"id": "2602.19107", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19107", "abs": "https://arxiv.org/abs/2602.19107", "authors": ["Yue Deng", "Changyang He"], "title": "A User-driven Design Framework for Robotaxi", "comment": null, "summary": "Robotaxis are emerging as a promising form of urban mobility, yet research has largely emphasized technical driving performance while leaving open how passengers experience and evaluate rides without a human driver. To address the limitations of prior work that often relies on simulated or hypothetical settings, we investigate real-world robotaxi use through 18 semi-structured interviews and autoethnographic ride experiences. We found that users were drawn to robotaxis by low cost, social recommendation, and curiosity. They valued a distinctive set of benefits, such as an increased sense of agency, and consistent driving behavioral consistency and standardized ride experiences. However, they encountered persistent challenges around limited flexibility, insufficient transparency, management difficulty, robustness concerns in edge cases, and emergency handling concerns. Robotaxi experiences were shaped by privacy, safety, ethics, and trust. Users were often privacy-indifferent yet sensitive to opaque access and leakage risks; safety perceptions were polarized; and ethical considerations surfaced round issues such as accountability, feedback responsibility and absence of human-like social norms. Based on these findings, we propose a user-driven design framework spanning the end-to-end journey, such as pre-ride configuration (hailing), context-aware pickup facilitation (pick-up) in-ride explainability (traveling), and accountable post-ride feedback (drop-off) to guide robotaxi interaction and service design.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u5730\u8bbf\u8c08\u548c\u4f53\u9a8c\u53d1\u73b0\uff0c\u7528\u6237\u9009\u62e9\u65e0\u4eba\u9a7e\u9a76\u51fa\u79df\u8f66\u4e3b\u8981\u56e0\u4e3a\u4f4e\u6210\u672c\u3001\u793e\u4ea4\u63a8\u8350\u548c\u597d\u5947\u5fc3\uff0c\u770b\u91cd\u81ea\u4e3b\u6027\u589e\u5f3a\u548c\u9a7e\u9a76\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u4f46\u9762\u4e34\u7075\u6d3b\u6027\u4e0d\u8db3\u3001\u900f\u660e\u5ea6\u4e0d\u591f\u3001\u7ba1\u7406\u56f0\u96be\u3001\u8fb9\u7f18\u60c5\u51b5\u9c81\u68d2\u6027\u5dee\u548c\u7d27\u6025\u5904\u7406\u7b49\u95ee\u9898\uff0c\u9690\u79c1\u3001\u5b89\u5168\u3001\u4f26\u7406\u548c\u4fe1\u4efb\u662f\u6838\u5fc3\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u65e0\u4eba\u9a7e\u9a76\u51fa\u79df\u8f66\u7684\u6280\u672f\u9a7e\u9a76\u6027\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u4e58\u5ba2\u5728\u65e0\u4eba\u7c7b\u53f8\u673a\u60c5\u51b5\u4e0b\u7684\u5b9e\u9645\u4f53\u9a8c\u548c\u8bc4\u4ef7\u3002\u5148\u524d\u7814\u7a76\u591a\u4f9d\u8d56\u6a21\u62df\u6216\u5047\u8bbe\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4e16\u754c\u4f7f\u7528\u60c5\u51b5\u7684\u7406\u89e3\u3002", "method": "\u91c7\u752818\u6b21\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u81ea\u6c11\u65cf\u5fd7\u4e58\u8f66\u4f53\u9a8c\uff0c\u8c03\u67e5\u771f\u5b9e\u4e16\u754c\u7684\u65e0\u4eba\u9a7e\u9a76\u51fa\u79df\u8f66\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u7528\u6237\u88ab\u4f4e\u6210\u672c\u3001\u793e\u4ea4\u63a8\u8350\u548c\u597d\u5947\u5fc3\u5438\u5f15\uff1b\u770b\u91cd\u81ea\u4e3b\u6027\u589e\u5f3a\u3001\u9a7e\u9a76\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u6807\u51c6\u5316\u4e58\u8f66\u4f53\u9a8c\uff1b\u4f46\u9762\u4e34\u7075\u6d3b\u6027\u6709\u9650\u3001\u900f\u660e\u5ea6\u4e0d\u8db3\u3001\u7ba1\u7406\u56f0\u96be\u3001\u8fb9\u7f18\u60c5\u51b5\u9c81\u68d2\u6027\u95ee\u9898\u548c\u7d27\u6025\u5904\u7406\u62c5\u5fe7\uff1b\u9690\u79c1\u3001\u5b89\u5168\u3001\u4f26\u7406\u548c\u4fe1\u4efb\u662f\u6838\u5fc3\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u9a71\u52a8\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u6db5\u76d6\u7aef\u5230\u7aef\u65c5\u7a0b\uff1a\u9884\u4e58\u8f66\u914d\u7f6e\uff08\u53eb\u8f66\uff09\u3001\u60c5\u5883\u611f\u77e5\u63a5\u5ba2\uff08\u63a5\u5ba2\uff09\u3001\u884c\u7a0b\u4e2d\u53ef\u89e3\u91ca\u6027\uff08\u884c\u9a76\uff09\u548c\u53ef\u8ffd\u6eaf\u7684\u4e58\u8f66\u540e\u53cd\u9988\uff08\u4e0b\u8f66\uff09\uff0c\u4ee5\u6307\u5bfc\u65e0\u4eba\u9a7e\u9a76\u51fa\u79df\u8f66\u7684\u4ea4\u4e92\u548c\u670d\u52a1\u8bbe\u8ba1\u3002"}}
{"id": "2602.19108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19108", "abs": "https://arxiv.org/abs/2602.19108", "authors": ["Anton R. Wagner", "Madhan Balaji Rao", "Xuesu Xiao", "S\u00f6ren Pirk"], "title": "Understanding Fire Through Thermal Radiation Fields for Mobile Robots", "comment": null, "summary": "Safely moving through environments affected by fire is a critical capability for autonomous mobile robots deployed in disaster response. In this work, we present a novel approach for mobile robots to understand fire through building real-time thermal radiation fields. We register depth and thermal images to obtain a 3D point cloud annotated with temperature values. From these data, we identify fires and use the Stefan-Boltzmann law to approximate the thermal radiation in empty spaces. This enables the construction of a continuous thermal radiation field over the environment. We show that this representation can be used for robot navigation, where we embed thermal constraints into the cost map to compute collision-free and thermally safe paths. We validate our approach on a Boston Dynamics Spot robot in controlled experimental settings. Our experiments demonstrate the robot's ability to avoid hazardous regions while still reaching navigation goals. Our approach paves the way toward mobile robots that can be autonomously deployed in fire-affected environments, with potential applications in search-and-rescue, firefighting, and hazardous material response.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u6784\u5efa\u5b9e\u65f6\u70ed\u8f90\u5c04\u573a\u7684\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u706b\u707e\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\uff0c\u907f\u514d\u70ed\u5371\u9669\u533a\u57df\u3002", "motivation": "\u5728\u707e\u96be\u54cd\u5e94\u4e2d\uff0c\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u5b89\u5168\u7a7f\u8d8a\u706b\u707e\u73af\u5883\u7684\u80fd\u529b\u3002\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u5b9e\u65f6\u611f\u77e5\u548c\u7406\u89e3\u706b\u707e\u70ed\u8f90\u5c04\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u914d\u51c6\u6df1\u5ea6\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u83b7\u5f97\u5e26\u6709\u6e29\u5ea6\u503c\u76843D\u70b9\u4e91\uff0c\u8bc6\u522b\u706b\u6e90\uff0c\u5229\u7528\u65af\u7279\u85e9-\u73bb\u5c14\u5179\u66fc\u5b9a\u5f8b\u4f30\u7b97\u7a7a\u57df\u70ed\u8f90\u5c04\uff0c\u6784\u5efa\u8fde\u7eed\u70ed\u8f90\u5c04\u573a\uff0c\u5e76\u5c06\u70ed\u7ea6\u675f\u5d4c\u5165\u5230\u4ee3\u4ef7\u5730\u56fe\u4e2d\u8ba1\u7b97\u5b89\u5168\u8def\u5f84\u3002", "result": "\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u673a\u5668\u4eba\u80fd\u591f\u907f\u5f00\u5371\u9669\u533a\u57df\u5e76\u6210\u529f\u5230\u8fbe\u5bfc\u822a\u76ee\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u90e8\u7f72\u5728\u706b\u707e\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5728\u641c\u6551\u3001\u6d88\u9632\u548c\u5371\u9669\u54c1\u54cd\u5e94\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.19173", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19173", "abs": "https://arxiv.org/abs/2602.19173", "authors": ["Ziwei Kang", "Yizhi Zhou"], "title": "Distributed and Consistent Multi-Robot Visual-Inertial-Ranging Odometry on Lie Groups", "comment": null, "summary": "Reliable localization is a fundamental requirement for multi-robot systems operating in GPS-denied environments. Visual-inertial odometry (VIO) provides lightweight and accurate motion estimation but suffers from cumulative drift in the absence of global references. Ultra-wideband (UWB) ranging offers complementary global observations, yet most existing UWB-aided VIO methods are designed for single-robot scenarios and rely on pre-calibrated anchors, which limits their robustness in practice. This paper proposes a distributed collaborative visual-inertial-ranging odometry (DC-VIRO) framework that tightly fuses VIO and UWB measurements across multiple robots. Anchor positions are explicitly included in the system state to address calibration uncertainty, while shared anchor observations are exploited through inter-robot communication to provide additional geometric constraints. By leveraging a right-invariant error formulation on Lie groups, the proposed approach preserves the observability properties of standard VIO, ensuring estimator consistency. Simulation results with multiple robots demonstrate that DC-VIRO significantly improves localization accuracy and robustness, while simultaneously enabling anchor self-calibration in distributed settings.", "AI": {"tldr": "\u63d0\u51faDC-VIRO\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u5bc6\u878d\u5408\u591a\u673a\u5668\u4eba\u7684\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u548cUWB\u6d4b\u8ddd\u6570\u636e\uff0c\u89e3\u51b3GPS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u6f02\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u951a\u70b9\u81ea\u6821\u51c6\u3002", "motivation": "\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\uff0c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u8981\u53ef\u9760\u7684\u5b9a\u4f4d\u3002\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u867d\u7136\u8f7b\u91cf\u51c6\u786e\u4f46\u5b58\u5728\u7d2f\u79ef\u6f02\u79fb\uff0c\u800cUWB\u6d4b\u8ddd\u63d0\u4f9b\u5168\u5c40\u89c2\u6d4b\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u673a\u5668\u4eba\u8bbe\u8ba1\u4e14\u4f9d\u8d56\u9884\u6821\u51c6\u951a\u70b9\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u9c81\u68d2\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u534f\u4f5c\u89c6\u89c9-\u60ef\u6027-\u6d4b\u8ddd\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u5c06VIO\u548cUWB\u6d4b\u91cf\u7d27\u5bc6\u878d\u5408\uff1b\u5c06\u951a\u70b9\u4f4d\u7f6e\u660e\u786e\u7eb3\u5165\u7cfb\u7edf\u72b6\u6001\u4ee5\u89e3\u51b3\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\uff1b\u901a\u8fc7\u673a\u5668\u4eba\u95f4\u901a\u4fe1\u5171\u4eab\u951a\u70b9\u89c2\u6d4b\u63d0\u4f9b\u989d\u5916\u51e0\u4f55\u7ea6\u675f\uff1b\u91c7\u7528\u674e\u7fa4\u4e0a\u7684\u53f3\u4e0d\u53d8\u8bef\u5dee\u516c\u5f0f\u5316\uff0c\u4fdd\u6301\u6807\u51c6VIO\u7684\u53ef\u89c2\u6d4b\u6027\u3002", "result": "\u591a\u673a\u5668\u4eba\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cDC-VIRO\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u951a\u70b9\u81ea\u6821\u51c6\u3002", "conclusion": "DC-VIRO\u6846\u67b6\u901a\u8fc7\u7d27\u5bc6\u878d\u5408\u591a\u673a\u5668\u4eba\u7684VIO\u548cUWB\u6d4b\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86GPS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u951a\u70b9\u81ea\u6821\u51c6\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5206\u5e03\u5f0f\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19193", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19193", "abs": "https://arxiv.org/abs/2602.19193", "authors": ["Hieu Bui", "Ziyan Gao", "Yuya Hosoda", "Joo-Ho Lee"], "title": "Visual Prompt Guided Unified Pushing Policy", "comment": null, "summary": "As one of the simplest non-prehensile manipulation skills, pushing has been widely studied as an effective means to rearrange objects. Existing approaches, however, typically rely on multi-step push plans composed of pre-defined pushing primitives with limited application scopes, which restrict their efficiency and versatility across different scenarios. In this work, we propose a unified pushing policy that incorporates a lightweight prompting mechanism into a flow matching policy to guide the generation of reactive, multimodal pushing actions. The visual prompt can be specified by a high-level planner, enabling the reuse of the pushing policy across a wide range of planning problems. Experimental results demonstrate that the proposed unified pushing policy not only outperforms existing baselines but also effectively serves as a low-level primitive within a VLM-guided planning framework to solve table-cleaning tasks efficiently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u63a8\u52a8\u7b56\u7565\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63d0\u793a\u673a\u5236\u6307\u5bfc\u751f\u6210\u53cd\u5e94\u5f0f\u3001\u591a\u6a21\u6001\u7684\u63a8\u52a8\u52a8\u4f5c\uff0c\u53ef\u4f5c\u4e3aVLM\u5f15\u5bfc\u89c4\u5212\u6846\u67b6\u4e2d\u7684\u4f4e\u7ea7\u539f\u8bed", "motivation": "\u73b0\u6709\u63a8\u52a8\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7531\u6709\u9650\u5e94\u7528\u8303\u56f4\u9884\u5b9a\u4e49\u63a8\u52a8\u539f\u8bed\u7ec4\u6210\u7684\u591a\u6b65\u63a8\u52a8\u8ba1\u5212\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6548\u7387\u548c\u901a\u7528\u6027", "method": "\u5c06\u8f7b\u91cf\u7ea7\u63d0\u793a\u673a\u5236\u6574\u5408\u5230\u6d41\u5339\u914d\u7b56\u7565\u4e2d\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u63a8\u52a8\u7b56\u7565\uff0c\u89c6\u89c9\u63d0\u793a\u53ef\u7531\u9ad8\u7ea7\u89c4\u5212\u5668\u6307\u5b9a\uff0c\u4f7f\u63a8\u52a8\u7b56\u7565\u80fd\u5728\u5404\u79cd\u89c4\u5212\u95ee\u9898\u4e2d\u91cd\u590d\u4f7f\u7528", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7edf\u4e00\u63a8\u52a8\u7b56\u7565\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fd8\u80fd\u6709\u6548\u4f5c\u4e3aVLM\u5f15\u5bfc\u89c4\u5212\u6846\u67b6\u4e2d\u7684\u4f4e\u7ea7\u539f\u8bed\uff0c\u9ad8\u6548\u89e3\u51b3\u684c\u9762\u6e05\u6d01\u4efb\u52a1", "conclusion": "\u901a\u8fc7\u6574\u5408\u63d0\u793a\u673a\u5236\u7684\u6d41\u5339\u914d\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u63a8\u52a8\u7b56\u7565\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u91cd\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.19260", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19260", "abs": "https://arxiv.org/abs/2602.19260", "authors": ["Timothy Duggan", "Pierrick Lorang", "Hong Lu", "Matthias Scheutz"], "title": "The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption", "comment": "Accepted at the 2026 IEEE International Conference on Robotics & Automation (ICRA 2026)", "summary": "Vision-Language-Action (VLA) models have recently been proposed as a pathway toward generalist robotic policies capable of interpreting natural language and visual inputs to generate manipulation actions. However, their effectiveness and efficiency on structured, long-horizon manipulation tasks remain unclear. In this work, we present a head-to-head empirical comparison between a fine-tuned open-weight VLA model \u03c00 and a neuro-symbolic architecture that combines PDDL-based symbolic planning with learned low-level control. We evaluate both approaches on structured variants of the Towers of Hanoi manipulation task in simulation while measuring both task performance and energy consumption during training and execution. On the 3-block task, the neuro-symbolic model achieves 95% success compared to 34% for the best-performing VLA. The neuro-symbolic model also generalizes to an unseen 4-block variant (78% success), whereas both VLAs fail to complete the task. During training, VLA fine-tuning consumes nearly two orders of magnitude more energy than the neuro-symbolic approach. These results highlight important trade-offs between end-to-end foundation-model approaches and structured reasoning architectures for long-horizon robotic manipulation, emphasizing the role of explicit symbolic structure in improving reliability, data efficiency, and energy efficiency. Code and models are available at https://price-is-not-right.github.io", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6bd4\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e0e\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u5728\u7ed3\u6784\u5316\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u6210\u529f\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u80fd\u6e90\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8eVLA\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u88ab\u63d0\u51fa\u4f5c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u9014\u5f84\uff0c\u4f46\u5176\u5728\u7ed3\u6784\u5316\u3001\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e0e\u4f20\u7edf\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u8fdb\u884c\u5b9e\u8bc1\u6bd4\u8f83\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5934\u5bf9\u5934\u5b9e\u8bc1\u6bd4\u8f83\u65b9\u6cd5\uff1a\u4e00\u65b9\u9762\u4f7f\u7528\u5fae\u8c03\u7684\u5f00\u6e90\u6743\u91cdVLA\u6a21\u578b\u03c00\uff0c\u53e6\u4e00\u65b9\u9762\u91c7\u7528\u7ed3\u5408PDDL\u7b26\u53f7\u89c4\u5212\u4e0e\u5b66\u4e60\u4f4e\u7ea7\u63a7\u5236\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bc4\u4f30\u4e24\u79cd\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u6c49\u8bfa\u5854\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u6d4b\u91cf\u8bad\u7ec3\u548c\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u4efb\u52a1\u6027\u80fd\u548c\u80fd\u6e90\u6d88\u8017\u3002", "result": "\u57283\u5757\u6c49\u8bfa\u5854\u4efb\u52a1\u4e2d\uff0c\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u8fbe\u523095%\u6210\u529f\u7387\uff0c\u800c\u6700\u4f73VLA\u6a21\u578b\u4ec534%\u3002\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u8fd8\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u76844\u5757\u53d8\u4f53\uff0878%\u6210\u529f\u7387\uff09\uff0c\u800c\u4e24\u79cdVLA\u6a21\u578b\u5747\u65e0\u6cd5\u5b8c\u6210\u4efb\u52a1\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cVLA\u5fae\u8c03\u6d88\u8017\u7684\u80fd\u6e90\u6bd4\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u9ad8\u51fa\u8fd1\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u7aef\u5230\u7aef\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u67b6\u6784\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u91cd\u8981\u6743\u8861\uff0c\u5f3a\u8c03\u4e86\u663e\u5f0f\u7b26\u53f7\u7ed3\u6784\u5728\u63d0\u9ad8\u53ef\u9760\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u80fd\u6e90\u6548\u7387\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.19273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19273", "abs": "https://arxiv.org/abs/2602.19273", "authors": ["Abhinav Gandhi", "Shou-Shan Chiang", "Cagdas D. Onal", "Berk Calli"], "title": "3D Shape Control of Extensible Multi-Section Soft Continuum Robots via Visual Servoing", "comment": null, "summary": "In this paper, we propose a novel vision-based control algorithm for regulating the whole body shape of extensible multisection soft continuum manipulators. Contrary to existing vision-based control algorithms in the literature that regulate the robot's end effector pose, our proposed control algorithm regulates the robot's whole body configuration, enabling us to leverage its kinematic redundancy. Additionally, our model-based 2.5D shape visual servoing provides globally stable asymptotic convergence in the robot's 3D workspace compared to the closest works in the literature that report local minima. Unlike existing visual servoing algorithms in the literature, our approach does not require information from proprioceptive sensors, making it suitable for continuum manipulators without such capabilities. Instead, robot state is estimated from images acquired by an external camera that observes the robot's whole body shape and is also utilized to close the shape control loop. Traditionally, visual servoing schemes require an image of the robot at its reference pose to generate the reference features. In this work, we utilize an inverse kinematics solver to generate reference features for the desired robot configuration and do not require images of the robot at the reference. Experiments are performed on a multisection continuum manipulator demonstrating the controller's capability to regulate the robot's whole body shape while precisely positioning the robot's end effector. Results validate our controller's ability to regulate the shape of continuum robots while demonstrating a smooth transient response and a steady-state error within 1 mm. Proof-of-concept object manipulation experiments including stacking, pouring, and pulling tasks are performed to demonstrate our controller's applicability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u8f6f\u4f53\u8fde\u7eed\u673a\u68b0\u81c2\u5168\u8eab\u5f62\u72b6\u63a7\u5236\u7b97\u6cd5\uff0c\u65e0\u9700\u672c\u4f53\u611f\u77e5\u4f20\u611f\u5668\uff0c\u5229\u7528\u5916\u90e8\u6444\u50cf\u5934\u5b9e\u73b0\u5168\u5c40\u7a33\u5b9a\u7684\u5f62\u72b6\u63a7\u5236", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a7\u5236\u7b97\u6cd5\u4e3b\u8981\u8c03\u8282\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8f6f\u4f53\u8fde\u7eed\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u5b66\u5197\u4f59\u6027\uff1b\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u4f20\u611f\u5668", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u76842.5D\u5f62\u72b6\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u7b97\u6cd5\uff0c\u5229\u7528\u5916\u90e8\u6444\u50cf\u5934\u83b7\u53d6\u673a\u5668\u4eba\u5168\u8eab\u5f62\u72b6\u56fe\u50cf\uff0c\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u751f\u6210\u53c2\u8003\u7279\u5f81\uff0c\u65e0\u9700\u53c2\u8003\u59ff\u6001\u56fe\u50cf", "result": "\u5728\u591a\u6bb5\u8fde\u7eed\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\u4e86\u63a7\u5236\u5668\u80fd\u7cbe\u786e\u8c03\u8282\u5168\u8eab\u5f62\u72b6\uff0c\u672b\u7aef\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e1\u6beb\u7c73\uff0c\u5b9e\u73b0\u4e86\u5806\u53e0\u3001\u503e\u5012\u3001\u62c9\u52a8\u7b49\u6982\u5ff5\u9a8c\u8bc1\u4efb\u52a1", "conclusion": "\u8be5\u89c6\u89c9\u63a7\u5236\u7b97\u6cd5\u80fd\u6709\u6548\u8c03\u8282\u8f6f\u4f53\u8fde\u7eed\u673a\u68b0\u81c2\u7684\u5168\u8eab\u5f62\u72b6\uff0c\u65e0\u9700\u672c\u4f53\u611f\u77e5\u4f20\u611f\u5668\uff0c\u5177\u6709\u5168\u5c40\u7a33\u5b9a\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u65e0\u4f20\u611f\u5668\u80fd\u529b\u7684\u8fde\u7eed\u673a\u68b0\u81c2"}}
{"id": "2602.19304", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19304", "abs": "https://arxiv.org/abs/2602.19304", "authors": ["Haojun Shi", "Suyu Ye", "Katherine M. Guerrerio", "Jianzhi Shen", "Yifan Yin", "Daniel Khashabi", "Chien-Ming Huang", "Tianmin Shu"], "title": "Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation", "comment": null, "summary": "Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In this work, we focus on path-level cooperation in which agents must adapt their paths to one another in order to avoid collisions or perform physical collaboration such as joint carrying. In particular, we propose a safe and interpretable multimodal path planning method, CaPE (Code as Path Editor), which generates and updates path plans for an agent based on the environment and language communication from other agents. CaPE leverages a vision-language model (VLM) to synthesize a path editing program verified by a model-based planner, grounding communication to path plan updates in a safe and interpretable way. We evaluate our approach in diverse simulated and real-world scenarios, including multi-robot and human-robot cooperation in autonomous driving, household, and joint carrying tasks. Experimental results demonstrate that CaPE can be integrated into different robotic systems as a plug-and-play module, greatly enhancing a robot's ability to align its plan to language communication from other robots or humans. We also show that the combination of the VLM-based path editing program synthesis and model-based planning safety enables robots to achieve open-ended cooperation while maintaining safety and interpretability.", "AI": {"tldr": "CaPE\uff1a\u57fa\u4e8e\u4ee3\u7801\u7f16\u8f91\u7684\u591a\u6a21\u6001\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u901a\u4fe1\u5b9e\u73b0\u5b89\u5168\u53ef\u89e3\u91ca\u7684\u673a\u5668\u4eba\u534f\u4f5c", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\uff0c\u5f53\u667a\u80fd\u4f53\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u5f7c\u6b64\u610f\u56fe\u65f6\uff0c\u8bed\u8a00\u901a\u4fe1\u5bf9\u786e\u4fdd\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u8def\u5f84\u7ea7\u534f\u4f5c\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u6839\u636e\u73af\u5883\u548c\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u8bed\u8a00\u901a\u4fe1\u8c03\u6574\u8def\u5f84\u4ee5\u907f\u514d\u78b0\u649e\u6216\u5b8c\u6210\u7269\u7406\u534f\u4f5c\u4efb\u52a1\u3002", "method": "\u63d0\u51faCaPE\uff08Code as Path Editor\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5408\u6210\u8def\u5f84\u7f16\u8f91\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u5c06\u8bed\u8a00\u901a\u4fe1\u5b89\u5168\u5730\u3001\u53ef\u89e3\u91ca\u5730\u6620\u5c04\u5230\u8def\u5f84\u8ba1\u5212\u66f4\u65b0\u4e2d\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u591a\u79cd\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u3001\u5bb6\u5ead\u73af\u5883\u548c\u8054\u5408\u642c\u8fd0\u4efb\u52a1\u4e2d\u7684\u591a\u673a\u5668\u4eba\u53ca\u4eba\u673a\u534f\u4f5c\u3002CaPE\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u96c6\u6210\u5230\u4e0d\u540c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u6839\u636e\u8bed\u8a00\u901a\u4fe1\u8c03\u6574\u8ba1\u5212\u7684\u80fd\u529b\u3002", "conclusion": "CaPE\u7ed3\u5408\u4e86VLM\u7684\u8def\u5f84\u7f16\u8f91\u7a0b\u5e8f\u5408\u6210\u548c\u57fa\u4e8e\u6a21\u578b\u89c4\u5212\u7684\u5b89\u5168\u6027\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u4fdd\u6301\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5f00\u653e\u5f0f\u534f\u4f5c\u3002"}}
{"id": "2602.19308", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.19308", "abs": "https://arxiv.org/abs/2602.19308", "authors": ["Hardik Shah", "Erica Tevere", "Deegan Atha", "Marcel Kaufmann", "Shehryar Khattak", "Manthan Patel", "Marco Hutter", "Jonas Frey", "Patrick Spieler"], "title": "WildOS: Open-Vocabulary Object Search in the Wild", "comment": "28 pages, 16 figures, 2 tables", "summary": "Autonomous navigation in complex, unstructured outdoor environments requires robots to operate over long ranges without prior maps and limited depth sensing. In such settings, relying solely on geometric frontiers for exploration is often insufficient. In such settings, the ability to reason semantically about where to go and what is safe to traverse is crucial for robust, efficient exploration. This work presents WildOS, a unified system for long-range, open-vocabulary object search that combines safe geometric exploration with semantic visual reasoning. WildOS builds a sparse navigation graph to maintain spatial memory, while utilizing a foundation-model-based vision module, ExploRFM, to score frontier nodes of the graph. ExploRFM simultaneously predicts traversability, visual frontiers, and object similarity in image space, enabling real-time, onboard semantic navigation tasks. The resulting vision-scored graph enables the robot to explore semantically meaningful directions while ensuring geometric safety. Furthermore, we introduce a particle-filter-based method for coarse localization of the open-vocabulary target query, that estimates candidate goal positions beyond the robot's immediate depth horizon, enabling effective planning toward distant goals. Extensive closed-loop field experiments across diverse off-road and urban terrains demonstrate that WildOS enables robust navigation, significantly outperforming purely geometric and purely vision-based baselines in both efficiency and autonomy. Our results highlight the potential of vision foundation models to drive open-world robotic behaviors that are both semantically informed and geometrically grounded. Project Page: https://leggedrobotics.github.io/wildos/", "AI": {"tldr": "WildOS\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u8ddd\u79bb\u5f00\u653e\u8bcd\u6c47\u7269\u4f53\u641c\u7d22\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b89\u5168\u51e0\u4f55\u63a2\u7d22\u4e0e\u8bed\u4e49\u89c6\u89c9\u63a8\u7406\uff0c\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5bfc\u822a\u3002", "motivation": "\u5728\u590d\u6742\u65e0\u7ed3\u6784\u7684\u6237\u5916\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u957f\u8ddd\u79bb\u8fd0\u884c\u4e14\u6ca1\u6709\u5148\u9a8c\u5730\u56fe\u548c\u6709\u9650\u7684\u6df1\u5ea6\u611f\u77e5\u3002\u4ec5\u4f9d\u8d56\u51e0\u4f55\u8fb9\u754c\u8fdb\u884c\u63a2\u7d22\u5f80\u5f80\u4e0d\u8db3\uff0c\u9700\u8981\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u6765\u786e\u5b9a\u5b89\u5168\u53ef\u901a\u884c\u7684\u8def\u5f84\u548c\u65b9\u5411\u3002", "method": "WildOS\u6784\u5efa\u7a00\u758f\u5bfc\u822a\u56fe\u7ef4\u62a4\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u5229\u7528\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u89c6\u89c9\u6a21\u5757ExploRFM\u5bf9\u56fe\u7684\u8fb9\u754c\u8282\u70b9\u8fdb\u884c\u8bc4\u5206\u3002ExploRFM\u540c\u65f6\u9884\u6d4b\u53ef\u901a\u884c\u6027\u3001\u89c6\u89c9\u8fb9\u754c\u548c\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u7269\u4f53\u76f8\u4f3c\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u7684\u65b9\u6cd5\u5bf9\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u67e5\u8be2\u8fdb\u884c\u7c97\u7565\u5b9a\u4f4d\uff0c\u4f30\u8ba1\u8d85\u51fa\u673a\u5668\u4eba\u6df1\u5ea6\u89c6\u91ce\u7684\u5019\u9009\u76ee\u6807\u4f4d\u7f6e\u3002", "result": "\u5728\u591a\u79cd\u8d8a\u91ce\u548c\u57ce\u5e02\u5730\u5f62\u4e2d\u7684\u5e7f\u6cdb\u95ed\u73af\u73b0\u573a\u5b9e\u9a8c\u8868\u660e\uff0cWildOS\u5b9e\u73b0\u4e86\u9c81\u68d2\u5bfc\u822a\uff0c\u5728\u6548\u7387\u548c\u81ea\u4e3b\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7eaf\u51e0\u4f55\u548c\u7eaf\u89c6\u89c9\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u9a71\u52a8\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8fd9\u4e9b\u884c\u4e3a\u65e2\u5177\u6709\u8bed\u4e49\u4fe1\u606f\u53c8\u57fa\u4e8e\u51e0\u4f55\u57fa\u7840\uff0c\u4e3a\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19313", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19313", "abs": "https://arxiv.org/abs/2602.19313", "authors": ["Shirui Chen", "Cole Harrison", "Ying-Chun Lee", "Angela Jin Yang", "Zhongzheng Ren", "Lillian J. Ratliff", "Jiafei Duan", "Dieter Fox", "Ranjay Krishna"], "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "comment": null, "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "AI": {"tldr": "TOPReward\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u7684\u65f6\u95f4\u4ef7\u503c\u51fd\u6570\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u4e16\u754c\u77e5\u8bc6\u6765\u4f30\u8ba1\u673a\u5668\u4eba\u4efb\u52a1\u8fdb\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u548c\u771f\u5b9e\u4e16\u754c\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65f6\u95f4\u4ef7\u503c\u51fd\u6570\u96be\u4ee5\u6cdb\u5316\u5230\u8bad\u7ec3\u57df\u4e4b\u5916\uff0c\u9700\u8981\u5f00\u53d1\u901a\u7528\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\u3002", "method": "\u63d0\u51faTOPReward\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8token\u5bf9\u6570\u6982\u7387\u4e2d\u63d0\u53d6\u4efb\u52a1\u8fdb\u5ea6\uff0c\u800c\u4e0d\u662f\u50cf\u5148\u524d\u65b9\u6cd5\u90a3\u6837\u63d0\u793a\u6a21\u578b\u76f4\u63a5\u8f93\u51fa\u8fdb\u5ea6\u503c\uff0c\u907f\u514d\u4e86\u6570\u503c\u8868\u793a\u9519\u8bef\u3002", "result": "\u5728130\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u548c\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0cTOPReward\u5728Qwen3-VL\u4e0a\u8fbe\u52300.947\u7684\u5e73\u5747\u4ef7\u503c\u987a\u5e8f\u76f8\u5173\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u63a5\u8fd1\u96f6\u76f8\u5173\u6027\u7684\u6700\u5148\u8fdbGVL\u57fa\u7ebf\u3002", "conclusion": "TOPReward\u4f5c\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u4e0b\u6e38\u5e94\u7528\u5982\u6210\u529f\u68c0\u6d4b\u548c\u5956\u52b1\u5bf9\u9f50\u7684\u884c\u4e3a\u514b\u9686\uff0c\u4e3a\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.19315", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19315", "abs": "https://arxiv.org/abs/2602.19315", "authors": ["Victor-Alexandru Darvariu", "Charlotte Z. Reed", "Jan Stratmann", "Bruno Lacerda", "Benjamin Allsup", "Stephen Woodward", "Elizabeth Siddle", "Trishna Saeharaseelan", "Owain Jones", "Dan Jones", "Tobias Ferreira", "Chloe Baker", "Kevin Chaplin", "James Kirk", "Ashley Morris", "Ryan Patmore", "Jeff Polton", "Charlotte Williams", "Alexandra Kokkinaki", "Alvaro Lorenzo Lopez", "Justin J. H. Buck", "Nick Hawes"], "title": "Online Navigation Planning for Long-term Autonomous Operation of Underwater Gliders", "comment": null, "summary": "Underwater glider robots have become an indispensable tool for ocean sampling. Although stakeholders are calling for tools to manage increasingly large fleets of gliders, successful autonomous long-term deployments have thus far been scarce, which hints at a lack of suitable methodologies and systems. In this work, we formulate glider navigation planning as a stochastic shortest-path Markov Decision Process and propose a sample-based online planner based on Monte Carlo Tree Search. Samples are generated by a physics-informed simulator that captures uncertain execution of controls and ocean current forecasts while remaining computationally tractable. The simulator parameters are fitted using historical glider data. We integrate these methods into an autonomous command-and-control system for Slocum gliders that enables closed-loop replanning at each surfacing. The resulting system was validated in two field deployments in the North Sea totalling approximately 3 months and 1000 km of autonomous operation. Results demonstrate improved efficiency compared to straight-to-goal navigation and show the practicality of sample-based planning for long-term marine autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u5728\u7ebf\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u6c34\u4e0b\u6ed1\u7fd4\u673a\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u6a21\u62df\u5668\u5904\u7406\u63a7\u5236\u4e0d\u786e\u5b9a\u6027\u548c\u6d0b\u6d41\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u5317\u6d77\u4e0a\u4e3a\u671f3\u4e2a\u6708\u30011000\u516c\u91cc\u7684\u81ea\u4e3b\u90e8\u7f72\u9a8c\u8bc1\u3002", "motivation": "\u6c34\u4e0b\u6ed1\u7fd4\u673a\u5df2\u6210\u4e3a\u6d77\u6d0b\u91c7\u6837\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u5927\u578b\u673a\u961f\u81ea\u4e3b\u957f\u671f\u90e8\u7f72\u7684\u6210\u529f\u6848\u4f8b\u5f88\u5c11\uff0c\u7f3a\u4e4f\u5408\u9002\u7684\u65b9\u6cd5\u8bba\u548c\u7cfb\u7edf\u3002\u5229\u76ca\u76f8\u5173\u8005\u9700\u8981\u7ba1\u7406\u65e5\u76ca\u5e9e\u5927\u7684\u6ed1\u7fd4\u673a\u673a\u961f\u7684\u5de5\u5177\u3002", "method": "\u5c06\u6ed1\u7fd4\u673a\u5bfc\u822a\u89c4\u5212\u5efa\u6a21\u4e3a\u968f\u673a\u6700\u77ed\u8def\u5f84\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u6837\u672c\u5728\u7ebf\u89c4\u5212\u5668\u3002\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u6a21\u62df\u5668\u751f\u6210\u6837\u672c\uff0c\u8be5\u6a21\u62df\u5668\u80fd\u6355\u6349\u63a7\u5236\u6267\u884c\u4e0d\u786e\u5b9a\u6027\u548c\u6d0b\u6d41\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u3002\u6a21\u62df\u5668\u53c2\u6570\u901a\u8fc7\u5386\u53f2\u6ed1\u7fd4\u673a\u6570\u636e\u62df\u5408\u3002\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u96c6\u6210\u5230Slocum\u6ed1\u7fd4\u673a\u7684\u81ea\u4e3b\u6307\u6325\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u6bcf\u6b21\u4e0a\u6d6e\u65f6\u7684\u95ed\u73af\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u7cfb\u7edf\u5728\u5317\u6d77\u4e0a\u8fdb\u884c\u4e86\u4e24\u6b21\u73b0\u573a\u90e8\u7f72\uff0c\u603b\u8ba1\u7ea63\u4e2a\u6708\u548c1000\u516c\u91cc\u7684\u81ea\u4e3b\u64cd\u4f5c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u76f4\u7ebf\u5bfc\u822a\u76f8\u6bd4\uff0c\u6548\u7387\u6709\u6240\u63d0\u9ad8\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u6837\u672c\u7684\u89c4\u5212\u5728\u957f\u671f\u6d77\u6d0b\u81ea\u4e3b\u6027\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u6837\u672c\u5728\u7ebf\u89c4\u5212\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u6c34\u4e0b\u6ed1\u7fd4\u673a\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u6a21\u62df\u5668\u548c\u5386\u53f2\u6570\u636e\u62df\u5408\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u81ea\u4e3b\u90e8\u7f72\uff0c\u4e3a\u6d77\u6d0b\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19346", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19346", "abs": "https://arxiv.org/abs/2602.19346", "authors": ["Erik Garcia Oyono", "Jialin Lin", "Dandan Zhang"], "title": "Design and Control of Modular Magnetic Millirobots for Multimodal Locomotion and Shape Reconfiguration", "comment": "Accepted by 2026 ICRA", "summary": "Modular small-scale robots offer the potential for on-demand assembly and disassembly, enabling task-specific adaptation in dynamic and constrained environments. However, existing modular magnetic platforms often depend on workspace collisions for reconfiguration, employ bulky three-dimensional electromagnetic systems, and lack robust single-module control, which limits their applicability in biomedical settings. In this work, we present a modular magnetic millirobotic platform comprising three cube-shaped modules with embedded permanent magnets, each designed for a distinct functional role: a free module that supports self-assembly and reconfiguration, a fixed module that enables flip-and-walk locomotion, and a gripper module for cargo manipulation. Locomotion and reconfiguration are actuated by programmable combinations of time-varying two-dimensional uniform and gradient magnetic field inputs. Experiments demonstrate closed-loop navigation using real-time vision feedback and A* path planning, establishing robust single-module control capabilities. Beyond locomotion, the system achieves self-assembly, multimodal transformations, and disassembly at low field strengths. Chain-to-gripper transformations succeeded in 90% of trials, while chain-to-square transformations were less consistent, underscoring the role of module geometry in reconfiguration reliability. These results establish a versatile modular robotic platform capable of multimodal behavior and robust control, suggesting a promising pathway toward scalable and adaptive task execution in confined environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u78c1\u6027\u6beb\u7c73\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5305\u542b\u4e09\u79cd\u529f\u80fd\u6a21\u5757\uff0c\u901a\u8fc7\u4e8c\u7ef4\u78c1\u573a\u63a7\u5236\u5b9e\u73b0\u8fd0\u52a8\u3001\u91cd\u6784\u548c\u8d27\u7269\u64cd\u4f5c\uff0c\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u591a\u6a21\u6001\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u5757\u5316\u78c1\u6027\u673a\u5668\u4eba\u5e73\u53f0\u4f9d\u8d56\u5de5\u4f5c\u7a7a\u95f4\u78b0\u649e\u8fdb\u884c\u91cd\u6784\uff0c\u4f7f\u7528\u7b28\u91cd\u7684\u4e09\u7ef4\u7535\u78c1\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u5355\u6a21\u5757\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5728\u751f\u7269\u533b\u5b66\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u7c7b\u7acb\u65b9\u4f53\u6a21\u5757\uff08\u81ea\u7531\u6a21\u5757\u3001\u56fa\u5b9a\u6a21\u5757\u3001\u5939\u6301\u6a21\u5757\uff09\uff0c\u901a\u8fc7\u7f16\u7a0b\u7ec4\u5408\u7684\u65f6\u53d8\u4e8c\u7ef4\u5747\u5300\u548c\u68af\u5ea6\u78c1\u573a\u8f93\u5165\u9a71\u52a8\u8fd0\u52a8\u548c\u91cd\u6784\uff0c\u7ed3\u5408\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\u548cA*\u8def\u5f84\u89c4\u5212\u5b9e\u73b0\u95ed\u73af\u5bfc\u822a\u3002", "result": "\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5355\u6a21\u5757\u63a7\u5236\u3001\u81ea\u7ec4\u88c5\u3001\u591a\u6a21\u6001\u53d8\u6362\u548c\u4f4e\u573a\u5f3a\u4e0b\u7684\u62c6\u5378\u3002\u94fe\u5230\u5939\u6301\u5668\u53d8\u6362\u6210\u529f\u738790%\uff0c\u94fe\u5230\u65b9\u5f62\u53d8\u6362\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u8868\u660e\u6a21\u5757\u51e0\u4f55\u5f62\u72b6\u5f71\u54cd\u91cd\u6784\u53ef\u9760\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u6a21\u5757\u5316\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5177\u5907\u591a\u6a21\u6001\u884c\u4e3a\u548c\u7a33\u5065\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2602.19359", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19359", "abs": "https://arxiv.org/abs/2602.19359", "authors": ["Kevin Qiu", "Yu Zhang", "Marek Cygan", "Josie Hughes"], "title": "Vid2Sid: Videos Can Help Close the Sim2Real Gap", "comment": null, "summary": "Calibrating a robot simulator's physics parameters (friction, damping, material stiffness) to match real hardware is often done by hand or with black-box optimizers that reduce error but cannot explain which physical discrepancies drive the error. When sensing is limited to external cameras, the problem is further compounded by perception noise and the absence of direct force or state measurements. We present Vid2Sid, a video-driven system identification pipeline that couples foundation-model perception with a VLM-in-the-loop optimizer that analyzes paired sim-real videos, diagnoses concrete mismatches, and proposes physics parameter updates with natural language rationales. We evaluate our approach on a tendon-actuated finger (rigid-body dynamics in MuJoCo) and a deformable continuum tentacle (soft-body dynamics in PyElastica). On sim2real holdout controls unseen during training, Vid2Sid achieves the best average rank across all settings, matching or exceeding black-box optimizers while uniquely providing interpretable reasoning at each iteration. Sim2sim validation confirms that Vid2Sid recovers ground-truth parameters most accurately (mean relative error under 13\\% vs. 28--98\\%), and ablation analysis reveals three calibration regimes. VLM-guided optimization excels when perception is clean and the simulator is expressive, while model-class limitations bound performance in more challenging settings.", "AI": {"tldr": "Vid2Sid\u662f\u4e00\u4e2a\u89c6\u9891\u9a71\u52a8\u7684\u7cfb\u7edf\u8bc6\u522b\u7ba1\u9053\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u611f\u77e5\u548cVLM-in-the-loop\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5206\u6790\u4eff\u771f-\u771f\u5b9e\u914d\u5bf9\u89c6\u9891\u6765\u8bca\u65ad\u7269\u7406\u53c2\u6570\u4e0d\u5339\u914d\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6821\u51c6\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u4eff\u771f\u5668\u7269\u7406\u53c2\u6570\u6821\u51c6\u901a\u5e38\u624b\u52a8\u5b8c\u6210\u6216\u4f7f\u7528\u9ed1\u76d2\u4f18\u5316\u5668\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c11\u8bef\u5dee\u4f46\u65e0\u6cd5\u89e3\u91ca\u54ea\u4e9b\u7269\u7406\u5dee\u5f02\u5bfc\u81f4\u4e86\u8bef\u5dee\u3002\u5728\u4ec5\u4f7f\u7528\u5916\u90e8\u76f8\u673a\u611f\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u95ee\u9898\u8fdb\u4e00\u6b65\u590d\u6742\u5316\uff0c\u56e0\u4e3a\u5b58\u5728\u611f\u77e5\u566a\u58f0\u4e14\u7f3a\u4e4f\u76f4\u63a5\u7684\u529b\u6216\u72b6\u6001\u6d4b\u91cf\u3002", "method": "\u63d0\u51faVid2Sid\u7cfb\u7edf\u8bc6\u522b\u7ba1\u9053\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u611f\u77e5\u548cVLM-in-the-loop\u4f18\u5316\u5668\u3002\u7cfb\u7edf\u5206\u6790\u914d\u5bf9\u4eff\u771f-\u771f\u5b9e\u89c6\u9891\uff0c\u8bca\u65ad\u5177\u4f53\u4e0d\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u63d0\u51fa\u7269\u7406\u53c2\u6570\u66f4\u65b0\u3002\u5728MuJoCo\u4e2d\u7684\u808c\u8171\u9a71\u52a8\u624b\u6307\uff08\u521a\u4f53\u52a8\u529b\u5b66\uff09\u548cPyElastica\u4e2d\u7684\u53ef\u53d8\u5f62\u8fde\u7eed\u4f53\u89e6\u624b\uff08\u8f6f\u4f53\u52a8\u529b\u5b66\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684sim2real\u4fdd\u6301\u63a7\u5236\u6d4b\u8bd5\u4e2d\uff0cVid2Sid\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u4f73\u5e73\u5747\u6392\u540d\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u9ed1\u76d2\u4f18\u5316\u5668\uff0c\u540c\u65f6\u72ec\u7279\u5730\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002Sim2sim\u9a8c\u8bc1\u786e\u8ba4Vid2Sid\u6700\u51c6\u786e\u5730\u6062\u590d\u5730\u9762\u771f\u5b9e\u53c2\u6570\uff08\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e13% vs. 28-98%\uff09\u3002\u6d88\u878d\u5206\u6790\u63ed\u793a\u4e86\u4e09\u79cd\u6821\u51c6\u673a\u5236\u3002", "conclusion": "\u5f53\u611f\u77e5\u6e05\u6670\u4e14\u4eff\u771f\u5668\u8868\u8fbe\u80fd\u529b\u8db3\u591f\u65f6\uff0cVLM\u5f15\u5bfc\u7684\u4f18\u5316\u8868\u73b0\u51fa\u8272\uff0c\u800c\u6a21\u578b\u7c7b\u522b\u9650\u5236\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u4e2d\u9650\u5236\u4e86\u6027\u80fd\u3002Vid2Sid\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7269\u7406\u53c2\u6570\u6821\u51c6\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u9ed1\u76d2\u4f18\u5316\u5668\u3002"}}
{"id": "2602.19372", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19372", "abs": "https://arxiv.org/abs/2602.19372", "authors": ["Yanting Yang", "Shenyuan Gao", "Qingwen Bu", "Li Chen", "Dimitris N. Metaxas"], "title": "Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization", "comment": "ICRA 2026", "summary": "Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u72b6\u6001\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\u89e3\u8026\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u52a8\u4f5c\u8ba1\u5212\u7684\u4f18\u52bf\u503c\uff0c\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u8bc4\u8bba\u5bb6\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u6ce2\u675f\u641c\u7d22\u63a2\u7d22\u591a\u4e2a\u672a\u6765\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u5c04\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u4ece\u566a\u58f0\u9884\u6d4b\u4e2d\u9690\u5f0f\u5b66\u4e60\u72b6\u6001\u4ef7\u503c\uff0c\u6548\u7387\u4f4e\u4e14\u4e0d\u51c6\u786e\uff1b2\uff09\u53ea\u8bc4\u4f30\u5355\u4e00\u8d2a\u5a6a\u672a\u6765\uff1b3\uff09\u63a8\u7406\u5ef6\u8fdf\u5927\u3002\u9700\u8981\u66f4\u76f4\u63a5\u3001\u7ec6\u7c92\u5ea6\u7684\u76d1\u7763\u4fe1\u53f7\u6765\u652f\u6301\u9c81\u68d2\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u72b6\u6001\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\u89e3\u8026\u3002\u663e\u5f0f\u5efa\u6a21\u52a8\u4f5c\u8ba1\u5212\u7684\u4f18\u52bf\uff08\u76ee\u6807\u8ddd\u79bb\u51cf\u5c11\u91cf\uff09\uff0c\u4f7f\u7528\u53ef\u6269\u5c55\u8bc4\u8bba\u5bb6\u4f30\u8ba1\u3002\u91c7\u7528\u6ce2\u675f\u641c\u7d22\u63a2\u7d22\u591a\u4e2a\u672a\u6765\u8def\u5f84\uff0c\u5728\u89e3\u7801\u65f6\u805a\u5408\u4ee5\u5efa\u6a21\u671f\u671b\u957f\u671f\u56de\u62a5\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u7f6e\u4fe1\u5ea6\u89e6\u53d1\u5668\uff0c\u5728\u76f4\u63a5\u9884\u6d4b\u53ef\u9760\u65f6\u63d0\u524d\u9000\u51fa\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8c03\u7528\u53cd\u5c04\u3002", "result": "\u5728\u591a\u6837\u672a\u89c1\u7684\u591a\u9636\u6bb5\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u534724.6%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1156.5%\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u72b6\u6001\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\u3001\u663e\u5f0f\u5efa\u6a21\u52a8\u4f5c\u4f18\u52bf\u3001\u591a\u8def\u5f84\u63a2\u7d22\u548c\u81ea\u9002\u5e94\u53cd\u5c04\u89e6\u53d1\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.19400", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19400", "abs": "https://arxiv.org/abs/2602.19400", "authors": ["Tamil Selvan Gurunathan", "Aryya Gangopadhyay"], "title": "Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration", "comment": null, "summary": "We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, \u03b8)), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.", "AI": {"tldr": "\u5c06\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u586b\u5145\u5148\u9a8c\u96c6\u6210\u5230\u53bb\u4e2d\u5fc3\u5316\u591a\u673a\u5668\u4eba\u5b66\u4e60\u4e0e\u6267\u884c\u4e2d\u7684\u8986\u76d6\u6846\u67b6\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7d22\u5f15\u589e\u5f3aDQN\u548cPPO\u7b97\u6cd5\uff0c\u63d0\u9ad8\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u63a2\u7d22\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u591a\u673a\u5668\u4eba\u8986\u76d6\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u5197\u4f59\u5ea6\u9ad8\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u6784\u5316\u63a2\u7d22\u3001\u51cf\u5c11\u5197\u4f59\u7684\u6846\u67b6\u6765\u63d0\u9ad8\u8986\u76d6\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8986\u76d6\u6846\u67b6\uff0c\u5c06\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u586b\u5145\u5148\u9a8c\u96c6\u6210\u5230\u53bb\u4e2d\u5fc3\u5316\u591a\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u4e2d\u3002\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7d22\u5f15\u589e\u5f3aDQN\u548cPPO\u7b97\u6cd5\uff0c\u7ed3\u6784\u5316\u63a2\u7d22\u8fc7\u7a0b\u3002\u5f00\u53d1\u822a\u70b9\u63a5\u53e3\uff0c\u5c06\u5e0c\u5c14\u4f2f\u7279\u6392\u5e8f\u8f6c\u6362\u4e3a\u66f2\u7387\u6709\u754c\u3001\u65f6\u95f4\u53c2\u6570\u5316\u7684SE(2)\u8f68\u8ff9\uff0c\u786e\u4fdd\u5728\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u4e0a\u7684\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u591a\u673a\u5668\u4eba\u7f51\u683c\u8986\u76d6\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8986\u76d6\u6548\u7387\u3001\u5197\u4f59\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u4f18\u4e8eDQN/PPO\u57fa\u7ebf\u3002\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u817f\u5f0f\u673a\u5668\u4eba\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u53ef\u9760\u6267\u884c\u751f\u6210\u7684\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4f4e\u5197\u4f59\u8986\u76d6\u3002", "conclusion": "\u51e0\u4f55\u5148\u9a8c\uff08\u7279\u522b\u662f\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u586b\u5145\uff09\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7fa4\u673a\u5668\u4eba\u548c\u817f\u5f0f\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u591a\u673a\u5668\u4eba\u8986\u76d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19518", "abs": "https://arxiv.org/abs/2602.19518", "authors": ["Nabanita Dash", "Ayush Kaura", "Shivam Singh", "Ramandeep Singh", "Snehasis Banerjee", "Mohan Sridharan", "K. Madhava Krishna"], "title": "Anticipate, Adapt, Act: A Hybrid Framework for Task Planning", "comment": "Accepted at IEEE European Conference on Mobile Robots (ECMR)", "summary": "Anticipating and adapting to failures is a key capability robots need to collaborate effectively with humans in complex domains. This continues to be a challenge despite the impressive performance of state of the art AI planning systems and Large Language Models (LLMs) because of the uncertainty associated with the tasks and their outcomes. Toward addressing this challenge, we present a hybrid framework that integrates the generic prediction capabilities of an LLM with the probabilistic sequential decision-making capability of Relational Dynamic Influence Diagram Language. For any given task, the robot reasons about the task and the capabilities of the human attempting to complete it; predicts potential failures due to lack of ability (in the human) or lack of relevant domain objects; and executes actions to prevent such failures or recover from them. Experimental evaluation in the VirtualHome 3D simulation environment demonstrates substantial improvement in performance compared with state of the art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u5c06LLM\u7684\u901a\u7528\u9884\u6d4b\u80fd\u529b\u4e0eRDDL\u7684\u6982\u7387\u5e8f\u5217\u51b3\u7b56\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u9884\u6d4b\u548c\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u6f5c\u5728\u5931\u8d25\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709AI\u89c4\u5212\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u9886\u57df\u4e0e\u4eba\u7c7b\u534f\u4f5c\u65f6\uff0c\u673a\u5668\u4eba\u9884\u6d4b\u548c\u9002\u5e94\u5931\u8d25\u7684\u80fd\u529b\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4efb\u52a1\u53ca\u5176\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u9884\u6d4b\u80fd\u529b\u548c\u5173\u7cfb\u52a8\u6001\u5f71\u54cd\u56fe\u8bed\u8a00\u7684\u6982\u7387\u5e8f\u5217\u51b3\u7b56\u80fd\u529b\u3002\u673a\u5668\u4eba\u80fd\u591f\u63a8\u7406\u4efb\u52a1\u548c\u4eba\u7c7b\u80fd\u529b\uff0c\u9884\u6d4b\u7531\u4e8e\u4eba\u7c7b\u80fd\u529b\u4e0d\u8db3\u6216\u9886\u57df\u5bf9\u8c61\u7f3a\u5931\u5bfc\u81f4\u7684\u6f5c\u5728\u5931\u8d25\uff0c\u5e76\u6267\u884c\u9884\u9632\u6216\u6062\u590d\u884c\u52a8\u3002", "result": "\u5728VirtualHome 3D\u4eff\u771f\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u9884\u6d4b\u548c\u9002\u5e94\u5931\u8d25\u7684\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u9886\u57df\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19538", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19538", "abs": "https://arxiv.org/abs/2602.19538", "authors": ["Arundhati Banerjee", "Jeff Schneider"], "title": "Cost-Aware Diffusion Active Search", "comment": "In submission", "summary": "Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thompson sampling based myopic, greedy approaches for agents to actively decide query or search locations when the number of targets is unknown. Decision making algorithms in such partially observable environments have also shown that agents capable of lookahead over a finite horizon outperform myopic policies for active search. Unfortunately, lookahead algorithms typically rely on building a computationally expensive search tree that is simulated and updated based on the agent's observations and a model of the environment dynamics. Instead, in this work, we leverage the sequence modeling abilities of diffusion models to sample lookahead action sequences that balance the exploration-exploitation trade-off for active search without building an exhaustive search tree. We identify the optimism bias in prior diffusion based reinforcement learning approaches when applied to the active search setting and propose mitigating solutions for efficient cost-aware decision making with both single and multi-agent teams. Our proposed algorithm outperforms standard baselines in offline reinforcement learning in terms of full recovery rate and is computationally more efficient than tree search in cost-aware active decision making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e3b\u52a8\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u524d\u77bb\u52a8\u4f5c\u5e8f\u5217\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6811\u641c\u7d22\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u56e2\u961f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6210\u672c\u611f\u77e5\u51b3\u7b56\u3002", "motivation": "\u4e3b\u52a8\u641c\u7d22\u9700\u8981\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fdb\u884c\u5728\u7ebf\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u548cThompson\u91c7\u6837\u7684\u8fd1\u89c6\u8d2a\u5a6a\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff1b\u6709\u9650\u89c6\u91ce\u524d\u77bb\u7b97\u6cd5\u867d\u7136\u6027\u80fd\u66f4\u597d\uff0c\u4f46\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u641c\u7d22\u6811\u6784\u5efa\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u6709\u6548\u524d\u77bb\u51b3\u7b56\u53c8\u907f\u514d\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u91c7\u6837\u524d\u77bb\u52a8\u4f5c\u5e8f\u5217\u6765\u5e73\u8861\u4e3b\u52a8\u641c\u7d22\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u65e0\u9700\u6784\u5efa\u8be6\u5c3d\u7684\u641c\u7d22\u6811\u3002\u9488\u5bf9\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e3b\u52a8\u641c\u7d22\u4e2d\u5b58\u5728\u7684\u4e50\u89c2\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7f13\u89e3\u65b9\u6848\uff0c\u652f\u6301\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u56e2\u961f\u7684\u9ad8\u6548\u6210\u672c\u611f\u77e5\u51b3\u7b56\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5b8c\u5168\u6062\u590d\u7387\u3002\u5728\u6210\u672c\u611f\u77e5\u4e3b\u52a8\u51b3\u7b56\u65b9\u9762\uff0c\u6bd4\u6811\u641c\u7d22\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u4e3b\u52a8\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u524d\u77bb\u51b3\u7b56\u65b9\u6cd5\uff0c\u65e2\u80fd\u5e73\u8861\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u53c8\u907f\u514d\u4e86\u4f20\u7edf\u6811\u641c\u7d22\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.19577", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19577", "abs": "https://arxiv.org/abs/2602.19577", "authors": ["Kordel K. France", "Ovidiu Daescu", "Latifur Khan", "Rohith Peddi"], "title": "Chasing Ghosts: A Simulation-to-Real Olfactory Navigation Stack with Optional Vision Augmentation", "comment": null, "summary": "Autonomous odor source localization remains a challenging problem for aerial robots due to turbulent airflow, sparse and delayed sensory signals, and strict payload and compute constraints. While prior unmanned aerial vehicle (UAV)-based olfaction systems have demonstrated gas distribution mapping or reactive plume tracing, they rely on predefined coverage patterns, external infrastructure, or extensive sensing and coordination. In this work, we present a complete, open-source UAV system for online odor source localization using a minimal sensor suite. The system integrates custom olfaction hardware, onboard sensing, and a learning-based navigation policy trained in simulation and deployed on a real quadrotor. Through our minimal framework, the UAV is able to navigate directly toward an odor source without constructing an explicit gas distribution map or relying on external positioning systems. Vision is incorporated as an optional complementary modality to accelerate navigation under certain conditions. We validate the proposed system through real-world flight experiments in a large indoor environment using an ethanol source, demonstrating consistent source-finding behavior under realistic airflow conditions. The primary contribution of this work is a reproducible system and methodological framework for UAV-based olfactory navigation and source finding under minimal sensing assumptions. We elaborate on our hardware design and open source our UAV firmware, simulation code, olfaction-vision dataset, and circuit board to the community. Code, data, and designs will be made available at https://github.com/KordelFranceTech/ChasingGhosts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u4f7f\u7528\u6700\u5c0f\u4f20\u611f\u5668\u5957\u4ef6\u8fdb\u884c\u5728\u7ebf\u6c14\u5473\u6e90\u5b9a\u4f4d\uff0c\u65e0\u9700\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u6216\u9884\u5b9a\u4e49\u8986\u76d6\u6a21\u5f0f\uff0c\u901a\u8fc7\u4eff\u771f\u8bad\u7ec3\u7684\u5bfc\u822a\u7b56\u7565\u5b9e\u73b0\u76f4\u63a5\u5bfc\u822a\u81f3\u6c14\u5473\u6e90\u3002", "motivation": "\u81ea\u4e3b\u6c14\u5473\u6e90\u5b9a\u4f4d\u5bf9\u7a7a\u4e2d\u673a\u5668\u4eba\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65e0\u4eba\u673a\u55c5\u89c9\u7cfb\u7edf\u4f9d\u8d56\u9884\u5b9a\u4e49\u8986\u76d6\u6a21\u5f0f\u3001\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u6216\u5927\u91cf\u4f20\u611f\u534f\u8c03\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u96c6\u6210\u5b9a\u5236\u55c5\u89c9\u786c\u4ef6\u3001\u673a\u8f7d\u4f20\u611f\u548c\u57fa\u4e8e\u4eff\u771f\u7684\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\uff0c\u53ef\u9009\u89c6\u89c9\u6a21\u6001\u52a0\u901f\u5bfc\u822a\uff0c\u65e0\u9700\u6784\u5efa\u663e\u5f0f\u6c14\u4f53\u5206\u5e03\u56fe\u6216\u5916\u90e8\u5b9a\u4f4d\u7cfb\u7edf\u3002", "result": "\u5728\u5927\u578b\u5ba4\u5185\u73af\u5883\u4e2d\u4f7f\u7528\u4e59\u9187\u6e90\u8fdb\u884c\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u73b0\u5b9e\u6c14\u6d41\u6761\u4ef6\u4e0b\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u6e90\u5bfb\u627e\u884c\u4e3a\uff0c\u7cfb\u7edf\u5177\u5907\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "\u4e3b\u8981\u8d21\u732e\u662f\u63d0\u4f9b\u4e86\u5728\u6700\u5c0f\u4f20\u611f\u5047\u8bbe\u4e0b\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u55c5\u89c9\u5bfc\u822a\u548c\u6e90\u5bfb\u627e\u7684\u53ef\u590d\u5236\u7cfb\u7edf\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u5f00\u6e90\u4e86\u786c\u4ef6\u8bbe\u8ba1\u3001\u56fa\u4ef6\u3001\u4eff\u771f\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u7535\u8def\u677f\u3002"}}
{"id": "2602.19651", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19651", "abs": "https://arxiv.org/abs/2602.19651", "authors": ["Lennart R\u00f6stel", "Berthold B\u00e4uml"], "title": "Denoising Particle Filters: Learning State Estimation with Single-Step Objectives", "comment": null, "summary": "Learning-based methods commonly treat state estimation in robotics as a sequence modeling problem. While this paradigm can be effective at maximizing end-to-end performance, models are often difficult to interpret and expensive to train, since training requires unrolling sequences of predictions in time. As an alternative to end-to-end trained state estimation, we propose a novel particle filtering algorithm in which models are trained from individual state transitions, fully exploiting the Markov property in robotic systems. In this framework, measurement models are learned implicitly by minimizing a denoising score matching objective. At inference, the learned denoiser is used alongside a (learned) dynamics model to approximately solve the Bayesian filtering equation at each time step, effectively guiding predicted states toward the data manifold informed by measurements. We evaluate the proposed method on challenging robotic state estimation tasks in simulation, demonstrating competitive performance compared to tuned end-to-end trained baselines. Importantly, our method offers the desirable composability of classical filtering algorithms, allowing prior information and external sensor models to be incorporated without retraining.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u7684\u65b0\u578b\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u8bad\u7ec3\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\uff0c\u5229\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\uff0c\u907f\u514d\u7aef\u5230\u7aef\u5e8f\u5217\u8bad\u7ec3\u7684\u9ad8\u6210\u672c", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u5c06\u95ee\u9898\u89c6\u4e3a\u5e8f\u5217\u5efa\u6a21\uff0c\u867d\u7136\u7aef\u5230\u7aef\u6027\u80fd\u53ef\u80fd\u8f83\u597d\uff0c\u4f46\u6a21\u578b\u96be\u4ee5\u89e3\u91ca\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\uff08\u9700\u8981\u5c55\u5f00\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff09\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\u53c8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u8bad\u7ec3\u6548\u7387\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u7684\u7b97\u6cd5\uff0c\u4ece\u5355\u4e2a\u72b6\u6001\u8f6c\u79fb\u8bad\u7ec3\u6a21\u578b\uff0c\u5145\u5206\u5229\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u53bb\u566a\u5206\u6570\u5339\u914d\u76ee\u6807\u9690\u5f0f\u5b66\u4e60\u6d4b\u91cf\u6a21\u578b\uff0c\u5728\u63a8\u7406\u65f6\u7ed3\u5408\u5b66\u4e60\u7684\u53bb\u566a\u5668\u548c\uff08\u5b66\u4e60\u7684\uff09\u52a8\u6001\u6a21\u578b\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u8fd1\u4f3c\u6c42\u89e3\u8d1d\u53f6\u65af\u6ee4\u6ce2\u65b9\u7a0b\uff0c\u5f15\u5bfc\u9884\u6d4b\u72b6\u6001\u5411\u6d4b\u91cf\u6570\u636e\u6d41\u5f62\u9760\u62e2\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u7684\u6311\u6218\u6027\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u4e0a\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4e0e\u8c03\u4f18\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7ecf\u5178\u6ee4\u6ce2\u7b97\u6cd5\u7684\u7406\u60f3\u53ef\u7ec4\u5408\u6027\uff0c\u5141\u8bb8\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6574\u5408\u5148\u9a8c\u4fe1\u606f\u548c\u5916\u90e8\u4f20\u611f\u5668\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2602.19653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19653", "abs": "https://arxiv.org/abs/2602.19653", "authors": ["Bailey Dacre", "Rodrigo Moreno", "J\u00f8rn Lambertsen", "Kasper Stoy", "Andr\u00e9s Fa\u00ed\u00f1a"], "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array", "comment": null, "summary": "Distributed Manipulator Systems, composed of arrays of robotic actuators necessitate dense actuator arrays to effectively manipulate small objects. This paper presents a system composed of modular 3-DoF robotic tiles interconnected by a compliant surface layer, forming a continuous, controllable manipulation surface. The compliant layer permits increased actuator spacing without compromising object manipulation capabilities, significantly reducing actuator density while maintaining robust control, even for smaller objects. We characterize the coupled workspace of the array and develop a manipulation strategy capable of translating objects to arbitrary positions within an N X N array. The approach is validated experimentally using a minimal 2 X 2 prototype, demonstrating the successful manipulation of objects with varied shapes and sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7531\u6a21\u5757\u53163\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u74e6\u7247\u548c\u67d4\u6027\u8868\u9762\u5c42\u7ec4\u6210\u7684\u5206\u5e03\u5f0f\u64cd\u7eb5\u7cfb\u7edf\uff0c\u901a\u8fc7\u67d4\u6027\u5c42\u5141\u8bb8\u66f4\u5927\u7684\u6267\u884c\u5668\u95f4\u8ddd\uff0c\u964d\u4f4e\u6267\u884c\u5668\u5bc6\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5c0f\u7269\u4f53\u7684\u9c81\u68d2\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f0f\u64cd\u7eb5\u7cfb\u7edf\u9700\u8981\u5bc6\u96c6\u7684\u6267\u884c\u5668\u9635\u5217\u6765\u6709\u6548\u64cd\u7eb5\u5c0f\u7269\u4f53\uff0c\u8fd9\u589e\u52a0\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u548c\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51cf\u5c11\u6267\u884c\u5668\u5bc6\u5ea6\u540c\u65f6\u4fdd\u6301\u64cd\u7eb5\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u53163\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u74e6\u7247\u901a\u8fc7\u67d4\u6027\u8868\u9762\u5c42\u4e92\u8fde\uff0c\u5f62\u6210\u8fde\u7eed\u53ef\u63a7\u7684\u64cd\u7eb5\u8868\u9762\u3002\u5206\u6790\u9635\u5217\u7684\u8026\u5408\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5f00\u53d1\u80fd\u591f\u5c06\u7269\u4f53\u79fb\u52a8\u5230N\u00d7N\u9635\u5217\u5185\u4efb\u610f\u4f4d\u7f6e\u7684\u64cd\u7eb5\u7b56\u7565\u3002", "result": "\u4f7f\u7528\u6700\u5c0f2\u00d72\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6210\u529f\u64cd\u7eb5\u4e86\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\uff0c\u8bc1\u660e\u67d4\u6027\u5c42\u5141\u8bb8\u589e\u52a0\u6267\u884c\u5668\u95f4\u8ddd\u800c\u4e0d\u5f71\u54cd\u64cd\u7eb5\u80fd\u529b\u3002", "conclusion": "\u67d4\u6027\u8868\u9762\u5c42\u4f7f\u5206\u5e03\u5f0f\u64cd\u7eb5\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6267\u884c\u5668\u5bc6\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5c0f\u7269\u4f53\u7684\u9c81\u68d2\u63a7\u5236\uff0c\u4e3a\u6784\u5efa\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u64cd\u7eb5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.19699", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.19699", "abs": "https://arxiv.org/abs/2602.19699", "authors": ["Elisa Alboni", "Pietro Noah Crestaz", "Elias Fontanari", "Andrea Del Prete"], "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization", "comment": null, "summary": "Trajectory Optimization (TO) and Reinforcement Learning (RL) offer complementary strengths for solving optimal control problems. TO efficiently computes locally optimal solutions but can struggle with non-convexity, while RL is more robust to non-convexity at the cost of significantly higher computational demands. CACTO (Continuous Actor-Critic with Trajectory Optimization) was introduced to combine these advantages by learning a warm-start policy that guides the TO solver towards low-cost trajectories. However, scalability remains a key limitation, as increasing system complexity significantly raises the computational cost of TO. This work introduces CACTO-BIC to address these challenges. CACTO-BIC improves data efficiency by biasing initial-state sampling leveraging a property of the value function associated with locally optimal policies; moreover, it reduces computation time by exploiting GPU acceleration. Empirical evaluations show improved sample efficiency and faster computation compared to CACTO. Comparisons with PPO demonstrate that our approach can achieve similar solutions in less time. Finally, experiments on the AlienGO quadruped robot demonstrate that CACTO-BIC can scale to high-dimensional systems and is suitable for real-time applications.", "AI": {"tldr": "CACTO-BIC\u901a\u8fc7\u6539\u8fdb\u521d\u59cb\u72b6\u6001\u91c7\u6837\u7b56\u7565\u548cGPU\u52a0\u901f\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u4f18\u5316\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u65b9\u6cd5CACTO\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u8ba1\u7b97\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u548c\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u8f68\u8ff9\u4f18\u5316\uff08TO\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u89e3\u51b3\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u5404\u5177\u4f18\u52bf\uff1aTO\u80fd\u9ad8\u6548\u8ba1\u7b97\u5c40\u90e8\u6700\u4f18\u89e3\u4f46\u96be\u4ee5\u5904\u7406\u975e\u51f8\u95ee\u9898\uff0cRL\u5bf9\u975e\u51f8\u95ee\u9898\u66f4\u9c81\u68d2\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002CACTO\u7ed3\u5408\u4e86\u4e24\u8005\u4f18\u52bf\uff0c\u4f46\u9762\u4e34\u6269\u5c55\u6027\u9650\u5236\uff0c\u7cfb\u7edf\u590d\u6742\u5ea6\u589e\u52a0\u4f1a\u663e\u8457\u63d0\u9ad8TO\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faCACTO-BIC\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u5c40\u90e8\u6700\u4f18\u7b56\u7565\u4ef7\u503c\u51fd\u6570\u7684\u7279\u6027\u6539\u8fdb\u521d\u59cb\u72b6\u6001\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff1b2\uff09\u5229\u7528GPU\u52a0\u901f\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002\u8be5\u65b9\u6cd5\u5728AlienGO\u56db\u8db3\u673a\u5668\u4eba\u7b49\u9ad8\u7ef4\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff1a\u76f8\u6bd4CACTO\uff0cCACTO-BIC\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u5feb\u4e86\u8ba1\u7b97\u901f\u5ea6\uff1b\u4e0ePPO\u76f8\u6bd4\uff0c\u80fd\u5728\u66f4\u77ed\u65f6\u95f4\u5185\u83b7\u5f97\u76f8\u4f3c\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u5728AlienGO\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6269\u5c55\u5230\u9ad8\u7ef4\u7cfb\u7edf\u5e76\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "conclusion": "CACTO-BIC\u901a\u8fc7\u6539\u8fdb\u91c7\u6837\u7b56\u7565\u548cGPU\u52a0\u901f\uff0c\u6709\u6548\u89e3\u51b3\u4e86CACTO\u7684\u6269\u5c55\u6027\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u548c\u8ba1\u7b97\u901f\u5ea6\uff0c\u80fd\u591f\u6269\u5c55\u5230\u9ad8\u7ef4\u673a\u5668\u4eba\u7cfb\u7edf\u5e76\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2602.19764", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19764", "abs": "https://arxiv.org/abs/2602.19764", "authors": ["Yirui Sun", "Guangyu Zhuge", "Keliang Liu", "Jie Gu", "Zhihao xia", "Qionglin Ren", "Chunxu tian", "Zhongxue Ga"], "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling", "comment": null, "summary": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.", "AI": {"tldr": "DeMUSE\u662f\u4e00\u4e2a\u6df1\u5ea6\u591a\u6a21\u6001\u7edf\u4e00\u7a00\u758f\u4e13\u5bb6\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563Transformer\u6574\u5408RGB\u3001\u6df1\u5ea6\u548c6\u8f74\u529b\u4f20\u611f\u5668\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u6001\u7279\u5b9a\u5f52\u4e00\u5316\u5e73\u8861\u591a\u6a21\u6001\u7279\u5f81\uff0c\u91c7\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u63d0\u5347\u6a21\u578b\u5bb9\u91cf\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\u5f80\u5f80\u5ffd\u7565\u4e86\u590d\u6742\u4efb\u52a1\u6240\u9700\u7684\u5173\u952e\u529b\u548c\u51e0\u4f55\u53cd\u9988\uff0c\u9700\u8981\u6df1\u5ea6\u6574\u5408\u5f02\u6784\u591a\u6a21\u6001\u611f\u5b98\u8f93\u5165\u6765\u5b9e\u73b0\u7075\u5de7\u7684\u5177\u8eab\u64cd\u4f5c\u3002", "method": "\u63d0\u51faDeMUSE\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6269\u6563Transformer\u5c06RGB\u3001\u6df1\u5ea6\u548c6\u8f74\u529b\u6574\u5408\u4e3a\u7edf\u4e00\u5e8f\u5217\u5316\u6d41\uff1b2\uff09\u91c7\u7528\u81ea\u9002\u5e94\u6a21\u6001\u7279\u5b9a\u5f52\u4e00\u5316\u91cd\u65b0\u6821\u51c6\u6a21\u6001\u611f\u77e5\u7279\u5f81\uff1b3\uff09\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u63d0\u5347\u6a21\u578b\u5bb9\u91cf\uff1b4\uff09\u8054\u5408\u53bb\u566a\u76ee\u6807\u540c\u6b65\u5408\u6210\u73af\u5883\u6f14\u5316\u548c\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u4e2d\u5206\u522b\u8fbe\u523083.2%\u548c72.5%\u7684\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6df1\u5ea6\u591a\u611f\u5b98\u6574\u5408\u5bf9\u590d\u6742\u7269\u7406\u4ea4\u4e92\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "DeMUSE\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u6574\u5408\u5f02\u6784\u591a\u6a21\u6001\u611f\u5b98\u8f93\u5165\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u89c6\u89c9\u4e2d\u5fc3\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u7269\u7406\u4ea4\u4e92\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19850", "abs": "https://arxiv.org/abs/2602.19850", "authors": ["Junhui Lee", "Hyosung Kim", "Saekwang Nam"], "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data", "comment": "6 pages, 7 figures, accepted at 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft)", "summary": "Real-time prediction of deformation in highly compliant soft materials remains a significant challenge in soft robotics. While vision-based soft tactile sensors can track internal marker displacements, learning-based models for 3D contact estimation heavily depend on their training datasets, inherently limiting their ability to generalize to complex scenarios such as multi-point sensing. To address this limitation, we introduce TactiVerse, a U-Net-based framework that formulates contact geometry estimation as a spatial heatmap prediction task. Even when trained exclusively on a limited dataset of single-point indentations, our architecture achieves highly accurate single-point sensing, yielding a superior mean absolute error of 0.0589 mm compared to the 0.0612 mm of a conventional regression-based CNN baseline. Furthermore, we demonstrate that augmenting the training dataset with multi-point contact data substantially enhances the sensor's multi-point sensing capabilities, significantly improving the overall mean MAE for two-point discrimination from 1.214 mm to 0.383 mm. By successfully extrapolating complex contact geometries from fundamental interactions, this methodology unlocks advanced multi-point and large-area shape sensing. Ultimately, it significantly streamlines the development of marker-based soft sensors, offering a highly scalable solution for real-world tactile perception.", "AI": {"tldr": "TactiVerse\uff1a\u57fa\u4e8eU-Net\u7684\u8f6f\u4f53\u89e6\u89c9\u4f20\u611f\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u70ed\u56fe\u9884\u6d4b\u5b9e\u73b0\u63a5\u89e6\u51e0\u4f55\u4f30\u8ba1\uff0c\u5373\u4f7f\u4ec5\u7528\u5355\u70b9\u538b\u75d5\u6570\u636e\u8bad\u7ec3\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5355\u70b9\u611f\u77e5\uff0c\u5e76\u80fd\u901a\u8fc7\u591a\u70b9\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u591a\u70b9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u9ad8\u5ea6\u67d4\u6027\u6750\u6599\u7684\u5b9e\u65f6\u53d8\u5f62\u9884\u6d4b\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u8f6f\u4f53\u89e6\u89c9\u4f20\u611f\u5668\u867d\u7136\u80fd\u8ddf\u8e2a\u5185\u90e8\u6807\u8bb0\u4f4d\u79fb\uff0c\u4f46\u57fa\u4e8e\u5b66\u4e60\u76843D\u63a5\u89e6\u4f30\u8ba1\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u590d\u6742\u573a\u666f\uff08\u5982\u591a\u70b9\u611f\u77e5\uff09\u3002", "method": "\u63d0\u51faTactiVerse\u6846\u67b6\uff0c\u57fa\u4e8eU-Net\u67b6\u6784\uff0c\u5c06\u63a5\u89e6\u51e0\u4f55\u4f30\u8ba1\u5efa\u6a21\u4e3a\u7a7a\u95f4\u70ed\u56fe\u9884\u6d4b\u4efb\u52a1\u3002\u5373\u4f7f\u4ec5\u4f7f\u7528\u6709\u9650\u7684\u5355\u70b9\u538b\u75d5\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u611f\u77e5\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u591a\u70b9\u63a5\u89e6\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\u96c6\u6765\u63d0\u5347\u591a\u70b9\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5355\u70b9\u611f\u77e5\u65b9\u9762\uff1a\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.0589 mm\uff0c\u4f18\u4e8e\u4f20\u7edf\u56de\u5f52CNN\u57fa\u7ebf\u76840.0612 mm\u3002\u591a\u70b9\u611f\u77e5\u65b9\u9762\uff1a\u901a\u8fc7\u6570\u636e\u589e\u5f3a\uff0c\u4e24\u70b9\u8fa8\u522b\u7684\u5e73\u5747MAE\u4ece1.214 mm\u663e\u8457\u63d0\u5347\u81f30.383 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u57fa\u672c\u76f8\u4e92\u4f5c\u7528\u4e2d\u6210\u529f\u63a8\u65ad\u590d\u6742\u63a5\u89e6\u51e0\u4f55\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u591a\u70b9\u548c\u5927\u9762\u79ef\u5f62\u72b6\u611f\u77e5\uff0c\u663e\u8457\u7b80\u5316\u4e86\u57fa\u4e8e\u6807\u8bb0\u7684\u8f6f\u4f53\u4f20\u611f\u5668\u5f00\u53d1\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.19898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19898", "abs": "https://arxiv.org/abs/2602.19898", "authors": ["Stefan Fabian", "Aljoscha Schmidt", "Jonas S\u00fc\u00df", "Dishant", "Aum Oza", "Oskar von Stryk"], "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform", "comment": "https://github.com/tu-darmstadt-ros-pkg/athena", "summary": "In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manipulators is limited. UGVs can carry required payloads to assess and manipulate the mission environment, but need to be able to deal with difficult and unstructured terrain such as rubble and stairs. The ability of tracked platforms with articulated arms (flippers) to reconfigure their geometry makes them particularly effective for navigating challenging terrain. In this paper, we present Athena, an open-hardware rescue ground robot research platform with four individually reconfigurable flippers and a reliable low-cost remote emergency stop (E-Stop) solution. A novel mounting solution using an industrial PU belt and tooth inserts allows the replacement and testing of different track profiles. The manipulator with a maximum reach of 1.54m can be used to operate doors, valves, and other objects of interest. Full CAD & PCB files, as well as all low-level software, are released as open-source contributions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Athena\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u6551\u63f4\u5730\u9762\u673a\u5668\u4eba\u7814\u7a76\u5e73\u53f0\uff0c\u5177\u6709\u56db\u4e2a\u72ec\u7acb\u53ef\u91cd\u6784\u7684\u5c65\u5e26\u81c2\u548c\u4f4e\u6210\u672c\u8fdc\u7a0b\u6025\u505c\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u707e\u5bb3\u54cd\u5e94\u4efb\u52a1\u3002", "motivation": "\u707e\u5bb3\u54cd\u5e94\u548c\u6001\u52bf\u8bc4\u4f30\u4e2d\uff0c\u673a\u5668\u4eba\u80fd\u964d\u4f4e\u6551\u63f4\u4eba\u5458\u98ce\u9669\u3002\u7531\u4e8e\u4efb\u52a1\u73af\u5883\u548c\u673a\u5668\u4eba\u80fd\u529b\u9700\u6c42\u5dee\u5f02\u5927\u4e14\u96be\u4ee5\u9884\u77e5\uff0c\u9700\u8981\u5f02\u6784\u673a\u5668\u4eba\u7f16\u961f\u3002\u65e0\u4eba\u673a\u80fd\u5feb\u901f\u4fa6\u5bdf\u4f46\u8f7d\u8377\u80fd\u529b\u6709\u9650\uff0c\u5730\u9762\u673a\u5668\u4eba\u80fd\u643a\u5e26\u4f20\u611f\u5668\u548c\u673a\u68b0\u81c2\u4f46\u9700\u5e94\u5bf9\u590d\u6742\u5730\u5f62\u3002", "method": "\u5f00\u53d1\u4e86Athena\u5f00\u6e90\u6551\u63f4\u5730\u9762\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5177\u6709\u56db\u4e2a\u72ec\u7acb\u53ef\u91cd\u6784\u7684\u5c65\u5e26\u81c2\uff0c\u91c7\u7528\u5de5\u4e1aPU\u76ae\u5e26\u548c\u9f7f\u5f62\u63d2\u4ef6\u7684\u65b0\u578b\u5b89\u88c5\u65b9\u6848\u53ef\u66f4\u6362\u6d4b\u8bd5\u4e0d\u540c\u5c65\u5e26\u8f6e\u5ed3\uff0c\u914d\u5907\u6700\u5927\u4f38\u5c551.54\u7c73\u7684\u673a\u68b0\u81c2\uff0c\u5e76\u63d0\u4f9b\u4f4e\u6210\u672c\u8fdc\u7a0b\u6025\u505c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u6551\u63f4\u673a\u5668\u4eba\u786c\u4ef6\u5e73\u53f0\uff0c\u5305\u62ec\u673a\u68b0\u8bbe\u8ba1\u3001\u7535\u5b50\u7cfb\u7edf\u548c\u5e95\u5c42\u8f6f\u4ef6\uff0c\u6240\u6709CAD\u3001PCB\u6587\u4ef6\u53ca\u5e95\u5c42\u8f6f\u4ef6\u5747\u5df2\u5f00\u6e90\u53d1\u5e03\u3002", "conclusion": "Athena\u5e73\u53f0\u4e3a\u6551\u63f4\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u91cd\u6784\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u590d\u6742\u5730\u5f62\u5bfc\u822a\u548c\u707e\u5bb3\u54cd\u5e94\u4efb\u52a1\uff0c\u901a\u8fc7\u5f00\u6e90\u5171\u4eab\u4fc3\u8fdb\u6551\u63f4\u673a\u5668\u4eba\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2602.19943", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19943", "abs": "https://arxiv.org/abs/2602.19943", "authors": ["Abulikemu Abuduweili", "Yuyang Pang", "Feihan Li", "Changliu Liu"], "title": "Scaling Law of Neural Koopman Operators", "comment": null, "summary": "Data-driven neural Koopman operator theory has emerged as a powerful tool for linearizing and controlling nonlinear robotic systems. However, the performance of these data-driven models fundamentally depends on the trade-off between sample size and model dimensions, a relationship for which the scaling laws have remained unclear. This paper establishes a rigorous framework to address this challenge by deriving and empirically validating scaling laws that connect sample size, latent space dimension, and downstream control quality. We derive a theoretical upper bound on the Koopman approximation error, explicitly decomposing it into sampling error and projection error. We show that these terms decay at specific rates relative to dataset size and latent dimension, providing a rigorous basis for the scaling law. Based on the theoretical results, we introduce two lightweight regularizers for the neural Koopman operator: a covariance loss to help stabilize the learned latent features and an inverse control loss to ensure the model aligns with physical actuation. The results from systematic experiments across six robotic environments confirm that model fitting error follows the derived scaling laws, and the regularizers improve dynamic model fitting fidelity, with enhanced closed-loop control performance. Together, our results provide a simple recipe for allocating effort between data collection and model capacity when learning Koopman dynamics for control.", "AI": {"tldr": "\u672c\u6587\u4e3a\u795e\u7ecfKoopman\u7b97\u5b50\u7406\u8bba\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6807\u5ea6\u5f8b\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6837\u672c\u91cf\u3001\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0e\u63a7\u5236\u8d28\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u5668\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecfKoopman\u7b97\u5b50\u7406\u8bba\u5df2\u6210\u4e3a\u7ebf\u6027\u5316\u548c\u63a7\u5236\u975e\u7ebf\u6027\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u5176\u6027\u80fd\u53d6\u51b3\u4e8e\u6837\u672c\u91cf\u4e0e\u6a21\u578b\u7ef4\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u800c\u8fd9\u4e00\u5173\u7cfb\u7684\u6807\u5ea6\u5f8b\u4e00\u76f4\u4e0d\u660e\u786e\u3002", "method": "1. \u63a8\u5bfcKoopman\u8fd1\u4f3c\u8bef\u5dee\u7684\u7406\u8bba\u4e0a\u754c\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u91c7\u6837\u8bef\u5dee\u548c\u6295\u5f71\u8bef\u5dee\uff1b2. \u57fa\u4e8e\u7406\u8bba\u7ed3\u679c\u5f15\u5165\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u5668\uff1a\u534f\u65b9\u5dee\u635f\u5931\uff08\u7a33\u5b9a\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7279\u5f81\uff09\u548c\u9006\u63a7\u5236\u635f\u5931\uff08\u786e\u4fdd\u6a21\u578b\u4e0e\u7269\u7406\u9a71\u52a8\u5bf9\u9f50\uff09\uff1b3. \u5728\u516d\u4e2a\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1. \u6a21\u578b\u62df\u5408\u8bef\u5dee\u9075\u5faa\u63a8\u5bfc\u51fa\u7684\u6807\u5ea6\u5f8b\uff1b2. \u6b63\u5219\u5316\u5668\u63d0\u9ad8\u4e86\u52a8\u6001\u6a21\u578b\u62df\u5408\u7684\u4fdd\u771f\u5ea6\uff1b3. \u589e\u5f3a\u4e86\u95ed\u73af\u63a7\u5236\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5b66\u4e60Koopman\u52a8\u529b\u5b66\u8fdb\u884c\u63a7\u5236\u65f6\uff0c\u5982\u4f55\u5728\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u5bb9\u91cf\u4e4b\u95f4\u5206\u914d\u52aa\u529b\u63d0\u4f9b\u4e86\u7b80\u5355\u6307\u5bfc\u65b9\u6848\u3002"}}
{"id": "2602.19983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19983", "abs": "https://arxiv.org/abs/2602.19983", "authors": ["Zachary Ravichadran", "David Snyder", "Alexander Robey", "Hamed Hassani", "Vijay Kumar", "George J. Pappas"], "title": "Contextual Safety Reasoning and Grounding for Open-World Robots", "comment": null, "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.", "AI": {"tldr": "CORE\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u673a\u5668\u4eba\u5b89\u5168\u884c\u4e3a\u7684\u5728\u7ebf\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u73af\u5883\u843d\u5730\u548c\u6267\u884c\uff0c\u65e0\u9700\u5148\u9a8c\u73af\u5883\u77e5\u8bc6", "motivation": "\u4f20\u7edf\u5b89\u5168\u65b9\u6cd5\u5728\u56fa\u5b9a\u7ea6\u675f\u548c\u7528\u6237\u6307\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u8fd0\u884c\uff0c\u65e0\u6cd5\u5904\u7406\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u4e0a\u4e0b\u6587\u53d8\u5f02\u6027", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u6301\u7eed\u63a8\u7406\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5b89\u5168\u89c4\u5219\uff0c\u5c06\u8fd9\u4e9b\u89c4\u5219\u5728\u7269\u7406\u73af\u5883\u4e2d\u843d\u5730\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6267\u884c\u7a7a\u95f4\u5b9a\u4e49\u7684\u5b89\u5168\u96c6", "result": "CORE\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5f3a\u5236\u6267\u884c\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u884c\u4e3a\uff0c\u663e\u8457\u4f18\u4e8e\u7f3a\u4e4f\u5728\u7ebf\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5148\u524d\u8bed\u4e49\u5b89\u5168\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u8003\u8651\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u7387\u5b89\u5168\u4fdd\u8bc1", "conclusion": "CORE\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u7a7a\u95f4\u843d\u5730\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u53d8\u5f02\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2602.20054", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20054", "abs": "https://arxiv.org/abs/2602.20054", "authors": ["A. Giordano", "G. De Meurichy", "V. Telazzi", "C. Mucignat", "I. Lunati", "D. A. L. M. Louchard", "M. Iovieno", "S. F. Armanini", "M. Kovac"], "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement", "comment": "Conference paper accepted at 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "This work assesses the hydrodynamic efficiency of Underwater Unmanned Vehicles (UUVs) equipped with soft morphing wings compared to conventional rigid wings. Unlike rigid wings, deformable counterparts can alter their aerodynamic properties on demand. Improvements in hydrodynamic efficiency extend a UUV's operational range and may determine mission feasibility. Structural and Computational Fluid Dynamics (CFD) simulations were conducted for both a soft morphing wing and a UUV incorporating it. The results show that a UUV employing soft wings achieves 9.75 percent higher overall efficiency than an equivalent vehicle with traditional rigid wings. These findings confirm the potential of soft robotics to enhance underwater vehicle performance, particularly in applications requiring pressure-agnostic operation.", "AI": {"tldr": "\u8f6f\u53d8\u5f62\u7ffcUUV\u76f8\u6bd4\u4f20\u7edf\u521a\u6027\u7ffcUUV\u6574\u4f53\u6548\u7387\u63d0\u53479.75%\uff0c\u8bc1\u5b9e\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u6280\u672f\u5728\u6c34\u4e0b\u822a\u884c\u5668\u6027\u80fd\u63d0\u5347\u65b9\u9762\u7684\u6f5c\u529b", "motivation": "\u8bc4\u4f30\u914d\u5907\u8f6f\u53d8\u5f62\u7ffc\u7684\u6c34\u4e0b\u65e0\u4eba\u822a\u884c\u5668\uff08UUV\uff09\u76f8\u6bd4\u4f20\u7edf\u521a\u6027\u7ffc\u7684\u6c34\u52a8\u529b\u6548\u7387\u3002\u8f6f\u53d8\u5f62\u7ffc\u80fd\u591f\u6309\u9700\u6539\u53d8\u6c14\u52a8\u7279\u6027\uff0c\u63d0\u9ad8\u6c34\u52a8\u529b\u6548\u7387\u53ef\u5ef6\u957fUUV\u7684\u4f5c\u4e1a\u822a\u7a0b\u5e76\u53ef\u80fd\u51b3\u5b9a\u4efb\u52a1\u53ef\u884c\u6027\u3002", "method": "\u5bf9\u8f6f\u53d8\u5f62\u7ffc\u53ca\u5176\u914d\u5907\u7684UUV\u8fdb\u884c\u4e86\u7ed3\u6784\u548c\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u6a21\u62df\uff0c\u6bd4\u8f83\u8f6f\u53d8\u5f62\u7ffc\u4e0e\u4f20\u7edf\u521a\u6027\u7ffc\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u91c7\u7528\u8f6f\u53d8\u5f62\u7ffc\u7684UUV\u76f8\u6bd4\u914d\u5907\u4f20\u7edf\u521a\u6027\u7ffc\u7684\u540c\u7b49\u822a\u884c\u5668\u5b9e\u73b0\u4e869.75%\u7684\u6574\u4f53\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u6280\u672f\u5728\u63d0\u5347\u6c34\u4e0b\u822a\u884c\u5668\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u538b\u529b\u65e0\u5173\u64cd\u4f5c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.20055", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20055", "abs": "https://arxiv.org/abs/2602.20055", "authors": ["Apoorva Vashisth", "Manav Kulshrestha", "Pranav Bakshi", "Damon Conover", "Guillaume Sartoretti", "Aniket Bera"], "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation", "comment": null, "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec8\u8eab\u4ea4\u4e92\u5f0f\u5bfc\u822a\u95ee\u9898\uff0c\u8ba9\u5177\u5907\u64cd\u4f5c\u80fd\u529b\u7684\u79fb\u52a8\u673a\u5668\u4eba\u80fd\u591f\u79fb\u52a8\u6742\u7269\u6765\u5f00\u8f9f\u8def\u5f84\uff0c\u5b8c\u6210\u987a\u5e8f\u7269\u4f53\u653e\u7f6e\u4efb\u52a1\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7ea6\u675f\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4e3b\u52a8\u611f\u77e5\uff0c\u5728\u7269\u7406\u6a21\u62df\u5668\u548c\u771f\u5b9e\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u573a\u666f\uff08\u5982\u5bb6\u5ead\u73af\u5883\u548c\u4ed3\u5e93\uff09\u4e2d\uff0c\u6742\u7269\u53ef\u80fd\u963b\u585e\u6240\u6709\u8def\u5f84\uff0c\u800c\u4f20\u7edf\u89c6\u89c9\u5bfc\u822a\u5047\u8bbe\u81f3\u5c11\u5b58\u5728\u4e00\u6761\u65e0\u969c\u788d\u8def\u5f84\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u5b9e\u9645\u6311\u6218\uff0c\u9700\u8981\u8ba9\u673a\u5668\u4eba\u5177\u5907\u79fb\u52a8\u6742\u7269\u5f00\u8f9f\u8def\u5f84\u7684\u80fd\u529b\u6765\u5b8c\u6210\u987a\u5e8f\u7269\u4f53\u653e\u7f6e\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u3001\u57fa\u4e8e\u7ea6\u675f\u7684\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4e3b\u52a8\u611f\u77e5\u3002LLM\u5728\u7ed3\u6784\u5316\u573a\u666f\u56fe\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u51b3\u5b9a\u79fb\u52a8\u54ea\u4e2a\u7269\u4f53\u3001\u653e\u7f6e\u5230\u54ea\u91cc\u3001\u4ee5\u53ca\u4e0b\u4e00\u6b65\u89c2\u5bdf\u54ea\u91cc\u6765\u53d1\u73b0\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002\u6846\u67b6\u5c06\u63a8\u7406\u4e0e\u4e3b\u52a8\u611f\u77e5\u7ed3\u5408\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u63a2\u7d22\u9884\u671f\u6709\u52a9\u4e8e\u4efb\u52a1\u5b8c\u6210\u7684\u533a\u57df\uff0c\u800c\u4e0d\u662f\u8be6\u5c3d\u5730\u6620\u5c04\u6574\u4e2a\u73af\u5883\u3002\u6807\u51c6\u8fd0\u52a8\u89c4\u5212\u5668\u6267\u884c\u76f8\u5e94\u7684\u5bfc\u822a-\u62fe\u53d6-\u653e\u7f6e\u6216\u7ed5\u884c\u5e8f\u5217\u3002", "result": "\u5728\u7269\u7406\u542f\u7528\u7684ProcTHOR-10k\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u975e\u5b66\u4e60\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u5728\u771f\u5b9e\u4e16\u754c\u786c\u4ef6\u4e0a\u8fdb\u884c\u4e86\u5b9a\u6027\u6f14\u793a\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u7ec8\u8eab\u4ea4\u4e92\u5f0f\u5bfc\u822a\u95ee\u9898\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u73b0\u5b9e\u6311\u6218\uff0c\u63d0\u51fa\u7684LLM\u9a71\u52a8\u7ea6\u675f\u89c4\u5212\u6846\u67b6\u7ed3\u5408\u4e3b\u52a8\u611f\u77e5\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u6742\u7269\u963b\u585e\u7684\u73af\u5883\u4e2d\u901a\u8fc7\u79fb\u52a8\u7269\u4f53\u6765\u5f00\u8f9f\u8def\u5f84\uff0c\u5b8c\u6210\u987a\u5e8f\u7269\u4f53\u653e\u7f6e\u4efb\u52a1\u3002"}}
{"id": "2602.20057", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20057", "abs": "https://arxiv.org/abs/2602.20057", "authors": ["Ge Yuan", "Qiyuan Qiao", "Jing Zhang", "Dong Xu"], "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation", "comment": "Homepage: https://AdaWorldPolicy.github.io", "summary": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.", "AI": {"tldr": "AdaWorldPolicy\u662f\u4e00\u4e2a\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u3001\u6269\u6563\u7b56\u7565\u548c\u5728\u7ebf\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6a21\u5757\u95f4\u6df1\u5ea6\u7279\u5f81\u4ea4\u6362\uff0c\u5e76\u5229\u7528\u529b-\u529b\u77e9\u53cd\u9988\u7f13\u89e3\u52a8\u6001\u529b\u504f\u79fb\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u80fd\u591f\u9884\u6d4b\u7269\u7406\u7ed3\u679c\u5e76\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u5728\u7ebf\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u51cf\u5c11\u4eba\u5de5\u53c2\u4e0e\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\u3002", "method": "\u63d0\u51faAdaWorldPolicy\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u57fa\u4e8eFlow Matching Diffusion Transformers\u7684\u6a21\u5757\uff1a\u4e16\u754c\u6a21\u578b\u3001\u52a8\u4f5c\u4e13\u5bb6\u548c\u529b\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u81ea\u6ce8\u610f\u529b\u5c42\u76f8\u4e92\u8fde\u63a5\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u5728\u7ebf\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff0c\u52a8\u6001\u5207\u6362\u52a8\u4f5c\u751f\u6210\u6a21\u5f0f\u548c\u672a\u6765\u60f3\u8c61\u6a21\u5f0f\uff0c\u9a71\u52a8\u4e09\u4e2a\u6a21\u5757\u7684\u5b9e\u65f6\u66f4\u65b0\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaWorldPolicy\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5bf9\u5206\u5e03\u5916\u573a\u666f\u5177\u6709\u52a8\u6001\u81ea\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u7ed3\u5408\u529b-\u529b\u77e9\u53cd\u9988\u53ef\u4ee5\u7f13\u89e3\u52a8\u6001\u529b\u504f\u79fb\u3002\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u95ed\u73af\u673a\u5236\u5b9e\u73b0\u4e86\u5bf9\u89c6\u89c9\u548c\u7269\u7406\u57df\u504f\u79fb\u7684\u81ea\u9002\u5e94\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.20119", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20119", "abs": "https://arxiv.org/abs/2602.20119", "authors": ["Jiahui Fu", "Junyu Nan", "Lingfeng Sun", "Hongyu Li", "Jianing Qian", "Jennifer L. Barry", "Kris Kitani", "George Konidaris"], "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning", "comment": "25 pages, 15 figures. Project webpage: https://nova-plan.github.io/", "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/", "AI": {"tldr": "NovaPlan\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u3001\u89c6\u9891\u751f\u6210\u548c\u51e0\u4f55\u57fa\u7840\u673a\u5668\u4eba\u6267\u884c\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u9700\u8981\u673a\u5668\u4eba\u6574\u5408\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u548c\u4f4e\u5c42\u7269\u7406\u4ea4\u4e92\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5206\u89e3\u4efb\u52a1\u548c\u60f3\u8c61\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6267\u884c\u6240\u9700\u7684\u7269\u7406\u57fa\u7840\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff1a\u9ad8\u5c42\u4f7f\u7528VLM\u89c4\u5212\u5668\u95ed\u73af\u5206\u89e3\u4efb\u52a1\u5e76\u76d1\u63a7\u6267\u884c\uff0c\u5b9e\u73b0\u5355\u6b65\u5931\u8d25\u540e\u7684\u81ea\u4e3b\u91cd\u89c4\u5212\uff1b\u4f4e\u5c42\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u5173\u952e\u70b9\u548c\u4eba\u624b\u59ff\u6001\u4f5c\u4e3a\u8fd0\u52a8\u5b66\u5148\u9a8c\uff0c\u901a\u8fc7\u5207\u6362\u673a\u5236\u9009\u62e9\u66f4\u597d\u7684\u53c2\u8003\u6765\u751f\u6210\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u529f\u80fd\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNovaPlan\u80fd\u591f\u6267\u884c\u590d\u6742\u88c5\u914d\u4efb\u52a1\u5e76\u5c55\u793a\u7075\u5de7\u7684\u9519\u8bef\u6062\u590d\u884c\u4e3a\uff0c\u65e0\u9700\u4efb\u4f55\u5148\u9a8c\u6f14\u793a\u6216\u8bad\u7ec3\u3002", "conclusion": "NovaPlan\u901a\u8fc7\u7edf\u4e00\u95ed\u73afVLM\u89c6\u9891\u89c4\u5212\u548c\u51e0\u4f55\u57fa\u7840\u673a\u5668\u4eba\u6267\u884c\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u957f\u65f6\u7a0b\u64cd\u4f5c\uff0c\u5728\u590d\u6742\u88c5\u914d\u548c\u9519\u8bef\u6062\u590d\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.20150", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20150", "abs": "https://arxiv.org/abs/2602.20150", "authors": ["Wei-Cheng Huang", "Jiaheng Han", "Xiaohan Ye", "Zherong Pan", "Kris Hauser"], "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization", "comment": "15 pages, 13 figures, in submission", "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u771f\u5b9e\u5230\u4eff\u771f\u573a\u666f\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u8054\u5408\u6062\u590d\u591a\u4e2a\u521a\u6027\u7269\u4f53\u7684\u5f62\u72b6\u548c\u59ff\u6001\uff0c\u540c\u65f6\u8003\u8651\u7269\u7406\u7ea6\u675f\uff0c\u9002\u7528\u4e8e\u6742\u4e71\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9c81\u68d2\u6027\u5dee\u3001\u6269\u5c55\u5230\u591a\u4ea4\u4e92\u7269\u4f53\u65f6\u901a\u7528\u6027\u53d7\u9650\u7684\u95ee\u9898\uff0c\u800c\u51c6\u786e\u4f30\u8ba1\u4eff\u771f\u5c31\u7eea\u573a\u666f\u5bf9\u4e0b\u6e38\u89c4\u5212\u548c\u7b56\u7565\u5b66\u4e60\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u771f\u5b9e\u5230\u4eff\u771f\u573a\u666f\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u72b6\u53ef\u5fae\u63a5\u89e6\u6a21\u578b\u5b9e\u73b0\u7269\u4f53\u51e0\u4f55\u548c\u59ff\u6001\u7684\u8054\u5408\u4f18\u5316\uff0c\u5229\u7528\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6d77\u68ee\u77e9\u9635\u7684\u7ed3\u6784\u7a00\u758f\u6027\u5f00\u53d1\u9ad8\u6548\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\uff0c\u5e76\u6784\u5efa\u7aef\u5230\u7aef\u6d41\u7a0b\u5305\u62ec\u5b66\u4e60\u5f0f\u7269\u4f53\u521d\u59cb\u5316\u3001\u7269\u7406\u7ea6\u675f\u7684\u8054\u5408\u5f62\u72b6-\u59ff\u6001\u4f18\u5316\u548c\u53ef\u5fae\u7eb9\u7406\u7ec6\u5316\u3002", "result": "\u5728\u5305\u542b\u6700\u591a5\u4e2a\u7269\u4f53\u548c22\u4e2a\u51f8\u5305\u7684\u6742\u4e71\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9c81\u68d2\u5730\u91cd\u5efa\u7269\u7406\u6709\u6548\u3001\u4eff\u771f\u5c31\u7eea\u7684\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\u548c\u5173\u952e\u6280\u672f\u521b\u65b0\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6742\u4e71\u73af\u5883\u4e2d\u591a\u7269\u4f53\u573a\u666f\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u4eff\u771f\u5c31\u7eea\u573a\u666f\u3002"}}
