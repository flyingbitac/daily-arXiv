<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors](https://arxiv.org/abs/2512.09065)
*Shivendra Agrawal,Jake Brawer,Ashutosh Naik,Alessandro Roncone,Bradley Hayes*

Main category: cs.RO

TL;DR: ShelfAware是一个语义粒子滤波器，用于在准静态室内环境中实现鲁棒的全局定位，通过将场景语义作为统计证据处理，结合深度似然和类别中心语义相似度，在低成本视觉硬件上实现快速定位。


<details>
  <summary>Details</summary>
Motivation: 许多室内工作空间是准静态的：全局布局稳定但局部语义不断变化，产生重复几何、动态杂乱和感知噪声，这些因素会破坏基于视觉的定位系统。

Method: ShelfAware采用语义粒子滤波器，将场景语义视为对象类别的统计证据而非固定地标。它融合深度似然和类别中心语义相似度，使用预计算的语义视点库在MCL内执行逆语义提议，实现快速、有针对性的假设生成。

Result: 在语义密集的零售环境中进行100次全局定位试验，涵盖四种条件（推车安装、可穿戴、动态障碍和稀疏语义），ShelfAware实现96%成功率（vs. MCL 22%和AMCL 10%），平均收敛时间1.91秒，在所有条件下获得最低平移RMSE，80%测试序列中保持稳定跟踪，在消费级笔记本电脑平台上实时运行。

Conclusion: 通过在类别级别对语义进行分布建模并利用逆提议，ShelfAware解决了准静态领域中常见的几何混叠和语义漂移问题。该方法仅需视觉传感器和VIO，可作为基础设施无关的移动机器人构建模块，支持创建为视障人士提供随时开始、共享控制的辅助导航设备。

Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.

</details>


### [2] [Inferring Operator Emotions from a Motion-Controlled Robotic Arm](https://arxiv.org/abs/2512.09086)
*Xinyu Qi,Zeyu Deng,Shaun Alexander Macdonald,Liying Li,Chen Wang,Muhammad Ali Imran,Philip G. Zhao*

Main category: cs.RO

TL;DR: 通过分析远程控制机器人运动数据，可以推断操作者的情绪状态，准确率达到83.3%


<details>
  <summary>Details</summary>
Motivation: 远程机器人操作者的情绪状态会显著影响机器人的运动，可能导致意外后果。当前的情绪识别方法依赖生理信号或身体语言，但这些方法在远程控制场景中存在设备限制和用户参与度问题。

Method: 提出了一种机器学习系统，通过分析远程控制机器人（非为情感表达设计）的功能性运动来推断人类操作者的情绪状态。系统基于操作者手部运动产生的机器人运动数据进行情感识别。

Result: 系统在识别用户通过机器人运动表达的情绪状态方面达到了83.3%的准确率。

Conclusion: 即使是未设计用于情感表达的机器人，其功能性运动也能有效反映操作者的情绪状态。这一发现对当前和未来的远程机器人操作及情感机器人应用具有重要意义。

Abstract: A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.

</details>


### [3] [Masked Generative Policy for Robotic Control](https://arxiv.org/abs/2512.09101)
*Lipeng Zhuang,Shiyu Fan,Florent P. Audonnet,Yingdong Ru,Gerardo Aragon Camarasa,Paul Henderson*

Main category: cs.RO

TL;DR: MGP是一个用于视觉运动模仿学习的新框架，将动作表示为离散标记，通过条件掩码变换器并行生成标记并快速优化低置信度标记，在150个机器人操作任务上实现了更快的推理速度和更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂和非马尔可夫任务时存在困难，需要开发能够实现全局一致预测和鲁棒自适应执行能力的框架。

Method: 将动作表示为离散标记，训练条件掩码变换器并行生成标记并快速优化低置信度标记；提出MGP-Short（并行掩码生成+基于分数的优化）和MGP-Long（单次预测完整轨迹+基于新观测动态优化低置信度动作标记）两种采样范式。

Result: 在Meta-World和LIBERO基准的150个机器人操作任务上，MGP相比最先进的扩散和自回归策略实现了更快的推理速度和更高的成功率：平均成功率提高9%，每序列推理时间减少高达35倍；在动态和缺失观测环境中平均成功率提高60%；解决了两个其他最先进方法无法处理的非马尔可夫场景。

Conclusion: MGP框架通过离散动作标记表示和条件掩码变换器，结合两种采样范式，实现了全局一致预测和鲁棒自适应执行，在复杂和非马尔可夫任务上表现出色，为视觉运动模仿学习提供了有效的解决方案。

Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.

</details>


### [4] [Cognitive Trust in HRI: "Pay Attention to Me and I'll Trust You Even if You are Wrong"](https://arxiv.org/abs/2512.09105)
*Adi Manor,Dan Cohen,Ziv Keidar,Avi Parush,Hadas Erel*

Main category: cs.RO

TL;DR: 研究发现机器人的高度专注力可以补偿其低能力表现，在建立认知信任中形成情感补偿机制


<details>
  <summary>Details</summary>
Motivation: 探索认知信任形成中机器人能力与专注力之间的相互作用，研究是否存在补偿机制，即一个因素能否弥补另一个因素的不足

Method: 采用2x2实验设计（能力：高/低 × 专注力：高/低），参与者与机器狗合作完成搜索任务，评估不同组合下的认知信任水平

Result: 高度专注力可以补偿低能力：与高度专注但能力低的机器人合作的参与者报告了与高能力机器人相当的信任水平；当机器人不专注时，低能力导致认知信任显著下降

Conclusion: 人机交互中的认知信任形成比传统认知更为复杂，涉及通常被忽视的情感过程；存在情感补偿机制，需要在传统基于能力的认知信任模型中增加这一层面的考量

Abstract: Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.

</details>


### [5] [Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous](https://arxiv.org/abs/2512.09111)
*Yuji Takubo,Arpit Dwivedi,Sukeerth Ramkumar,Luis A. Pabon,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: cs.RO

TL;DR: SAGES框架将自然语言指令转换为满足非凸约束的航天器轨迹，实现语言条件化的轨迹生成，减少专家负担。


<details>
  <summary>Details</summary>
Motivation: 当前自主轨迹优化方法依赖大量专家输入（如航点、约束、任务时间线等），限制了实际交会任务中的操作可扩展性，需要更直观的自然语言交互方式。

Method: 提出SAGES（语义自主制导引擎），将自然语言命令转换为反映高层意图的航天器轨迹，同时尊重非凸约束。

Result: 在两个场景中验证：具有连续时间约束执行的容错接近操作和自由飞行机器人平台。SAGES可靠地产生与人类命令一致的轨迹，在不同行为模式下实现超过90%的语义行为一致性。

Conclusion: 这是迈向语言条件化、约束感知的航天器轨迹生成的初步步骤，使操作员能够通过直观的自然语言命令交互式指导安全性和行为，减少专家负担。

Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.

</details>


### [6] [UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects](https://arxiv.org/abs/2512.09283)
*Fan Wu,Chenguang Yang,Haibin Yang,Shuo Wang,Yanrui Xu,Xing Zhou,Meng Gao,Yaoqi Xian,Zhihong Zhu,Shifeng Huang*

Main category: cs.RO

TL;DR: UPETrack是一个基于单向位置估计的几何驱动框架，用于实时跟踪可变形线性物体，无需物理建模、虚拟仿真或视觉标记，在定位精度和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可变形线性物体在工业装配、医疗手术和日常应用中需要实时状态跟踪，但其高维配置空间、非线性动力学和频繁部分遮挡给鲁棒实时跟踪带来了根本性障碍。

Method: UPETrack框架包含两个阶段：(1) 基于高斯混合模型和期望最大化算法的可见段跟踪；(2) 使用提出的单向位置估计算法进行遮挡区域预测。UPE利用DLO形状的几何连续性和时间演化模式，通过局部线性组合位移项、近端线性约束项和历史曲率项三个主要机制推导出闭式位置估计器。

Result: 实验结果表明，UPETrack在定位精度和计算效率上都超越了两种最先进的跟踪算法（TrackDLO和CDCPD2）。

Conclusion: UPETrack通过几何驱动的单向位置估计框架，为可变形线性物体的实时跟踪提供了一种高效稳定的解决方案，无需复杂的物理建模或迭代优化。

Abstract: Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.

</details>


### [7] [One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation](https://arxiv.org/abs/2512.09297)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: BiDemoSyn框架从单个真实世界演示合成数千个接触丰富的双手操作演示，通过协调块分解和视觉引导优化实现高效且物理可行的数据生成


<details>
  <summary>Details</summary>
Motivation: 当前双手操作策略学习面临两难：遥操作提供物理真实数据但劳动密集，仿真合成可扩展但存在仿真到现实的差距。需要一种既能高效扩展又能保持物理真实性的方法。

Method: 将任务分解为不变的协调块和可变的物体依赖调整，通过视觉引导对齐和轻量级轨迹优化，从单个真实演示合成数千个物理可行的双手操作演示。

Result: 在六个双臂任务中，使用BiDemoSyn数据训练的策略对新物体姿态和形状具有鲁棒泛化能力，显著优于现有基线方法。

Conclusion: BiDemoSyn在效率和真实世界保真度之间架起桥梁，为复杂双手操作提供了一种可扩展的模仿学习路径，无需在物理真实性上妥协。

Abstract: Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.

</details>


### [8] [Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning](https://arxiv.org/abs/2512.09310)
*Kwang Bin Lee,Jiho Kang,Sung-Hee Lee*

Main category: cs.RO

TL;DR: 提出一个场景无关的双手机器人任务规划统一框架，通过视觉点定位、双手机器人子目标规划和交互点驱动的双手机器人提示三个模块，实现从高级指令到可执行双手机器人行为的转换。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划器主要是单手机器人操作，无法解决场景无关设置中双手机器人操作的空间、几何和协调挑战。需要在开放环境中将高级指令转化为可执行的双手机器人行为。

Method: 集成三个关键模块：1) 视觉点定位(VPG)：分析单张场景图像检测相关物体并生成世界对齐的交互点；2) 双手机器人子目标规划器(BSP)：基于空间邻接和跨物体可达性推理，生成紧凑、运动中立化的子目标；3) 交互点驱动的双手机器人提示(IPBP)：将子目标绑定到结构化技能库，实例化满足手部状态和可供性约束的同步单手机器人或双手机器人动作序列。

Result: 实验表明，该方法能够产生连贯、可行且紧凑的双手机器人规划，无需重新训练即可泛化到杂乱场景，展示了双手机器人任务的鲁棒场景无关可供性推理能力。

Conclusion: 提出的统一框架能够使智能体在杂乱、未见过的场景中规划语义上有意义、物理上可行且可并行的双手机器人行为，解决了双手机器人操作中的空间、几何和协调挑战。

Abstract: Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.

</details>


### [9] [COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.09349)
*Lin Li,Yuxin Cai,Jianwu Fang,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: COVLM-RL：结合关键目标导向推理与VLM引导强化学习的端到端自动驾驶框架，通过CoT提示策略生成语义决策先验，加速训练并提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架存在泛化能力不足、训练效率低和可解释性差的问题。基于视觉语言模型的方法在新场景中缺乏鲁棒性，而强化学习方法虽然适应性强但数据效率低且决策不透明。

Method: 提出COVLM-RL框架：1) 设计链式思维提示策略，让VLM对关键交通元素进行推理并生成高级语义决策先验；2) 引入一致性损失函数，确保VLM的语义规划与RL智能体的控制输出对齐；3) 将语义先验注入RL循环，降低输入维度并加速训练。

Result: 在CARLA模拟器中的实验表明：在训练过的驾驶环境中成功率提升30%，在未见环境中成功率提升50%，显示出强大的泛化能力。

Conclusion: COVLM-RL通过整合VLM的语义推理与RL的适应性控制，有效解决了自动驾驶框架的泛化、效率和可解释性问题，为端到端自动驾驶提供了新的解决方案。

Abstract: End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\% in trained driving environments and by 50\% in previously unseen environments, highlighting its strong generalization capability.

</details>


### [10] [Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances](https://arxiv.org/abs/2512.09377)
*Lidan Xu,Dadong Fan,Junhong Wang,Wenshuo Li,Hao Lu,Jianzhong Qiao*

Main category: cs.RO

TL;DR: 该研究证明在仅有无人机里程计信息的情况下，双无人机-杆系统的载荷姿态在多源扰动下是可观测的，并设计了基于扰动观测器的误差状态扩展卡尔曼滤波器进行状态和扰动估计。


<details>
  <summary>Details</summary>
Motivation: 现有的协同悬吊空中运输系统对多源扰动（如空气动力效应和推力不确定性）高度敏感。为实现精确载荷操纵，现有方法通常依赖额外传感器测量缆绳方向或载荷姿态，这增加了系统成本和复杂性。核心问题是：仅使用无人机里程计信息，在多源扰动下载荷姿态是否可观测？

Method: 1. 针对双无人机-杆系统，使用可观测性秩判据证明当仅存在两种或更少类型的集总扰动时，整个系统是可观测的；2. 针对扰动仅作用于无人机的情况，开发复合扰动滤波方案，设计了基于扰动观测器的误差状态扩展卡尔曼滤波器，用于在流形$(mathbb{R}^3)^2times(TS^2)^3$上演化的整个系统的状态和扰动估计。

Result: 仿真和实验测试验证了仅使用无人机里程计信息可以完全估计系统的状态和扰动。这是首次证明此类结论，为通过最小化传感器套件实现更具成本效益和鲁棒性的系统铺平了道路。

Conclusion: 该研究证明了在特定扰动条件下，仅使用无人机里程计信息即可实现双无人机-杆系统的完全状态和扰动估计，为减少传感器依赖、降低成本和提高系统鲁棒性提供了理论基础和实用方法。

Abstract: Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\mathbb{R}^3)^2\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.

</details>


### [11] [H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos](https://arxiv.org/abs/2512.09406)
*Hai Ci,Xiaokang Liu,Pei Yang,Yiren Song,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 提出视频到视频转换框架，将普通人-物交互视频转换为运动一致、物理基础的机器人操作视频，无需配对训练数据


<details>
  <summary>Details</summary>
Motivation: 让机器人通过观察日常人类视频学习操作技能，避免繁琐的机器人数据收集，实现大规模技能获取

Method: 使用可迁移表示桥接具身鸿沟：通过修复训练视频中的机器人手臂获得干净背景，叠加简单视觉线索（标记和箭头表示夹爪位置和方向），训练生成模型将机器人手臂重新插入场景；在测试时对人物视频应用相同流程，生成模仿人类动作的高质量机器人视频；采用上下文学习方式微调SOTA视频扩散模型确保时间一致性和利用其丰富先验知识

Result: 实验结果表明，相比基线方法，该方法能生成更真实、更物理基础的机器人运动，为从无标签人类视频扩展机器人学习提供了有前景的方向

Conclusion: 提出的视频到视频转换框架能够有效利用日常人类视频生成物理基础的机器人操作视频，无需配对训练数据，为大规模机器人技能学习提供了可行方案

Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/

</details>


### [12] [Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation](https://arxiv.org/abs/2512.09410)
*Jialin Ying,Zhihao Li,Zicheng Dong,Guohua Wu,Yihuan Liao*

Main category: cs.RO

TL;DR: PGF-MAPPO：一种分层强化学习框架，结合拓扑规划和反应控制，解决多智能体在复杂环境中的追逃问题，通过A*势场奖励塑形和方向性前沿分配提升探索效率。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习在复杂环境追逃任务中面临稀疏奖励和视野受限的挑战，导致探索效率低下且难以扩展到大规模场景。

Method: 提出PGF-MAPPO分层框架，将拓扑规划与反应控制结合。采用A*势场进行密集奖励塑形解决局部极小值和稀疏奖励问题；引入方向性前沿分配（结合最远点采样和几何角度抑制）实现空间分散；使用参数共享的分散式批评器保持O(1)模型复杂度。

Result: PGF-MAPPO在追捕更快逃逸者时表现出优越的捕获效率。在10x10地图上训练的策略能够零样本泛化到未见过的20x20环境，显著优于基于规则和学习的基础方法。

Conclusion: PGF-MAPPO通过分层设计和前沿分配机制有效解决了复杂环境中的多智能体追逃问题，实现了高效的探索和良好的泛化能力，适用于机器人群体应用。

Abstract: Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.

</details>


### [13] [D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM](https://arxiv.org/abs/2512.09411)
*Siting Zhu,Yuxiang Huang,Wenhua Wu,Chaokang Jiang,Yongbo Chen,I-Ming Chen,Hesheng Wang*

Main category: cs.RO

TL;DR: D²GSLAM是一种基于高斯表示的动态SLAM系统，能够在动态环境中同时进行准确的动态重建和鲁棒跟踪，通过几何提示动态分离、动静复合表示、渐进位姿优化和运动一致性损失四个关键组件实现。


<details>
  <summary>Details</summary>
Motivation: 现有密集SLAM方法在静态环境中表现出色，但在动态环境中面临挑战。大多数方法直接移除动态物体，忽略了这些物体包含的运动信息，无法同时进行动态重建和跟踪。

Method: 提出四个关键组件：1) 几何提示动态分离方法，利用高斯表示的几何一致性区分动静元素；2) 动静复合表示，整合静态3D高斯和动态4D高斯；3) 渐进位姿优化策略，利用静态场景几何的多视角一致性和动态物体运动信息；4) 运动一致性损失，利用物体运动的时序连续性。

Result: D²GSLAM在动态场景中展现出优越的建图和跟踪精度，同时能够进行准确的动态建模。

Conclusion: 该系统成功解决了动态环境中SLAM的挑战，能够同时处理静态场景重建和动态物体建模，为动态SLAM提供了新的解决方案。

Abstract: Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.

</details>


### [14] [A Hierarchical, Model-Based System for High-Performance Humanoid Soccer](https://arxiv.org/abs/2512.09431)
*Quanyou Wang,Mingzhang Zhu,Ruochen Hou,Kay Gillespie,Alvin Zhu,Shiqi Wang,Yicheng Wang,Gaberiel I. Fernandez,Yeting Liu,Colin Togashi,Hyunwoo Nam,Aditya Navghare,Alex Xu,Taoyuanmin Zhu,Min Sung Ahn,Arturo Flores Alvarez,Justin Quan,Ethan Hong,Dennis W. Hong*

Main category: cs.RO

TL;DR: 本文介绍了赢得RoboCup 2024成人尺寸人形机器人足球赛冠军的ARTEMIS机器人系统，包括其轻量化硬件设计、高扭矩准直驱执行器、专用足部设计，以及集成的感知定位、导航和行为管理系统。


<details>
  <summary>Details</summary>
Motivation: 随着驱动、传感和控制技术的进步，竞技人形机器人的发展受到广泛关注。RoboCup作为完全自主人形机器人的国际竞赛，为实现2050年与人类足球运动员对抗的长期目标提供了独特的挑战性基准。本文旨在展示团队赢得RoboCup 2024成人尺寸人形足球赛冠军所依赖的硬件和软件创新。

Method: 硬件方面：采用轻量化结构组件、高扭矩准直驱执行器和专用足部设计，能够在保持运动稳定性的同时实现强有力的步态内踢球。软件方面：开发了集成的感知和定位框架，结合立体视觉、目标检测和基于地标的融合技术，可靠估计球、球门、队友和对手位置。中层导航栈生成碰撞感知的动态可行轨迹，集中式行为管理器根据游戏状态协调高级决策、角色选择和踢球执行。

Result: 这些子系统的无缝集成实现了快速、精确和战术有效的游戏玩法，在真实比赛的动态对抗条件下表现出稳健性能。ARTEMIS机器人系统成功赢得了2024年成人尺寸人形足球赛冠军。

Conclusion: 本文展示了ARTEMIS机器人系统的设计原则、系统架构和实验结果，这些创新为赢得RoboCup 2024成人尺寸人形足球赛冠军做出了重要贡献，推动了竞技人形机器人技术的发展。

Abstract: The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.

</details>


### [15] [Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments](https://arxiv.org/abs/2512.09447)
*Jaehyun Kim,Seungwon Choi,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出了一种基于截断序贯概率比检验(SPRT)的激光雷达闭环验证方法，通过累积多帧描述符相似度证据进行自适应决策，旨在抑制室内重复结构环境中的误报。


<details>
  <summary>Details</summary>
Motivation: 现有激光雷达闭环检测方法通常基于单帧描述符比较或使用固定阈值配合后期ICP验证，在结构重复的室内环境中容易产生误报。需要一种能够有效抑制误报的验证方法。

Method: 将激光雷达闭环验证建模为截断序贯概率比检验(SPRT)，通过累积查询帧与候选帧之间短时序列的描述符相似度证据，根据用户指定的I/II类错误目标自适应地做出接受/拒绝决策。

Result: 在五个序列的图书馆数据集上评估，使用固定检索前端和多种代表性激光雷达全局描述符。与单帧和启发式多帧基线相比，序贯验证器在所有描述符上都一致提高了精度，减少了混淆闭环的影响。

Conclusion: 提出的基于SPRT的多帧闭环验证方法能够有效抑制室内重复结构环境中的误报，相比传统方法显著提高了闭环检测的精度和可靠性。

Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.

</details>


### [16] [Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing](https://arxiv.org/abs/2512.09462)
*Jayant Unde,Takumi Inden,Yuki Wakayama,Jacinto Colan,Yaonan Zhu,Tadayoshi Aoyama,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 开发用于帮助老年人或偏瘫患者穿脱裤子的夹持器系统，平衡柔顺性与抓取力，实现安全精准操作


<details>
  <summary>Details</summary>
Motivation: 人口老龄化背景下，保持老年人生活质量成为重要关切。对于身体能力受损的老年人，如厕辅助是最重要的问题之一，特别是帮助老年人或偏瘫患者穿脱裤子的需求

Method: 设计开发夹持器系统，平衡柔顺性与抓取力，确保精确操作同时保持与用户的安全柔顺交互。将夹持器集成到定制机器人操纵器系统中，为偏瘫患者提供穿脱裤子的全面解决方案

Result: 实验评估和与现有研究的比较表明，该夹持器能够在有限空间内成功辅助穿脱裤子，具有较高的成功率

Conclusion: 这项研究有助于推进辅助机器人技术的发展，帮助老年人和身体受损者保持独立性并提高生活质量

Abstract: In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.

</details>


### [17] [ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics](https://arxiv.org/abs/2512.09510)
*Donato Caramia,Florian T. Pokorny,Giuseppe Triggiani,Denis Ruffino,David Naso,Paolo Roberto Massenio*

Main category: cs.RO

TL;DR: ViTA-Seg是一个基于Vision Transformer的类无关实时amodal分割框架，用于机器人箱体抓取中的遮挡处理，包含单头和双头架构，并提出了专门的合成数据集ViTA-SimData。


<details>
  <summary>Details</summary>
Motivation: 机器人箱体抓取中的遮挡问题会影响准确可靠的抓取规划，需要恢复包括隐藏区域在内的完整物体掩码。

Method: 提出ViTA-Seg框架，利用全局注意力机制恢复完整物体掩码，包含两种架构：单头用于amodal掩码预测，双头用于amodal和遮挡掩码预测；同时提出ViTA-SimData合成数据集。

Result: 在COOCA和KINS两个amodal基准测试中，ViTA-Seg双头架构在保持计算效率的同时，实现了强大的amodal和遮挡分割精度。

Conclusion: ViTA-Seg能够实现鲁棒的实时机器人操作，为工业箱体抓取场景提供了有效的遮挡处理解决方案。

Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.

</details>


### [18] [Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing](https://arxiv.org/abs/2512.09571)
*Feng Yu,Yu Hu,Yang Su,Yang Deng,Linzuo Zhang,Danping Zou*

Main category: cs.RO

TL;DR: 提出两阶段学习框架解决无人机竞速在未知杂乱环境中的泛化问题，通过软碰撞训练和硬碰撞精炼平衡速度与避障，实现敏捷飞行和跨环境泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的无人机竞速方法主要针对固定、无障碍的赛道，难以泛化到未知、杂乱的复杂环境。主要挑战包括：平衡竞速速度与碰撞避免的矛盾；可行空间有限导致策略探索陷入局部最优；深度图中门与障碍物的感知模糊性（特别是当门位置仅粗略指定时）。

Method: 提出两阶段学习框架：1）初始软碰撞训练阶段，保持高速飞行的策略探索；2）硬碰撞精炼阶段，强制鲁棒的障碍物避免。采用自适应噪声增强课程学习和非对称actor-critic架构，逐步将策略依赖从特权门状态信息转移到基于深度的视觉输入。施加Lipschitz约束并集成赛道基元生成器，增强运动稳定性和跨环境泛化能力。

Result: 通过广泛的仿真和消融研究评估框架，并在计算受限的四旋翼无人机上进行真实世界实验验证。系统实现了敏捷飞行，同时对门位置误差保持鲁棒性，开发出能够在多样化、部分未知和杂乱环境中运行的可泛化无人机竞速框架。

Conclusion: 该研究成功解决了无人机竞速在未知杂乱环境中的泛化挑战，通过创新的两阶段学习框架和自适应课程学习方法，平衡了速度与安全，实现了跨环境的鲁棒性能，为实际应用中的无人机竞速提供了有效的解决方案。

Abstract: Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/

</details>


### [19] [GLaD: Geometric Latent Distillation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09619)
*Minghao Guo,Meng Cao,Jiachen Tao,Rongtao Xu,Yan Yan,Xiaodan Liang,Ivan Laptev,Xiaojun Chang*

Main category: cs.RO

TL;DR: GLaD是一个几何感知的视觉-语言-动作模型框架，通过知识蒸馏在预训练中融入3D几何先验，在机器人任务中超越了仅使用RGB信息的现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖RGB信息，忽略了空间推理和操作中至关重要的几何线索。几何信息对于机器人理解3D空间和执行精确操作至关重要。

Method: 引入GLaD框架，通过知识蒸馏将3D几何先验融入预训练过程。不是简单地将几何特征蒸馏到视觉编码器，而是将LLM中对应视觉token的隐藏状态与冻结的几何感知视觉变换器(VGGT)的特征对齐，确保几何理解深度集成到驱动动作预测的多模态表示中。

Result: 在Bridge数据集上预训练后，GLaD在四个LIBERO任务套件中实现了94.1%的平均成功率，优于使用相同预训练数据的UniVLA(92.5%)。这表明几何感知预训练增强了空间推理和策略泛化能力。

Conclusion: 几何感知预训练能够显著提升VLA模型的空间推理能力和策略泛化性能，且无需显式深度传感器或3D标注，为机器人学习提供了有效的几何信息整合方法。

Abstract: Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.

</details>


### [20] [ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat](https://arxiv.org/abs/2512.09656)
*Nicolas Marticorena,Tobias Fischer,Niko Suenderhauf*

Main category: cs.RO

TL;DR: ReMoSPLAT：基于高斯泼溅表示和二次规划的反应式移动机械臂控制器，能在复杂场景中实现末端执行器姿态跟踪同时避障


<details>
  <summary>Details</summary>
Motivation: 反应式控制能优雅协调移动机械臂基座和手臂运动，但如何在不涉及昂贵规划的情况下融入精确环境表示以避障仍是挑战

Method: 提出ReMoSPLAT反应式控制器，基于二次规划公式，利用高斯泼溅表示进行碰撞避免，通过优化公式中的额外约束和成本实现目标

Result: 在合成和真实世界扫描的仿真实验中验证方法可行性，性能接近依赖完美地面真实信息的控制器，比较了两种高效计算机器人-障碍物距离方法的权衡

Conclusion: 提出的方法能有效实现移动机械臂在复杂场景中的末端姿态跟踪和避障，展示了高斯泼溅表示在反应式控制中的实用性

Abstract: Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.

</details>


### [21] [High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle](https://arxiv.org/abs/2512.09798)
*Misael Mamani,Mariel Fernandez,Grace Luna,Steffani Limachi,Leonel Apaza,Carolina Montes-Dávalos,Marcelo Herrera,Edwin Salcedo*

Main category: cs.RO

TL;DR: 本文提出了一种太阳能驱动的全自主无人水面艇，配备新型注射器采样系统，单次任务可采集72个离散水样，结合ROS 2自主导航栈和模块化采样架构，实现了高空间分辨率的水质监测。


<details>
  <summary>Details</summary>
Motivation: 现有无人水面艇采样能力有限，通常只能采集少量样本或依赖代表性差的单点传感器，无法满足准确水质评估所需的空间分辨率要求。

Method: 开发太阳能驱动全自主USV，采用注射器采样架构（6×12模块化系统），集成ROS 2自主栈（GPS-RTK导航、LiDAR和立体视觉障碍检测、Nav2任务规划）、行为树自主架构和分布式micro-ROS节点控制。

Result: 在玻利维亚拉巴斯Achocalla泻湖的实地测试显示：航点精度达87%，稳定自主导航，采集的物理化学参数（温度、pH、电导率、总溶解固体）与手动参考数据相当，采样空间覆盖范围超过现有USV采样器。

Conclusion: 该平台实现了可靠的高分辨率采样和自主任务执行，为偏远环境的水生监测提供了可扩展解决方案，显著提升了水质评估的空间代表性。

Abstract: Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.

</details>


### [22] [Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration](https://arxiv.org/abs/2512.09833)
*Elias Krantz,Ngai Nam Chan,Gunnar Tibert,Huina Mao,Christer Fuglesang*

Main category: cs.RO

TL;DR: 开发了一个轻量级开源通信桥，连接Basilisk航天器动力学仿真器和ROS 2系统，实现实时双向数据交换，支持从仿真到硬件的无缝过渡


<details>
  <summary>Details</summary>
Motivation: 将高保真航天器仿真器与模块化机器人框架集成仍然是自主性开发的一个挑战，需要解决两者之间的通信和数据交换问题

Method: 开发了一个轻量级开源通信桥，无需修改Basilisk核心代码，可与ROS 2节点无缝集成，支持实时双向数据交换

Result: 成功应用于领导者-跟随者编队飞行场景，使用非线性模型预测控制，在仿真和ATMOS平面微重力测试平台上实现相同部署

Conclusion: 该通信桥为模块化航天器自主性和可重复研究工作流程提供了一个灵活、可扩展的平台，支持快速开发、硬件在环测试和从仿真到硬件的无缝过渡

Abstract: Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.

</details>


### [23] [Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation](https://arxiv.org/abs/2512.09851)
*Yuyang Li,Yinghan Chen,Zihang Zhao,Puhao Li,Tengyu Liu,Siyuan Huang,Yixin Zhu*

Main category: cs.RO

TL;DR: TacThru传感器实现同时视觉触觉感知，TacThru-UMI框架利用多模态信号进行模仿学习，在5个真实任务中达到85.5%成功率，显著优于交替感知和纯视觉基线。


<details>
  <summary>Details</summary>
Motivation: 现有透皮传感器缺乏同时多模态感知能力且触觉跟踪不可靠，同时将丰富的多模态信号集成到基于学习的操作管道中仍是一个开放挑战。

Method: 提出TacThru传感器（全透明弹性体、持续照明、新型关键线标记、高效跟踪）和TacThru-UMI模仿学习框架（基于Transformer的扩散策略集成多模态信号）。

Result: 在5个具有挑战性的真实世界任务中，TacThru-UMI平均成功率达到85.5%，显著优于交替触觉-视觉感知（66.3%）和纯视觉（55.4%）基线。在薄软物体接触检测和需要多模态协调的精确操作等关键场景中表现出色。

Conclusion: 同时多模态感知与现代学习框架的结合能够实现更精确、适应性更强的机器人操作。

Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.

</details>


### [24] [YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos](https://arxiv.org/abs/2512.09903)
*Ryan Meegan,Adam D'Souza,Bryan Bo Cao,Shubham Jain,Kristin Dana*

Main category: cs.RO

TL;DR: YOPO-Nav：一种基于视频轨迹的视觉导航方法，使用3D高斯泼溅模型编码环境，通过分层定位和姿态细化实现机器人导航


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航依赖详细地图和路径规划，构建和维护3D地图计算成本高、内存密集。本文利用探索视频作为视觉参考，让机器人能够重走已探索轨迹而无需依赖度量地图。

Method: 提出YOPO-Nav方法，将环境编码为相互连接的局部3D高斯泼溅模型组成的紧凑空间表示。采用分层设计：视觉地点识别模块提供粗略定位，局部3DGS模型细化目标和中间姿态以生成控制动作。

Result: 引入YOPO-Campus数据集（4小时自我中心视频，超过6公里人工遥控机器人轨迹）。在Clearpath Jackal机器人上测试，YOPO-Nav在真实场景的图像目标导航中表现出色。

Conclusion: YOPO-Nav提供了一种基于视频轨迹的高效视觉导航解决方案，避免了传统3D地图的构建和维护成本，在真实机器人平台上验证了有效性。

Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.

</details>


### [25] [Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/abs/2512.09927)
*Yifan Ye,Jiaqi Ma,Jun Cen,Zhihe Lu*

Main category: cs.RO

TL;DR: TEAM-VLA是一种无需训练的动态令牌压缩框架，通过扩展和合并视觉语言动作模型中的令牌来加速推理，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言动作模型通常有数十亿参数，在动态环境中实时部署时面临计算开销大、延迟敏感的问题，需要在不重新训练的情况下提高推理效率。

Method: 提出TEAM-VLA框架，包含动态令牌扩展机制（在注意力高亮区域附近采样额外信息令牌）和选择性令牌合并机制（在深层进行动作感知指导的令牌合并），在单次前向传播中完成扩展与合并。

Result: 在LIBERO基准测试中，TEAM-VLA在保持甚至超越完整VLA模型任务成功率的同时，显著提高了推理速度。

Conclusion: TEAM-VLA提供了一种无需训练的高效令牌压缩方案，在效率与效果之间取得了良好平衡，为VLA模型的实时部署提供了可行方案。

Abstract: Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}

</details>


### [26] [HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09928)
*Minghui Lin,Pengxiang Ding,Shu Wang,Zifeng Zhuang,Yang Liu,Xinyang Tong,Wenxuan Song,Shangke Lyu,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: HiF-VLA提出一个利用运动信息进行双向时序推理的框架，通过后见、洞见和预见机制提升VLA模型的长时程操作能力，在仿真和真实机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型大多假设马尔可夫性，仅依赖当前观测，导致时序短视问题，影响长时程操作的连贯性。运动作为更紧凑且信息丰富的时序上下文表示，能捕捉状态间变化并过滤静态像素噪声。

Method: 提出HiF-VLA统一框架，利用运动进行双向时序推理：1）通过后见先验编码过去动态；2）通过预见推理预测未来运动；3）通过后见调制的联合专家整合两者，实现"边行动边思考"范式。

Result: 在LIBERO-Long和CALVIN ABC-D基准测试中超越强基线，推理延迟增加可忽略。在真实世界长时程操作任务中取得显著改进，展示了在实际机器人场景中的广泛有效性。

Conclusion: HiF-VLA通过利用运动作为时序上下文表示，有效解决了VLA模型的时序短视问题，为长时程机器人操作提供了有效的双向时序推理框架，在实际应用中表现出色。

Abstract: Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.

</details>
