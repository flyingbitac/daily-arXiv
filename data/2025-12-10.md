<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VLD: Visual Language Goal Distance for Reinforcement Learning Navigation](https://arxiv.org/abs/2512.07976)
*Lazar Milikic,Manthan Patel,Jonas Frey*

Main category: cs.RO

TL;DR: 提出Vision-Language Distance (VLD)学习框架，通过解耦感知学习和策略学习来解决机器人导航中端到端策略训练的困难，利用互联网规模视频数据训练距离预测器，在仿真中训练RL策略，实现多模态目标导航。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像数据的端到端导航策略训练存在两个主要问题：1) 策略迁移时的仿真到现实差距(sim-to-real gap)；2) 带有动作标签的训练数据有限。这些问题使得直接从图像数据训练导航策略变得困难。

Method: 提出VLD学习框架，包含两个解耦阶段：1) 在互联网规模视频数据上训练自监督的距离到目标预测器，该预测器能泛化到图像和文本目标；2) 在仿真环境中使用特权几何距离信号训练RL策略，并注入噪声模拟距离预测器的不确定性。部署时，策略使用VLD预测作为输入。

Result: VLD在距离函数评估上优于先前的时序距离方法（如ViNT和VIP）。实验表明，该解耦设计在仿真中实现了有竞争力的导航性能，同时支持灵活的目标模态（图像和文本），为可靠的多模态导航策略提供了可扩展的路径。

Conclusion: VLD框架通过解耦感知学习和策略学习，利用大规模视觉训练获取语义目标信息，同时在仿真中学习鲁棒的低层导航行为，为解决机器人导航中的端到端策略训练问题提供了可扩展的替代方案。

Abstract: Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.

</details>


### [2] [DIJIT: A Robotic Head for an Active Observer](https://arxiv.org/abs/2512.07998)
*Mostafa Kamali Tabrizi,Mingshi Chi,Bir Bikram Dey,Yu Qing Yuan,Markus D. Solbach,Yiqian Liu,Michael Jenkin,John K. Tsotsos*

Main category: cs.RO

TL;DR: DIJIT是一个专为移动智能体设计的双目机器人头部系统，具有9个机械自由度和4个光学自由度，能够模拟人类眼-头-颈运动，用于研究主动视觉和人类视觉机制。


<details>
  <summary>Details</summary>
Motivation: 设计一个能够模拟人类眼-头-颈运动的机器人头部系统，用于研究主动视觉、人类视觉机制以及比较人类视觉与计算机视觉在解决视觉任务时的差异。

Method: 设计了具有9个机械自由度和4个光学自由度的双目机器人头部系统，运动范围和速度与人类性能相当，支持会聚立体视觉所需的运动范围（辐辏、版本和旋转）。开发了新的扫视相机运动方法，建立了相机方向与电机值之间的直接关系。

Result: DIJIT系统能够实现接近人类运动的扫视相机运动精度，为研究人类眼-头运动关系及其对视觉能力的影响提供了实验平台。

Conclusion: DIJIT是一个功能全面的机器人头部系统，能够有效模拟人类眼-头-颈运动，为主动视觉研究和人类与机器视觉比较提供了重要工具，其扫视运动方法实现了接近人类水平的精度。

Abstract: We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.

</details>


### [3] [Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations](https://arxiv.org/abs/2512.08028)
*Lampis Papakostas,Aristeidis Geladaris,Athanasios Mastrogeorgiou,Jim Sharples,Gautier Hattenberger,Panagiotis Chatzakos,Panagiotis Polygerinos*

Main category: cs.RO

TL;DR: 本文提出了一种用于灾害场景的无人机群系统，通过分布式传感器延长飞行时间，采用ESDF地图进行避障，结合TSP优化区域覆盖，优先扫描重要兴趣点。


<details>
  <summary>Details</summary>
Motivation: 在野火等灾害场景中，第一响应者需要高效、可靠的无人机系统来收集关键信息。传统单无人机系统存在飞行时间有限、碰撞风险高、覆盖效率低等问题，需要开发能够延长任务时间、确保安全、优化覆盖的无人机群系统。

Method: 1. 采用分布式传感器部署在多个无人机上，延长整体飞行时间；2. 使用局部欧几里得符号距离场（ESDF）地图进行实时障碍物避让；3. 引入旅行商问题（TSP）变体优化区域覆盖路径；4. 基于环境行为和关键基础设施为兴趣点（POI）分配优先级值。

Result: 通过不同规模无人机群的仿真验证，系统能够最大化区域覆盖，同时确保无人机之间以及无人机与障碍物之间的碰撞避免，证明了系统的有效性和可扩展性。

Conclusion: 提出的无人机群系统通过分布式传感器、ESDF避障和TSP优化覆盖，为灾害响应提供了可靠、高效的解决方案，能够延长任务时间、降低碰撞风险、优先扫描关键区域。

Abstract: This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.

</details>


### [4] [An Introduction to Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2512.08052)
*Pedro Santana*

Main category: cs.RO

TL;DR: 本文介绍了深度强化学习和深度模仿学习在具身智能体中的应用，采用深度优先的方法讲解基础算法，从马尔可夫决策过程到PPO算法，以及从行为克隆到GAIL算法。


<details>
  <summary>Details</summary>
Motivation: 具身智能体需要解决复杂的顺序决策问题，手动设计控制器困难，因此基于学习的方法成为有前景的替代方案，特别是深度强化学习和深度模仿学习。

Method: 采用深度优先的文献研究方法，聚焦于基础算法和技术：深度强化学习方面涵盖马尔可夫决策过程、REINFORCE和近端策略优化；深度模仿学习方面涵盖行为克隆、数据集聚合和生成对抗模仿学习。

Result: 文档提供了自包含的数学和机器学习概念介绍，深入讲解了深度强化学习和深度模仿学习的核心算法，为理解具身智能体的控制器设计提供了理论基础。

Conclusion: 本文通过深度优先的方法系统介绍了深度强化学习和深度模仿学习在具身智能体中的应用，为研究人员提供了深入理解这些基础算法的框架，而非广泛覆盖整个领域。

Abstract: Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.

</details>


### [5] [Chat with UAV -- Human-UAV Interaction Based on Large Language Models](https://arxiv.org/abs/2512.08145)
*Haoran Wang,Zhuohang Chen,Guang Li,Bo Ma,Chuanghuang Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于双智能体LLM框架的新型人机交互系统，通过任务规划智能体和执行智能体的分离设计，解决了传统LLM框架在混合任务规划和执行中的困难，提升了无人机交互的流畅性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 无人机交互系统正从工程师驱动转向用户驱动，但用户与无人机之间缺乏共同语言，难以实现个性化交互。虽然基于大语言模型的HUI框架已被提出，但现有方案在混合任务规划和执行方面存在困难，导致复杂场景适应性低。

Method: 提出双智能体HUI框架，构建两个独立的LLM智能体（任务规划智能体和执行智能体），应用不同的提示工程技术分别处理任务的理解、规划和执行。建立了覆盖四个典型无人机应用场景的任务数据库，并使用三个独立指标量化框架性能。

Result: 用户研究实验结果表明，该框架在设定的任务场景中提高了HUI的流畅性和任务执行的灵活性，有效满足了用户的个性化需求。同时比较了不同LLM模型控制无人机的性能表现。

Conclusion: 提出的双智能体LLM框架能够有效解决传统HUI框架在混合任务规划和执行中的困难，提升无人机交互体验，为个性化人机交互提供了可行的技术方案。

Abstract: The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.

</details>


### [6] [RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features](https://arxiv.org/abs/2512.08170)
*Haoxin Zhang,Shuaixin Li,Xiaozhou Zhu,Hongbo Chen,Wen Yao*

Main category: cs.RO

TL;DR: 提出一个无需标定板、兼容多种LiDAR和相机传感器的标定工具包，仅需一对激光点和相机图像，通过Gluestick管道建立2D-3D特征对应，自适应加权特征成本，实现高精度外参标定。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR-相机标定方法通常需要标定板或初始变换参数，且对传感器位置和旋转变化敏感。现有方法在无标定板环境下鲁棒性和精度不足，需要更通用、自动化的标定解决方案。

Method: 使用Gluestick管道建立2D-3D点和线特征对应关系，无需初始变换。定量分析特征分布对标定结果的影响，基于度量指标自适应加权每个特征的成本，通过过滤劣质特征优化外参参数。

Result: 在室内外多种LiDAR-相机传感器上进行广泛实验验证，结果表明该方法在鲁棒性和精度方面优于现有最先进技术，且无需标定板或初始变换。

Conclusion: 提出了一种用户友好、兼容性强、无需标定板的LiDAR-相机标定方法，通过自适应特征加权实现高精度外参估计，代码已开源供社区使用。

Abstract: In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.

</details>


### [7] [Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation](https://arxiv.org/abs/2512.08186)
*Meng Wei,Chenyang Wan,Jiaqi Peng,Xiqian Yu,Yuqiang Yang,Delin Feng,Wenzhe Cai,Chenming Zhu,Tai Wang,Jiangmiao Pang,Xihui Liu*

Main category: cs.RO

TL;DR: DualVLN提出首个双系统视觉语言导航基础模型，将高层推理与低层动作执行相结合，解决现有端到端方法动作碎片化、延迟高、难以应对动态障碍等问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法通常采用端到端管道，直接将视觉语言输入映射为短期离散动作，导致动作碎片化、延迟高，且难以应对动态障碍规避等现实挑战。

Method: 提出双系统架构：System 2（基于VLM的全局规划器）通过图像基础推理预测中期路径点目标；System 1（轻量级多模态条件扩散Transformer策略）利用System 2提供的显式像素目标和潜在特征生成平滑准确轨迹。

Result: DualVLN在所有VLN基准测试中优于先前方法，真实世界实验展示了在动态环境中鲁棒的长时程规划和实时适应能力。

Conclusion: 双系统设计实现了稳健的实时控制和复杂动态环境中的自适应局部决策，通过解耦训练保持VLM的泛化能力，同时实现可解释且有效的局部导航。

Abstract: While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.

</details>


### [8] [Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model](https://arxiv.org/abs/2512.08188)
*Wenjiang Xu,Cindy Wang,Rui Fang,Mingkang Zhang,Lusong Li,Jing Xu,Jiayuan Gu,Zecui Zeng,Rui Chen*

Main category: cs.RO

TL;DR: 提出Embodied Tree of Thoughts (EToT)框架，通过物理模拟器作为具身世界模型，将机器人操作规划构建为树搜索，结合先验分支和反思分支机制，在Real2Sim2Real框架中解决视频生成模型缺乏物理基础的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在机器人操作规划中缺乏严格的物理基础，容易产生幻觉，难以维持长时域物理约束的一致性。需要一种能够确保物理动力学和碰撞约束的规划框架。

Method: 提出EToT框架，采用Real2Sim2Real规划方法，利用基于物理的交互式数字孪生作为具身世界模型。将操作规划构建为树搜索，包含两个协同机制：1) 先验分支：基于语义和空间分析生成多样化候选执行路径；2) 反思分支：利用视觉语言模型诊断模拟器中的执行失败，并通过纠正动作迭代优化规划树。

Result: 在短期和长期操作任务套件上验证EToT，相比基线方法持续表现出色，能够有效预测物理动力学并适应潜在失败。

Conclusion: EToT通过将高级推理基于物理模拟器，确保生成的计划符合刚体动力学和碰撞约束，解决了视频生成模型缺乏物理基础的问题，为机器人操作规划提供了更可靠的解决方案。

Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .

</details>


### [9] [High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement](https://arxiv.org/abs/2512.08206)
*Duo Zhang,Junshan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: SDAR是一个用于桌面重排任务的双臂任务与运动规划框架，通过紧密集成的任务规划和同步运动规划，在复杂纠缠场景中实现100%成功率和高质量解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决双臂机器人在紧密协作环境下处理物体起始和目标配置高度纠缠的桌面重排任务，传统方法难以处理这种复杂的非单调、长视野任务。

Method: SDAR包含两个核心组件：SDAR-T任务规划器采用依赖图分解策略生成优化的双臂任务计划；SDAR-M运动规划器利用GPU SIMD工具和分层运动规划策略，从众多任务计划中筛选最佳同步双臂运动方案。

Result: SDAR在复杂非单调长视野桌面重排任务中实现了100%成功率，解决方案质量远超先前最先进方法，并在UR-5e双臂机器人上验证了直接可靠的实际部署能力。

Conclusion: SDAR框架通过紧密集成的任务与运动规划，有效解决了双臂机器人在复杂纠缠场景下的桌面重排问题，展现了优越的性能和实际应用价值。

Abstract: We propose Synchronous Dual-Arm Rearrange- ment Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal config- urations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR- M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state- of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.

</details>


### [10] [Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior](https://arxiv.org/abs/2512.08233)
*Timothy Chen,Marcus Dominguez-Kuhne,Aiden Swann,Xu Liu,Mac Schwager*

Main category: cs.RO

TL;DR: 提出基于贝叶斯框架的风险建模方法，通过人类演示视频和视觉语言模型学习空间变化的语义风险，用于机器人规划任务


<details>
  <summary>Details</summary>
Motivation: 人类对安全的理解不是二元信号，而是连续、上下文相关且空间依赖的风险概念。需要让自主系统内化人类类似的风险认知，以产生更符合人类偏好的行为

Method: 提出贝叶斯风险建模框架：先验来自预训练的视觉语言模型，似然函数通过学习的ViT将预训练特征映射到像素对齐的风险值。输入RGB图像和查询对象字符串，输出像素密集的风险图像

Result: 框架能够产生与人类偏好一致的情境化风险估计，可泛化到新对象和上下文，并支持快速适应额外观察或常识规则。在下游应用中作为视觉运动规划的价值学习器或与传统轨迹优化算法结合使用

Conclusion: 该框架是让自主系统内化人类类似风险认知的重要一步，具有扩展到更大训练数据集的潜力，为机器人产生人类类似运动提供了新方法

Abstract: Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.

</details>


### [11] [Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation](https://arxiv.org/abs/2512.08271)
*Srijan Dokania,Dharini Raghavan*

Main category: cs.RO

TL;DR: Zero-Splat TeleAssist是一个零样本传感器融合管道，将普通CCTV视频流转换为共享的6自由度世界模型，用于多边遥操作


<details>
  <summary>Details</summary>
Motivation: 现有的遥操作系统通常需要专用传感器或标记物，限制了在普通监控摄像头环境下的应用。需要一种能够在没有深度传感器或标记的情况下，从普通监控视频中实时重建6自由度世界模型的方法

Method: 整合视觉语言分割、单目深度估计、加权主成分分析姿态提取和3D高斯泼溅技术，将CCTV视频流转换为共享的6自由度世界模型

Result: 系统能够为每个操作员提供多个机器人的实时全局位置和方向，无需标记物或深度传感器，适用于交互为中心的遥操作设置

Conclusion: Zero-Splat TeleAssist提供了一种创新的零样本传感器融合方法，使普通监控摄像头能够支持多边遥操作，降低了系统部署成本和技术门槛

Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.

</details>


### [12] [Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making](https://arxiv.org/abs/2512.08280)
*Haldun Balim,Na Li,Yilun Du*

Main category: cs.RO

TL;DR: MPDiffuser是一个基于扩散模型的组合式模型预测框架，用于离线决策，通过规划器、动力学模型和排序器的组合，生成既符合任务目标又满足动力学可行性的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式方法在离线决策中经常产生动力学不可行的轨迹，需要一种能够同时保证任务对齐和动力学可行性的方法。

Method: 提出模型预测扩散器（MPDiffuser），包含三个组件：1）生成多样化任务对齐轨迹的规划器；2）确保与底层系统动力学一致性的动力学模型；3）选择与任务目标对齐行为的排序器。采用交替扩散采样方案，在采样过程中交替进行规划器和动力学更新，逐步优化轨迹。

Result: 在无约束（D4RL）和约束（DSRL）离线决策基准测试中均优于现有方法，组合设计提高了样本效率，能够利用低质量数据进行动力学学习并适应新动力学。初步研究显示可扩展到基于视觉的控制任务，并在真实四足机器人上验证了实用性。

Conclusion: MPDiffuser通过组合式模型预测扩散框架，有效解决了离线决策中轨迹动力学可行性的问题，在多个基准测试中表现优异，并展示了向高维感官输入扩展和实际机器人应用的潜力。

Abstract: Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.

</details>


### [13] [Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging](https://arxiv.org/abs/2512.08333)
*Yajat Yadav,Zhiyuan Zhou,Andrew Wagenmaker,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 通过简单的模型权重插值方法，在微调通用机器人策略时保持其泛化能力，使单个策略既能学习新任务又能保留预训练获得的广泛能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略在微调新任务时容易过拟合到特定演示，不仅失去原有广泛任务解决能力，也无法在新任务内部泛化。需要一种方法在微调时保持策略的泛化能力。

Method: 采用简单有效的策略：将微调后的模型权重与预训练模型权重进行插值，通过模型合并产生单一模型。

Result: 模型合并产生的单一模型继承了基础模型的通用能力，并能稳健地解决新任务，在新任务的分布外变化上优于预训练和微调模型。同时支持持续学习新技能而不牺牲已有能力。

Conclusion: 权重插值是一种简单而有效的方法，能够在微调通用机器人策略时保持其泛化能力，实现单个策略同时学习新任务和保留预训练获得的广泛能力。

Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.

</details>


### [14] [Learning Robot Manipulation from Audio World Models](https://arxiv.org/abs/2512.08405)
*Fan Zhang,Michael Gienger*

Main category: cs.RO

TL;DR: 提出一种生成式潜在流匹配模型来预测未来音频观测，使机器人策略能够推理长期后果，在需要感知真实世界音频或音乐信号的操作任务中表现优于无前瞻的方法。


<details>
  <summary>Details</summary>
Motivation: 许多机器人学习任务需要多模态推理，例如灌水任务仅凭视觉信息可能模糊或不完整，需要基于音频的时间演化进行推理，考虑其物理属性和音高模式。现有方法缺乏对未来音频状态的准确预测能力。

Method: 提出生成式潜在流匹配模型来预测未来音频观测，该模型能够集成到机器人策略中，使系统能够推理长期后果。模型专注于准确预测体现内在节奏模式的未来音频状态。

Result: 在两个需要感知真实世界音频或音乐信号的操作任务中，该系统表现出优于无前瞻方法的性能。实验表明成功的机器人动作学习不仅依赖多模态输入，更关键的是准确预测体现内在节奏模式的未来音频状态。

Conclusion: 生成式潜在流匹配模型能够有效预测未来音频观测，使机器人策略能够进行长期推理，在需要音频感知的操作任务中具有优越性能，强调了准确预测未来音频状态对机器人动作学习的重要性。

Abstract: World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.

</details>


### [15] [A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems](https://arxiv.org/abs/2512.08476)
*Po-An Shih,Shao-Hua Wang,Yung-Che Li,Chia-Heng Tu,Chih-Han Chang*

Main category: cs.RO

TL;DR: 提出基于多智能体大语言模型的自动驾驶系统设计空间探索框架，通过多模态推理和3D仿真自动化分析执行输出，相比遗传算法基线找到更多帕累托最优解


<details>
  <summary>Details</summary>
Motivation: 传统设计空间探索方法难以处理多模态执行输出和复杂性能权衡，需要人工参与评估正确性，限制了自动驾驶系统设计的效率和自动化程度

Method: 开发多智能体LLM框架，集成多模态推理、3D仿真和性能分析工具，利用专门LLM智能体处理用户输入解释、设计点生成、执行编排以及视觉和文本执行输出分析

Result: 在机器人出租车案例研究中，相比遗传算法基线，该框架在相同探索预算下识别出更多帕累托最优、成本效益更高的解决方案，并减少了导航时间

Conclusion: 该框架为自动驾驶系统设计自动化铺平了道路，展示了LLM方法在设计空间探索中的效率和潜力

Abstract: Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.

</details>


### [16] [Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception](https://arxiv.org/abs/2512.08481)
*Yixiang Lin,Tiancheng Yang,Jonathan Eden,Ying Tan*

Main category: cs.RO

TL;DR: 人类在物理人机交互中对不确定性的响应存在个体差异：研究发现两种行为模式——"权衡"组根据干扰概率调整物理响应，"总是补偿"组则表现出强烈的风险规避倾向。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何应对不确定性对于设计安全有效的物理人机交互至关重要。传统的最优控制框架假设人类行为最小化成本函数，但人类在不确定性下的行为往往偏离这种最优模式，因此需要更深入理解人类在不确定性下的行为。

Method: 实施了一项物理耦合目标到达任务，机器人以系统变化的概率（10%到90%）提供协助或干扰。通过分析参与者的力输入和决策策略来研究人类行为。

Result: 发现了两种不同的行为集群："权衡"组根据干扰可能性调整物理响应，"总是补偿"组则表现出强烈的风险规避倾向，无论概率如何。这表明人类在物理人机交互中的决策高度个性化，对概率的感知可能与真实值不同。

Conclusion: 研究强调了需要更可解释的行为模型（如累积前景理论）来准确捕捉这些行为，并为未来自适应机器人控制器的设计提供信息。人类对不确定性的响应存在显著个体差异，这对物理人机交互系统的设计具有重要意义。

Abstract: Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\% to 90\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a "trade-off" group that modulated their physical responses according to disturbance likelihood, and an "always-compensate" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.

</details>


### [17] [SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking](https://arxiv.org/abs/2512.08518)
*Nadezhda Kushina,Ko Watanabe,Aarthi Kannan,Ashita Ashok,Andreas Dengel,Karsten Berns*

Main category: cs.RO

TL;DR: 研究使用眼动追踪和机器学习评估人与机器人互动中的舒适度，发现决策树模型在预测舒适度方面表现最佳，表明人机互动的生理舒适阈值与人际互动不同。


<details>
  <summary>Details</summary>
Motivation: 社交机器人需要适应人类的近体空间规范以确保用户舒适和参与。虽然先前研究表明眼动追踪特征能可靠估计人际互动中的舒适度，但这些方法在人形机器人互动中的适用性尚未探索。

Method: 使用移动眼动追踪和主观报告（N=19），在四个实验控制距离（0.5米至2.0米）下评估用户与机器人"Ameca"的舒适度。评估多种机器学习和深度学习模型基于凝视特征估计舒适度。

Result: 与先前人际互动研究中Transformer模型表现优异不同，决策树分类器获得了最高性能（F1分数=0.73），最小瞳孔直径被确定为最重要的预测因子。

Conclusion: 人机互动中的生理舒适阈值与人际互动动态不同，可以使用可解释的逻辑进行有效建模。

Abstract: Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.

</details>


### [18] [vEDGAR - Can CARLA Do HiL?](https://arxiv.org/abs/2512.08541)
*Nils Gehrke,David Brecht,Dominik Kulmer,Dheer Patel,Frank Diermeyer*

Main category: cs.RO

TL;DR: 开发了一个基于CARLA的vEDGAR仿真框架，用于在专用硬件上实时测试自动驾驶软件，填补了CARLA在硬件在环测试方面的空白。


<details>
  <summary>Details</summary>
Motivation: CARLA等开源仿真器在自动驾驶算法训练和评估中广泛应用，但缺乏对完整传感器和执行器堆栈的实时硬件在环测试能力，限制了其在完整开发流程中的应用。

Method: 首先推导需求并指定仿真架构，然后实现vEDGAR软件框架，基于修改的CARLA分支，支持在专用硬件上进行实时硬件在环测试。

Result: 开发了开源工具：修改的CARLA分支和vEDGAR框架，评估了CARLA在自动驾驶车辆硬件在环测试中的适用性。

Conclusion: vEDGAR框架使CARLA能够作为一致的评估工具贯穿整个自动驾驶功能开发流程，特别是在硬件在环测试方面具有应用价值。

Abstract: Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR

</details>


### [19] [Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations](https://arxiv.org/abs/2512.08548)
*Yuchi Zhang,Churui Sun,Shiqi Liang,Diyuan Liu,Chao Ji,Wei-Nan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: 提出语义基础的语言表示来标准化机器人动作，通过忽略数值尺度效应、强调方向性来缓解分布偏移，提高预训练表示的可泛化性


<details>
  <summary>Details</summary>
Motivation: 当前端到端机器人操作研究采用大语言模型架构，但面临严重的分布偏移问题，主要源于不同机器人平台和任务间动作命令的数值变化，阻碍了预训练知识的有效迁移

Method: 提出语义基础的语言表示来标准化动作，该运动表示忽略数值尺度效应，强调方向性，缓解分布偏移，缩小动作标记与标准词汇标记之间的特征距离

Result: 在两个基准测试上的多任务实验表明，该方法显著提高了机器人操作任务的泛化性能和可迁移性

Conclusion: 通过语义基础的语言表示标准化机器人动作，能够有效缓解分布偏移问题，提高预训练模型在跨平台和跨任务场景下的泛化能力

Abstract: Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.

</details>


### [20] [RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight](https://arxiv.org/abs/2512.08574)
*Vit Kratky,Robert Penicka,Parakh M. Gupta,Ondrej Prochazka,Martin Saska*

Main category: cs.RO

TL;DR: 该论文提出了一种基于非线性模型预测控制（NMPC）和时间相关互惠速度约束（RVCs）的无人机互避碰撞方法，无需过多通信，运行频率达100Hz，在10架无人机、25m/s速度场景下实现31%飞行时间减少。


<details>
  <summary>Details</summary>
Motivation: 现有无人机互避碰撞方法通常依赖大量通信，而本文旨在开发一种仅基于可观测信息、无需过多通信的高效避碰方法，特别适用于敏捷飞行的无人机场景。

Method: 采用非线性模型预测控制（NMPC）框架，结合时间相关的互惠速度约束（RVCs），通过计算高效的RVCs算法直接集成到控制器层面的NMPC问题中，利用无人机的非线性动力学模型。

Result: 系统能以100Hz频率运行，在模拟实验中支持最多10架无人机、速度达25m/s，真实实验中加速度达30m/s²。与现有技术相比，在挑战性场景中飞行时间减少31%，所有试验中保持无碰撞导航。

Conclusion: 该方法通过NMPC与RVCs的有效结合，实现了仅基于可观测信息的高频互避碰撞控制，显著提升了无人机在密集动态环境中的飞行效率和安全性。

Abstract: This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.

</details>


### [21] [Mind to Hand: Purposeful Robotic Control via Embodied Reasoning](https://arxiv.org/abs/2512.08580)
*Peijun Tang,Shangjin Xie,Binyan Sun,Baifu Huang,Kuncheng Luo,Haotian Yang,Weiqi Jin,Jianan Wang*

Main category: cs.RO

TL;DR: Lumo-1是一个统一的视觉-语言-动作模型，将机器人推理与动作相结合，通过三阶段预训练流程增强具身推理能力，在真实世界机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 虽然互联网规模数据使AI系统具备了广泛的推理能力，但将这些能力与物理动作结合仍然是一个重大挑战。人类行为具有上下文和意图，推理在其中扮演核心角色。

Method: 采用三阶段预训练流程：1) 在精选的视觉-语言数据上继续预训练VLM以增强具身推理技能；2) 在跨具身机器人数据和视觉-语言数据上共同训练；3) 在Astribot S1机器人收集的轨迹上进行带推理过程的动作训练。最后整合强化学习来优化推理-动作一致性。

Result: Lumo-1在具身视觉-语言推理方面取得显著性能提升，这是通用机器人控制的关键组件。真实世界评估显示，Lumo-1在广泛挑战性机器人任务中超越强基线，对新物体和环境具有强大泛化能力，在长时程任务和需要策略、概念和空间推理的人类自然指令响应方面表现优异。

Conclusion: Lumo-1成功统一了机器人推理与动作，通过渐进式训练方法实现了具身推理与物理动作的有效结合，为通用机器人控制提供了有前景的解决方案。

Abstract: Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.

</details>


### [22] [Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems](https://arxiv.org/abs/2512.08630)
*Marta Manzoni,Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 该论文研究使用多任务贝叶斯优化来调整多无人机系统中的分散式轨迹生成算法，将每个任务视为特定无人机交互数量的轨迹生成场景，通过多任务高斯过程建模场景间关系，比较了优化所有任务平均任务时间和单独优化每个任务两种策略。


<details>
  <summary>Details</summary>
Motivation: 在多无人机系统中，分散式轨迹生成算法的参数调优是一个复杂问题，特别是当无人机群规模变化时。传统单任务优化方法需要为每个场景单独优化，计算成本高。需要一种能够跨不同无人机交互场景共享信息的高效优化方法。

Method: 采用多任务贝叶斯优化框架，将每个轨迹生成场景（由特定无人机交互数量定义）视为一个任务。使用多任务高斯过程建模任务间关系，捕获共享结构并实现优化过程中的信息传递。比较了两种优化策略：1）优化所有任务的平均任务时间；2）单独优化每个任务。

Result: 通过全面的仿真实验表明：单任务优化随着无人机群规模增长能够实现逐渐缩短的任务时间，但需要比平均任务优化方法显著更多的优化时间。多任务方法通过跨场景信息共享实现了更高效的优化。

Conclusion: 多任务贝叶斯优化为多无人机系统中的轨迹生成算法调优提供了一种有效方法，能够在保持性能的同时显著减少优化时间。平均任务优化策略在计算效率方面具有优势，而单任务优化在性能优化方面表现更好但计算成本更高。

Abstract: This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.

</details>


### [23] [A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation](https://arxiv.org/abs/2512.08653)
*Doumegna Mawuto Koudjo Felix,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一个传感器感知的现象学框架，用于在真实点云上模拟可解释的激光雷达退化，实现对SLAM系统的可控、可重复压力测试。


<details>
  <summary>Details</summary>
Motivation: 现有的激光雷达SLAM鲁棒性评估方法要么缺乏物理基础，要么无法捕捉传感器特定行为。激光雷达SLAM系统对遮挡、噪声和视场退化等不利条件高度敏感，需要更有效的评估工具。

Method: 开发了一个传感器感知的现象学框架，直接在真实点云上模拟可解释的激光雷达退化。该方法保留每个点的几何、强度和时间结构，同时应用结构化丢弃、视场减少、高斯噪声、遮挡掩码、稀疏化和运动失真。框架具有自主主题和传感器检测功能，模块化配置包含四个严重级别（轻度-极端），实时性能（每帧小于20毫秒）兼容ROS工作流程。

Result: 在三种激光雷达架构和五种最先进的SLAM系统上进行实验验证，揭示了由传感器设计和环境背景塑造的独特鲁棒性模式。开源实现为在物理意义明确的退化场景下基准测试激光雷达SLAM提供了实用基础。

Conclusion: 该框架提供了一种传感器感知、可解释的方法来模拟激光雷达退化，实现了对SLAM系统的可控、可重复压力测试，填补了现有评估方法的空白，为激光雷达SLAM的鲁棒性评估提供了实用工具。

Abstract: Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.

</details>


### [24] [Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes](https://arxiv.org/abs/2512.08656)
*Lauritz Rismark Fosso,Herman Biørn Amundsen,Marios Xanthidis,Sveinung Johan Ohrem*

Main category: cs.RO

TL;DR: 首个零样本sim2real深度强化学习速度控制器，仅需3分钟训练即可实现AUV的6自由度敏捷机动和路径跟踪


<details>
  <summary>Details</summary>
Motivation: 全向自主水下航行器具有硬件能力实现平移和旋转自由度的敏捷机动，但由于水下环境的复杂性（复杂流体静力学和动力学、参数不确定性、负载变化频繁等），控制具有挑战性。现有方法通常需要针对特定平台配置精心调校控制器，并在负载和流体条件变化时需要重新调校，导致实践中很少使用6自由度敏捷机动。

Method: 提出Sim2Swim方法，基于最先进的DRL位置控制，利用领域随机化和大规模并行训练，仅需3分钟训练即可收敛到可直接部署的控制策略，无需后处理或调校。该方法适用于具有可变特性的AUV。

Result: 在池试验中对多种配置进行了广泛验证，展示了高度敏捷运动的鲁棒控制能力。这是首个实现零样本sim2real深度强化学习速度控制器的方法。

Conclusion: Sim2Swim方法成功解决了AUV敏捷6自由度机动的控制挑战，通过仅3分钟的快速训练实现了可直接部署的控制策略，为水下机器人的实际应用提供了新的可能性。

Abstract: Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.

</details>


### [25] [Ergodic Trajectory Planning with Dynamic Sensor Footprints](https://arxiv.org/abs/2512.08661)
*Ziyue Zheng,Yongce Liu,Hesheng Wang,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 提出了一种考虑动态变化传感器足迹的遍历规划新方法，能同时优化轨迹和传感器配置，相比传统方法提升一个数量级的遍历性。


<details>
  <summary>Details</summary>
Motivation: 现有遍历规划方法通常假设点传感器或固定形状分辨率的传感器足迹，但实际应用中传感器足迹会随机器人运动动态变化（如无人机搭载的下视相机，其视场取决于姿态和高度）。

Method: 提出了考虑动态传感器足迹的新度量标准，分析了理论局部最优条件，并开发了数值轨迹优化算法。

Result: 实验结果显示，该方法能同时优化轨迹和传感器足迹，遍历性比传统方法提升一个数量级，并在多无人机系统中成功应用于3D空间物体覆盖。

Conclusion: 该方法有效解决了动态变化传感器足迹的遍历规划问题，为实际机器人信息采集任务提供了更精确的规划框架。

Abstract: This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.

</details>


### [26] [Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility](https://arxiv.org/abs/2512.08688)
*Mark Pustilnik,Francesco Borrelli*

Main category: cs.RO

TL;DR: 提出了一种用于协作人机导航的动态博弈框架，通过非归一化均衡结构让双方以不同努力程度共同满足安全约束


<details>
  <summary>Details</summary>
Motivation: 在共享工作空间中，人机协作导航需要同时满足安全约束（如避障和间距保持）并完成共同任务，但现有方法通常要求双方贡献均等，缺乏灵活性

Method: 提出动态博弈框架，引入非归一化均衡结构处理共享约束，允许人机以不同努力程度执行安全要求，并将其嵌入滚动时域最优控制方案

Result: 该方法能够实现更灵活的人机协作导航，双方可根据各自能力和状态以不同强度执行安全约束，同时完成共同任务

Conclusion: 非归一化均衡结构为协作人机导航提供了更灵活的安全约束执行机制，增强了实际应用中的适应性和效率

Abstract: This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.

</details>


### [27] [A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models](https://arxiv.org/abs/2512.08754)
*Jason Hughes,Marcel Hussing,Edward Zhang,Shenbagaraj Kannapiran,Joshua Caswell,Kenneth Chaney,Ruichen Deng,Michaela Feehery,Agelos Kratimenos,Yi Fan Li,Britny Major,Ethan Sanchez,Sumukh Shrote,Youkang Wang,Jeremy Wang,Daudi Zein,Luying Zhang,Ruijun Zhang,Alex Zhou,Tenzi Zhouga,Jeremy Cannon,Zaffir Qasim,Jay Yelon,Fernando Cladera,Kostas Daniilidis,Camillo J. Taylor,Eric Eaton*

Main category: cs.RO

TL;DR: 异构机器人系统用于大规模伤亡事件中的远程初级分诊，通过无人机和地面机器人协同工作，在不危及救援人员的情况下定位伤员、评估伤情并确定医疗优先级。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件中，传统救援方式可能使救援人员面临危险，且人工分诊效率有限。需要一种能够远程、快速、准确评估伤员状况的系统，以最大化救援效果并保护救援人员安全。

Method: 采用异构机器人系统，包括无人机和地面机器人协同工作。无人机负责空中侦察、定位伤员并提供俯视图；地面机器人配备专业传感器，测量生命体征、检测和定位物理损伤。系统实现了完整的远程分诊流程。

Result: 该系统能够完成完整的远程分诊过程：伤员定位、生命体征测量、伤情严重程度分类、精神状态评估，并将数据整合提供给救援人员。作为DARPA分诊挑战赛的一部分，展示了多机器人系统在灾难响应中的实际应用价值。

Conclusion: 异构机器人系统能够有效增强人类在灾难响应中的能力，通过远程分诊保护救援人员安全，提高救援效率，最大化拯救生命的机会。这种多机器人协同方法为未来灾难响应提供了新的技术解决方案。

Abstract: This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.

</details>


### [28] [Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks](https://arxiv.org/abs/2512.08813)
*Connor York,Zachary R Madin,Paul O'Dowd,Edmund R Hunt*

Main category: cs.RO

TL;DR: 多机器人系统在执行连续任务时，遇到紧急子任务会产生性能权衡。研究通过角色专业化（巡逻者与搜索者）和感知异质性来优化这种权衡，发现行为异质性团队在多数情况下表现最优。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在执行连续任务（如区域巡逻）时，经常会被紧急的、时间关键的子任务（如定位异常信号）打断，这导致系统需要在两个任务之间做出性能权衡。如何设计机器人团队来优化这种权衡是一个重要问题。

Method: 通过模拟实验评估两种异质性策略：1）行为异质性 - 通过角色专业化（"巡逻者"和"搜索者"）；2）感知异质性 - 只有搜索者能够感知无线电信号。研究在不同团队组成下，分析帕累托最优权衡。

Result: 1）行为异质性团队在大多数情况下表现出最平衡的权衡；2）当感知能力受限时，拥有半数感知能力机器人的异质性团队与同质性团队表现相当，这为限制传感器部署提供了成本节约的理由。

Conclusion: 预部署的角色和感知专业化是多机器人系统面对时间冲突任务时的重要设计考虑因素，通过调整行为异质性程度可以调节系统性能偏向于特定任务。

Abstract: Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization ("patrollers" and "searchers") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.

</details>


### [29] [IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams](https://arxiv.org/abs/2512.08877)
*Ryan LeRoy,Jack Kolb*

Main category: cs.RO

TL;DR: 研究发现，在异构多智能体环境中，即使没有在训练中暴露于多样化的队友策略，简单的自博弈PPO基线（IPPO）也能很好地泛化到新队友算法，其表现与采用多样化训练方案（RPT）的方法相当。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究多智能体强化学习（MARL）中，通过自博弈训练的同质智能体是否真正学习了基于底层游戏的一般协调策略，还是仅仅过拟合到训练伙伴的行为。特别是在异构智能体环境中，这个问题尤为重要。

Method: 使用异构多智能体挑战（HeMAC）环境，该环境包含具有互补能力的Observer和Drone智能体。提出了旋转策略训练（RPT）方法，在训练过程中轮换不同学习算法的异构队友策略，使智能体暴露于更广泛的伙伴策略范围。与标准的自博弈基线IPPO（所有智能体共享单个PPO策略）进行比较。

Result: 当与保留的队友策略（DDQN）一起游戏时，RPT实现了与标准自博弈基线IPPO相似的性能。这表明在这个异构多智能体设置中，IPPO基线能够泛化到新的队友算法，尽管在训练中没有经历队友多样性。

Conclusion: 简单的IPPO基线可能具备与多样化训练方案设计达到的相同水平的泛化能力，能够适应新的队友。这表明在某些异构多智能体环境中，简单的自博弈方法可能已经足够，而不需要复杂的多样化训练方案。

Abstract: Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.

</details>
