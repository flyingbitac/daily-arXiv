<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control](https://arxiv.org/abs/2512.11047)
*Haoran Jiang,Jin Chen,Qingwen Bu,Li Chen,Modi Shi,Yanjie Zhang,Delong Li,Chuanzhe Suo,Chuang Wang,Zhihui Peng,Hongyang Li*

Main category: cs.RO

TL;DR: 提出WholeBodyVLA框架，通过统一潜在学习从低成本无动作视频获取人形机器人全身运动操作知识，结合专门的运动操作强化学习策略，实现大空间人形机器人运动操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人形机器人的运动操作任务中存在局限性：缺乏操作感知的运动能力，限制了机器人的工作空间；难以获取运动操作知识（缺乏遥操作数据）；现有RL控制器精度和稳定性不足，难以可靠执行运动命令。

Method: 1. 提出统一潜在学习框架，让视觉-语言-动作系统从低成本无动作的自我中心视频中学习；2. 设计高效的人类数据收集流程来扩展数据集；3. 提出专门的运动操作导向RL策略，用于精确稳定的核心运动操作动作；4. 整合为WholeBodyVLA统一框架。

Result: 在AgiBot X2人形机器人上验证，相比先前基线提升21.3%；展示了强大的泛化能力和高扩展性，能够处理广泛的任务范围；实现了大空间人形机器人运动操作。

Conclusion: WholeBodyVLA是首个实现大空间人形机器人运动操作的统一框架，通过从无动作视频学习知识和专门的运动操作RL策略，显著提升了人形机器人在复杂运动操作任务中的性能。

Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.

</details>


### [2] [Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations](https://arxiv.org/abs/2512.11080)
*Cedric-Pascal Sommer,Robert J. Wood,Justin Werfel*

Main category: cs.RO

TL;DR: 研究人员提出了一个模块化工具系统，让标准双指夹爪能够执行按压、摩擦、刮擦等非抓取操作，扩展了简单机械手的多功能性。


<details>
  <summary>Details</summary>
Motivation: 虽然平行夹爪等简单机械手在抓取任务中表现出色，但它们并不擅长执行按压、摩擦、刮擦等非抓取操作。人类会根据任务需求使用不同身体部位或工具，而现有机械手缺乏这种适应性。

Method: 首先提出了非驱动末端执行器关键属性的分类法，然后基于该分类法设计了一个模块化工具系统，可以让标准双指夹爪通过更换工具来执行各种非抓取操作。

Result: 在航空航天和家庭场景中展示了该工具系统的应用，证明它能够有效执行一系列非抓取和抓取操作，扩展了简单机械手的多功能性。

Conclusion: 通过模块化工具系统，简单的双指夹爪可以扩展其功能范围，执行各种非抓取操作，为通用机器人末端执行器提供了更全面的解决方案。

Abstract: General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations.

</details>


### [3] [Design and Experimental Validation of Closed-Form CBF-Based Safe Control for Stewart Platform Under Multiple Constraints](https://arxiv.org/abs/2512.11125)
*Benedictus C. G. Cinun,Tua A. Tamba,Immanuel R. Santjoko,Xiaofeng Wang,Michael A. Gunarso,Bin Hu*

Main category: cs.RO

TL;DR: 该论文提出了一种用于Stewart机器人平台的CBF闭式解方法，通过显式控制律同时处理多个位置和速度约束，无需在每个控制步骤求解二次规划，实现了高效实时控制。


<details>
  <summary>Details</summary>
Motivation: 传统基于二次规划的CBF方法在实时控制中计算开销大，限制了在并行机器人系统中的应用。需要一种计算效率更高、能保证安全性的控制方法。

Method: 提出了CBF框架的闭式解方法，推导了多约束问题的显式控制律，并建立了确保解非奇异的充要条件，从而保证CBF解的适定性。

Result: 在自定义Stewart平台原型上进行了仿真和硬件实验验证，结果表明该方法在保证安全性能与QP方法相当的同时，计算时间减少了一个数量级以上。

Conclusion: 该方法为并行机器人系统的实时安全控制提供了一个可靠且计算轻量的框架，显著提升了计算效率，适合实时应用。

Abstract: This letter presents a closed-form solution of Control Barrier Function (CBF) framework for enforcing safety constraints on a Stewart robotic platform. The proposed method simultaneously handles multiple position and velocity constraints through an explicit closed-form control law, eliminating the need to solve a Quadratic Program (QP) at every control step and enabling efficient real-time implementation. This letter derives necessary and sufficient conditions under which the closed-form expression remains non-singular, thereby ensuring well-posedness of the CBF solution to multi-constraint problem. The controller is validated in both simulation and hardware experiments on a custom-built Stewart platform prototype, demonstrating safetyguaranteed performance that is comparable to the QP-based formulation, while reducing computation time by more than an order of magnitude. The results confirm that the proposed approach provides a reliable and computationally lightweight framework for real-time safe control of parallel robotic systems. The experimental videos are available on the project website. (https://nail-uh.github.io/StewartPlatformSafeControl.github.io/)

</details>


### [4] [Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance](https://arxiv.org/abs/2512.11173)
*Tzu-Hsien Lee,Fidan Mahmudova,Karthik Desingh*

Main category: cs.RO

TL;DR: 提出基于物体中心的模仿学习框架，让四足移动机械臂仅使用RGB相机实现最后一米精确导航，达到可操作定位精度


<details>
  <summary>Details</summary>
Motivation: 现有RGB导航系统通常只有米级精度，不适合移动机械臂的精确定位需求，导致后续操作失败率高。需要解决最后一米导航的精度问题

Method: 使用目标图像、多视角RGB观测和文本提示作为导航策略输入，结合语言驱动分割模块和空间得分矩阵解码器，实现物体定位和相对位姿推理

Result: 在未见过的物体实例上，边缘对齐成功率达到73.47%，物体对齐成功率达到96.94%，证明无需深度、激光雷达或地图先验即可实现类别级精确导航

Conclusion: 仅使用RGB观测即可实现最后一米精确导航，为统一移动操作提供了可扩展的途径，解决了移动机械臂精确定位的关键问题

Abstract: Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/

</details>


### [5] [Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy](https://arxiv.org/abs/2512.11218)
*Kechun Xu,Zhenjie Zhu,Anzhe Chen,Shuqi Zhao,Qing Huang,Yifei Yang,Haojian Lu,Rong Xiong,Masayoshi Tomizuka,Yue Wang*

Main category: cs.RO

TL;DR: BayesVLA通过贝叶斯分解解决VLA模型中的模态不平衡问题，将策略分解为视觉-动作先验和语言条件似然，有效缓解灾难性遗忘并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: VLA模型在微调过程中常因模态不平衡（语言多样性远低于视觉和动作多样性）导致视觉捷径学习和语言遗忘，现有方法依赖外部推理数据且需要经验性调参。

Method: 提出BayesVLA，采用贝叶斯分解将策略分解为视觉-动作先验（支持"看到即行动"）和语言条件似然（支持"提示即指定"），并引入接触前和接触后阶段以更好利用预训练基础模型。

Result: 信息论分析正式验证了该方法在缓解捷径学习方面的有效性，大量实验显示在未见指令、对象和环境上相比现有方法具有更优越的泛化能力。

Conclusion: BayesVLA通过内在的贝叶斯分解解决了VLA模型中的模态不平衡问题，无需外部依赖即可保持泛化能力并促进指令跟随，为VLA模型的分布外泛化提供了有效解决方案。

Abstract: The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.

</details>


### [6] [Elevation Aware 2D/3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics](https://arxiv.org/abs/2512.11249)
*Chandra Raskoti,Weizi Li*

Main category: cs.RO

TL;DR: 提出自动化、考虑高程的协同仿真框架，集成SUMO和CARLA，融合OpenStreetMap路网和USGS高程数据，生成物理一致的3D环境，用于自动驾驶系统的高保真测试。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶测试仿真工具很少考虑真实世界的高程数据，限制了在复杂地形城市中的实用性。需要能够结合大规模交通建模与真实3D感知和地形的仿真环境。

Method: 开发自动化高程感知协同仿真框架，通过管道将OpenStreetMap路网和USGS高程数据融合到物理一致的3D环境中。系统生成平滑的高程剖面，验证几何精度，并实现跨平台的同步2D-3D仿真。

Result: 在旧金山多个区域进行演示，展示了框架的可扩展性和再现陡峭不规则地形的能力。为在真实、高程丰富的城市环境中进行高保真自动驾驶车辆测试提供了实用基础。

Conclusion: 该框架为自动驾驶系统在真实复杂地形城市环境中的可靠测试提供了有效的解决方案，填补了现有工具在高程建模方面的不足。

Abstract: Reliable testing of autonomous driving systems requires simulation environments that combine large-scale traffic modeling with realistic 3D perception and terrain. Existing tools rarely capture real-world elevation, limiting their usefulness in cities with complex topography. This paper presents an automated, elevation-aware co-simulation framework that integrates SUMO with CARLA using a pipeline that fuses OpenStreetMap road networks and USGS elevation data into physically consistent 3D environments. The system generates smooth elevation profiles, validates geometric accuracy, and enables synchronized 2D-3D simulation across platforms. Demonstrations on multiple regions of San Francisco show the framework's scalability and ability to reproduce steep and irregular terrain. The result is a practical foundation for high-fidelity autonomous vehicle testing in realistic, elevation-rich urban settings.

</details>


### [7] [Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator](https://arxiv.org/abs/2512.11250)
*Brock Marcinczyk,Logan E. Beaver*

Main category: cs.RO

TL;DR: 提出了一种结合降阶庞特里亚金极大值原理控制器与物理信息梯度下降阶段的控制中心框架，用于4自由度刚性机械臂，实现高效计算的同时保持严格的控制理论结构。


<details>
  <summary>Details</summary>
Motivation: 需要开发一种既能保持严格控制理论结构，又能嵌入机械臂物理约束和负载行为的计算高效控制框架，以解决传统方法在动态一致性和计算效率方面的不足。

Method: 1. 使用降阶PMP模型提供关节加速度的闭式最优控制律；2. 梯度下降模块通过最小化基于完整刚体动力学的成本函数确定对应时间范围；3. 使用结构力学反应分析初始化可行关节速度；4. 将运动轨迹和时间范围输入符号欧拉-拉格朗日模型生成闭式逆动力学输入。

Result: 开发了一个完整的控制流水线，能够在保持严格控制理论结构的同时，以计算高效的方式嵌入机械臂的物理约束和负载行为，实现了动态一致的时间范围和运动轨迹。

Conclusion: 该框架成功地将降阶最优控制与物理信息优化相结合，为刚性机械臂提供了一种既保持理论严谨性又具有计算效率的控制解决方案，实现了控制理论与物理约束的有效融合。

Abstract: This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way.

</details>


### [8] [Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing](https://arxiv.org/abs/2512.11275)
*Suchang Chen,Daqiang Guo*

Main category: cs.RO

TL;DR: 提出面向机器人操作的物体中心化操作逻辑模式τ，将接触力/阻抗等关键参数作为知识信号，应用于3D打印机线轴移除任务，支持数据增强和逻辑感知提示检索


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在机器人操作中注重语义泛化，但缺乏制造场景中接触密集型动作所需的关键执行参数，需要更结构化的知识表示方法

Method: 定义八字段元组τ作为物体中心化操作逻辑模式，包含物体、接口、轨迹、容差、力/阻抗等信息，构建知识库并在3D打印机线轴移除任务中实例化

Result: τ模式支持训练时的分类标记数据增强和测试时的逻辑感知检索增强提示，作为智能制造助手系统的构建模块，通过适应VLM/LLM规划基准的指标评估规划质量

Conclusion: 物体中心化操作逻辑模式τ为制造场景中的接触密集型机器人操作提供了结构化知识表示，连接了人类操作员、VLM助手和机器人控制器，支持更可靠的智能制造系统

Abstract: Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.

</details>


### [9] [Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains](https://arxiv.org/abs/2512.11351)
*Steffen Schäfer,Martin Cichon*

Main category: cs.RO

TL;DR: 论文提出了一种将运行设计域（ODD）细分为微ODD（mODD）的结构化方法，用于生成感知系统测试用例，通过简化障碍物表示和闭环仿真验证自动驾驶功能的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前基于场景的自动驾驶系统测试面临挑战：从ODD分类到具体测试用例的转换过程缺乏结构化方法，完整性难以保证。需要系统化的方法来验证感知系统在各种真实条件下的正确性。

Method: 1. 将ODD细分为可管理的微ODD（mODD）部分；2. 使用抽象对象表示（如不同尺寸的通用立方体）生成测试用例；3. 在闭环协同仿真虚拟环境中进行测试，包含逼真渲染和模拟传感器（LiDAR、GNSS、摄像头）；4. 以碰撞与安全停止作为结果指标评估感知质量。

Result: 该方法能够系统性地探索障碍物检测的边缘情况，基于观察到的车辆行为评估感知质量。通过受限ODD中的侧向引导机动案例（涉及调车机车）展示了mODD的定义和细化过程，成功生成测试用例并验证了方法的有效性。

Conclusion: 提出的mODD方法和抽象障碍物表示为自动驾驶功能的安全论证提供了标准化框架基础，是验证和授权自动驾驶功能的重要实践步骤，支持系统化的安全评估。

Abstract: The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions.

</details>


### [10] [An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges](https://arxiv.org/abs/2512.11362)
*Chao Xu,Suyu Zhang,Yang Liu,Baigui Sun,Weihong Chen,Bo Xu,Qi Liu,Juncheng Wang,Shujun Wang,Shan Luo,Jan Peters,Athanasios V. Vasilakos,Stefanos Zafeiriou,Jiankang Deng*

Main category: cs.RO

TL;DR: 这篇论文是一篇关于视觉-语言-动作（VLA）模型的综述性文章，为机器人领域的研究者提供了结构化的学习路径，重点分析了该领域的五大核心挑战。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型在机器人领域的快速发展，新模型和数据集层出不穷，研究人员难以跟上最新进展。本文旨在为研究者提供清晰的结构化指南，帮助新人快速入门，并为经验丰富的研究者提供战略路线图。

Method: 采用自然学习路径的结构设计：首先介绍VLA模型的基本模块，然后追溯关键里程碑，最后深入分析五大核心挑战（表示、执行、泛化、安全、数据集与评估）。为每个挑战回顾现有方法并指出未来机会。

Result: 创建了一个结构化的VLA领域综述框架，将发展路线图分为三个层次：建立基本的感知-动作循环、在不同具身和环境中的能力扩展、确保可信部署。提供了持续更新的在线版本。

Conclusion: 本文既是新研究者的基础指南，也是经验丰富研究者的战略路线图，旨在加速具身智能领域的学习并激发新想法。通过分析五大核心挑战，为VLA模型的未来发展提供了清晰的框架。

Abstract: Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/Survery/}{project page}.

</details>


### [11] [CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios](https://arxiv.org/abs/2512.11551)
*Jörg Gamerdinger,Sven Teufel,Simon Roller,Oliver Bringmann*

Main category: cs.RO

TL;DR: 该研究提出了一个评估基础设施辅助集体感知对弱势道路使用者安全改善的框架，包含包含1.1万帧EuroNCAP安全关键场景的数据集，证明基础设施感知可将事故避免率从33%提升至100%。


<details>
  <summary>Details</summary>
Motivation: 道路使用者数量增加导致事故风险上升，弱势道路使用者在城市环境中尤其危险，常被停车或建筑物遮挡。自动驾驶和集体感知技术有望降低这些风险，但需要全面研究和数据集来证明基础设施辅助集体感知对弱势道路使用者的安全改善效果，以鼓励决策者采用该技术。

Method: 提出一个评估基础设施辅助集体感知对弱势道路使用者安全改善的框架，创建包含EuroNCAP安全关键场景的CarlaNCAP数据集（1.1万帧），通过深度仿真研究分析基础设施感知单元对事故避免的效果。

Result: 基础设施辅助集体感知在安全关键场景中能显著降低事故率，相比仅配备传感器的车辆（33%事故避免率），基础设施感知可实现高达100%的事故避免率。

Conclusion: 基础设施辅助集体感知技术能有效提高弱势道路使用者的安全性，特别是在遮挡严重的城市环境中，为决策者采用该技术提供了有力的实证支持。

Abstract: The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap

</details>


### [12] [Cross-Entropy Optimization of Physically Grounded Task and Motion Plans](https://arxiv.org/abs/2512.11571)
*Andreu Matoses Gimenez,Nils Wilde,Chris Pek,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 提出一种使用GPU并行物理模拟器计算计划实现的方法，通过交叉熵优化采样控制器参数，使机器人能够直接执行计算出的计划


<details>
  <summary>Details</summary>
Motivation: 现有TAMP算法主要关注计算性能、完备性或最优性，通过简化和抽象使问题可处理，但这可能导致计划无法考虑动力学或复杂接触，从而在实际执行任务时失败。同时，忽略底层控制器影响的方法可能无法获得最优或可行的计划实现。

Method: 使用GPU并行化物理模拟器计算带有运动控制器的计划实现，显式考虑动力学和环境接触。通过交叉熵优化采样控制器参数（动作），获得低成本解决方案。该方法使用与实际系统相同的控制器，使机器人能够直接执行计算出的计划。

Result: 该方法在一系列任务中展示，机器人能够利用环境几何形状移动物体。提供了网站和代码链接供进一步参考。

Conclusion: 通过结合GPU并行物理模拟和交叉熵优化，提出的方法能够生成考虑动力学和接触的可行计划，使机器人能够直接执行这些计划，特别适用于需要物体操作的任务。

Abstract: Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization

</details>


### [13] [UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations](https://arxiv.org/abs/2512.11609)
*Tingyu Yuan,Biaoliang Guan,Wen Ye,Ziyan Tian,Yi Yang,Weijie Zhou,Yan Huang,Peng Wang,Chaoyang Zhao,Jinqiao Wang*

Main category: cs.RO

TL;DR: UniBYD是一个统一框架，通过动态强化学习算法发现符合机器人物理特性的操作策略，使用统一形态表示支持不同机械手形态，在成功率上比现有方法提升67.90%。


<details>
  <summary>Details</summary>
Motivation: 在具身智能中，机器人手与人类手之间的具身差距给从人类演示中学习带来了重大挑战。现有研究虽然尝试用强化学习弥合这一差距，但仅限于复制人类操作，导致任务性能有限。

Method: 提出UniBYD统一框架，包含：1) 统一形态表示(UMR)实现跨不同机械手形态的一致建模；2) 基于UMR设计带有退火奖励调度的动态PPO算法，使强化学习从模仿人类演示过渡到探索适应机器人形态的策略；3) 设计混合马尔可夫阴影引擎，在早期训练阶段精细模仿人类操作。

Result: 提出了首个涵盖多种手形态的机器人操作基准UniManip。实验表明，UniBYD在成功率上比当前最先进方法提高了67.90%。

Conclusion: UniBYD框架能够超越单纯模仿人类手，发现适应不同机器人形态的操作策略，显著提升了机器人操作性能，为跨形态机器人操作学习提供了有效解决方案。

Abstract: In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.

</details>


### [14] [Architecting Large Action Models for Human-in-the-Loop Intelligent Robots](https://arxiv.org/abs/2512.11620)
*Kanisorn Sangchai,Methasit Boonpun,Withawin Kraipetchara,Paulo Garcia*

Main category: cs.RO

TL;DR: 通过组合现成的基础模型并加入符号化封装和验证，可以构建出具有控制性、可解释性和可验证性的神经符号化大型动作模型，无需大规模端到端训练。


<details>
  <summary>Details</summary>
Motivation: 传统符号化AI方法在计算和内存成本上存在可扩展性问题，而大型语言模型等神经方法虽然能力强大但缺乏控制性、可解释性和可靠性。大型动作模型试图扩展LLM以涵盖完整的感知-推理-动作循环，但通常需要大量训练且存在同样的可靠性缺陷。

Method: 通过组合现成的基础模型构建大型动作模型，并加入符号化封装和对输出的验证。具体方法包括：集成高效的感知模型与逻辑驱动核心，通过生成规划域定义语言（PDDL）代码来驱动动作执行，并引入人机交互验证阶段。

Result: 在多模态机器人上的实验表明，大型动作模型的智能可以通过集成高效感知模型和逻辑驱动核心实现，无需大规模端到端训练。通过PDDL代码生成和人机交互验证，有效减轻了动作幻觉问题。

Conclusion: 通过神经符号化方法构建的大型动作模型具有控制性、可解释性和可验证性，为智能机器人设计提供了新途径。这种方法支持从业者在各行业开发机器人大型动作模型，并指出了确保现场安全需要解决的挑战。

Abstract: The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.

</details>


### [15] [Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots](https://arxiv.org/abs/2512.11736)
*Ninghan Zhong,Steven Caro,Megnath Ramesh,Rishi Bhatnagar,Avraiem Iskandar,Stephen L. Smith*

Main category: cs.RO

TL;DR: Bench-Push是首个用于推动式移动机器人导航和操作任务的统一基准测试平台，包含模拟环境、评估指标和基线实现


<details>
  <summary>Details</summary>
Motivation: 移动机器人在杂乱环境中需要与可移动物体交互，而现有推动式机器人研究缺乏标准化评估方法，限制了可重复性和跨研究比较

Method: 开发了Bench-Push基准测试平台，包括：1）多种模拟环境（迷宫导航、冰区船舶导航、箱子递送、区域清理）；2）新颖评估指标（效率、交互努力、部分任务完成度）；3）示例基线实现评估

Result: Bench-Push作为开源Python库发布，具有模块化设计，提供了代码、文档和训练模型，为推动式机器人研究提供了标准化评估框架

Conclusion: Bench-Push填补了推动式移动机器人导航和操作任务标准化评估的空白，促进了该领域研究的可重复性和跨方法比较

Abstract: Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.

</details>


### [16] [The Influence of Human-like Appearance on Expected Robot Explanations](https://arxiv.org/abs/2512.11746)
*Hana Kopecka,Jose Such*

Main category: cs.RO

TL;DR: 研究机器人外观（人形程度）如何影响用户对机器人解释的期望，发现人形外观与拟人化解释呈正相关


<details>
  <summary>Details</summary>
Motivation: 机器人外观已知会影响用户心智模型和人机交互，但尚未研究其对预期机器人解释的影响。本研究探讨机器人的人形外观是否及在多大程度上引发拟人化（即心理能力归因），以及拟人化水平如何体现在人们期望从机器人获得的解释中。

Method: 设计了组间研究，包含三种具有不同人形外观的家用服务机器人的视觉刺激条件。要求受访者为相同的机器人行为提供他们期望从机器人获得的解释。通过分析解释内容来评估拟人化程度。

Result: 在所有条件下，大多数解释都是拟人化的。但拟人化解释与人形外观之间存在正相关关系。还观察到非拟人化解释和机器人描述中更细微的趋势。

Conclusion: 机器人的人形外观确实会影响用户期望从机器人获得的解释类型，人形外观越强，用户越倾向于期望拟人化的解释。这为人机交互设计和机器人解释系统开发提供了重要见解。

Abstract: A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.

</details>


### [17] [BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models](https://arxiv.org/abs/2512.11769)
*Xiaoyu Ma,Zhengqing Yuan,Zheyuan Zhang,Kaiwen Shi,Lichao Sun,Yanfang Ye*

Main category: cs.RO

TL;DR: BLURR是一个轻量级推理包装器，可加速现有视觉-语言-动作模型，保持任务成功率同时显著降低计算成本和延迟


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型推理堆栈过于笨重，难以在普通GPU上实现响应式网页演示或高频机器人控制，需要轻量级解决方案

Method: BLURR作为轻量级推理包装器，无需重新训练或更改模型检查点，通过指令前缀键值缓存、混合精度执行和单步展开调度来加速控制

Result: 在SimplerEnv评估中，BLURR保持与原始控制器相当的任务成功率，同时显著降低有效FLOPs和实际延迟，并构建了交互式网页演示

Conclusion: BLURR是在有限计算预算下部署现代VLA策略的实用方法，特别适合网页演示和实时机器人控制场景

Abstract: Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.

</details>


### [18] [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](https://arxiv.org/abs/2512.11773)
*Britton Jordan,Jordan Thompson,Jesse F. d'Almeida,Hao Li,Nithesh Kumar,Susheela Sharma Stern,Ipek Oguz,Robert J. Webster,Daniel Brown,Alan Kuntz,James Ferguson*

Main category: cs.RO

TL;DR: ProbeMDE：一种成本感知的主动感知框架，通过结合RGB图像和稀疏本体感知测量来改进单目深度估计，在手术场景等挑战性环境中实现更高精度


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在手术等挑战性环境中存在不确定性和不准确性问题，主要由于纹理缺失表面、镜面反射和遮挡等因素。需要一种能结合稀疏本体感知测量的方法来提高深度估计精度

Method: 提出ProbeMDE框架，使用MDE模型集成预测基于RGB图像和稀疏已知深度测量的密集深度图。通过集成方差量化预测不确定性，测量不确定性相对于候选测量位置的梯度。利用Stein变分梯度下降在梯度图上选择最具信息量的本体感知位置，避免模式崩溃

Result: 在模拟和物理实验中的中央气道阻塞手术模型上验证，方法在标准深度估计指标上优于基线方法，实现更高精度同时最小化所需本体感知测量数量

Conclusion: ProbeMDE通过成本感知的主动感知框架有效结合RGB图像和稀疏本体感知测量，显著提高了单目深度估计在挑战性手术环境中的准确性和可靠性

Abstract: Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.

</details>


### [19] [Agile Flight Emerges from Multi-Agent Competitive Racing](https://arxiv.org/abs/2512.11781)
*Vineet Pasumarti,Lorenzo Bianchi,Antonio Loquercio*

Main category: cs.RO

TL;DR: 通过多智能体竞争和赢得比赛的稀疏高层目标，强化学习训练出的智能体能够同时掌握敏捷飞行（如高速运动）和策略（如超车或阻挡），在复杂环境中表现优于传统的单智能体训练方法，并且具有更好的仿真到现实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常训练单智能体使用规定行为的奖励函数（如沿赛道线前进），但在复杂环境中效果有限。本研究探索通过多智能体竞争和稀疏高层目标（赢得比赛）是否能够同时产生高级飞行技能和策略行为，并改善仿真到现实的迁移能力。

Method: 使用强化学习训练多智能体竞争系统，仅提供赢得比赛的稀疏高层目标作为奖励。在仿真环境中进行训练，采用相同的仿真环境、随机化策略和硬件配置，与基于进度奖励的单智能体训练方法进行对比。在仿真和现实世界中进行验证。

Result: 多智能体竞争方法在复杂环境（如有障碍物时）中表现优于单智能体进度奖励方法。多智能体策略具有更好的仿真到现实迁移能力，即使两种方法使用相同的仿真设置。此外，多智能体策略还能在一定程度上泛化到训练时未见过的对手。

Conclusion: 稀疏任务级奖励足以训练出能够在物理世界中执行高级低级控制的智能体。多智能体竞争不仅能够同时产生敏捷飞行技能和策略行为，还能显著改善仿真到现实迁移能力，为物理世界中的高级控制任务提供了有前景的方法。

Abstract: Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent

</details>


### [20] [AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797)
*Junjie Ye,Rong Xue,Basile Van Hoorick,Pavel Tokmakov,Muhammad Zubair Irshad,Yue Wang,Vitor Guizilini*

Main category: cs.RO

TL;DR: AnchorDream利用预训练视频扩散模型，通过机器人运动渲染作为条件，从少量人类遥操作演示中生成大规模、多样化的机器人数据集，显著提升下游策略学习性能。


<details>
  <summary>Details</summary>
Motivation: 机器人演示数据收集成本高且困难，仿真器存在多样性和保真度限制以及明显的仿真到现实差距。现有生成方法要么只改变视觉外观而不创造新行为，要么存在本体不一致导致不合理的运动。

Method: 提出AnchorDream，一种本体感知的世界模型，重新利用预训练视频扩散模型进行机器人数据合成。该方法以机器人运动渲染为条件，锚定本体以防止幻觉，同时合成与机器人运动学一致的对象和环境。

Result: 实验表明，生成的数据在下游策略学习中带来持续改进，在仿真基准中相对提升36.4%，在现实世界研究中性能几乎翻倍。

Conclusion: 将生成世界模型基于机器人运动，为扩展模仿学习提供了一条实用路径。

Abstract: The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.

</details>
