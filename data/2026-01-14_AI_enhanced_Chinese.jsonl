{"id": "2601.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07945", "abs": "https://arxiv.org/abs/2601.07945", "authors": ["Aabha Tamhankar", "Ron Alterovitz", "Ajit S. Puri", "Giovanni Pittiglio"], "title": "Contact-aware Path Planning for Autonomous Neuroendovascular Navigation", "comment": "8 pages, 7 figures, IROS(R-AL)", "summary": "We propose a deterministic and time-efficient contact-aware path planner for neurovascular navigation. The algorithm leverages information from pre- and intra-operative images of the vessels to navigate pre-bent passive tools, by intelligently predicting and exploiting interactions with the anatomy. A kinematic model is derived and employed by the sampling-based planner for tree expansion that utilizes simplified motion primitives. This approach enables fast computation of the feasible path, with negligible loss in accuracy, as demonstrated in diverse and representative anatomies of the vessels. In these anatomical demonstrators, the algorithm shows a 100% convergence rate within 22.8s in the worst case, with sub-millimeter tracking errors (less than 0.64 mm), and is found effective on anatomical phantoms representative of around 94% of patients.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u786e\u5b9a\u6027\u3001\u65f6\u95f4\u9ad8\u6548\u7684\u63a5\u89e6\u611f\u77e5\u8def\u5f84\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\uff0c\u901a\u8fc7\u5229\u7528\u8840\u7ba1\u7684\u672f\u524d\u548c\u672f\u4e2d\u56fe\u50cf\u4fe1\u606f\uff0c\u667a\u80fd\u9884\u6d4b\u548c\u5229\u7528\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u5bfc\u822a\u9884\u5f2f\u66f2\u88ab\u52a8\u5de5\u5177\u3002", "motivation": "\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\u9700\u8981\u7cbe\u786e\u7684\u8def\u5f84\u89c4\u5212\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u8840\u7ba1\u89e3\u5256\u7ed3\u6784\u65f6\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u53ef\u80fd\u8ba1\u7b97\u6548\u7387\u4f4e\u6216\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u8ba1\u7b97\u53ef\u884c\u8def\u5f84\u7684\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u4ee5\u652f\u6301\u795e\u7ecf\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u786e\u5b9a\u6027\u3001\u65f6\u95f4\u9ad8\u6548\u7684\u63a5\u89e6\u611f\u77e5\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u8840\u7ba1\u7684\u672f\u524d\u548c\u672f\u4e2d\u56fe\u50cf\u4fe1\u606f\uff0c\u901a\u8fc7\u667a\u80fd\u9884\u6d4b\u548c\u5229\u7528\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u5bfc\u822a\u9884\u5f2f\u66f2\u88ab\u52a8\u5de5\u5177\u3002\u63a8\u5bfc\u4e86\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\u8fdb\u884c\u6811\u6269\u5c55\uff0c\u5229\u7528\u7b80\u5316\u7684\u8fd0\u52a8\u539f\u8bed\u3002", "result": "\u5728\u591a\u6837\u5316\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u8840\u7ba1\u89e3\u5256\u7ed3\u6784\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u572822.8\u79d2\u5185\u8fbe\u5230100%\u7684\u6536\u655b\u7387\uff0c\u8ddf\u8e2a\u8bef\u5dee\u5c0f\u4e8e0.64\u6beb\u7c73\uff08\u4e9a\u6beb\u7c73\u7ea7\uff09\u3002\u5728\u4ee3\u8868\u7ea694%\u60a3\u8005\u7684\u89e3\u5256\u6a21\u578b\u4e2d\uff0c\u8be5\u7b97\u6cd5\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u63a5\u89e6\u611f\u77e5\u8def\u5f84\u89c4\u5212\u5668\uff0c\u80fd\u591f\u5728\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\u4e2d\u5feb\u901f\u8ba1\u7b97\u53ef\u884c\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u8be5\u7b97\u6cd5\u5728\u591a\u79cd\u89e3\u5256\u7ed3\u6784\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08034", "abs": "https://arxiv.org/abs/2601.08034", "authors": ["Cameron Smith", "Basile Van Hoorick", "Vitor Guizilini", "Yue Wang"], "title": "Fiducial Exoskeletons: Image-Centric Robot State Estimation", "comment": null, "summary": "We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.\n  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.\n  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.", "AI": {"tldr": "Fiducial Exoskeletons\uff1a\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u76843D\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u5f20RGB\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u5404\u8fde\u6746\u76846D\u4f4d\u59ff\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7e41\u7410\u7684\u624b\u773c\u6807\u5b9a\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u6267\u884c\u5668\u548c\u8017\u65f6\u7684\u624b\u773c\u6807\u5b9a\u6d41\u7a0b\uff0c\u800c\u73b0\u4ee3\u57fa\u4e8e\u5b66\u4e60\u7684\u673a\u5668\u4eba\u63a7\u5236\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528RGB\u89c2\u6d4b\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u8bad\u7ec3\u548c\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u9c81\u68d2\u7684\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "1. \u5c06\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ece\u5355\u5f20RGB\u56fe\u50cf\u4f30\u8ba1\u6bcf\u4e2a\u8fde\u6746\u76846D\u4f4d\u59ff\uff1b2. \u5f15\u5165\"\u57fa\u51c6\u5916\u9aa8\u9abc\"\uff1a\u5728\u6bcf\u4e2a\u8fde\u6746\u4e0a\u5b89\u88c5\u5e26\u6709\u57fa\u51c6\u6807\u8bb0\u76843D\u6253\u5370\u652f\u67b6\uff0c\u5df2\u77e5\u6807\u8bb0-\u8fde\u6746\u51e0\u4f55\u5173\u7cfb\uff1b3. \u901a\u8fc7\u8f7b\u91cf\u7ea7\u5168\u5c40\u4f18\u5316\u6062\u590d\u5173\u8282\u72b6\u6001\uff0c\u786e\u4fdd\u4e0e\u89c2\u6d4b\u5230\u7684\u8fde\u6746\u4f4d\u59ff\u4fdd\u6301\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4f4e\u6210\u672c\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\uff0c\u57fa\u51c6\u5916\u9aa8\u9abc\u663e\u8457\u7b80\u5316\u4e86\u8bbe\u7f6e\u6d41\u7a0b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6807\u5b9a\u7cbe\u5ea6\u3001\u72b6\u6001\u51c6\u786e\u6027\u548c\u4e0b\u6e383D\u63a7\u5236\u6027\u80fd\u3002\u5373\u4f7f\u673a\u5668\u4eba\u65ad\u7535\u4e5f\u80fd\u5b9e\u73b0\u9c81\u68d2\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "conclusion": "\u57fa\u51c6\u5916\u9aa8\u9abc\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9c81\u68d2\u7684\u56fe\u50cf\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u83b7\u53d6\u76f8\u673a-\u673a\u5668\u4eba\u5916\u53c2\u3001\u6bcf\u8fde\u6746SE(3)\u4f4d\u59ff\u548c\u5173\u8282\u89d2\u5ea6\u72b6\u6001\uff0c\u4fc3\u8fdb\u4e86\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u3002"}}
{"id": "2601.08110", "categories": ["cs.RO", "cs.IT", "eess.SP", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.08110", "abs": "https://arxiv.org/abs/2601.08110", "authors": ["Reza Arablouei"], "title": "Efficient Incremental SLAM via Information-Guided and Selective Optimization", "comment": null, "summary": "We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u589e\u91cfSLAM\u540e\u7aef\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u5f15\u5bfc\u95e8\u63a7\u548c\u9009\u62e9\u6027\u90e8\u5206\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u5168\u6279\u91cf\u4f18\u5316\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u589e\u91cfSLAM\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5168\u5c40\u4e00\u81f4\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u5168\u6279\u91cf\u4f18\u5316\u867d\u7136\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u5e38\u89c4\u589e\u91cf\u65b9\u6cd5\u53ef\u80fd\u727a\u7272\u7cbe\u5ea6\u6216\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u91cd\u65b0\u7ebf\u6027\u5316\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u53c8\u80fd\u9ad8\u6548\u5904\u7406\u52a8\u6001\u6570\u636e\u6d41\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1a1) \u4fe1\u606f\u5f15\u5bfc\u95e8\u63a7(IGG)\uff1a\u57fa\u4e8e\u4fe1\u606f\u77e9\u9635\u5bf9\u6570\u884c\u5217\u5f0f\u7684\u4fe1\u606f\u7406\u8bba\u51c6\u5219\u91cf\u5316\u65b0\u6d4b\u91cf\u7684\u8d21\u732e\uff0c\u4ec5\u5728\u89c2\u5bdf\u5230\u663e\u8457\u4fe1\u606f\u589e\u76ca\u65f6\u89e6\u53d1\u5168\u5c40\u4f18\u5316\uff1b2) \u9009\u62e9\u6027\u90e8\u5206\u4f18\u5316(SPO)\uff1a\u6267\u884c\u591a\u8f6e\u9ad8\u65af\u725b\u987f\u66f4\u65b0\uff0c\u4f46\u6bcf\u8f6e\u4ec5\u4f18\u5316\u53d7\u65b0\u6d4b\u91cf\u5f71\u54cd\u6700\u5927\u7684\u53d8\u91cf\u5b50\u96c6\uff0c\u52a8\u6001\u8c03\u6574\u6d3b\u8dc3\u96c6\u76f4\u81f3\u6536\u655b\u3002", "result": "\u5728\u57fa\u51c6SLAM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u5339\u914d\u6279\u91cf\u6c42\u89e3\u5668\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u589e\u91cf\u65b9\u6cd5\u5b9e\u73b0\u663e\u8457\u8ba1\u7b97\u8282\u7701\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u5b8c\u6574\u9ad8\u65af\u725b\u987f\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u5e73\u8861\uff0c\u662f\u52a8\u6001\u6570\u636e\u4e30\u5bcc\u73af\u5883\u4e2d\u5b9e\u65f6\u64cd\u4f5c\u7684\u7a33\u5065\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u7559\u4e86\u6240\u6709\u6d4b\u91cf\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u96c6\u4e2d\u5728\u6536\u76ca\u6700\u5927\u7684\u56fe\u90e8\u5206\u3002"}}
{"id": "2601.08143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08143", "abs": "https://arxiv.org/abs/2601.08143", "authors": ["Takuya Kato", "Kentaro Uno", "Kazuya Yoshida"], "title": "A Pin-Array Structure for Gripping and Shape Recognition of Convex and Concave Terrain Profiles", "comment": "Author's version of a manuscript accepted at the 2022 IEEE International Conference on Robotics and Biomimetics (ROBIO). (c) IEEE", "summary": "This paper presents a gripper capable of grasping and recognizing terrain shapes for mobile robots in extreme environments. Multi-limbed climbing robots with grippers are effective on rough terrains, such as cliffs and cave walls. However, such robots may fall over by misgrasping the surface or getting stuck owing to the loss of graspable points in unknown natural environments. To overcome these issues, we need a gripper capable of adaptive grasping to irregular terrains, not only for grasping but also for measuring the shape of the terrain surface accurately. We developed a gripper that can grasp both convex and concave terrains and simultaneously measure the terrain shape by introducing a pin-array structure. We demonstrated the mechanism of the gripper and evaluated its grasping and terrain recognition performance using a prototype. Moreover, the proposed pin-array design works well for 3D terrain mapping as well as adaptive grasping for irregular terrains.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5939\u6301\u5668\uff0c\u80fd\u591f\u6293\u63e1\u548c\u8bc6\u522b\u6781\u7aef\u73af\u5883\u4e2d\u7684\u5730\u5f62\u5f62\u72b6\uff0c\u901a\u8fc7\u9500\u9489\u9635\u5217\u7ed3\u6784\u5b9e\u73b0\u81ea\u9002\u5e94\u6293\u63e1\u548c\u5730\u5f62\u6d4b\u91cf\u3002", "motivation": "\u591a\u80a2\u6500\u722c\u673a\u5668\u4eba\u5728\u7c97\u7cd9\u5730\u5f62\uff08\u5982\u60ac\u5d16\u548c\u6d1e\u7a74\u5899\u58c1\uff09\u4e0a\u5f88\u6709\u6548\uff0c\u4f46\u5728\u672a\u77e5\u81ea\u7136\u73af\u5883\u4e2d\u53ef\u80fd\u56e0\u8bef\u6293\u8868\u9762\u6216\u5931\u53bb\u53ef\u6293\u63e1\u70b9\u800c\u6454\u5012\u6216\u5361\u4f4f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u6293\u63e1\u4e0d\u89c4\u5219\u5730\u5f62\u5e76\u51c6\u786e\u6d4b\u91cf\u5730\u5f62\u5f62\u72b6\u7684\u5939\u6301\u5668\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u91c7\u7528\u9500\u9489\u9635\u5217\u7ed3\u6784\u7684\u5939\u6301\u5668\uff0c\u80fd\u591f\u6293\u63e1\u51f8\u9762\u548c\u51f9\u9762\u5730\u5f62\uff0c\u5e76\u540c\u65f6\u6d4b\u91cf\u5730\u5f62\u5f62\u72b6\u3002\u901a\u8fc7\u539f\u578b\u673a\u8bc4\u4f30\u4e86\u5176\u6293\u63e1\u548c\u5730\u5f62\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u9500\u9489\u9635\u5217\u8bbe\u8ba1\u4e0d\u4ec5\u9002\u7528\u4e8e\u81ea\u9002\u5e94\u6293\u63e1\u4e0d\u89c4\u5219\u5730\u5f62\uff0c\u8fd8\u80fd\u5f88\u597d\u5730\u7528\u4e8e3D\u5730\u5f62\u6620\u5c04\u3002\u539f\u578b\u673a\u9a8c\u8bc1\u4e86\u8be5\u5939\u6301\u5668\u7684\u6293\u63e1\u548c\u5730\u5f62\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u9500\u9489\u9635\u5217\u5939\u6301\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6781\u7aef\u73af\u5883\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u6293\u63e1\u548c\u5730\u5f62\u8bc6\u522b\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2601.08161", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08161", "abs": "https://arxiv.org/abs/2601.08161", "authors": ["Jing Tao", "Banglei Guan", "Yang Shang", "Shunkun Liang", "Qifeng Yu"], "title": "Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching", "comment": "This paper has been accepted by Applied Optics", "summary": "This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u98de\u884c\u5bfc\u822a\u7684\u9c81\u68d2\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c42\u89d2\u70b9\u7b5b\u9009\u548c\u81ea\u9002\u5e94\u6a21\u677f\u5339\u914d\u89e3\u51b3\u590d\u6742\u80cc\u666f\u5e72\u6270\u548c\u4f20\u7edf\u6ed1\u52a8\u7a97\u53e3\u5339\u914d\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u98de\u884c\u5bfc\u822a\u4e2d\u590d\u6742\u80cc\u666f\u5e72\u6270\u5bfc\u81f4\u7684\u5b9a\u4f4d\u5931\u8d25\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u6ed1\u52a8\u7a97\u53e3\u5339\u914d\u6280\u672f\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u5c40\u9650\u6027", "method": "\u91c7\u7528\u4e09\u5c42\u6846\u67b6\uff1a1) \u901a\u8fc7\u5149\u7167\u5747\u8861\u548c\u7ed3\u6784\u4fe1\u606f\u63d0\u53d6\u964d\u7ef4\uff1b2) \u7c97\u5230\u7cbe\u5019\u9009\u70b9\u9009\u62e9\u7b56\u7565\u51cf\u5c11\u6ed1\u52a8\u7a97\u53e3\u8ba1\u7b97\u6210\u672c\uff1b3) \u4e3a\u5019\u9009\u70b9\u751f\u6210\u81ea\u9002\u5e94\u6a21\u677f\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6a21\u677f\u5339\u914d\u548c\u76f8\u5173\u7cfb\u6570\u6781\u503c\u62df\u5408\u5b9e\u73b0\u4e9a\u50cf\u7d20\u7cbe\u5ea6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u80fd\u6709\u6548\u63d0\u53d6\u548c\u5b9a\u4f4d\u5bf9\u89d2\u6807\u8bb0\uff0c\u9002\u5408\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u89c6\u573a\u6d4b\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u98de\u884c\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u3001\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u80cc\u666f\u73af\u5883\u4e0b\u7684\u6807\u8bb0\u5b9a\u4f4d\u4efb\u52a1"}}
{"id": "2601.08246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08246", "abs": "https://arxiv.org/abs/2601.08246", "authors": ["Yifan Han", "Pengfei Yi", "Junyan Li", "Hanqing Wang", "Gaojing Zhang", "Qi Peng Liu", "Wenzhao Lian"], "title": "FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models", "comment": null, "summary": "Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u7075\u5de7\u6293\u53d6\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u4ece\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u63d0\u53d6\u6293\u53d6\u80fd\u529b\uff0c\u5b9e\u73b0\u8de8\u624b\u90e8\u786c\u4ef6\u7684\u6cdb\u5316\uff0c\u65e0\u9700\u4e3a\u6bcf\u79cd\u624b\u578b\u6536\u96c6\u5927\u91cf\u6293\u53d6\u6570\u636e\u3002", "motivation": "\u7075\u5de7\u6293\u53d6\u5408\u6210\u9762\u4e34\u9ad8\u7ef4\u5ea6\u548c\u8fd0\u52a8\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u786c\u4ef6\u7279\u5b9a\u7684\u5927\u89c4\u6a21\u6293\u53d6\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u65b0\u7075\u5de7\u624b\u8bbe\u8ba1\u7684\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7ed5\u8fc7\u673a\u5668\u4eba\u6293\u53d6\u6570\u636e\u6536\u96c6\u3002", "method": "1) \u4ece\u539f\u59cb\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u63d0\u53d6\u65f6\u95f4\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u6293\u53d6\u80fd\u529b\uff1b2) \u5c06\u6293\u53d6\u80fd\u529b\u4e0e\u6df1\u5ea6\u56fe\u50cf\u76843D\u573a\u666f\u51e0\u4f55\u878d\u5408\uff0c\u63a8\u65ad\u8bed\u4e49\u63a5\u5730\u70b9\uff1b3) \u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u91cd\u5b9a\u5411\u6a21\u5757\u5c06\u80fd\u529b\u8868\u793a\u6620\u5c04\u5230\u4e0d\u540c\u7684\u7075\u5de7\u624b\uff0c\u65e0\u9700\u4e3a\u6bcf\u79cd\u624b\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u7cfb\u7edf\u4ea7\u751f\u7a33\u5b9a\u3001\u529f\u80fd\u9002\u5f53\u7684\u591a\u63a5\u89e6\u6293\u53d6\uff0c\u5728\u5e38\u89c1\u7269\u4f53\u548c\u5de5\u5177\u4e0a\u4fdd\u6301\u53ef\u9760\u6210\u529f\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5b9e\u4f8b\u3001\u59ff\u6001\u53d8\u5316\u548c\u591a\u79cd\u624b\u90e8\u5b9e\u73b0\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u548c\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u786c\u4ef6\u65e0\u5173\u7684\u7075\u5de7\u64cd\u4f5c\u8def\u5f84\uff0c\u8bc1\u660e\u4e86\u5355\u4e00\u6df1\u5ea6\u6a21\u6001\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u8bed\u4e49\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u6293\u53d6\u5408\u6210\u3002"}}
{"id": "2601.08325", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08325", "abs": "https://arxiv.org/abs/2601.08325", "authors": ["Zhenyang Liu", "Yongchong Gu", "Yikai Wang", "Xiangyang Xue", "Yanwei Fu"], "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation", "comment": null, "summary": "Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.", "AI": {"tldr": "ActiveVLA\u662f\u4e00\u4e2a\u7ed3\u5408\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u5173\u952e\u533a\u57df\u5b9a\u4f4d\u548c\u4e3b\u52a8\u611f\u77e5\u4f18\u5316\uff09\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7ec6\u7c92\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u9759\u6001\u8155\u90e8\u6444\u50cf\u5934\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u6216\u5206\u8fa8\u7387\uff0c\u9650\u5236\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u573a\u666f\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a1\uff09\u5173\u952e\u533a\u57df\u5b9a\u4f4d\uff1a\u5c063D\u8f93\u5165\u6295\u5f71\u5230\u591a\u89c6\u89d22D\u6295\u5f71\uff0c\u8bc6\u522b\u5173\u952e3D\u533a\u57df\uff0c\u652f\u6301\u52a8\u6001\u7a7a\u95f4\u611f\u77e5\uff1b2\uff09\u4e3b\u52a8\u611f\u77e5\u4f18\u5316\uff1a\u57fa\u4e8e\u5b9a\u4f4d\u7684\u5173\u952e\u533a\u57df\uff0c\u4f7f\u7528\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u7b56\u7565\u9009\u62e9\u6700\u4f73\u89c6\u89d2\uff0c\u6700\u5927\u5316\u6a21\u6001\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\uff0c\u6700\u5c0f\u5316\u906e\u6321\uff0c\u5e76\u5bf9\u5173\u952e\u533a\u57df\u8fdb\u884c3D\u653e\u5927\u4ee5\u63d0\u9ad8\u5206\u8fa8\u7387\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cActiveVLA\u5b9e\u73b0\u4e86\u7cbe\u786e\u76843D\u64cd\u4f5c\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\u80fd\u591f\u65e0\u7f1d\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b66\u4e60\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u3002", "conclusion": "ActiveVLA\u901a\u8fc7\u8d4b\u4e88\u673a\u5668\u4eba\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08405", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08405", "abs": "https://arxiv.org/abs/2601.08405", "authors": ["Yizhan Feng", "Hichem Snoussi", "Jing Teng", "Abel Cherouat", "Tian Wang"], "title": "Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments", "comment": "1st International Conference on Drones and Unmanned Systems (DAUS' 2025)", "summary": "Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u8c03CodeT5\u6a21\u578b\u4e0eAirSim\u65e0\u4eba\u673a\u6a21\u62df\u5668\u7684\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7ChatGPT\u751f\u6210\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u6267\u884c\u65e0\u4eba\u673a\u4ee3\u7801\u7684\u81ea\u52a8\u7ffb\u8bd1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u673a\u4ea4\u4e92\u8fce\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u673a\u9047\u3002\u4f20\u7edf\u65e0\u4eba\u673a\u64cd\u4f5c\u9700\u8981\u4e13\u4e1a\u6280\u80fd\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u964d\u4f4e\u65e0\u4eba\u673a\u64cd\u4f5c\u95e8\u69db\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u66f4\u4fbf\u6377\u5730\u63a7\u5236\u65e0\u4eba\u673a\u72b6\u6001\u3002", "method": "1. \u96c6\u6210\u5fae\u8c03\u7684CodeT5\u6a21\u578b\u4e0e\u57fa\u4e8eUnreal Engine\u7684AirSim\u65e0\u4eba\u673a\u6a21\u62df\u5668\uff1b2. \u4f7f\u7528ChatGPT\u751f\u6210\u7684\u5927\u89c4\u6a21\uff08\u81ea\u7136\u8bed\u8a00\uff0c\u7a0b\u5e8f\u4ee3\u7801\uff09\u547d\u4ee4-\u6267\u884c\u5bf9\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5f00\u53d1\u8005\u7f16\u5199\u7684\u65e0\u4eba\u673a\u4ee3\u7801\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff1b3. \u5728AirSim\u4e2d\u6784\u5efa\u89c6\u89c9\u903c\u771f\u7684\u52a8\u6001\u73af\u5883\u6a21\u62df\u590d\u6742\u573a\u666f\uff1b4. \u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u6267\u884c\u65e0\u4eba\u673a\u4ee3\u7801\u7684\u81ea\u52a8\u7ffb\u8bd1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u4efb\u52a1\u6267\u884c\u6548\u7387\u548c\u547d\u4ee4\u7406\u89e3\u80fd\u529b\u3002\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u4efb\u52a1\u64cd\u4f5c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65e0\u4eba\u673a\u64cd\u4f5c\u7684\u6280\u672f\u95e8\u69db\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u6a21\u62df\u65e0\u4eba\u673a\uff0c\u672a\u6765\u8ba1\u5212\u4ee5\u6a21\u5757\u5316\u65b9\u5f0f\u6269\u5c55\u6a21\u578b\u529f\u80fd\uff0c\u589e\u5f3a\u5bf9\u590d\u6742\u573a\u666f\u7684\u9002\u5e94\u6027\uff0c\u63a8\u52a8\u65e0\u4eba\u673a\u6280\u672f\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.08422", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08422", "abs": "https://arxiv.org/abs/2601.08422", "authors": ["Taerim Yoon", "Dongho Kang", "Jin Cheng", "Fatemeh Zargarbashi", "Yijiang Huang", "Minsung Ahn", "Stelian Coros", "Sungjoon Choi"], "title": "Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech", "comment": "10 pages, 7 figures", "summary": "In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u901a\u8fc7\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u5b66\u4e60\u7406\u89e3\u793e\u4ea4\u7ebf\u7d22\u548c\u5bfc\u822a\u884c\u4e3a\uff0c\u652f\u6301\u624b\u52bf\u548c\u8bed\u97f3\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u654f\u6377\u5bfc\u822a\u573a\u666f\u4e2d\u8fbe\u523097.15%\u6210\u529f\u7387\u3002", "motivation": "\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u4eba\u7c7b\u793e\u4ea4\u7ebf\u7d22\u5e76\u4ea7\u751f\u9002\u5f53\u884c\u4e3a\uff0c\u4f46\u4f20\u7edf\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4eba\u7c7b\u6570\u636e\uff0c\u7ed9\u7528\u6237\u5e26\u6765\u6c89\u91cd\u8d1f\u62c5\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u91cd\u5efa\u4ea4\u4e92\u573a\u666f\u5e76\u805a\u5408\u6570\u636e\u4ee5\u51cf\u5c11\u5206\u5e03\u504f\u79fb\uff1b\u91c7\u7528\u6e10\u8fdb\u5f0f\u76ee\u6807\u63d0\u793a\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u4e2d\u81ea\u9002\u5e94\u63d0\u4f9b\u9002\u5f53\u547d\u4ee4\u548c\u5bfc\u822a\u76ee\u6807\uff1b\u652f\u6301\u624b\u52bf\u548c\u8bed\u97f3\u591a\u6a21\u6001\u81ea\u7136\u8f93\u5165\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u654f\u6377\u5bfc\u822a\u573a\u666f\uff08\u5305\u62ec\u8df3\u8dc3\u969c\u788d\u548c\u907f\u969c\uff09\u4e2d\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u8bd5\u9a8c\u4e2d\u90fd\u6210\u529f\uff0c\u603b\u6f14\u793a\u6570\u636e\u5c11\u4e8e1\u5c0f\u65f6\u7684\u60c5\u51b5\u4e0b\u8fbe\u523097.15%\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6570\u636e\u9ad8\u6548\u7684\u65b9\u5f0f\u8ba9\u673a\u5668\u4eba\u5b66\u4e60\u5bfc\u822a\u884c\u4e3a\uff0c\u5b9e\u73b0\u4eba\u7c7b\u8f93\u5165\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u4e4b\u95f4\u7684\u5f3a\u5bf9\u9f50\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.08434", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08434", "abs": "https://arxiv.org/abs/2601.08434", "authors": ["Long Zhang", "Yuchen Xia"], "title": "Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?", "comment": null, "summary": "The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u4e0e\u7b56\u7565\u53cc\u9a71\u52a8\u7684\u6df7\u5408\u51b3\u7b56\u6846\u67b6\uff0c\u5c06\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7528\u4e8e\u8bed\u4e49\u7406\u89e3\u548c\u8ba4\u77e5\u8868\u793a\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5b9e\u65f6\u7b56\u7565\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u548c\u8054\u5408\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u867d\u7136\u80fd\u89e3\u51b3\u6a21\u5757\u5316\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4f46\u4ec5\u4f9d\u8d56LMMs\u63d0\u5347\u5177\u8eab\u667a\u80fd\u9a7e\u9a76\u80fd\u529b\u4f1a\u53d7\u5230\u9650\u5236\uff0c\u7f3a\u4e4f\u8054\u5408\u51b3\u7b56\u80fd\u529b\u3002\u9700\u8981\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u548c\u8054\u5408\u51b3\u7b56\u6765\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\u5411\u5177\u8eab\u667a\u80fd\u9a7e\u9a76\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u4e0e\u7b56\u7565\u53cc\u9a71\u52a8\u7684\u6df7\u5408\u51b3\u7b56\u6846\u67b6\uff0c\u878d\u5408LMMs\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\u548c\u8ba4\u77e5\u8868\u793a\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5b9e\u65f6\u7b56\u7565\u4f18\u5316\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u8f66\u9053\u53d8\u6362\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u5b8c\u6210\u8f66\u9053\u53d8\u6362\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5177\u8eab\u667a\u80fd\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bed\u4e49\u4e0e\u7b56\u7565\u53cc\u9a71\u52a8\u7684\u6df7\u5408\u51b3\u7b56\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u548c\u8054\u5408\u51b3\u7b56\u95ee\u9898\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u8bc6\u522b\u4e86\u591a\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u6307\u5bfc\u540e\u7eed\u5de5\u4f5c\u3002"}}
{"id": "2601.08454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08454", "abs": "https://arxiv.org/abs/2601.08454", "authors": ["Alessandro Adami", "Sebastian Zudaire", "Ruggero Carli", "Pietro Falco"], "title": "Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees", "comment": null, "summary": "Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u6811\u7684Real2Sim\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u7269\u7406\u4ea4\u4e92\u884c\u4e3a\u6811\uff0c\u81ea\u4e3b\u83b7\u53d6\u4eff\u771f\u6240\u9700\u7269\u7406\u53c2\u6570\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u4efb\u52a1\u6a21\u677f\u6216\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63a2\u7d22\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edfReal2Sim\u6d41\u6c34\u7ebf\u4f9d\u8d56\u624b\u52a8\u6d4b\u91cf\u6216\u56fa\u5b9a\u7684\u9884\u7f16\u7a0b\u63a2\u7d22\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u548c\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u5177\u4f53\u4eff\u771f\u76ee\u6807\u81ea\u4e3b\u83b7\u53d6\u6240\u9700\u7269\u7406\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u8bc6\u522b\u76f8\u5173\u7269\u4f53\u3001\u63a8\u65ad\u6240\u9700\u7269\u7406\u53c2\u6570\uff0c\u5e76\u751f\u6210\u7531\u57fa\u672c\u673a\u5668\u4eba\u52a8\u4f5c\u7ec4\u6210\u7684\u884c\u4e3a\u6811\u3002\u5728\u626d\u77e9\u63a7\u5236\u7684Franka Emika Panda\u673a\u68b0\u81c2\u4e0a\u6267\u884c\u8fd9\u4e9b\u884c\u4e3a\uff0c\u901a\u8fc7\u987a\u5e94\u6027\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u673a\u68b0\u81c2\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u79cd\u573a\u666f\u4e0b\uff08\u5305\u62ec\u906e\u6321\u7269\u4f53\u548c\u4e0d\u5b8c\u6574\u5148\u9a8c\u6a21\u578b\uff09\u51c6\u786e\u4f30\u8ba1\u7269\u4f53\u8d28\u91cf\u3001\u8868\u9762\u9ad8\u5ea6\u548c\u6469\u64e6\u76f8\u5173\u53c2\u6570\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u3001\u610f\u56fe\u9a71\u52a8\u4e14\u81ea\u4e3b\u7684Real2Sim\u6d41\u6c34\u7ebf\uff0c\u5c06\u9ad8\u7ea7\u63a8\u7406\u4e0e\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u76f8\u7ed3\u5408\uff0c\u4e3a\u6784\u5efa\u51c6\u786e\u7684\u73af\u5883\u4eff\u771f\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.08485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08485", "abs": "https://arxiv.org/abs/2601.08485", "authors": ["Chong Zhang", "Victor Klemm", "Fan Yang", "Marco Hutter"], "title": "AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding", "comment": "under review", "summary": "Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.", "AI": {"tldr": "AME-2\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7684\u5730\u56fe\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u6620\u5c04\u7ba1\u9053\uff0c\u5b9e\u73b0\u4e86\u56db\u8db3\u548c\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u654f\u6377\u548c\u6cdb\u5316\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7aef\u5230\u7aef\u4f20\u611f\u5668\u8fd0\u52a8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff1b\u9488\u5bf9\u6cdb\u5316\u8fd0\u52a8\u7684\u65b9\u6cd5\u901a\u5e38\u654f\u6377\u6027\u4e0d\u8db3\u4e14\u96be\u4ee5\u5904\u7406\u89c6\u89c9\u906e\u6321\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u654f\u6377\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5e76\u80fd\u5904\u7406\u906e\u6321\u548c\u7a00\u758f\u7acb\u8db3\u70b9\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faAME-2\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u6ce8\u610f\u529b\u673a\u5236\u7684\u5730\u56fe\u7f16\u7801\u5668\uff0c\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u5730\u56fe\u7279\u5f81\uff0c\u805a\u7126\u5173\u952e\u533a\u57df\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5d4c\u5165\u8868\u793a\uff1b2\uff09\u57fa\u4e8e\u5b66\u4e60\u7684\u6620\u5c04\u7ba1\u9053\uff0c\u5c06\u6df1\u5ea6\u89c2\u6d4b\u5feb\u901f\u8f6c\u6362\u4e3a\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u5c40\u90e8\u9ad8\u7a0b\u56fe\uff0c\u5e76\u4e0e\u91cc\u7a0b\u8ba1\u878d\u5408\uff0c\u5bf9\u566a\u58f0\u548c\u906e\u6321\u5177\u6709\u9c81\u68d2\u6027\uff1b3\uff09\u4e0e\u5e76\u884c\u4eff\u771f\u96c6\u6210\uff0c\u652f\u6301\u5728\u7ebf\u6620\u5c04\u8bad\u7ec3\uff0c\u4fc3\u8fdb\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "result": "\u5728\u56db\u8db3\u548c\u53cc\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86AME-2\u6846\u67b6\uff0c\u63a7\u5236\u5668\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u654f\u6377\u6027\u548c\u5bf9\u672a\u89c1\u5730\u5f62\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AME-2\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7684\u5730\u56fe\u7f16\u7801\u5668\u548c\u9c81\u68d2\u7684\u6620\u5c04\u7ba1\u9053\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u611f\u77e5\u4e0e\u63a7\u5236\u7684\u7d27\u5bc6\u96c6\u6210\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u654f\u6377\u548c\u6cdb\u5316\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08491", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08491", "abs": "https://arxiv.org/abs/2601.08491", "authors": ["Mohamed Afouene Melki", "Mohammad Shehab", "Mohamed-Slim Alouini"], "title": "AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization", "comment": null, "summary": "Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain's fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6301\u7eed\u7684\u6c34\u4e0b\u7269\u8054\u7f51(IoUT)\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668(AUV)\u540c\u65f6\u5b9e\u73b0\u4fe1\u606f\u4e0a\u884c\u548c\u58f0\u5b66\u80fd\u91cf\u4f20\u8f93\uff0c\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u5e74\u9f84\u4fe1\u606f(AoI)\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u7269\u8054\u7f51\u8bbe\u5907\u4f9d\u8d56\u7535\u6c60\u4f9b\u7535\uff0c\u5b58\u5728\u5bff\u547d\u9650\u5236\u548c\u5e9f\u5f03\u540e\u7684\u73af\u5883\u5371\u5bb3\u95ee\u9898\uff0c\u9700\u8981\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u957f\u671f\u6c34\u4e0b\u76d1\u6d4b\u548c\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u901a\u8fc7AUV\u540c\u65f6\u8fdb\u884c\u4fe1\u606f\u4e0a\u884c\u548c\u58f0\u5b66\u80fd\u91cf\u4f20\u8f93\u7684\u53ef\u6301\u7eed\u65b9\u6848\uff0c\u91c7\u7528\u5e74\u9f84\u4fe1\u606f(AoI)\u548cJain\u516c\u5e73\u6027\u6307\u6570\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1a\u9ad8\u6027\u80fd\u7684\u9891\u5206\u53cc\u5de5(FDD)\u65b9\u6848\u548c\u4e2d\u7b49\u6027\u80fd\u7684\u65f6\u5206\u53cc\u5de5(TDD)\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684FDD\u548cTDD\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747AoI\uff0c\u63d0\u9ad8\u4e86\u80fd\u91cf\u6536\u96c6\u6548\u7387\u548c\u6570\u636e\u6536\u96c6\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6c34\u4e0b\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7AUV\u540c\u65f6\u8fdb\u884c\u4fe1\u606f\u4f20\u8f93\u548c\u80fd\u91cf\u8865\u7ed9\uff0c\u6709\u671b\u5b9e\u73b0\u8bbe\u5907\u7684\u65e0\u9650\u671f\u8fd0\u884c\uff0c\u5e76\u901a\u8fc7DRL\u7b97\u6cd5\u4f18\u5316\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.08514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08514", "abs": "https://arxiv.org/abs/2601.08514", "authors": ["Davide Risi", "Vincenzo Petrone", "Antonio Langella", "Lorenzo Pagliara", "Enrico Ferrentino", "Pasquale Chiacchio"], "title": "Simplifying ROS2 controllers with a modular architecture for robot-agnostic reference generation", "comment": "5 pages, 7 figures", "summary": "This paper introduces a novel modular architecture for ROS2 that decouples the logic required to acquire, validate, and interpolate references from the control laws that track them. The design includes a dedicated component, named Reference Generator, that receives references, in the form of either single points or trajectories, from external nodes (e.g., planners), and writes single-point references at the controller's sampling period via the existing ros2_control chaining mechanism to downstream controllers. This separation removes duplicated reference-handling code from controllers and improves reusability across robot platforms. We implement two reference generators: one for handling joint-space references and one for Cartesian references, along with a set of new controllers (PD with gravity compensation, Cartesian pose, and admittance controllers) and validate the approach on simulated and real Universal Robots and Franka Emika manipulators. Results show that (i) references are tracked reliably in all tested scenarios, (ii) reference generators reduce duplicated reference-handling code across chained controllers to favor the construction and reuse of complex controller pipelines, and (iii) controller implementations remain focused only on control laws.", "AI": {"tldr": "\u63d0\u51faROS2\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7Reference Generator\u7ec4\u4ef6\u5206\u79bb\u53c2\u8003\u8f68\u8ff9\u83b7\u53d6\u4e0e\u63a7\u5236\u5668\u903b\u8f91\uff0c\u51cf\u5c11\u4ee3\u7801\u91cd\u590d\uff0c\u63d0\u9ad8\u63a7\u5236\u5668\u590d\u7528\u6027", "motivation": "\u89e3\u51b3ROS2\u63a7\u5236\u5668\u4e2d\u53c2\u8003\u8f68\u8ff9\u5904\u7406\u4ee3\u7801\u91cd\u590d\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u63a7\u5236\u5668\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u590d\u7528\u6027", "method": "\u8bbe\u8ba1Reference Generator\u7ec4\u4ef6\uff0c\u5206\u79bb\u53c2\u8003\u8f68\u8ff9\u83b7\u53d6\u3001\u9a8c\u8bc1\u548c\u63d2\u503c\u903b\u8f91\uff1b\u5b9e\u73b0\u5173\u8282\u7a7a\u95f4\u548c\u7b1b\u5361\u5c14\u7a7a\u95f4\u4e24\u79cd\u53c2\u8003\u751f\u6210\u5668\uff1b\u5f00\u53d1PD\u91cd\u529b\u8865\u507f\u3001\u7b1b\u5361\u5c14\u4f4d\u59ff\u548c\u5bfc\u7eb3\u63a7\u5236\u5668", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u53ef\u9760\u8ddf\u8e2a\u53c2\u8003\u8f68\u8ff9\uff1b\u663e\u8457\u51cf\u5c11\u63a7\u5236\u5668\u4e2d\u7684\u91cd\u590d\u4ee3\u7801\uff1b\u63a7\u5236\u5668\u5b9e\u73b0\u4e13\u6ce8\u4e8e\u63a7\u5236\u7b97\u6cd5\u672c\u8eab", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316\u67b6\u6784\u6709\u6548\u5206\u79bb\u4e86\u53c2\u8003\u8f68\u8ff9\u5904\u7406\u4e0e\u63a7\u5236\u903b\u8f91\uff0c\u63d0\u9ad8\u4e86ROS2\u63a7\u5236\u7cfb\u7edf\u7684\u53ef\u91cd\u7528\u6027\u548c\u53ef\u7ef4\u62a4\u6027"}}
{"id": "2601.08520", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08520", "abs": "https://arxiv.org/abs/2601.08520", "authors": ["Krzysztof Zielinski", "Dominik Belter"], "title": "Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps", "comment": "Accepted in ICRA 2020", "summary": "In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u5e27\u7684NDT\u5730\u56fe\u6784\u5efa\u7cfb\u7edf\uff0c\u5229\u7528RGB-D\u4f20\u611f\u5668\u6570\u636e\u66f4\u65b0\u5c40\u90e8NDT\u5730\u56fe\uff0c\u901a\u8fc7\u59ff\u6001\u56fe\u5b58\u50a8\u5c40\u90e8\u5730\u56fe\u5b9e\u73b0\u56de\u73af\u68c0\u6d4b\u540e\u7684\u5168\u5c40\u5730\u56fe\u6821\u6b63\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u5229\u7528RGB-D\u76f8\u673a\u7684\u7279\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u63d0\u9ad8\u8fd1\u8ddd\u79bb\u7269\u4f53\u7684\u8868\u793a\u7cbe\u5ea6\uff0c\u5e76\u5b9e\u73b0\u56de\u73af\u68c0\u6d4b\u540e\u7684\u5168\u5c40\u5730\u56fe\u6821\u6b63\u3002", "method": "\u4f7f\u7528RGB-D\u4f20\u611f\u5668\u6570\u636e\u66f4\u65b0\u5c40\u90e8NDT\u5730\u56fe\uff0c\u5c06NDT\u5355\u5143\u5b58\u50a8\u57282D\u89c6\u56fe\u76f8\u5173\u7ed3\u6784\u4e2d\uff0c\u5c40\u90e8\u5730\u56fe\u5b58\u50a8\u5728\u59ff\u6001\u56fe\u4e2d\uff0c\u63d0\u51fa\u5c40\u90e8\u5730\u56fe\u5408\u5e76\u548c\u8fc7\u6ee4\u7a0b\u5e8f\u4ee5\u83b7\u5f97\u5168\u5c40\u73af\u5883\u5730\u56fe\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u8868\u793a\u9760\u8fd1\u76f8\u673a\u539f\u70b9\u7684\u7269\u4f53\uff0c\u5b9e\u73b0\u4e86\u56de\u73af\u68c0\u6d4b\u540e\u7684\u5168\u5c40\u5730\u56fe\u6821\u6b63\uff0c\u5e76\u4e0eOctomap\u548cNDT-OM\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5173\u952e\u5e27\u7684NDT\u5730\u56fe\u6784\u5efa\u7cfb\u7edf\u6709\u6548\u5229\u7528\u4e86RGB-D\u76f8\u673a\u7279\u6027\uff0c\u63d0\u9ad8\u4e86\u5730\u56fe\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u59ff\u6001\u56fe\u5b9e\u73b0\u4e86\u5168\u5c40\u5730\u56fe\u7684\u4f18\u5316\u6821\u6b63\u3002"}}
{"id": "2601.08523", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08523", "abs": "https://arxiv.org/abs/2601.08523", "authors": ["Nesserine Laribi", "Mohammed Rida Mokhtari", "Abdelaziz Benallegue", "Abdelhafid El-Hadri", "Mehdi Benallegue"], "title": "QP-Based Control of an Underactuated Aerial Manipulator under Constraints", "comment": null, "summary": "This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6b20\u9a71\u52a8\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u7ea6\u675f\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8003\u8651\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u7684\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u9488\u5bf9\u6b20\u9a71\u52a8\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u5728\u5b9e\u73b0\u7cbe\u786e\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u8ddf\u8e2a\u7684\u540c\u65f6\uff0c\u663e\u5f0f\u5730\u8003\u8651\u5b89\u5168\u6027\u3001\u53ef\u884c\u6027\u7ea6\u675f\u4ee5\u53ca\u7cfb\u7edf\u9c81\u68d2\u6027\u9700\u6c42\u3002", "method": "\u5c06\u63a7\u5236\u95ee\u9898\u8868\u8ff0\u4e3a\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff0c\u8ba1\u7b97\u6ee1\u8db3\u6b20\u9a71\u52a8\u3001\u6267\u884c\u5668\u9650\u5236\u548c\u7cfb\u7edf\u7ea6\u675f\u7684\u52a8\u6001\u4e00\u81f4\u5e7f\u4e49\u52a0\u901f\u5ea6\uff1b\u5728\u626d\u77e9\u5c42\u9762\u5f15\u5165\u57fa\u4e8e\u88ab\u52a8\u6027\u7684\u79ef\u5206\u4f5c\u7528\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u7269\u7406\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u5305\u542b\u53c2\u6570\u6270\u52a8\u3001\u7c98\u6027\u5173\u8282\u6469\u64e6\u548c\u5b9e\u9645\u4f20\u611f\u72b6\u6001\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u8ddf\u8e2a\u3001\u5e73\u6ed1\u63a7\u5236\u8f93\u5165\u548c\u53ef\u9760\u7684\u7ea6\u675f\u6ee1\u8db3\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ea6\u675f\u611f\u77e5\u63a7\u5236\u6846\u67b6\u80fd\u591f\u4e3a\u6b20\u9a71\u52a8\u7a7a\u4e2d\u673a\u68b0\u81c2\u5728\u5b9e\u9645\u64cd\u4f5c\u6761\u4ef6\u4e0b\u63d0\u4f9b\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u5b89\u5168\u7684\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2601.08665", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08665", "abs": "https://arxiv.org/abs/2601.08665", "authors": ["Shaoan Wang", "Yuanfei Luo", "Xingyu Chen", "Aocheng Luo", "Dongyue Li", "Chang Liu", "Sheng Chen", "Yangang Zhang", "Junzhi Yu"], "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory", "comment": "Project page: https://wsakobe.github.io/VLingNav-web/", "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.", "AI": {"tldr": "VLingNav\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u9a71\u52a8\u8ba4\u77e5\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u673a\u5236\u548c\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\uff0c\u5728\u5177\u8eab\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5927\u591a\u91c7\u7528\u4ece\u89c2\u5bdf\u5230\u52a8\u4f5c\u7684\u88ab\u52a8\u6620\u5c04\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u548c\u6301\u4e45\u8bb0\u5fc6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u3001\u957f\u89c6\u91ce\u7684\u5bfc\u822a\u4efb\u52a1\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u4eba\u7c7b\u8ba4\u77e5\u53cc\u8fc7\u7a0b\u7406\u8bba\u7684\u65b9\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u5feb\u901f\u76f4\u89c9\u6267\u884c\u548c\u6162\u901f\u6df1\u601d\u89c4\u5212\u4e4b\u95f4\u7075\u6d3b\u5207\u6362\u3002", "method": "1. \u5f15\u5165\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u673a\u5236\uff0c\u52a8\u6001\u89e6\u53d1\u663e\u5f0f\u63a8\u7406\uff1b2. \u5f00\u53d1\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u8bed\u4e49\u8bb0\u5fc6\uff1b3. \u6784\u5efaNav-AdaCoT-2.9M\u6570\u636e\u96c6\uff0c\u5305\u542b\u63a8\u7406\u6807\u6ce8\uff1b4. \u91c7\u7528\u5728\u7ebf\u4e13\u5bb6\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u8d85\u8d8a\u7eaf\u6a21\u4eff\u5b66\u4e60\u3002", "result": "VLingNav\u5728\u5e7f\u6cdb\u7684\u5177\u8eab\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6267\u884c\u5404\u79cd\u5bfc\u822a\u4efb\u52a1\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VLingNav\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u8ba4\u77e5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u63a8\u7406\u548c\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u957f\u89c6\u91ce\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u5411\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\u8fc1\u79fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08713", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08713", "abs": "https://arxiv.org/abs/2601.08713", "authors": ["Naren Medarametla", "Sreejon Mondal"], "title": "Real-Time Localization Framework for Autonomous Basketball Robots", "comment": "8 pages, 12 figures, Project code: https://github.com/NarenTheNumpkin/Basketball-robot-localization", "summary": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u7ecf\u5178\u6280\u672f\u4e0e\u5b66\u4e60\u65b9\u6cd5\u7684\u6df7\u5408\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u4ec5\u5229\u7528\u7403\u573a\u5730\u9762\u89c6\u89c9\u6570\u636e\u5b9e\u73b0\u7bee\u7403\u573a\u4e0a\u7684\u81ea\u4e3b\u5b9a\u4f4d", "motivation": "\u5b9a\u4f4d\u662f\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u57fa\u672c\u80fd\u529b\uff0c\u5728Robocon 2025\u7ade\u8d5b\u4e2d\uff0c\u51c6\u786e\u53ef\u9760\u7684\u5b9a\u4f4d\u5bf9\u4e8e\u63d0\u9ad8\u5c04\u51fb\u7cbe\u5ea6\u3001\u907f\u514d\u4e0e\u5176\u4ed6\u673a\u5668\u4eba\u78b0\u649e\u4ee5\u53ca\u9ad8\u6548\u5bfc\u822a\u6bd4\u8d5b\u573a\u5730\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u6df7\u5408\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u96c6\u6210\u7ecf\u5178\u6280\u672f\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u7403\u573a\u5730\u9762\u89c6\u89c9\u6570\u636e\u8fdb\u884c\u81ea\u5b9a\u4f4d", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c", "conclusion": "\u8be5\u6df7\u5408\u5b9a\u4f4d\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7bee\u7403\u573a\u4e0a\u7684\u81ea\u4e3b\u5b9a\u4f4d\uff0c\u6ee1\u8db3Robocon\u7ade\u8d5b\u9700\u6c42"}}
{"id": "2601.08819", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08819", "abs": "https://arxiv.org/abs/2601.08819", "authors": ["Roshni Kaushik", "Reid Simmons"], "title": "Older Adults' Preferences for Feedback Cadence from an Exercise Coach Robot", "comment": "Nonarchival submission to RO-MAN 2024 - poster session", "summary": "People can respond to feedback and guidance in different ways, and it is important for robots to personalize their interactions and utilize verbal and nonverbal communication cues. We aim to understand how older adults respond to different cadences of verbal and nonverbal feedback of a robot exercise coach. We conducted an online study of older adults, where participants evaluated videos of the robot giving feedback at different cadences for each modality. The results indicate that changing the cadence of one modality affects the perception of both it and the other modality. We can use the results from this study to better design the frequency of the robot coach's feedback during an exercise session with this population.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u8001\u5e74\u4eba\u5bf9\u673a\u5668\u4eba\u8fd0\u52a8\u6559\u7ec3\u4e0d\u540c\u8282\u594f\u53cd\u9988\u7684\u611f\u77e5\uff0c\u53d1\u73b0\u6539\u53d8\u4e00\u79cd\u6a21\u6001\u7684\u8282\u594f\u4f1a\u5f71\u54cd\u5bf9\u8be5\u6a21\u6001\u53ca\u53e6\u4e00\u79cd\u6a21\u6001\u7684\u611f\u77e5", "motivation": "\u4eba\u4eec\u4ee5\u4e0d\u540c\u65b9\u5f0f\u56de\u5e94\u53cd\u9988\u548c\u6307\u5bfc\uff0c\u673a\u5668\u4eba\u9700\u8981\u4e2a\u6027\u5316\u4e92\u52a8\u5e76\u5229\u7528\u8a00\u8bed\u548c\u975e\u8a00\u8bed\u6c9f\u901a\u7ebf\u7d22\u3002\u7279\u522b\u5173\u6ce8\u8001\u5e74\u4eba\u5bf9\u673a\u5668\u4eba\u8fd0\u52a8\u6559\u7ec3\u53cd\u9988\u8282\u594f\u7684\u54cd\u5e94\u65b9\u5f0f", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\uff0c\u8ba9\u8001\u5e74\u4eba\u89c2\u770b\u673a\u5668\u4eba\u4ee5\u4e0d\u540c\u8282\u594f\u63d0\u4f9b\u8a00\u8bed\u548c\u975e\u8a00\u8bed\u53cd\u9988\u7684\u89c6\u9891\uff0c\u8bc4\u4f30\u4ed6\u4eec\u5bf9\u4e0d\u540c\u8282\u594f\u53cd\u9988\u7684\u611f\u77e5", "result": "\u6539\u53d8\u4e00\u79cd\u6a21\u6001\u7684\u8282\u594f\u4f1a\u5f71\u54cd\u5bf9\u8be5\u6a21\u6001\u53ca\u53e6\u4e00\u79cd\u6a21\u6001\u7684\u611f\u77e5\uff0c\u8fd9\u8868\u660e\u6a21\u6001\u95f4\u5b58\u5728\u76f8\u4e92\u5f71\u54cd", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u6559\u7ec3\u5728\u8001\u5e74\u4eba\u8fd0\u52a8\u8bad\u7ec3\u4e2d\u7684\u53cd\u9988\u9891\u7387\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u4e92\u52a8"}}
