<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 31]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Beyond the Vehicle: Cooperative Localization by Fusing Point Clouds for GPS-Challenged Urban Scenarios](https://arxiv.org/abs/2602.03908)
*Kuo-Yi Chao,Ralph Rasshofer,Alois Christian Knoll*

Main category: cs.RO

TL;DR: 提出了一种融合V2V和V2I数据的协同多传感器多模态定位方法，通过点云配准SLAM算法提升城市环境中GPS不可靠情况下的车辆定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GPS信号不可靠，需要更准确的车辆定位解决方案来应对复杂的城市场景。

Method: 采用协同多传感器多模态定位方法，融合V2V和V2I数据，结合点云配准SLAM算法，处理来自车载LiDAR、立体相机和交叉路口部署传感器的点云数据。

Result: 通过利用基础设施共享数据，该方法在复杂、GPS噪声严重的城市场景中显著提高了定位精度和鲁棒性。

Conclusion: 协同多传感器多模态定位方法能够有效解决城市环境中GPS不可靠的车辆定位问题，通过数据融合和基础设施共享提升系统性能。

Abstract: Accurate vehicle localization is a critical challenge in urban environments where GPS signals are often unreliable. This paper presents a cooperative multi-sensor and multi-modal localization approach to address this issue by fusing data from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Our approach integrates cooperative data with a point cloud registration-based simultaneous localization and mapping (SLAM) algorithm. The system processes point clouds generated from diverse sensor modalities, including vehicle-mounted LiDAR and stereo cameras, as well as sensors deployed at intersections. By leveraging shared data from infrastructure, our method significantly improves localization accuracy and robustness in complex, GPS-noisy urban scenarios.

</details>


### [2] [How Users Understand Robot Foundation Model Performance through Task Success Rates and Beyond](https://arxiv.org/abs/2602.03920)
*Isaac Sheidlower,Jindan Huang,James Staley,Bingyu Wu,Qicong Chen,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 研究非机器人专家如何理解机器人基础模型评估中的任务成功率信息，发现用户不仅正确使用成功率数据，还高度关注评估中通常不报告的其他信息类型。


<details>
  <summary>Details</summary>
Motivation: 随着机器人基础模型的发展，用户可能会要求机器人执行模型未训练或评估过的任务。在这些情况下，用户理解尝试新任务的风险至关重要。了解非机器人专家如何解读RFM评估中的性能信息，特别是任务成功率这一主要指标，对于确保用户正确理解机器人能力非常重要。

Method: 进行了一项研究，让用户查看来自多个已发表RFM研究项目的真实评估数据，包括任务成功率、失败案例描述和视频。研究非专家用户如何解读这些信息。

Result: 研究发现非专家用户不仅以与专家预期一致的方式使用任务成功率信息，而且高度重视其他类型的信息，如RFM评估中通常不报告的失败案例。用户希望同时获得RFM先前评估的真实数据和机器人对新任务表现的估计。

Conclusion: 为了帮助用户更好地理解机器人基础模型的能力和风险，评估报告应该包含更多类型的信息，特别是失败案例，而不仅仅是任务成功率。同时，用户需要机器人提供对新任务表现的估计，以做出更明智的决策。

Abstract: Robot Foundation Models (RFMs) represent a promising approach to developing general-purpose home robots. Given the broad capabilities of RFMs, users will inevitably ask an RFM-based robot to perform tasks that the RFM was not trained or evaluated on. In these cases, it is crucial that users understand the risks associated with attempting novel tasks due to the relatively high cost of failure. Furthermore, an informed user who understands an RFM's capabilities will know what situations and tasks the robot can handle. In this paper, we study how non-roboticists interpret performance information from RFM evaluations. These evaluations typically report task success rate (TSR) as the primary performance metric. While TSR is intuitive to experts, it is necessary to validate whether novices also use this information as intended. Toward this end, we conducted a study in which users saw real evaluation data, including TSR, failure case descriptions, and videos from multiple published RFM research projects. The results highlight that non-experts not only use TSR in a manner consistent with expert expectations but also highly value other information types, such as failure cases that are not often reported in RFM evaluations. Furthermore, we find that users want access to both real data from previous evaluations of the RFM and estimates from the robot about how well it will do on a novel task.

</details>


### [3] [VLS: Steering Pretrained Robot Policies via Vision-Language Models](https://arxiv.org/abs/2602.03973)
*Shuo Liu,Ishneet Sukhvinder Singh,Yiqing Xu,Jiafei Duan,Ranjay Krishna*

Main category: cs.RO

TL;DR: VLS提出无需训练即可在推理时调整预训练扩散/流匹配机器人策略的框架，通过视觉语言模型合成轨迹可微奖励函数来引导去噪过程，适应测试时的空间和任务变化。


<details>
  <summary>Details</summary>
Motivation: 预训练的扩散或流匹配策略在遇到障碍物、支撑面偏移或轻度杂乱等训练-测试分布偏移时容易失败，这些失败并非缺少运动技能，而是模仿学习在分布偏移下的局限性。重新训练或微调成本高且概念上不匹配，因为所需行为已存在但无法在测试时有选择地适应。

Method: VLS将适应视为推理时控制问题，在不修改策略参数的情况下，通过视觉语言模型合成轨迹可微奖励函数，引导预训练扩散或流匹配策略的采样过程，使其响应分布外观察-语言输入，满足测试时的空间和任务要求。

Result: 在仿真和真实世界评估中，VLS始终优于先前的引导方法，在CALVIN上实现了31%的改进，在LIBERO-PRO上获得了13%的提升。在Franka机器人上的真实世界部署进一步展示了在测试时空间和语义偏移下的鲁棒推理时适应能力。

Conclusion: VLS提供了一个无需训练即可在推理时适应冻结生成机器人策略的框架，通过视觉语言模型引导采样过程，有效解决了模仿学习在训练-测试分布偏移下的局限性，实现了对测试时空间和任务变化的鲁棒适应。

Abstract: Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/

</details>


### [4] [Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement](https://arxiv.org/abs/2602.03983)
*Weikang Qiu,Tinglin Huang,Aosong Feng,Rex Ying*

Main category: cs.RO

TL;DR: SD-VLA框架通过将视觉输入分解为多级静态和动态token，显著减少上下文长度并重用KV缓存，实现了高效的长期上下文建模和推理加速。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型面临两个主要挑战：有限的长期上下文建模能力和因二次注意力复杂度和大量参数导致的低效推理。研究发现轨迹中的大部分视觉信息在时间步之间保持静态（如背景），这为优化提供了机会。

Method: 提出SD-VLA框架，将视觉输入分解为多级静态和动态token，保留静态token的单一副本以减少上下文长度，并通过轻量级重缓存门仅在必要时更新静态token的KV缓存，实现高效的多帧集成和推理。

Result: 在新基准测试中比基线模型成功率提升39.8%，在SimplerEnv基准测试中提升3.9%。推理速度比基础VLA模型提升2.26倍。

Conclusion: SD-VLA通过有效分离静态和动态视觉信息，显著提升了VLA模型的长期上下文建模能力和推理效率，为实际部署提供了更实用的解决方案。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.

</details>


### [5] [FDA Flocking: Future Direction-Aware Flocking via Velocity Prediction](https://arxiv.org/abs/2602.04012)
*Hossein B. Jond,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于生物启发的未来方向感知（FDA）集群模型，通过结合反应式对齐和基于邻居未来速度预测的项，增强了集群的协调性、鲁棒性和抗延迟能力。


<details>
  <summary>Details</summary>
Motivation: 受鸟类姿态和翼拍信号以及多旋翼飞行器在方向改变前的姿态倾斜等前瞻性线索启发，现有集群模型大多为反应式，忽略了这些能增强协调性的预测性信号。

Method: 提出FDA集群框架，智能体将反应式对齐与基于邻居短期未来速度估计的预测项相结合，通过可调混合参数在反应式和前瞻性行为之间插值。

Result: 仿真结果表明，FDA相比纯反应式模型实现了更快更高的对齐度、增强的集群平移位移，以及对延迟和噪声的更好鲁棒性。

Conclusion: FDA框架通过生物启发的预测性增强提升了集群性能，未来工作将研究自适应混合策略、加权预测方案以及在多旋翼无人机集群上的实验验证。

Abstract: Understanding self-organization in natural collectives such as bird flocks inspires swarm robotics, yet most flocking models remain reactive, overlooking anticipatory cues that enhance coordination. Motivated by avian postural and wingbeat signals, as well as multirotor attitude tilts that precede directional changes, this work introduces a principled, bio-inspired anticipatory augmentation of reactive flocking termed Future Direction-Aware (FDA) flocking. In the proposed framework, agents blend reactive alignment with a predictive term based on short-term estimates of neighbors' future velocities, regulated by a tunable blending parameter that interpolates between reactive and anticipatory behaviors. This predictive structure enhances velocity consensus and cohesion-separation balance while mitigating the adverse effects of sensing and communication delays and measurement noise that destabilize reactive baselines. Simulation results demonstrate that FDA achieves faster and higher alignment, enhanced translational displacement of the flock, and improved robustness to delays and noise compared to a purely reactive model. Future work will investigate adaptive blending strategies, weighted prediction schemes, and experimental validation on multirotor drone swarms.

</details>


### [6] [An Anatomy-specific Guidewire Shaping Robot for Improved Vascular Navigation](https://arxiv.org/abs/2602.04050)
*Aabha Tamhankar,Jay Patil,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 开发了一种用于神经血管介入手术的导丝成形机器人系统，能够根据导航需求自动成形导丝尖端，提高手术标准化程度


<details>
  <summary>Details</summary>
Motivation: 神经血管介入手术中，导丝成形依赖外科医生的经验和手工操作，特别是在复杂解剖结构中，手术效果高度依赖医生技术水平。需要实现标准化、自动化的导丝成形技术。

Method: 开发了桌面导丝成形机器人系统，建立模型将所需导丝形状映射到机器人动作，通过实验数据进行校准。系统能够产生临床常见的尖端几何形状（C形、S形、角度形、钩形）。

Result: 模型预测形状与实验结果相比，均方根误差为0.56mm。展示了3D尖端成形能力，以及从岩段颈内动脉到后交通动脉的复杂腔内导航能力。

Conclusion: 该机器人系统能够实现标准化、自主的导丝成形，有望减少对医生经验的依赖，提高神经血管介入手术的可靠性和可重复性。

Abstract: Neuroendovascular access often relies on passive microwires that are hand-shaped at the back table and then used to track a microcatheter to the target. Neuroendovascular surgeons determine the shape of the wire by examining the patient pre-operative images and using their experience to identify anatomy specific shapes of the wire that would facilitate reaching the target. This procedure is particularly complex in convoluted anatomical structures and is heavily dependent on the level of expertise of the surgeon. Towards enabling standardized autonomous shaping, we present a bench-top guidewire shaping robot capable of producing navigation-specific desired wire configurations. We present a model that can map the desired wire shape into robot actions, calibrated using experimental data. We show that the robot can produce clinically common tip geometries (C, S, Angled, Hook) and validate them with respect to the model-predicted shapes in 2D. Our model predicts the shape with a Root Mean Square (RMS) error of 0.56mm across all shapes when compared to the experimental results. We also demonstrate 3D tip shaping capabilities and the ability to traverse complex endoluminal navigation from the petrous Internal Carotid Artery (ICA) to the Posterior Communicating Artery (PComm).

</details>


### [7] [Control and State Estimation of Vehicle-Mounted Aerial Systems in GPS-Denied, Non-Inertial Environments](https://arxiv.org/abs/2602.04057)
*Riming Xu,Obadah Wali,Yasmine Marani,Eric Feron*

Main category: cs.RO

TL;DR: 提出一种在GNSS拒止、非惯性环境下的四旋翼鲁棒控制与估计框架，使用EKF-UI处理平台运动，无需IMU和GNSS，仅依赖外部位置测量


<details>
  <summary>Details</summary>
Motivation: 在GNSS拒止、非惯性环境中，传统IMU无法区分四旋翼自身加速度和平台运动导致的加速度，导致估计漂移和控制性能下降，需要新的解决方案

Method: 采用带未知输入的扩展卡尔曼滤波器(EKF-UI)处理平台运动，结合级联PID控制器实现3D跟踪，仅依赖外部位置测量，无需IMU和GNSS

Result: 在移动推车测试平台上验证了方法在X轴和Y轴平移扰动下的有效性，相比标准EKF显著提高了稳定性和轨迹跟踪性能

Conclusion: 该方法能够在非惯性环境下实现稳定控制，无需惯性反馈，适用于卡车、电梯等移动平台的实际部署

Abstract: We present a robust control and estimation framework for quadrotors operating in Global Navigation Satellite System(GNSS)-denied, non-inertial environments where inertial sensors such as Inertial Measurement Units (IMUs) become unreliable due to platform-induced accelerations. In such settings, conventional estimators fail to distinguish whether the measured accelerations arise from the quadrotor itself or from the non-inertial platform, leading to drift and control degradation. Unlike conventional approaches that depend heavily on IMU and GNSS, our method relies exclusively on external position measurements combined with a Extended Kalman Filter with Unknown Inputs (EKF-UI) to account for platform motion. The estimator is paired with a cascaded PID controller for full 3D tracking. To isolate estimator performance from localization errors, all tests are conducted using high-precision motion capture systems. Experimental results in a moving-cart testbed validate our approach under both translational in X-axis and Y-axis dissonance. Compared to standard EKF, the proposed method significantly improves stability and trajectory tracking without requiring inertial feedback, enabling practical deployment on moving platforms such as trucks or elevators.

</details>


### [8] [Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study](https://arxiv.org/abs/2602.04076)
*Daniyal Maroufi,Yash Kulkarni,Justin E. Bird,Jeffrey H. Siewerdsen,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 开发了自主超声骶骨截骨机器人系统，相比手动操作显著提高了轨迹精度和深度控制精度


<details>
  <summary>Details</summary>
Motivation: 手动骶骨截骨存在轨迹精度差和深度控制不准确的问题，需要开发更安全、更精确的机器人系统来克服这些限制

Method: 集成超声截骨刀与七自由度机械臂，通过光学跟踪系统引导，在Sawbones模型上进行手动与机器人操作的定量比较

Result: 机器人系统轨迹精度达0.11 mm RMSE，比手动操作(1.10 mm)提高一个数量级；深度控制方面，手动操作过度穿透16.0 mm(目标8.0 mm)，而机器人系统精确达到8.1 mm

Conclusion: 机器人系统能有效克服手动截骨的关键限制，为更安全、更精确的骶骨切除术奠定了基础

Abstract: In this paper, we introduce an autonomous Ultrasonic Sacral Osteotomy (USO) robotic system that integrates an ultrasonic osteotome with a seven-degree-of-freedom (DoF) robotic manipulator guided by an optical tracking system. To assess multi-directional control along both the surface trajectory and cutting depth of this system, we conducted quantitative comparisons between manual USO (MUSO) and robotic USO (RUSO) in Sawbones phantoms under identical osteotomy conditions. The RUSO system achieved sub-millimeter trajectory accuracy (0.11 mm RMSE), an order of magnitude improvement over MUSO (1.10 mm RMSE). Moreover, MUSO trials showed substantial over-penetration (16.0 mm achieved vs. 8.0 mm target), whereas the RUSO system maintained precise depth control (8.1 mm). These results demonstrate that robotic procedures can effectively overcome the critical limitations of manual osteotomy, establishing a foundation for safer and more precise sacral resections.

</details>


### [9] [KGLAMP: Knowledge Graph-guided Language model for Adaptive Multi-robot Planning and Replanning](https://arxiv.org/abs/2602.04129)
*Chak Lam Shek,Faizan M. Tariq,Sangjae Bae,David Isele,Piyush Gupta*

Main category: cs.RO

TL;DR: KGLAMP是一个基于知识图谱引导的LLM规划框架，用于异构多机器人团队，通过结构化知识图谱编码对象关系、空间可达性和机器人能力，指导LLM生成准确的PDDL问题规范，在动态环境中实现自适应规划。


<details>
  <summary>Details</summary>
Motivation: 异构多机器人系统在长期任务中需要协调，但现有规划方法难以构建准确的符号表示并在动态环境中保持计划一致性。经典PDDL规划器需要手动构建符号模型，而基于LLM的规划器往往忽略智能体异质性和环境不确定性。

Method: 提出KGLAMP框架，维护一个结构化知识图谱，编码对象关系、空间可达性和机器人能力。该知识图谱作为持久化、动态更新的记忆，整合新观察并在检测到不一致时触发重新规划，指导LLM生成准确的PDDL问题规范。

Result: 在MAT-THOR基准测试中，KGLAMP相比纯LLM和基于PDDL的变体，性能提升至少25.5%。

Conclusion: KGLAMP通过知识图谱引导的LLM规划，有效解决了异构多机器人系统在动态环境中的规划问题，提高了规划准确性和适应性。

Abstract: Heterogeneous multi-robot systems are increasingly deployed in long-horizon missions that require coordination among robots with diverse capabilities. However, existing planning approaches struggle to construct accurate symbolic representations and maintain plan consistency in dynamic environments. Classical PDDL planners require manually crafted symbolic models, while LLM-based planners often ignore agent heterogeneity and environmental uncertainty. We introduce KGLAMP, a knowledge-graph-guided LLM planning framework for heterogeneous multi-robot teams. The framework maintains a structured knowledge graph encoding object relations, spatial reachability, and robot capabilities, which guides the LLM in generating accurate PDDL problem specifications. The knowledge graph serves as a persistent, dynamically updated memory that incorporates new observations and triggers replanning upon detecting inconsistencies, enabling symbolic plans to adapt to evolving world states. Experiments on the MAT-THOR benchmark show that KGLAMP improves performance by at least 25.5% over both LLM-only and PDDL-based variants.

</details>


### [10] [Shaping Expressiveness in Robotics: The Role of Design Tools in Crafting Embodied Robot Movements](https://arxiv.org/abs/2602.04137)
*Elisabetta Zibetti,Alexandra Mercader,Hélène Duval,Florent Levillain,Audrey Rochette,David St-Onge*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人手臂表达性运动的交互式设计教学法，通过结合舞蹈分析框架和定制工具，帮助工程师创造更具表现力和吸引力的机器人运动。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地进入人类共享空间，其运动需要超越基本功能，融入表达性特质以增强参与度和沟通效果。当前工程师在创造表达性机器人运动方面面临挑战。

Method: 采用跨学科方法设计交互式工作坊，整合舞蹈分析框架，开发定制手动遥控器和专用动画软件，支持实时操作、可视化、详细运动序列和精确参数控制。

Result: 定性分析表明，提出的"工具箱"有效弥合了人类意图与机器人表达性之间的差距，产生了更直观、更具吸引力的表达性机器人手臂运动。

Conclusion: 该运动中心设计教学法成功支持工程师创造表达性机器人运动，通过迭代方法和跨学科框架实现了更自然的人机交互表达。

Abstract: As robots increasingly become part of shared human spaces, their movements must transcend basic functionality by incorporating expressive qualities to enhance engagement and communication. This paper introduces a movement-centered design pedagogy designed to support engineers in creating expressive robotic arm movements. Through a hands-on interactive workshop informed by interdisciplinary methodologies, participants explored various creative possibilities, generating valuable insights into expressive motion design. The iterative approach proposed integrates analytical frameworks from dance, enabling designers to examine motion through dynamic and embodied dimensions. A custom manual remote controller facilitates interactive, real-time manipulation of the robotic arm, while dedicated animation software supports visualization, detailed motion sequencing, and precise parameter control. Qualitative analysis of this interactive design process reveals that the proposed "toolbox" effectively bridges the gap between human intent and robotic expressiveness resulting in more intuitive and engaging expressive robotic arm movements.

</details>


### [11] [MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments](https://arxiv.org/abs/2602.04152)
*Yirum Kim,Jaewoo Kim,Ue-Hwan Kim*

Main category: cs.RO

TL;DR: 提出了首个多智能体3D场景图生成框架MA3DSG，通过无训练图对齐算法整合多个智能体的局部查询图，并建立了支持多样化配置的基准测试MA3DSG-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图生成方法严重依赖单智能体假设和小规模环境，难以扩展到真实世界场景。需要解决可扩展性问题。

Method: 开发了无需训练参数的多智能体框架MA3DSG，包含图对齐算法来高效合并多个智能体的局部查询图，形成统一的全局场景图。

Result: 建立了MA3DSG-Bench基准测试，支持多样化智能体配置、领域大小和环境条件，为可扩展的多智能体3DSGG研究奠定了基础。

Conclusion: 该工作首次解决了3D场景图生成的可扩展性挑战，通过多智能体协作框架和无参数图对齐算法，为大规模真实世界应用提供了可行方案。

Abstract: Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle this scalability challenge using multiple agents. We develop a training-free graph alignment algorithm that efficiently merges partial query graphs from individual agents into a unified global scene graph. Leveraging extensive analysis and empirical insights, our approach enables conventional single-agent systems to operate collaboratively without requiring any learnable parameters. To rigorously evaluate 3DSGG performance, we propose MA3DSG-Bench-a benchmark that supports diverse agent configurations, domain sizes, and environmental conditions-providing a more general and extensible evaluation framework. This work lays a solid foundation for scalable, multi-agent 3DSGG research.

</details>


### [12] [A Modern System Recipe for Situated Embodied Human-Robot Conversation with Real-Time Multimodal LLMs and Tool-Calling](https://arxiv.org/abs/2602.04157)
*Dong Won Lee,Sarah Gillet,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.RO

TL;DR: 提出了一个简单的最小系统配方，将实时多模态语言模型与注意力工具接口配对，用于需要频繁注意力转移的居家场景中的具身对话


<details>
  <summary>Details</summary>
Motivation: 具身对话需要机器人在严格延迟约束下，将实时对话与主动感知交织：决定看什么、何时看以及说什么。现有系统在这方面存在挑战

Method: 提出一个简单的最小系统配方，将实时多模态语言模型与一小套用于注意力和主动感知的工具接口配对。研究了六个需要频繁注意力转移和增加感知范围的居家场景

Result: 评估了四个系统变体，对比人类标注评估回合级工具决策正确性，并收集交互质量的主观评分。结果表明实时多模态大语言模型和用于主动感知的工具使用是实用具身对话的有前景方向

Conclusion: 实时多模态大语言模型结合主动感知工具使用是实现实用具身对话的有效途径，特别是在需要频繁注意力转移的居家场景中

Abstract: Situated embodied conversation requires robots to interleave real-time dialogue with active perception: deciding what to look at, when to look, and what to say under tight latency constraints. We present a simple, minimal system recipe that pairs a real-time multimodal language model with a small set of tool interfaces for attention and active perception. We study six home-style scenarios that require frequent attention shifts and increasing perceptual scope. Across four system variants, we evaluate turn-level tool-decision correctness against human annotations and collect subjective ratings of interaction quality. Results indicate that real-time multimodal large language models and tool use for active perception is a promising direction for practical situated embodied conversation.

</details>


### [13] [SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models](https://arxiv.org/abs/2602.04208)
*Hyeonbeom Choi,Daechul Ahn,Youhan Lee,Taewook Kang,Seongwon Cho,Jonghyun Choi*

Main category: cs.RO

TL;DR: SCALE是一种无需额外训练、验证器或多次前向传播的推理策略，通过自我不确定性联合调节视觉感知和动作，在单次前向传播中提高VLA模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的测试时扩展方法需要额外训练、验证器和多次前向传播，不实用且仅在动作解码阶段干预，无法处理感知模糊性，而感知重新考虑与动作决策同等重要。

Method: SCALE基于主动推理理论中的不确定性驱动探索思想，利用"自我不确定性"联合调节视觉感知和动作，在高不确定性时扩大感知和动作探索，在置信度高时专注于利用，无需额外训练、验证器，仅需单次前向传播。

Result: 在模拟和真实世界基准测试中，SCALE提升了最先进的VLA模型性能，优于现有测试时扩展方法，同时保持单次前向传播的效率。

Conclusion: SCALE提供了一种简单高效的推理策略，通过联合调节感知和动作来增强VLA模型的鲁棒性，解决了现有测试时扩展方法的局限性，为实际部署提供了实用解决方案。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.

</details>


### [14] [ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator](https://arxiv.org/abs/2602.04214)
*Zhihai Bi,Yushan Zhang,Kai Chen,Guoyang Zhao,Yulin Li,Jun Ma*

Main category: cs.RO

TL;DR: ALORE是一个用于腿式机械臂的自主大型物体重排系统，能够高效重排各种大型物体，通过分层强化学习、统一交互配置表示和任务运动规划框架实现多物体环境中的稳定操作。


<details>
  <summary>Details</summary>
Motivation: 让机器人具备重排大型重型物体（如家具）的能力可以大大减轻人类工作负担，但由于需要与多样物体交互、在复杂环境中高效重排多个物体并确保无碰撞的移动操作，这一任务极具挑战性。

Method: 系统包含三个主要特征：(1) 用于多物体环境学习的分层强化学习训练管道，高层物体速度控制器在低层全身控制器之上训练；(2) 统一交互配置表示和物体速度估计器两个关键模块，使单个策略能准确调节多样物体的平面速度；(3) 任务与运动规划框架，联合优化物体访问顺序和物体到目标的分配。

Result: 与强基线相比，在策略泛化、物体速度跟踪精度和多物体重排效率方面表现一致优越。系统成功完成8个连续循环重排32把椅子近40分钟无失败，并在约40米路线上执行长距离自主重排。

Conclusion: ALORE系统通过分层强化学习、统一交互表示和任务运动规划，实现了腿式机械臂对多样大型物体的高效自主重排，在仿真和真实实验中验证了系统的鲁棒性和有效性。

Abstract: Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at https://zhihaibi.github.io/Alore/.

</details>


### [15] [OAT: Ordered Action Tokenization](https://arxiv.org/abs/2602.04215)
*Chaoqi Liu,Xiaoshen Han,Jiawei Gao,Yue Zhao,Haonan Chen,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了Ordered Action Tokenization (OAT)方法，为连续机器人动作提供高效的自回归建模，通过满足压缩性、完全可解码性和因果有序性三个需求，显著提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 自回归策略为可扩展的机器人学习提供了有前景的基础，但将其应用于连续机器人动作需要有效的动作标记化方案。现有方法要么产生过长的标记序列，要么缺乏结构限制与下一个标记预测的兼容性。

Method: 提出了Ordered Action Tokenization (OAT)，使用带有寄存器的transformer、有限标量量化和有序诱导训练机制，将动作块离散化为有序标记序列，满足高压缩性、完全可解码性和从左到右因果有序性三个需求。

Result: 在超过20个任务、涵盖四个仿真基准和真实世界设置中，配备OAT的自回归策略始终优于先前的标记化方案和基于扩散的基线方法，同时在推理时提供更大的灵活性。

Conclusion: OAT为连续机器人动作提供了有效的自回归建模框架，通过满足三个关键需求的标记化方案，实现了性能提升和推理灵活性，为可扩展的机器人学习提供了有力工具。

Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.

</details>


### [16] [Reshaping Action Error Distributions for Reliable Vision-Language-Action Models](https://arxiv.org/abs/2602.04228)
*Shuanghao Bai,Dakai Wang,Cheng Chi,Wanqi Zhou,Jing Lyu,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Lei Xing,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 该论文提出在连续动作的视觉-语言-动作模型中引入最小误差熵目标，替代传统的均方误差回归，以提高策略的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要使用均方误差进行连续动作回归，这种点对点约束限制了模型的性能。需要探索更有效的训练目标来改善动作误差分布。

Method: 基于信息论原理，将最小误差熵引入VLA架构，提出轨迹级MEE目标及其两个加权变体，并与MSE结合用于连续动作VLA训练。

Result: 在标准、少样本和噪声设置下，多个VLA架构在LIBERO、SimplerEnv仿真基准和真实机器人操作任务中均表现出成功率和鲁棒性的持续提升。

Conclusion: MEE监督在数据不平衡情况下仍能保持性能提升，且训练成本增加可忽略不计，不影响推理效率。理论分析解释了MEE的有效性并界定了其适用范围。

Abstract: In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: https://cognition2actionlab.github.io/VLA-TMEE.github.io/

</details>


### [17] [GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning](https://arxiv.org/abs/2602.04231)
*Rui Tang,Guankun Wang,Long Bai,Huxin Gao,Jiewen Lai,Chi Kit Ng,Jiazheng Wang,Fan Zhang,Hongliang Ren*

Main category: cs.RO

TL;DR: GeoLanG是一个基于CLIP架构的端到端多任务框架，通过深度引导几何模块和自适应密集通道集成，在复杂遮挡和低纹理场景中实现鲁棒的语言引导抓取。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导抓取方法通常采用分离对象感知和抓取的多阶段流程，导致跨模态融合有限、计算冗余，在杂乱、遮挡或低纹理场景中泛化能力差。

Method: 1. 基于CLIP架构构建端到端多任务框架，统一视觉和语言输入到共享表示空间；2. 深度引导几何模块将深度信息转换为显式几何先验并注入注意力机制；3. 自适应密集通道集成平衡多层特征的贡献，生成更具判别性和泛化能力的视觉表示。

Result: 在OCID-VLG数据集、仿真和真实硬件上的大量实验表明，GeoLanG能够在复杂杂乱环境中实现精确鲁棒的语言引导抓取。

Conclusion: GeoLanG通过深度引导几何先验和自适应特征集成，提高了在遮挡和低纹理条件下的目标识别能力，为现实世界人本环境中的可靠多模态机器人操作铺平了道路。

Abstract: Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings.

</details>


### [18] [Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation](https://arxiv.org/abs/2602.04243)
*Pengfei Yi,Yifan Han,Junyan Li,Litao Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: MAE-Select是一个用于单摄像头机器人系统的主动视角选择框架，利用预训练的多视角掩码自编码器表示，动态选择信息最丰富的视角，无需标注的视角数据。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法通常依赖固定摄像头设置，摄像头被手动放置在静态位置，这严重限制了系统的适应性和覆盖范围。受人类主动感知的启发，人类会动态调整视角以捕捉最相关、噪声最少的信息。

Method: 提出MAE-Select框架，利用预训练的多视角掩码自编码器表示，在每个时间块动态选择下一个信息最丰富的视角，无需标注的视角数据。

Result: 大量实验表明，MAE-Select提升了单摄像头系统的能力，在某些情况下甚至超越了多摄像头设置。

Conclusion: MAE-Select通过主动视角选择，为机器人模仿学习提供了更灵活、适应性更强的视觉感知方案，减少了对外部多摄像头设置的依赖。

Abstract: Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io.

</details>


### [19] [Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions](https://arxiv.org/abs/2602.04251)
*Li Wang,Ruixuan Gong,Yumo Han,Lei Yang,Lu Yang,Ying Li,Bin Xu,Huaping Liu,Rong Fu*

Main category: cs.RO

TL;DR: 这篇综述论文全面回顾了3D高斯泼溅与SLAM系统集成的关键技术方法，分析了在渲染质量、跟踪精度、重建速度和内存消耗四个关键维度的性能优化，并探讨了在复杂环境下的鲁棒性增强方法。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统存在渲染质量粗糙、场景细节恢复不足、动态环境鲁棒性差等局限性。3D高斯泼溅以其高效的显式表示和高质量渲染能力，为SLAM提供了新的重建范式，需要系统性地总结这一新兴领域的技术进展。

Method: 采用综述研究方法，全面回顾3DGS与SLAM集成的关键技术方法，从四个关键维度（渲染质量、跟踪精度、重建速度、内存消耗）分析代表性方法的性能优化，深入探讨其设计原理和突破点，并研究在运动模糊和动态环境等复杂场景下的鲁棒性增强方法。

Result: 系统梳理了3DGS-SLAM领域的关键技术进展，分析了不同方法在四个关键维度的性能表现，总结了在复杂环境下的鲁棒性增强策略，为研究人员提供了全面的技术参考。

Conclusion: 3DGS-SLAM代表了下一代SLAM系统的发展方向，具有高保真度、高效性和鲁棒性的特点。未来需要进一步解决该领域面临的挑战，推动高保真、高效、鲁棒的下一代SLAM系统发展。

Abstract: Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.

</details>


### [20] [AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models](https://arxiv.org/abs/2602.04256)
*Yuxuan Han,Kunyuan Wu,Qianyi Shao,Renxiang Xiao,Zilu Wang,Cansen Jiang,Yi Xiao,Liang Hu,Yunjiang Lou*

Main category: cs.RO

TL;DR: AppleVLM是一种用于端到端自动驾驶的先进视觉语言模型，通过改进的视觉编码器和规划策略编码器，结合层次化思维链微调，在CARLA基准测试中实现了最先进的驾驶性能，并在真实户外环境中成功部署。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的端到端自动驾驶方法存在车道感知不理想、语言理解偏差和处理极端情况困难等问题。为了解决这些挑战，需要开发更鲁棒的感知和规划增强模型。

Method: AppleVLM引入了新颖的视觉编码器和规划策略编码器。视觉编码器使用可变形transformer机制融合多视角图像在多个时间步的空间-时间信息；规划编码器编码显式的鸟瞰图空间信息；最后通过层次化思维链微调的VLM解码器集成视觉、语言和规划特征，输出鲁棒的驾驶路径点。

Result: 在CARLA基准测试的两个数据集上进行闭环实验，实现了最先进的驾驶性能。成功在AGV平台上部署，并在复杂的户外环境中展示了真实的端到端自动驾驶能力。

Conclusion: AppleVLM通过增强的感知和规划能力，有效解决了现有VLM方法在车道感知、语言偏差和极端情况处理方面的局限性，为鲁棒的端到端自动驾驶提供了有前景的解决方案。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird's-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments.

</details>


### [21] [Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition](https://arxiv.org/abs/2602.04401)
*Dhyey Manish Rajani,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 提出一种自动选择视觉位置识别系统阈值的方法，通过量化归一化技术转移相似度分数分布，在满足用户定义精度要求下最大化召回率


<details>
  <summary>Details</summary>
Motivation: 当前视觉位置识别系统的阈值通常需要离线手动调优，且针对特定环境固定设置，导致在环境变化时性能下降。需要一种能自动适应新环境并满足精度要求的阈值选择方法。

Method: 使用带有已知对应关系的小型校准遍历数据，通过量化归一化技术将相似度分数分布进行转移，从而将阈值从校准环境转移到部署环境。该方法确保阈值在校准大小和查询子集之间保持稳定，对采样变异性具有鲁棒性。

Result: 在多个最先进的VPR技术和数据集上的实验表明，该方法始终优于现有技术，在高精度操作机制下召回率提高了25%。方法消除了手动调优，能够适应新环境并在不同操作条件下泛化。

Conclusion: 提出的方法能够根据用户定义的精度要求自动选择VPR系统的操作点以最大化召回率，通过量化转移技术确保阈值稳定性，显著提升了视觉位置识别系统在环境变化下的性能表现。

Abstract: Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall. Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change. We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall. The method uses a small calibration traversal with known correspondences and transfers thresholds to deployment via quantile normalisation of similarity score distributions. This quantile transfer ensures that thresholds remain stable across calibration sizes and query subsets, making the method robust to sampling variability. Experiments with multiple state-of-the-art VPR techniques and datasets show that the proposed approach consistently outperforms the state-of-the-art, delivering up to 25% higher recall in high-precision operating regimes. The method eliminates manual tuning by adapting to new environments and generalising across operating conditions. Our code will be released upon acceptance.

</details>


### [22] [HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation](https://arxiv.org/abs/2602.04412)
*Puyue Wang,Jiawei Hu,Yan Gao,Junyan Wang,Yu Zhang,Gillian Dobbie,Tao Gu,Wafa Johal,Ting Dang,Hong Jia*

Main category: cs.RO

TL;DR: HoRD是一个两阶段学习框架，通过历史条件强化学习训练教师策略，再通过在线蒸馏将鲁棒控制能力转移到基于Transformer的学生策略中，实现人形机器人在未见领域中的零样本适应。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动力学、任务规范或环境设置发生微小变化时性能会显著下降，需要一种能够适应领域偏移的鲁棒控制方法。

Method: 采用两阶段学习框架：1）通过历史条件强化学习训练教师策略，使其能够从最近的状态-动作轨迹推断潜在动力学上下文并在线适应随机化动力学；2）通过在线蒸馏将教师策略的鲁棒控制能力转移到基于Transformer的学生策略中，该学生策略基于稀疏的根相对3D关节关键点轨迹。

Result: HoRD在未见领域和外部扰动下表现出色，超越了现有基线方法，实现了零样本适应而无需针对每个领域重新训练。

Conclusion: 通过结合历史条件适应和在线蒸馏，HoRD能够使单个策略在未见领域中实现零样本适应，为人形机器人的鲁棒控制提供了有效解决方案。

Abstract: Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.

</details>


### [23] [Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning](https://arxiv.org/abs/2602.04419)
*Heqing Yang,Ziyuan Jiao,Shu Wang,Yida Niu,Si Liu,Hangxin Liu*

Main category: cs.RO

TL;DR: EPoG是一个基于场景图的探索式顺序操作规划框架，结合图全局规划和LLM局部规划，在部分已知环境中实现高效的机器人探索与任务执行。


<details>
  <summary>Details</summary>
Motivation: 在部分已知环境中，机器人需要同时进行探索获取信息和任务规划以实现高效执行。现有方法难以无缝结合探索和顺序操作规划，特别是在复杂家庭场景中。

Method: 提出EPoG框架：1) 使用图全局规划器和基于大语言模型的情境局部规划器；2) 持续更新信念图来表示已知和未知物体；3) 通过计算目标图和信念图之间的图编辑操作生成动作序列，并按时间依赖性和移动成本排序。

Result: 在46个真实家庭场景和5个长时域日常物体运输任务中，EPoG达到91.3%的成功率，平均减少36.1%的移动距离。物理移动机械臂在未知和动态环境中成功执行复杂任务。

Conclusion: EPoG能够无缝结合探索和顺序操作规划，在部分已知环境中表现出色，展示了在现实世界应用中的潜力。

Abstract: In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG's potential for real-world applications.

</details>


### [24] [Gust Estimation and Rejection with a Disturbance Observer for Proprioceptive Underwater Soft Morphing Wings](https://arxiv.org/abs/2602.04438)
*Tobias Cook,Leo Micklem,Huazhi Dong,Yunjie Yang,Michael Mistry,Francesco Giorgio Serchi*

Main category: cs.RO

TL;DR: 该论文提出了一种受海洋生物启发的软变形翼，通过本体感知传感和扰动观测器来抵消水下环境扰动，提高无人水下航行器在浅水区的稳定性。


<details>
  <summary>Details</summary>
Motivation: 无人水下航行器在浅水区作业时，常受到波浪、水流和湍流等流体动力扰动的影响，导致方向速度突变、稳定性下降。海洋生物通过结合本体感知反馈与柔性鳍尾来应对类似扰动，这启发了本研究。

Method: 提出软变形翼，其连续变形可推断动态扰动：翼型曲率的突然变化直接反映来流变化。开发并实验验证了液压驱动的可控曲率软翼动态模型，利用曲率传感准确估计攻角扰动，设计基于本体感知估计的控制器来抑制软翼升力响应中的扰动。

Result: 实验验证了软翼动态模型的有效性，曲率传感能够准确估计攻角扰动，基于本体感知估计的控制器成功抑制了软翼升力响应中的环境扰动。

Conclusion: 通过结合本体感知传感与扰动观测器，该技术模仿了生物策略，为软体水下航行器在危险环境中保持稳定性提供了有效途径。

Abstract: Unmanned underwater vehicles are increasingly employed for maintenance and surveying tasks at sea, but their operation in shallow waters is often hindered by hydrodynamic disturbances such as waves, currents, and turbulence. These unsteady flows can induce rapid changes in direction and speed, compromising vehicle stability and manoeuvrability. Marine organisms contend with such conditions by combining proprioceptive feedback with flexible fins and tails to reject disturbances. Inspired by this strategy, we propose soft morphing wings endowed with proprioceptive sensing to mitigate environmental perturbations. The wing's continuous deformation provides a natural means to infer dynamic disturbances: sudden changes in camber directly reflect variations in the oncoming flow. By interpreting this proprioceptive signal, a disturbance observer can reconstruct flow parameters in real time. To enable this, we develop and experimentally validate a dynamic model of a hydraulically actuated soft wing with controllable camber. We then show that curvature-based sensing allows accurate estimation of disturbances in the angle of attack. Finally, we demonstrate that a controller leveraging these proprioceptive estimates can reject disturbances in the lift response of the soft wing. By combining proprioceptive sensing with a disturbance observer, this technique mirrors biological strategies and provides a pathway for soft underwater vehicles to maintain stability in hazardous environments.

</details>


### [25] [EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models](https://arxiv.org/abs/2602.04515)
*Yu Bai,MingMing Yu,Chaojie Li,Ziyi Bai,Xinlong Wang,Börje F. Karlsson*

Main category: cs.RO

TL;DR: 提出EgoActing任务和EgoActor模型，将高级指令直接转换为具身化的人形机器人动作，实现感知、运动和操作的实时协调


<details>
  <summary>Details</summary>
Motivation: 在现实世界中部署人形机器人面临根本性挑战，需要在部分信息观察和动态变化环境中紧密集成感知、运动和操作，并能在不同类型子任务间稳健过渡

Method: 提出EgoActing任务，并开发EgoActor统一视觉语言模型，预测运动基元、头部运动、操作命令和人机交互；利用真实世界第一人称RGB数据、空间推理问答和仿真环境演示进行广泛监督训练

Result: EgoActor能够做出稳健、上下文感知的决策，在8B和4B参数模型上实现流畅的动作推理（低于1秒），在仿真和真实环境中有效桥接抽象任务规划和具体运动执行，并能泛化到多样化任务和未见环境

Conclusion: EgoActor通过统一的视觉语言模型框架，成功解决了人形机器人在现实世界部署中的感知-运动-操作集成挑战，实现了从高级指令到具体动作的直接转换

Abstract: Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.

</details>


### [26] [TACO: Temporal Consensus Optimization for Continual Neural Mapping](https://arxiv.org/abs/2602.04516)
*Xunlan Zhou,Hongrui Zhao,Negar Mehr*

Main category: cs.RO

TL;DR: TACO是一个无需重放历史数据的持续神经地图构建框架，通过时间共识优化实现动态环境下的自适应地图更新。


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式地图构建方法无法在内存和计算受限条件下适应动态环境变化，且大多依赖历史数据重放和静态场景假设。

Method: 将地图构建重新定义为时间共识优化问题，将过去模型快照视为时间邻居，通过加权共识约束当前地图更新，允许可靠历史几何约束优化同时修订不可靠区域。

Result: 在模拟和真实世界实验中，TACO能够稳健适应场景变化，在持续学习基准测试中表现优于其他方法。

Conclusion: TACO框架在无需存储或重放历史数据的情况下，实现了内存效率和自适应性的平衡，为动态机器人环境中的持续神经地图构建提供了有效解决方案。

Abstract: Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.

</details>


### [27] [A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction](https://arxiv.org/abs/2602.04522)
*Bingkun Huang,Xin Ma,Nilanjan Chakraborty,Riddhiman Laha*

Main category: cs.RO

TL;DR: 提出Unicomp统一建模框架，将自由运动和摩擦接触统一为互补性问题，支持实时优化规划


<details>
  <summary>Details</summary>
Motivation: 现有规划框架通常分离自由运动和接触状态，或依赖简化的接触表示，限制了接触模式转换的保真度，阻碍了接触丰富行为在实时环境中的鲁棒执行

Method: 基于互补性刚体动力学，将自由运动和接触交互建模为耦合的线性和非线性互补性问题；针对平面面接触，从最大功率耗散原理推导摩擦接触模型，使用椭球极限曲面表示可接受接触力矩

Result: 提出的方法能够在交互速度下实现稳定、物理一致的行为，适用于从平面推送到接触丰富的全身操作等各种任务

Conclusion: Unicomp框架为机器人操作提供了统一的离散时间建模方法，能够一致地捕获自由运动和摩擦接触，支持实时优化规划，提高了接触丰富行为的执行鲁棒性

Abstract: Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.

</details>


### [28] [Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data](https://arxiv.org/abs/2602.04600)
*Jialiang Li,Yi Qiao,Yunhan Guo,Changwen Chen,Wenzhao Lian*

Main category: cs.RO

TL;DR: CoMe-VLA：基于认知记忆的视觉语言动作框架，通过人类自我中心数据学习主动感知和操作先验，解决非结构化环境中的信息不确定性


<details>
  <summary>Details</summary>
Motivation: 现有主动感知方法局限于有限类型的感知行为，难以适应复杂环境。需要解决机器人在非约束环境中信息不确定性的问题，实现通用化操作能力

Method: 将主动感知形式化为信息增益和决策分支驱动的非马尔可夫过程；提出CoMe-VLA框架，包含认知辅助头实现自主子任务转换，双轨记忆系统融合本体感觉和视觉时序上下文；在统一自我中心动作空间中对齐人类和机器人手眼协调行为，分三阶段渐进训练

Result: 在轮式人形机器人上的广泛实验表明，该方法在跨越多个主动感知场景的多样化长时程任务中表现出强大的鲁棒性和适应性

Conclusion: CoMe-VLA框架通过结构化主动感知分类和认知记忆系统，有效提升了机器人在复杂环境中的主动感知和操作能力，为通用化操作提供了新思路

Abstract: Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.

</details>


### [29] [Radar-Inertial Odometry For Computationally Constrained Aerial Navigation](https://arxiv.org/abs/2602.04631)
*Jan Michalczyk*

Main category: cs.RO

TL;DR: 该论文提出了一种雷达-惯性里程计（RIO）算法，用于融合IMU和雷达数据来估计无人机导航状态，能够在资源受限的嵌入式计算机上实时运行，并使用低成本消费级传感器。


<details>
  <summary>Details</summary>
Motivation: 传统外感知传感器（如LiDAR、相机）在极端环境条件（如极端光照、烟雾、雾）下容易失效，而雷达利用电磁波特性对这些因素具有较强免疫力。需要开发能够在恶劣环境下可靠工作的导航系统。

Method: 提出了基于多状态紧耦合扩展卡尔曼滤波（EKF）和因子图（FG）的雷达-惯性里程计方法，融合低成本FMCW雷达提供的3D点瞬时速度和距离信息与IMU读数。还展示了利用深度学习从稀疏噪声雷达点云中提取3D点对应关系的新方法。

Result: 开发了能够在资源受限的嵌入式计算机上实时运行的雷达-惯性里程计算法，使用低成本消费级传感器，能够在极端环境条件下提供可靠的导航状态估计。

Conclusion: 雷达-惯性里程计为解决恶劣环境下的机器人自主导航问题提供了有效解决方案，通过融合雷达和IMU数据，结合先进滤波技术和深度学习方法，实现了在资源受限平台上的实时可靠导航。

Abstract: Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.

</details>


### [30] [Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics](https://arxiv.org/abs/2602.04746)
*Nozomi Nakajima,Pedro Reynolds-Cuéllar,Caitrin Lynch,Kate Darling*

Main category: cs.RO

TL;DR: 对1980-2024年机器人学文献中"枯燥、肮脏、危险"（DDD）概念使用的实证分析，发现仅有少量文献明确定义或提供具体DDD任务示例，并提出框架帮助机器人学界更全面地考虑工作背景。


<details>
  <summary>Details</summary>
Motivation: 机器人学领域长期使用"枯燥、肮脏、危险"（DDD）工作作为机器人应用的主要动机，但缺乏对这一概念的明确定义和系统性分析。本研究旨在通过实证分析了解DDD概念在机器人学文献中的使用情况，并为更全面地考虑机器人技术对人类劳动的影响提供理论基础。

Method: 1. 对1980年至2024年间提及DDD的机器人学出版物进行实证分析；2. 统计明确定义DDD的文献比例（2.7%）和提供具体DDD任务示例的文献比例（8.7%）；3. 回顾社会科学文献中对"枯燥"、"肮脏"、"危险"工作的研究；4. 提出帮助机器人学界考虑工作背景的框架。

Result: 实证分析显示：1. 只有2.7%的机器人学文献明确定义了DDD概念；2. 仅有8.7%的文献提供了具体的DDD任务或工作示例；3. DDD概念在机器人学领域的使用缺乏严谨性和系统性；4. 需要更全面的框架来理解机器人技术对劳动的影响。

Conclusion: 机器人学领域对DDD概念的使用存在明显不足，缺乏明确定义和具体示例。通过回顾社会科学文献和提出新的框架，本研究鼓励机器人学界更全面地考虑工作背景，从而更明智地评估机器人技术对人类劳动的影响，促进更负责任的技术发展。

Abstract: In robotics, the concept of "dull, dirty, and dangerous" (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on "dull," "dirty," and "dangerous" work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.

</details>


### [31] [PDF-HR: Pose Distance Fields for Humanoid Robots](https://arxiv.org/abs/2602.04851)
*Yi Gu,Yukang Gao,Yangchen Zhou,Xingyu Chen,Yixiao Feng,Mingle Zhao,Yunyang Mo,Zhaorui Wang,Lixin Xu,Renjing Xu*

Main category: cs.RO

TL;DR: PDF-HR是一个轻量级的人形机器人姿态先验模型，通过连续可微的流形表示机器人姿态分布，能够预测任意姿态与大规模重定向机器人姿态数据集的距离，为优化和控制提供平滑的姿态合理性度量。


<details>
  <summary>Details</summary>
Motivation: 人形机器人领域缺乏高质量的运动数据，导致姿态和运动先验的研究受限。虽然人体运动恢复领域已有多种模型，但这些先验在人形机器人上的应用仍然有限。

Method: 提出Pose Distance Fields for Humanoid Robots (PDF-HR)，将机器人姿态分布表示为连续可微的流形。给定任意姿态，PDF-HR预测其与大规模重定向机器人姿态数据集的距离，生成平滑的姿态合理性度量。

Result: PDF-HR在多种人形机器人任务中表现优异，包括单轨迹运动跟踪、通用运动跟踪、基于风格的运动模仿和通用运动重定向。实验表明这个即插即用的先验模型能显著增强现有基线方法。

Conclusion: PDF-HR作为一个轻量级姿态先验，能够有效提升人形机器人的运动性能，可作为奖励塑造项、正则化器或独立的合理性评分器集成到各种流程中。

Abstract: Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.

</details>
