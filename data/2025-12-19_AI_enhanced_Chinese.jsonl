{"id": "2512.15840", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15840", "abs": "https://arxiv.org/abs/2512.15840", "authors": ["Boyuan Chen", "Tianyuan Zhang", "Haoran Geng", "Kiwhan Song", "Caiyi Zhang", "Peihao Li", "William T. Freeman", "Jitendra Malik", "Pieter Abbeel", "Russ Tedrake", "Vincent Sitzmann", "Yilun Du"], "title": "Large Video Planner Enables Generalizable Robot Control", "comment": "29 pages, 16 figures", "summary": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u6784\u5efa\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u4eba\u7c7b\u6d3b\u52a8\u89c6\u9891\u6570\u636e\u96c6\u8bad\u7ec3\u751f\u6210\u5f0f\u673a\u5668\u4eba\u89c4\u5212\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u89c6\u9891\u89c4\u5212\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6267\u884c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u52a8\u4f5c\u8f93\u51fa\uff08VLA\u7cfb\u7edf\uff09\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u89c6\u9891\u4f5c\u4e3a\u4e3b\u8981\u6a21\u6001\u66f4\u9002\u5408\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u56e0\u4e3a\u89c6\u9891\u80fd\u6355\u6349\u7269\u7406\u4e16\u754c\u4e2d\u7684\u65f6\u7a7a\u72b6\u6001\u548c\u52a8\u4f5c\u5e8f\u5217\uff0c\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u81ea\u7136\u5bf9\u9f50\u3002", "method": "\u6536\u96c6\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u4eba\u7c7b\u6d3b\u52a8\u548c\u4efb\u52a1\u6f14\u793a\u89c6\u9891\u6570\u636e\u96c6\uff0c\u9996\u6b21\u4ee5\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u8bad\u7ec3\u5f00\u653e\u5f0f\u89c6\u9891\u6a21\u578b\u7528\u4e8e\u751f\u6210\u5f0f\u673a\u5668\u4eba\u89c4\u5212\u3002\u6a21\u578b\u4e3a\u96f6\u6837\u672c\u89c6\u9891\u89c4\u5212\u751f\u6210\u89c6\u9891\u8ba1\u5212\uff0c\u540e\u5904\u7406\u63d0\u53d6\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u6a21\u578b\u5728\u91ce\u5916\u7b2c\u4e09\u65b9\u9009\u62e9\u7684\u4efb\u52a1\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3001\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u73b0\u5b9e\u4e16\u754c\u53ef\u884c\u6027\u3002\u6210\u529f\u5b9e\u73b0\u4e86\u7269\u7406\u6267\u884c\u3002", "conclusion": "\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u662f\u6784\u5efa\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u6709\u6548\u66ff\u4ee3\u8303\u5f0f\uff0c\u89c6\u9891\u6a21\u6001\u80fd\u66f4\u597d\u5730\u6355\u6349\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u5bf9\u9f50\u7684\u65f6\u7a7a\u4fe1\u606f\u3002\u4f5c\u8005\u5f00\u6e90\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u5f00\u653e\u3001\u53ef\u91cd\u590d\u7684\u89c6\u9891\u673a\u5668\u4eba\u5b66\u4e60\u3002"}}
{"id": "2512.15994", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15994", "abs": "https://arxiv.org/abs/2512.15994", "authors": ["Manuel Mekkattu", "Mike Y. Michelis", "Robert K. Katzschmann"], "title": "SORS: A Modular, High-Fidelity Simulator for Soft Robots", "comment": "This work has been submitted to the IEEE for possible publication. Code and data are available at github.com/srl-ethz/sors", "summary": "The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.", "AI": {"tldr": "SORS\u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f\u8f6f\u4f53\u673a\u5668\u4eba\u4eff\u771f\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u6709\u9650\u5143\u7684\u80fd\u91cf\u6846\u67b6\u548c\u7ea6\u675f\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u5efa\u6a21\u4e2d\u7684\u975e\u7ebf\u6027\u53d8\u5f62\u3001\u6750\u6599\u4e0d\u53ef\u538b\u7f29\u6027\u548c\u63a5\u89e6\u4ea4\u4e92\u7b49\u6311\u6218\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u591a\u7269\u7406\u573a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9700\u8981\u5148\u8fdb\u7684\u4eff\u771f\u6846\u67b6\uff0c\u4f46\u73b0\u6709\u4eff\u771f\u5668\u96be\u4ee5\u5904\u7406\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5927\u975e\u7ebf\u6027\u53d8\u5f62\u3001\u6750\u6599\u4e0d\u53ef\u538b\u7f29\u6027\u548c\u63a5\u89e6\u4ea4\u4e92\u7b49\u590d\u6742\u73b0\u8c61\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u5e94\u7528\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51faSORS\u4eff\u771f\u5668\uff0c\u91c7\u7528\u57fa\u4e8e\u6709\u9650\u5143\u65b9\u6cd5\u7684\u80fd\u91cf\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u6269\u5c55\u4ee5\u5305\u542b\u81ea\u5b9a\u4e49\u6750\u6599\u548c\u9a71\u52a8\u6a21\u578b\u3002\u901a\u8fc7\u57fa\u4e8e\u5e8f\u5217\u4e8c\u6b21\u89c4\u5212\u7684\u7ea6\u675f\u975e\u7ebf\u6027\u4f18\u5316\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u7684\u63a5\u89e6\u5904\u7406\u3002", "result": "\u901a\u8fc7\u60ac\u81c2\u6881\u504f\u8f6c\u3001\u8f6f\u4f53\u673a\u68b0\u81c2\u538b\u529b\u9a71\u52a8\u3001PokeFlex\u6570\u636e\u96c6\u63a5\u89e6\u4ea4\u4e92\u7b49\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4eff\u771f\u5668\u7684\u9ad8\u7269\u7406\u4fdd\u771f\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u817f\u63a7\u5236\u4f18\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "SORS\u4eff\u771f\u5668\u80fd\u591f\u51c6\u786e\u6355\u6349\u57fa\u672c\u6750\u6599\u884c\u4e3a\u548c\u590d\u6742\u9a71\u52a8\u52a8\u529b\u5b66\uff0c\u586b\u8865\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u751f\u6001\u7cfb\u7edf\u5728\u53ef\u6269\u5c55\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u53ef\u7528\u6027\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8f6f\u4f53\u673a\u5668\u4eba\u539f\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u5de5\u5177\u3002"}}
{"id": "2512.16011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16011", "abs": "https://arxiv.org/abs/2512.16011", "authors": ["Jack Naylor", "Raghav Mishra", "Nicholas H. Barbara", "Donald G. Dansereau"], "title": "dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection", "comment": "13 pages, 9 images", "summary": "Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u2202LITE\uff0c\u4e00\u79cd\u7528\u4e8e\u5728\u8f68\u68c0\u67e5\u64cd\u4f5c\u7684\u53ef\u5fae\u5206\u7aef\u5230\u7aef\u4eff\u771f\u7ba1\u9053\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u8f68\u9053\u4f20\u64ad\u5668\u4f18\u5316\u8f68\u9053\u53c2\u6570\uff0c\u81ea\u52a8\u8bbe\u8ba1\u975e\u76f4\u89c2\u8f68\u8ff9\u4ee5\u63d0\u5347\u56fe\u50cf\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u73af\u5883\u4e2d\u7684\u955c\u9762\u53cd\u5c04\u3001\u81ea\u9634\u5f71\u548c\u52a8\u6001\u5149\u7167\u4e25\u91cd\u5f71\u54cd\u5728\u8f68\u68c0\u67e5\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u800c\u73b0\u6709\u4eff\u771f\u65b9\u6cd5\u96be\u4ee5\u9488\u5bf9\u56fe\u50cf\u8d28\u91cf\u4f18\u5316\u68c0\u67e5\u8f68\u8ff9\u3002", "method": "\u5f00\u53d1\u2202LITE\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u4eff\u771f\u7ba1\u9053\uff0c\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u5de5\u5177\u548c\u81ea\u5b9a\u4e49\u8f68\u9053\u4f20\u64ad\u5668\uff0c\u5b9e\u73b0\u57fa\u4e8e\u89c6\u89c9\u4f20\u611f\u5668\u6570\u636e\u7684\u8f68\u9053\u53c2\u6570\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u2202LITE\u80fd\u591f\u81ea\u52a8\u8bbe\u8ba1\u975e\u76f4\u89c2\u7684\u68c0\u67e5\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u9ad8\u83b7\u53d6\u6570\u636e\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u822a\u5929\u5668\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u65b0\u7684\u8ba1\u7b97\u601d\u8def\u3002", "conclusion": "\u8be5\u53ef\u5fae\u5206\u68c0\u67e5\u89c4\u5212\u7ba1\u9053\u662f\u9996\u4e2a\u6b64\u7c7b\u65b9\u6cd5\uff0c\u4e3a\u73b0\u4ee3\u822a\u5929\u5668\u4efb\u52a1\u89c4\u5212\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u80fd\u591f\u4f18\u5316\u5728\u8f68\u68c0\u67e5\u64cd\u4f5c\u7684\u6570\u636e\u83b7\u53d6\u8d28\u91cf\u3002"}}
{"id": "2512.16019", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16019", "abs": "https://arxiv.org/abs/2512.16019", "authors": ["Qiping Zhang", "Nathan Tsoi", "Mofeed Nagib", "Hao-Tien Lewis Chiang", "Marynel V\u00e1zquez"], "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios", "comment": null, "summary": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.", "AI": {"tldr": "LLMs\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u884c\u4e3a\u7684\u8bc4\u4ef7\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\uff0c\u4e14\u4e2a\u6027\u5316\u793a\u4f8b\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6765\u8bc4\u4f30\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u611f\u77e5\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u9650\u5236\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u7528\u6237\u5bf9\u673a\u5668\u4eba\u6027\u80fd\u7684\u8bc4\u4ef7\u3002", "method": "\u5229\u7528LLM\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u57fa\u4e8e\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u884c\u4e3a\u7684\u611f\u77e5\u3002\u6269\u5c55SEAN TOGETHER\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u4e2aLLM\u6a21\u578b\uff0c\u5206\u6790\u8f93\u5165\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e2a\u6027\u5316\u793a\u4f8b\u7684\u5e94\u7528\u3002", "result": "LLM\u5728\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u6027\u80fd\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u4e14\u6240\u9700\u6807\u6ce8\u6570\u636e\u91cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u66f4\u591a\u4e0a\u4e0b\u6587\u793a\u4f8b\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4e2a\u6027\u5316\u793a\u4f8b\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "LLM\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\u4e3a\u901a\u8fc7\u7528\u6237\u53cd\u9988\u6539\u8fdb\u673a\u5668\u4eba\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u4e2a\u6027\u5316\u793a\u4f8b\u7684\u5e94\u7528\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.16024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16024", "abs": "https://arxiv.org/abs/2512.16024", "authors": ["Rishabh Dev Yadav", "Shrey Agrawal", "Kamalakar Karlapalem"], "title": "Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface", "comment": null, "summary": "In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u8d1f\u8f7d\u8fd0\u8f93\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u672a\u77e5\u548c\u4e0d\u5e73\u5766\u7684\u503e\u659c\u73af\u5883\u4e2d\u8fd0\u8f93\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u8d1f\u8f7d\u7684\u671f\u671b\u65b9\u5411\u3002\u7cfb\u7edf\u4f7f\u7528\u5e26\u6709\u7ebf\u6027\u6267\u884c\u5668\u7684\u5b9a\u5236\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u5f00\u73af\u63a7\u5236\u5668\u7ed3\u5408\u95ed\u73afPID\u63a7\u5236\u5668\u6765\u7ef4\u6301\u8d1f\u8f7d\u65b9\u5411\u3002", "motivation": "\u5728\u672a\u77e5\u548c\u4e0d\u5e73\u5766\u7684\u503e\u659c\u73af\u5883\u4e2d\u8fd0\u8f93\u8d1f\u8f7d\u65f6\uff0c\u4fdd\u6301\u8d1f\u8f7d\u7684\u671f\u671b\u65b9\u5411\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bf9\u5730\u5f62\u7c7b\u578b\u6709\u5047\u8bbe\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4efb\u4f55\u672a\u77e5\u548c\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u5de5\u4f5c\u7684\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u673a\u5668\u4eba\uff0c\u6bcf\u4e2a\u673a\u5668\u4eba\u9876\u90e8\u5b89\u88c5\u7ebf\u6027\u6267\u884c\u5668\uff08\u6d3b\u585e\uff09\u3002\u7cfb\u7edf\u6301\u7eed\u76d1\u6d4b\u8d1f\u8f7d\u65b9\u5411\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u673a\u5668\u4eba\u6240\u9700\u7684\u6d3b\u585e\u9ad8\u5ea6\u4ee5\u7ef4\u6301\u8d1f\u8f7d\u671f\u671b\u65b9\u5411\u3002\u91c7\u7528\u5f00\u73af\u63a7\u5236\u5668\u4e0e\u95ed\u73afPID\u63a7\u5236\u5668\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5efa\u6a21\u4e0d\u5bf9\u5730\u5f62\u7c7b\u578b\u505a\u4efb\u4f55\u5047\u8bbe\u3002", "result": "\u5728\u591a\u79cd\u5177\u6709\u53d8\u5316\u548c\u590d\u6742\u5730\u5f62\u7684\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002\u7cfb\u7edf\u80fd\u591f\u5728\u4efb\u4f55\u672a\u77e5\u548c\u4e0d\u5e73\u5766\u7684\u5730\u5f62\u548c\u503e\u659c\u5ea6\u4e0a\u5de5\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u673a\u5668\u4eba\u8d1f\u8f7d\u8fd0\u8f93\u7cfb\u7edf\u80fd\u591f\u5728\u672a\u77e5\u548c\u4e0d\u5e73\u5766\u7684\u503e\u659c\u73af\u5883\u4e2d\u6709\u6548\u8fd0\u8f93\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u8d1f\u8f7d\u7684\u671f\u671b\u65b9\u5411\u3002\u5f00\u73af\u4e0e\u95ed\u73afPID\u63a7\u5236\u5668\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.16027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16027", "abs": "https://arxiv.org/abs/2512.16027", "authors": ["Shuaidong Ji", "Mahdi Bamdad", "Francisco Cruz"], "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments", "comment": "10 pages, Accepted at Australasian Conference on Robotics and Automation (ACRA) 2025", "summary": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.", "AI": {"tldr": "SWIFT-Nav\uff1a\u4e00\u79cd\u57fa\u4e8eTD3\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u5b89\u5168\u5c42\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u6536\u655b\uff0c\u5728\u6742\u4e71\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272", "motivation": "\u5728\u6742\u4e71\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8def\u5f84\u89c4\u5212\u3001\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u54cd\u5e94\u7b49\u95ee\u9898", "method": "\u63d0\u51faSWIFT-Nav\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u9a71\u52a8\u611f\u77e5\u524d\u7aef\u548cTD3\u822a\u70b9\u7b56\u7565\uff0c\u4f7f\u7528\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u548c\u8870\u51cf\u03b5-\u8d2a\u5a6a\u63a2\u7d22\uff0c\u52a0\u5165\u8f7b\u91cf\u7ea7\u6a21\u7cca\u903b\u8f91\u5c42\u8ba1\u7b97\u5b89\u5168\u5206\u6570\u5e76\u63a7\u5236\u6a21\u5f0f\u5207\u6362", "result": "\u5728Webots\u4e2d\u5b9e\u73b0\u7684\u65b9\u6cd5\u5728\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u548c\u5bf9\u672a\u89c1\u5e03\u5c40\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u54cd\u5e94\u6027", "conclusion": "\u5c06TD3\u4e0e\u56de\u653e\u4f18\u5148\u7ea7\u3001\u6821\u51c6\u63a2\u7d22\u548c\u6a21\u7cca\u5b89\u5168\u89c4\u5219\u76f8\u7ed3\u5408\uff0c\u4e3a\u6742\u4e71\u573a\u666f\u4e2d\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.16069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16069", "abs": "https://arxiv.org/abs/2512.16069", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Rui Dai", "Matteo Dalle Vedove", "Jiatao Ding", "Daniele Fontanelli", "Nikos Tsagarakis"], "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators", "comment": null, "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u4efb\u52a1\u9a71\u52a8\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u5757\u5316\u673a\u68b0\u81c2\u7684\u5f62\u6001\u4e0e\u5b89\u88c5\u59ff\u6001\u534f\u540c\u4f18\u5316\uff0c\u91c7\u7528\u5206\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u5f15\u5165\u865a\u62df\u6a21\u5757\u62bd\u8c61\u5b9e\u73b0\u53cc\u5206\u652f\u5f62\u6001\u4ee5\u6269\u5c55\u5de5\u4f5c\u7a7a\u95f4\u3002", "motivation": "\u6a21\u5757\u5316\u673a\u68b0\u81c2\u867d\u7136\u5177\u6709\u9ad8\u9002\u5e94\u6027\uff0c\u4f46\u5728\u90e8\u7f72\u65f6\u9700\u8981\u540c\u65f6\u751f\u6210\u53ef\u884c\u8fd0\u52a8\u5e76\u4f18\u5316\u5f62\u6001\u4e0e\u5b89\u88c5\u59ff\u6001\uff0c\u4e14\u4f20\u7edf\u5355\u5206\u652f\u8bbe\u8ba1\u901a\u8fc7\u589e\u52a0\u8fde\u6746\u957f\u5ea6\u6765\u6269\u5c55\u5de5\u4f5c\u7a7a\u95f4\u5bb9\u6613\u8fdd\u53cd\u57fa\u5173\u8282\u626d\u77e9\u9650\u5236\u3002", "method": "1) \u63d0\u51fa\u7edf\u4e00\u4efb\u52a1\u9a71\u52a8\u8ba1\u7b97\u6846\u67b6\uff0c\u96c6\u6210\u8f68\u8ff9\u89c4\u5212\u4e0e\u5f62\u6001/\u59ff\u6001\u534f\u540c\u4f18\u5316\uff1b2) \u5f00\u53d1\u5206\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u652f\u6301\u5197\u4f59\u548c\u975e\u5197\u4f59\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\uff1b3) \u4f7f\u7528CMA-ES\u7b97\u6cd5\u5728\u79bb\u6563\u5f62\u6001\u914d\u7f6e\u548c\u8fde\u7eed\u5b89\u88c5\u59ff\u6001\u7684\u6df7\u5408\u641c\u7d22\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bbe\u8ba1\u4f18\u5316\uff1b4) \u5f15\u5165\u865a\u62df\u6a21\u5757\u62bd\u8c61\u5b9e\u73b0\u53cc\u5206\u652f\u5f62\u6001\uff0c\u8f85\u52a9\u5206\u652f\u5206\u62c5\u4e3b\u5206\u652f\u626d\u77e9\u3002", "result": "\u901a\u8fc7\u629b\u5149\u3001\u94bb\u5b54\u548c\u62fe\u653e\u4efb\u52a1\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff1a1) \u80fd\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u751f\u6210\u6ee1\u8db3\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u7ea6\u675f\u4e14\u907f\u514d\u73af\u5883\u78b0\u649e\u7684\u591a\u79cd\u53ef\u884c\u8bbe\u8ba1\uff1b2) \u901a\u8fc7\u5b9a\u5236\u6210\u672c\u51fd\u6570\u53ef\u5b9e\u73b0\u6700\u5927\u5316\u53ef\u64cd\u4f5c\u6027\u3001\u6700\u5c0f\u5316\u5173\u8282\u52aa\u529b\u6216\u51cf\u5c11\u6a21\u5757\u6570\u91cf\u7b49\u7075\u6d3b\u8bbe\u8ba1\u76ee\u6807\uff1b3) \u65e0\u9700\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u5757\u5373\u53ef\u5b9e\u73b0\u5927\u5de5\u4f5c\u7a7a\u95f4\u7684\u53cc\u5206\u652f\u5f62\u6001\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6a21\u5757\u5316\u673a\u68b0\u81c2\u90e8\u7f72\u4e2d\u7684\u5f62\u6001\u4e0e\u59ff\u6001\u534f\u540c\u4f18\u5316\u95ee\u9898\uff0c\u53cc\u5206\u652f\u8bbe\u8ba1\u6709\u6548\u6269\u5c55\u4e86\u5de5\u4f5c\u7a7a\u95f4\u800c\u4e0d\u589e\u52a0\u5355\u4e2a\u5173\u8282\u6a21\u5757\u7684\u5bb9\u91cf\uff0c\u4e3a\u6a21\u5757\u5316\u673a\u68b0\u81c2\u7684\u4efb\u52a1\u9002\u5e94\u6027\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16076", "abs": "https://arxiv.org/abs/2512.16076", "authors": ["Jia Hu", "Junqi Li", "Xuerun Yan", "Jintao Lai", "Lianhua An"], "title": "A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level", "comment": null, "summary": "Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4eff\u771f\u5e73\u53f0\u6821\u51c6\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8f66\u8f86\u548c\u4ea4\u901a\u6d41\u4e24\u4e2a\u5c42\u9762\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u590d\u73b0\u51c6\u786e\u6027\u548c\u6821\u51c6\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u6d4b\u8bd5\u6821\u51c6\u65b9\u6cd5\u5728\u51c6\u786e\u590d\u73b0\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u80cc\u666f\u4ea4\u901a\u4e4b\u95f4\u7684\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u4eff\u771f\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8f66\u8f86\u95f4\u4ea4\u4e92\u6821\u51c6\u80fd\u529b\u3001\u7cbe\u5ea6\u4fdd\u8bc1\u3001\u6548\u7387\u63d0\u5347\u548c\u6d41\u6c34\u7ebf\u6821\u51c6\u80fd\u529b\u7684\u4eff\u771f\u5e73\u53f0\u6821\u51c6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5168\u81ea\u52a8\u5316\u6821\u51c6\u8fc7\u7a0b\u3002", "result": "\u4e0e\u65e0\u6821\u51c6\u57fa\u7ebf\u548c\u73b0\u6709\u5148\u8fdb\u6821\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u4ea4\u4e92\u590d\u73b0\u51c6\u786e\u6027\u63d0\u5347\u4e8683.53%\uff0c\u6821\u51c6\u6548\u7387\u63d0\u9ad8\u4e8676.75%\uff0c\u5728\u8f66\u8f86\u7ea7\u548c\u4ea4\u901a\u6d41\u7ea7\u6307\u6807\u4e0a\u4fdd\u6301\u4e8651.9%\u7684\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u6d4b\u8bd5\u4e2d\u7684\u6821\u51c6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5168\u81ea\u52a8\u5316\u3001\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u7684\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.16302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16302", "abs": "https://arxiv.org/abs/2512.16302", "authors": ["Zixuan Chen", "Chongkai Gao", "Lin Shao", "Jieqi Shi", "Jing Huo", "Yang Gao"], "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation", "comment": "Accepted by AAAI 2026", "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.", "AI": {"tldr": "ManiLong-Shot\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u91ce\u9884\u6293\u53d6\u64cd\u4f5c\u4efb\u52a1\u7684\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u4ea4\u4e92\u4e8b\u4ef6\u7684\u539f\u59cb\u5206\u89e3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u77ed\u89c6\u91ce\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u89c6\u91ce\u9884\u6293\u53d6\u64cd\u4f5c\u4efb\u52a1\u7684\u65b0\u6846\u67b6\u3002", "method": "ManiLong-Shot\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u4e8b\u4ef6\u6765\u7ed3\u6784\u5316\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e8f\u5217\u5316\u4ea4\u4e92\u611f\u77e5\u7684\u539f\u59cb\u52a8\u4f5c\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u6a21\u4eff\u8fde\u7eed\u8f68\u8ff9\u3002\u539f\u59cb\u5206\u89e3\u53ef\u4ee5\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u5c42\u63a8\u7406\u6216\u57fa\u4e8e\u673a\u5668\u4eba\u72b6\u6001\u53d8\u5316\u7684\u89c4\u5219\u542f\u53d1\u5f0f\u65b9\u6cd5\u9a71\u52a8\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u539f\u59cb\u52a8\u4f5c\uff0c\u9884\u6d4b\u4ea4\u4e92\u5173\u952e\u7684\u4e0d\u53d8\u533a\u57df\uff0c\u5efa\u7acb\u6f14\u793a\u4e0e\u5f53\u524d\u89c2\u5bdf\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u8ba1\u7b97\u76ee\u6807\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3002", "result": "\u5728\u4ec5\u4f7f\u752810\u4e2a\u77ed\u89c6\u91ce\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0cManiLong-Shot\u901a\u8fc7\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u80fd\u591f\u6cdb\u5316\u523020\u4e2a\u672a\u89c1\u8fc7\u7684\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u6db5\u76d6\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u5b9e\u73b0\u4e8622.8%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u7a33\u5065\u5730\u6267\u884c\u4e09\u4e2a\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "ManiLong-Shot\u901a\u8fc7\u57fa\u4e8e\u4ea4\u4e92\u4e8b\u4ef6\u7684\u539f\u59cb\u5206\u89e3\u65b9\u6cd5\uff0c\u6210\u529f\u6269\u5c55\u4e86\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u5728\u957f\u89c6\u91ce\u9884\u6293\u53d6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.16367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16367", "abs": "https://arxiv.org/abs/2512.16367", "authors": ["Sijia Chen", "Wei Dong"], "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion", "comment": "accept by IEEE Transactions on Industrial Electronics", "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5730\u7a7a\u534f\u540c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u52a8\u89c6\u89c9\u3001\u5355\u70b9\u6d4b\u8ddd\u3001\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u5149\u6d41\u878d\u5408\uff0c\u589e\u5f3a\u98de\u884c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4f20\u611f\u5668\u9000\u5316\u65f6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u6444\u50cf\u5934\u89c2\u6d4b\u9884\u88c5\u6807\u8bb0\u6765\u4f30\u8ba1\u98de\u884c\u673a\u5668\u4eba\u4f4d\u7f6e\uff0c\u4f46\u53d7\u9650\u4e8e\u8ddd\u79bb\u4e14\u5bb9\u6613\u6355\u83b7\u5931\u8d25\u3002\u5728\u89c6\u89c9\u4f20\u611f\u5668\u9000\u5316\u7684\u590d\u6742\u73af\u5883\u4e2d\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "1) \u5730\u9762\u8f66\u8f86\u642d\u8f7d\u53ef\u52a8\u6001\u65cb\u8f6c\u7684\u4e3b\u52a8\u89c6\u89c9\u5b50\u7cfb\u7edf\uff0c\u68c0\u6d4b\u8ddf\u8e2a\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u7ea2\u5916\u6807\u8bb0\uff1b2) \u7ed3\u5408\u5355\u70b9\u6d4b\u8ddd\u6269\u5c55\u53ef\u884c\u8ddd\u79bb\u548c\u91cd\u6355\u83b7\u80fd\u529b\uff1b3) \u57fa\u4e8e\u591a\u9879\u5f0f\u8fd1\u4f3c\u7684\u964d\u7ef4\u4f30\u8ba1\u5668\u878d\u5408\u591a\u6e90\u6d4b\u91cf\uff1b4) \u81ea\u9002\u5e94\u6ed1\u52a8\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6743\u91cd\u53c2\u6570\u3002", "result": "\u5728\u70df\u96fe\u5e72\u6270\u3001\u5149\u7167\u53d8\u5316\u3001\u969c\u788d\u7269\u906e\u6321\u3001\u957f\u65f6\u95f4\u89c6\u89c9\u4e22\u5931\u548c\u6269\u5c55\u64cd\u4f5c\u8303\u56f4\u7b49\u6761\u4ef6\u4e0b\uff0c\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u7ea6\u4e3a0.09\u7c73\uff0c\u4fdd\u6301\u5bf9\u6355\u83b7\u4e22\u5931\u548c\u4f20\u611f\u5668\u6545\u969c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5730\u7a7a\u534f\u540c\u5b9a\u4f4d\u6846\u67b6\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u81ea\u9002\u5e94\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u98de\u884c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.16446", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16446", "abs": "https://arxiv.org/abs/2512.16446", "authors": ["Enis Yalcin", "Joshua O'Hara", "Maria Stamatopoulou", "Chengxu Zhou", "Dimitrios Kanoulas"], "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion", "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)", "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.", "AI": {"tldr": "E-SDS\u6846\u67b6\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5b9e\u65f6\u5730\u5f62\u611f\u77e5\uff0c\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0b\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u5c06\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u7684\u65f6\u95f4\u4ece\u6570\u5929\u7f29\u77ed\u81f32\u5c0f\u65f6\u5185\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5e94\u5bf9\u590d\u6742\u5730\u5f62\u5bfc\u822a\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u611f\u77e5\u73af\u5883\u7684\u81ea\u52a8\u5316\u5956\u52b1\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ee5\u66ff\u4ee3\u7e41\u7410\u7684\u624b\u5de5\u5de5\u7a0b\u3002", "method": "\u63d0\u51faE-SDS\uff08\u73af\u5883\u611f\u77e5\u7684\"\u770b\u5230\u5373\u505a\u5230\"\u6392\u5e8f\uff09\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5b9e\u65f6\u5730\u5f62\u4f20\u611f\u5668\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u57fa\u4e8e\u793a\u4f8b\u89c6\u9891\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u8bad\u7ec3\u5177\u6709\u73af\u5883\u611f\u77e5\u80fd\u529b\u7684\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u56db\u7c7b\u5730\u5f62\u6d4b\u8bd5\u4e2d\uff0cE-SDS\u9996\u6b21\u5b9e\u73b0\u4e86\u6210\u529f\u7684\u697c\u68af\u4e0b\u964d\u4efb\u52a1\uff0c\u800c\u624b\u5de5\u8bbe\u8ba1\u5956\u52b1\u548c\u975e\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\u5747\u5931\u8d25\u3002\u5728\u6240\u6709\u5730\u5f62\u4e2d\uff0c\u901f\u5ea6\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e8651.9-82.6%\u3002", "conclusion": "E-SDS\u6846\u67b6\u5c06\u5956\u52b1\u8bbe\u8ba1\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u4ece\u6570\u5929\u51cf\u5c11\u52302\u5c0f\u65f6\u4ee5\u5185\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u9c81\u68d2\u548c\u66f4\u5f3a\u5927\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5f53\u524dVLM\u65b9\u6cd5\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u7684\u95ee\u9898\u3002"}}
{"id": "2512.16454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16454", "abs": "https://arxiv.org/abs/2512.16454", "authors": ["Tianhao Shao", "Kaixing Zhao", "Feng Liu", "Lixin Yang", "Bin Guo"], "title": "AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems", "comment": null, "summary": "As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable \"user\". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.", "AI": {"tldr": "MPBS\u6846\u67b6\u901a\u8fc7\u884c\u4e3a\u5206\u7c7b\u548c\u65f6\u7a7a\u9884\u6d4b\uff0c\u4e3a\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\u7b49\u65e0\u4eba\u7cfb\u7edf\u63d0\u4f9b\u667a\u80fd\u4efb\u52a1\u62db\u52df\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\u5728\u57ce\u5e02\u573a\u666f\u611f\u77e5\u548c\u5e94\u6025\u54cd\u5e94\u7b49\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u5982\u4f55\u9ad8\u6548\u62db\u52df\u8fd9\u4e9b\u81ea\u4e3b\u8bbe\u5907\u6267\u884c\u65f6\u6548\u6027\u4efb\u52a1\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faMPBS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u884c\u4e3a\u611f\u77e5KNN\u5206\u7c7b\u5668\u3001\u65f6\u53d8\u9a6c\u5c14\u53ef\u592b\u79fb\u52a8\u9884\u6d4b\u6a21\u578b\u3001\u8003\u8651\u4efb\u52a1\u7d27\u6025\u6027\u548c\u57fa\u7ad9\u6027\u80fd\u7684\u52a8\u6001\u4f18\u5148\u7ea7\u8c03\u5ea6\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9eGeoLife\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMPBS\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "MPBS\u4e3a\u65e0\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9884\u6d4b\u6027\u3001\u884c\u4e3a\u611f\u77e5\u7684\u667a\u80fd\u534f\u540c\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16469", "abs": "https://arxiv.org/abs/2512.16469", "authors": ["Jiayu Zhang", "Kaixing Zhao", "Tianhao Shao", "Bin Guo", "Liang He"], "title": "Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing", "comment": null, "summary": "Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.", "AI": {"tldr": "Tri-Select\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u89c6\u89c9\u4f17\u5305\u7684\u4e09\u9636\u6bb5\u89c6\u89c9\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u7a7a\u95f4\u76f8\u4f3c\u6027\u805a\u7c7b\u548c\u89c6\u89c9\u7279\u5f81\u5f15\u5bfc\u9009\u62e9\uff0c\u6709\u6548\u8fc7\u6ee4\u5197\u4f59\u548c\u4f4e\u8d28\u91cf\u56fe\u50cf\uff0c\u63d0\u9ad8\u9009\u62e9\u6548\u7387\u548c\u6570\u636e\u96c6\u8d28\u91cf\u3002", "motivation": "\u79fb\u52a8\u89c6\u89c9\u4f17\u5305\u901a\u8fc7\u5206\u5e03\u5f0f\u79fb\u52a8\u8bbe\u5907\u6536\u96c6\u56fe\u50cf\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u7684\u73af\u5883\u76d1\u6d4b\uff0c\u4f46\u6570\u636e\u5b58\u5728\u5197\u4f59\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5305\u62ec\u91c7\u96c6\u89c6\u89d2\u91cd\u53e0\u3001\u5206\u8fa8\u7387\u53d8\u5316\u548c\u7528\u6237\u884c\u4e3a\u591a\u6837\u6027\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faTri-Select\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u5143\u6570\u636e\u8fc7\u6ee4\u4e22\u5f03\u4e0d\u76f8\u5173\u6837\u672c\uff1b2) \u57fa\u4e8e\u7a7a\u95f4\u76f8\u4f3c\u6027\u7684\u8c31\u805a\u7c7b\u7ec4\u7ec7\u5019\u9009\u56fe\u50cf\uff1b3) \u57fa\u4e8e\u6700\u5927\u72ec\u7acb\u96c6\u641c\u7d22\u7684\u89c6\u89c9\u7279\u5f81\u5f15\u5bfc\u9009\u62e9\uff0c\u4fdd\u7559\u9ad8\u8d28\u91cf\u4ee3\u8868\u6027\u56fe\u50cf\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTri-Select\u63d0\u9ad8\u4e86\u9009\u62e9\u6548\u7387\u548c\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u4f17\u5305\u5e94\u7528\u3002", "conclusion": "Tri-Select\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u89c6\u89c9\u4f17\u5305\u4e2d\u7684\u6570\u636e\u5197\u4f59\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u9009\u62e9\u7b56\u7565\u4f18\u5316\u4e86\u6570\u636e\u8d28\u91cf\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u4f17\u5305\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16555", "categories": ["cs.RO", "cs.FL", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16555", "abs": "https://arxiv.org/abs/2512.16555", "authors": ["Marcelo Rosa", "Jos\u00e9 E. R. Cury", "Fabio L. Baldissera"], "title": "A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots", "comment": null, "summary": "In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots", "AI": {"tldr": "\u57fa\u4e8e\u76d1\u7763\u63a7\u5236\u7406\u8bba\u7684\u591a\u673a\u5668\u4eba3D\u7ed3\u6784\u534f\u540c\u5efa\u9020\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u591a\u79fb\u52a8\u673a\u5668\u4eba\u534f\u540c\u5efa\u9020\u9884\u5b9a3D\u7ed3\u6784\u7684\u534f\u8c03\u95ee\u9898\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5efa\u9020", "method": "\u91c7\u7528\u76d1\u7763\u63a7\u5236\u7406\u8bba\uff0c\u4ece\u5355\u4e2a\u673a\u5668\u4eba\u548c\u76ee\u6807\u7ed3\u6784\u6a21\u578b\u4e2d\u5408\u6210\u6b63\u786e\u6784\u9020\u7684\u76d1\u7763\u63a7\u5236\u5668", "result": "\u4e3a\u6bcf\u4e2a\u673a\u5668\u4eba\u590d\u5236\u76d1\u7763\u63a7\u5236\u5668\u540e\uff0c\u6240\u6709\u673a\u5668\u4eba\u80fd\u591f\u534f\u540c\u5b8c\u6210\u76ee\u6807\u7ed3\u6784\u7684\u5efa\u9020", "conclusion": "\u76d1\u7763\u63a7\u5236\u7406\u8bba\u4e3a\u591a\u673a\u5668\u4eba3D\u7ed3\u6784\u5efa\u9020\u63d0\u4f9b\u4e86\u4e00\u79cd\u6b63\u786e\u6784\u9020\u7684\u534f\u8c03\u63a7\u5236\u65b9\u6cd5"}}
{"id": "2512.16705", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16705", "abs": "https://arxiv.org/abs/2512.16705", "authors": ["David M\u00fcller", "Espen Knoop", "Dario Mylonopoulos", "Agon Serifi", "Michael A. Hopkins", "Ruben Grandia", "Moritz B\u00e4cher"], "title": "Olaf: Bringing an Animated Character to Life in the Physical World", "comment": null, "summary": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u52a8\u753b\u89d2\u8272\u5965\u62c9\u592b\u5b9e\u4f53\u5316\u4e3a\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5b9e\u73b0\u52a8\u753b\u98ce\u683c\u7684\u884c\u8d70\uff0c\u91c7\u7528\u521b\u65b0\u7684\u673a\u68b0\u8bbe\u8ba1\uff08\u9690\u85cf\u5f0f\u817f\u90e8\u3001\u7403\u5f62/\u5e73\u9762\u8fde\u6746\uff09\u548c\u6e29\u5ea6\u611f\u77e5\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89d2\u8272\u673a\u5668\u4eba\u7684\u771f\u5b9e\u611f\u3002", "motivation": "\u52a8\u753b\u89d2\u8272\u901a\u5e38\u5177\u6709\u975e\u7269\u7406\u7684\u8fd0\u52a8\u65b9\u5f0f\u548c\u7279\u6b8a\u7684\u8eab\u4f53\u6bd4\u4f8b\uff0c\u8fd9\u4e3a\u673a\u68b0\u8bbe\u8ba1\u548c\u98ce\u683c\u5316\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u521b\u65b0\u5e73\u53f0\u3002\u7814\u7a76\u65e8\u5728\u5c06\u52a8\u753b\u89d2\u8272\u5965\u62c9\u592b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u5b9e\u73b0\uff0c\u521b\u9020\u5177\u6709\u9ad8\u5ea6\u53ef\u4fe1\u5ea6\u7684\u89d2\u8272\u673a\u5668\u4eba\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u52a8\u753b\u53c2\u8003\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff1b\u673a\u68b0\u8bbe\u8ba1\u4e0a\u4f7f\u7528\u9690\u85cf\u5f0f\u4e0d\u5bf9\u79f0\u817f\u90e8\uff08\u85cf\u5728\u8f6f\u6ce1\u6cab\u88d9\u4e0b\uff09\u5b9e\u73b0\u811a\u90e8\u6cbf\u8eab\u4f53\u79fb\u52a8\u7684\u9519\u89c9\uff1b\u4f7f\u7528\u7403\u5f62\u548c\u5e73\u9762\u8fde\u6746\u5c06\u6267\u884c\u5668\u96c6\u6210\u5230\u89d2\u8272\u624b\u81c2\u3001\u5634\u5df4\u548c\u773c\u775b\u4e2d\uff1b\u5f15\u5165\u51cf\u5c11\u51b2\u51fb\u566a\u97f3\u7684\u5956\u52b1\u51fd\u6570\uff1b\u5c06\u6e29\u5ea6\u503c\u4f5c\u4e3a\u7b56\u7565\u8f93\u5165\uff0c\u6dfb\u52a0\u4fdd\u6301\u6e29\u5ea6\u5728\u5b89\u5168\u8303\u56f4\u5185\u7684\u5956\u52b1\u4ee5\u9632\u6b62\u6267\u884c\u5668\u8fc7\u70ed\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u89d2\u8272\u673a\u5668\u4eba\u524d\u6240\u672a\u6709\u7684\u53ef\u4fe1\u5ea6\u6c34\u5e73\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u753b\u98ce\u683c\u7684\u884c\u8d70\u8fd0\u52a8\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u673a\u68b0\u96c6\u6210\u3001\u566a\u97f3\u63a7\u5236\u548c\u70ed\u7ba1\u7406\u7b49\u591a\u91cd\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c06\u52a8\u753b\u89d2\u8272\u5965\u62c9\u592b\u5b9e\u4f53\u5316\u4e3a\u7269\u7406\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u673a\u68b0\u8bbe\u8ba1\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u53ef\u4fe1\u7684\u52a8\u753b\u98ce\u683c\u8fd0\u52a8\uff0c\u4e3a\u89d2\u8272\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u7a81\u7834\u3002"}}
{"id": "2512.16724", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16724", "abs": "https://arxiv.org/abs/2512.16724", "authors": ["Yixiang Chen", "Yan Huang", "Keji He", "Peiyan Li", "Liang Wang"], "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation", "comment": "Accepted at RA-L 2025", "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .", "AI": {"tldr": "\u63d0\u51faVERM\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u4ece3D\u70b9\u4e91\u60f3\u8c61\u865a\u62df\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89d2\uff0c\u8fc7\u6ee4\u5197\u4f59\u4fe1\u606f\uff0c\u63d0\u9ad8\u673a\u5668\u4eba3D\u64cd\u4f5c\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\u5f15\u5165\u5927\u91cf\u5197\u4f59\u548c\u65e0\u5173\u4fe1\u606f\uff0c\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u8feb\u4f7f\u6a21\u578b\u82b1\u8d39\u989d\u5916\u8bad\u7ec3\u65f6\u95f4\u63d0\u53d6\u5173\u952e\u4efb\u52a1\u76f8\u5173\u7ec6\u8282", "method": "\u63d0\u51faVERM\u65b9\u6cd5\uff1a1) \u5229\u7528\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u4ece3D\u70b9\u4e91\u60f3\u8c61\u865a\u62df\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89d2\uff1b2) \u8bbe\u8ba1\u6df1\u5ea6\u611f\u77e5\u6a21\u5757\uff1b3) \u8bbe\u8ba1\u52a8\u6001\u7c97\u5230\u7ec6\u5904\u7406\u6d41\u7a0b", "result": "\u5728RLBench\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bad\u7ec3\u65f6\u95f4\u52a0\u901f1.89\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u901f1.54\u500d", "conclusion": "VERM\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u5197\u4f59\u4fe1\u606f\uff0c\u51c6\u786e\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u4fc3\u8fdb3D\u52a8\u4f5c\u89c4\u5212\u548c\u7ec6\u7c92\u5ea6\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd"}}
{"id": "2512.16760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16760", "abs": "https://arxiv.org/abs/2512.16760", "authors": ["Tianshuai Hu", "Xiaolu Liu", "Song Wang", "Yiyao Zhu", "Ao Liang", "Lingdong Kong", "Guoyang Zhao", "Zeying Gong", "Jun Cen", "Zhiyu Huang", "Xiaoshuai Hao", "Linfeng Li", "Hang Song", "Xiangtai Li", "Jun Ma", "Shaojie Shen", "Jianke Zhu", "Dacheng Tao", "Ziwei Liu", "Junwei Liang"], "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future", "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad", "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4ece\u4f20\u7edf\u6a21\u5757\u5316\u65b9\u6cd5\u5230\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6\u7684\u6f14\u8fdb\uff0c\u5c06VLA\u65b9\u6cd5\u5206\u4e3a\u7aef\u5230\u7aef\u548c\u53cc\u7cfb\u7edf\u4e24\u5927\u8303\u5f0f\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3001\u57fa\u51c6\u53ca\u672a\u6765\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7684\u6a21\u5757\u5316\"\u611f\u77e5-\u51b3\u7b56-\u52a8\u4f5c\"\u6d41\u6c34\u7ebf\u5b58\u5728\u624b\u5de5\u63a5\u53e3\u548c\u89c4\u5219\u7ec4\u4ef6\u5728\u590d\u6742\u573a\u666f\u4e2d\u5931\u6548\u3001\u611f\u77e5\u8bef\u5dee\u7ea7\u8054\u4f20\u64ad\u7b49\u95ee\u9898\u3002\u89c6\u89c9-\u52a8\u4f5c\u6a21\u578b\u867d\u6709\u6240\u6539\u8fdb\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u5bf9\u5206\u5e03\u504f\u79fb\u654f\u611f\u4e14\u7f3a\u5c11\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u8fdb\u5c55\u4fc3\u4f7fVLA\u6846\u67b6\u51fa\u73b0\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u3001\u6cdb\u5316\u6027\u5f3a\u4e14\u7b26\u5408\u4eba\u7c7b\u610f\u56fe\u7684\u9a7e\u9a76\u7b56\u7565\u3002", "method": "\u672c\u6587\u5bf9\u81ea\u52a8\u9a7e\u9a76VLA\u6846\u67b6\u8fdb\u884c\u4e86\u7ed3\u6784\u5316\u5206\u7c7b\uff1a1\uff09\u7aef\u5230\u7aefVLA\uff1a\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\u96c6\u6210\u5728\u5355\u4e00\u6a21\u578b\u4e2d\uff1b2\uff09\u53cc\u7cfb\u7edfVLA\uff1a\u5c06\u6162\u901f\u6df1\u601d\u719f\u8651\uff08\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u4e0e\u5feb\u901f\u5b89\u5168\u5173\u952e\u6267\u884c\uff08\u901a\u8fc7\u89c4\u5212\u5668\uff09\u5206\u79bb\u3002\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e3a\u6587\u672cvs\u6570\u503c\u52a8\u4f5c\u751f\u6210\u5668\u3001\u663e\u5f0fvs\u9690\u5f0f\u5f15\u5bfc\u673a\u5236\u7b49\u5b50\u7c7b\u3002\u540c\u65f6\u603b\u7ed3\u4e86\u8bc4\u4f30VLA\u9a7e\u9a76\u7cfb\u7edf\u7684\u4ee3\u8868\u6027\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "result": "\u5efa\u7acb\u4e86\u81ea\u52a8\u9a7e\u9a76VLA\u9886\u57df\u7684\u7cfb\u7edf\u5316\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u4e24\u79cd\u4e3b\u8981\u8303\u5f0f\u53ca\u5176\u5b50\u7c7b\uff0c\u6574\u7406\u4e86\u76f8\u5173\u8bc4\u4f30\u8d44\u6e90\uff0c\u4e3a\u63a8\u8fdb\u4eba\u7c7b\u517c\u5bb9\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "VLA\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u3001\u6cdb\u5316\u6027\u5f3a\u4e14\u7b26\u5408\u4eba\u7c7b\u610f\u56fe\u7684\u65b0\u9014\u5f84\u3002\u672a\u6765\u6311\u6218\u5305\u62ec\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6307\u4ee4\u5fe0\u5b9e\u5ea6\u7b49\u65b9\u9762\u3002\u672c\u6587\u65e8\u5728\u4e3a\u63a8\u8fdb\u4eba\u7c7b\u517c\u5bb9\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5efa\u7acb\u7edf\u4e00\u57fa\u7840\u3002"}}
{"id": "2512.16793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16793", "abs": "https://arxiv.org/abs/2512.16793", "authors": ["Xiaopeng Lin", "Shijie Lian", "Bin Yu", "Ruoqi Yang", "Changti Wu", "Yuzhuo Miao", "Yurun Jin", "Yukun Shi", "Cong Huang", "Bojun Cheng", "Kai Chen"], "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "comment": "17 pages, 4 figures", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u5177\u8eab\u667a\u80fd\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaE2E-3M\u6570\u636e\u96c6\u8bad\u7ec3PhysBrain\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u548c\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u6cdb\u5316\u9700\u8981\u7269\u7406\u667a\u80fd\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u7b2c\u4e09\u4eba\u79f0\u6570\u636e\u8bad\u7ec3\uff0c\u4e0e\u4eff\u4eba\u673a\u5668\u4eba\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002\u6536\u96c6\u673a\u5668\u4eba\u81ea\u6211\u4e2d\u5fc3\u6570\u636e\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u800c\u5927\u89c4\u6a21\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u5177\u8eab\u8bad\u7ec3\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u63d0\u51faEgocentric2Embodiment\u7ffb\u8bd1\u7ba1\u9053\uff0c\u5c06\u539f\u59cb\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u5316\u4e3a\u591a\u5c42\u6b21\u3001\u6a21\u5f0f\u9a71\u52a8\u7684\u89c6\u89c9\u95ee\u7b54\u76d1\u7763\uff0c\u5f3a\u8c03\u8bc1\u636e\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u6784\u5efa\u4e86E2E-3M\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u7684\u5177\u8eab\u5927\u8111PhysBrain\u3002", "result": "PhysBrain\u5728\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728EgoThink\u89c4\u5212\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u4f5c\u4e3a\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u7684\u521d\u59cb\u5316\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5fae\u8c03\uff0c\u5728SimplerEnv\u4efb\u52a1\u4e0a\u8fbe\u523053.9%\u7684\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u4ece\u4eba\u7c7b\u81ea\u6211\u4e2d\u5fc3\u76d1\u7763\u5230\u4e0b\u6e38\u673a\u5668\u4eba\u63a7\u5236\u7684\u6709\u6548\u8fc1\u79fb\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5927\u89c4\u6a21\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8bad\u7ec3\u76d1\u7763\uff0c\u6210\u529f\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5177\u8eab\u667a\u80fd\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u7269\u7406\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
