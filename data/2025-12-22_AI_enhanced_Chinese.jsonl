{"id": "2512.17062", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17062", "abs": "https://arxiv.org/abs/2512.17062", "authors": ["Muhayy Ud Din", "Jan Rosell", "Waseem Akram", "Irfan Hussain"], "title": "Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation", "comment": "Submitted to ICARA", "summary": "Simulation is essential for developing robotic manipulation systems, particularly for task and motion planning (TAMP), where symbolic reasoning interfaces with geometric, kinematic, and physics-based execution. Recent advances in Large Language Models (LLMs) enable robots to generate symbolic plans from natural language, yet executing these plans in simulation often requires robot-specific engineering or planner-dependent integration. In this work, we present a unified pipeline that connects an LLM-based symbolic planner with the Kautham motion planning framework to achieve generalizable, robot-agnostic symbolic-to-geometric manipulation. Kautham provides ROS-compatible support for a wide range of industrial manipulators and offers geometric, kinodynamic, physics-driven, and constraint-based motion planning under a single interface. Our system converts language instructions into symbolic actions and computes and executes collision-free trajectories using any of Kautham's planners without additional coding. The result is a flexible and scalable tool for language-driven TAMP that is generalized across robots, planning modalities, and manipulation tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7ba1\u9053\uff0c\u5c06\u57fa\u4e8eLLM\u7684\u7b26\u53f7\u89c4\u5212\u5668\u4e0eKautham\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u8fde\u63a5\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u65e0\u5173\u7684\u7b26\u53f7\u5230\u51e0\u4f55\u64cd\u4f5c\uff0c\u652f\u6301\u8bed\u8a00\u9a71\u52a8\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7b26\u53f7\u8ba1\u5212\uff0c\u4f46\u5728\u6a21\u62df\u4e2d\u6267\u884c\u8fd9\u4e9b\u8ba1\u5212\u901a\u5e38\u9700\u8981\u673a\u5668\u4eba\u7279\u5b9a\u7684\u5de5\u7a0b\u6216\u89c4\u5212\u5668\u4f9d\u8d56\u7684\u96c6\u6210\uff0c\u7f3a\u4e4f\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7ba1\u9053\uff0c\u5c06LLM\u7b26\u53f7\u89c4\u5212\u5668\u4e0eKautham\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u8fde\u63a5\u3002\u7cfb\u7edf\u5c06\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u7b26\u53f7\u52a8\u4f5c\uff0c\u7136\u540e\u4f7f\u7528Kautham\u7684\u4efb\u4f55\u89c4\u5212\u5668\u8ba1\u7b97\u548c\u6267\u884c\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u65e0\u9700\u989d\u5916\u7f16\u7801\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u652f\u6301\u8bed\u8a00\u9a71\u52a8\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u80fd\u591f\u8de8\u673a\u5668\u4eba\u3001\u89c4\u5212\u6a21\u5f0f\u548c\u64cd\u4f5c\u4efb\u52a1\u8fdb\u884c\u901a\u7528\u5316\u5904\u7406\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u65e0\u5173\u7684\u7b26\u53f7\u5230\u51e0\u4f55\u64cd\u4f5c\uff0c\u4e3a\u8bed\u8a00\u9a71\u52a8\u7684TAMP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2512.17136", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17136", "abs": "https://arxiv.org/abs/2512.17136", "authors": ["Chunyang Meng", "Eduardo B. Sandoval", "Ricardo Sosa", "Francisco Cruz"], "title": "Towards Senior-Robot Interaction: Reactive Robot Dog Gestures", "comment": "Accepted at the Australasian Conference on Robotics and Automation (ACRA) 2025", "summary": "As the global population ages, many seniors face the problem of loneliness. Companion robots offer a potential solution. However, current companion robots often lack advanced functionality, while task-oriented robots are not designed for social interaction, limiting their suitability and acceptance by seniors. Our work introduces a senior-oriented system for quadruped robots that allows for more intuitive user input and provides more socially expressive output. For user input, we implemented a MediaPipe-based module for hand gesture and head movement recognition, enabling control without a remote. For output, we designed and trained robotic dog gestures using curriculum-based reinforcement learning in Isaac Gym, progressing from simple standing to three-legged balancing and leg extensions, and more. The final tests achieved over 95\\% success on average in simulation, and we validated a key social gesture (the paw-lift) on a Unitree robot. Real-world tests demonstrated the feasibility and social expressiveness of this framework, while also revealing sim-to-real challenges in joint compliance, load distribution, and balance control. These contributions advance the development of practical quadruped robots as social companions for the senior and outline pathways for sim-to-real adaptation and inform future user studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8001\u5e74\u4eba\u7684\u56db\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u624b\u52bf\u548c\u5934\u90e8\u8fd0\u52a8\u8bc6\u522b\u5b9e\u73b0\u76f4\u89c2\u63a7\u5236\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u793e\u4ea4\u8868\u8fbe\u6027\u52a8\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u793e\u4ea4\u8868\u73b0\u529b\u3002", "motivation": "\u968f\u7740\u5168\u7403\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u8bb8\u591a\u8001\u5e74\u4eba\u9762\u4e34\u5b64\u72ec\u95ee\u9898\u3002\u966a\u4f34\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u966a\u4f34\u673a\u5668\u4eba\u529f\u80fd\u6709\u9650\uff0c\u800c\u4efb\u52a1\u5bfc\u5411\u673a\u5668\u4eba\u53c8\u4e0d\u9002\u5408\u793e\u4ea4\u4e92\u52a8\uff0c\u9650\u5236\u4e86\u8001\u5e74\u4eba\u5bf9\u673a\u5668\u4eba\u7684\u63a5\u53d7\u5ea6\u3002", "method": "1. \u8f93\u5165\u65b9\u9762\uff1a\u5b9e\u73b0\u57fa\u4e8eMediaPipe\u7684\u624b\u52bf\u548c\u5934\u90e8\u8fd0\u52a8\u8bc6\u522b\u6a21\u5757\uff0c\u65e0\u9700\u9065\u63a7\u5668\u5373\u53ef\u63a7\u5236\u673a\u5668\u4eba\u30022. \u8f93\u51fa\u65b9\u9762\uff1a\u5728Isaac Gym\u4e2d\u4f7f\u7528\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u548c\u8bad\u7ec3\u673a\u5668\u4eba\u72d7\u59ff\u52bf\uff0c\u4ece\u7b80\u5355\u7ad9\u7acb\u9010\u6b65\u5230\u4e09\u817f\u5e73\u8861\u548c\u817f\u90e8\u4f38\u5c55\u7b49\u590d\u6742\u52a8\u4f5c\u30023. \u5728Unitree\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u5173\u952e\u793e\u4ea4\u59ff\u52bf\uff08\u62ac\u722a\u52a8\u4f5c\uff09\u3002", "result": "\u4eff\u771f\u6d4b\u8bd5\u5e73\u5747\u6210\u529f\u7387\u8d85\u8fc795%\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u62ac\u722a\u793e\u4ea4\u59ff\u52bf\u7684\u53ef\u884c\u6027\u3002\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u793e\u4ea4\u8868\u73b0\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u6362\u4e2d\u7684\u5173\u8282\u67d4\u987a\u6027\u3001\u8d1f\u8f7d\u5206\u5e03\u548c\u5e73\u8861\u63a7\u5236\u7b49\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u56db\u8db3\u673a\u5668\u4eba\u4f5c\u4e3a\u8001\u5e74\u4eba\u793e\u4ea4\u4f34\u4fa3\u7684\u5b9e\u7528\u5316\u53d1\u5c55\uff0c\u4e3a\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u9002\u5e94\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u5e76\u4e3a\u672a\u6765\u7528\u6237\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.17183", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17183", "abs": "https://arxiv.org/abs/2512.17183", "authors": ["Gang Zhang"], "title": "Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots", "comment": null, "summary": "We present an innovative end-to-end framework for synthesizing semantically meaningful co-speech gestures and deploying them in real-time on a humanoid robot. This system addresses the challenge of creating natural, expressive non-verbal communication for robots by integrating advanced gesture generation techniques with robust physical control. Our core innovation lies in the meticulous integration of a semantics-aware gesture synthesis module, which derives expressive reference motions from speech input by leveraging a generative retrieval mechanism based on large language models (LLMs) and an autoregressive Motion-GPT model. This is coupled with a high-fidelity imitation learning control policy, the MotionTracker, which enables the Unitree G1 humanoid robot to execute these complex motions dynamically and maintain balance. To ensure feasibility, we employ a robust General Motion Retargeting (GMR) method to bridge the embodiment gap between human motion data and the robot platform. Through comprehensive evaluation, we demonstrate that our combined system produces semantically appropriate and rhythmically coherent gestures that are accurately tracked and executed by the physical robot. To our knowledge, this work represents a significant step toward general real-world use by providing a complete pipeline for automatic, semantic-aware, co-speech gesture generation and synchronized real-time physical deployment on a humanoid robot.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u8bed\u4e49\u76f8\u5173\u7684\u4f34\u968f\u8bed\u97f3\u624b\u52bf\u5e76\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u65f6\u90e8\u7f72\uff0c\u5b9e\u73b0\u81ea\u7136\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u975e\u8bed\u8a00\u4ea4\u6d41\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u81ea\u7136\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u975e\u8bed\u8a00\u4ea4\u6d41\u6311\u6218\uff0c\u5c06\u5148\u8fdb\u7684\u624b\u52bf\u751f\u6210\u6280\u672f\u4e0e\u7a33\u5065\u7684\u7269\u7406\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u4f34\u968f\u8bed\u97f3\u624b\u52bf\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "1) \u8bed\u4e49\u611f\u77e5\u624b\u52bf\u5408\u6210\u6a21\u5757\uff1a\u5229\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u68c0\u7d22\u673a\u5236\u548c\u81ea\u56de\u5f52Motion-GPT\u6a21\u578b\uff0c\u4ece\u8bed\u97f3\u8f93\u5165\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u53c2\u8003\u52a8\u4f5c\uff1b2) \u9ad8\u4fdd\u771f\u6a21\u4eff\u5b66\u4e60\u63a7\u5236\u7b56\u7565MotionTracker\uff1a\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u52a8\u6001\u6267\u884c\u590d\u6742\u52a8\u4f5c\u5e76\u4fdd\u6301\u5e73\u8861\uff1b3) \u901a\u7528\u8fd0\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5\uff1a\u5f25\u5408\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u4e0e\u673a\u5668\u4eba\u5e73\u53f0\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8bed\u4e49\u6070\u5f53\u3001\u8282\u594f\u8fde\u8d2f\u7684\u624b\u52bf\uff0c\u5e76\u80fd\u88ab\u7269\u7406\u673a\u5668\u4eba\u51c6\u786e\u8ddf\u8e2a\u548c\u6267\u884c\uff0c\u5b9e\u73b0\u4e86\u4f34\u968f\u8bed\u97f3\u624b\u52bf\u7684\u81ea\u52a8\u751f\u6210\u4e0e\u5b9e\u65f6\u7269\u7406\u90e8\u7f72\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5411\u901a\u7528\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u81ea\u52a8\u3001\u8bed\u4e49\u611f\u77e5\u7684\u4f34\u968f\u8bed\u97f3\u624b\u52bf\u751f\u6210\u548c\u540c\u6b65\u5b9e\u65f6\u7269\u7406\u90e8\u7f72\u7684\u5b8c\u6574\u6d41\u7a0b\u3002"}}
{"id": "2512.17212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17212", "abs": "https://arxiv.org/abs/2512.17212", "authors": ["Yan Gao", "Jiliang Wang", "Ming Cheng", "Tianyun Huang"], "title": "Design and Research of a Self-Propelled Pipeline Robot Based on Force Analysis and Dynamic Simulation", "comment": "7 pages, 14 figures", "summary": "In pipeline inspection, traditional tethered inspection robots are severely constrained by cable length and weight, which greatly limit their travel range and accessibility. To address these issues, this paper proposes a self-propelled pipeline robot design based on force analysis and dynamic simulation, with a specific focus on solving core challenges including vertical climbing failure and poor passability in T-branch pipes. Adopting a wheeled configuration and modular design, the robot prioritizes the core demand of body motion control. Specifically, 3D modeling of the robot was first completed using SolidWorks. Subsequently, the model was imported into ADAMS for dynamic simulation, which provided a basis for optimizing the drive module and motion control strategy.To verify the robot's dynamic performance, an experimental platform with acrylic pipes was constructed. Through adjusting its body posture to surmount obstacles and select directions, the robot has demonstrated its ability to stably traverse various complex pipeline scenarios. Notably, this work offers a technical feasibility reference for the application of pipeline robots in the inspection of medium and low-pressure urban gas pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529b\u5206\u6790\u548c\u52a8\u6001\u4eff\u771f\u7684\u81ea\u9a71\u52a8\u7ba1\u9053\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u91cd\u70b9\u89e3\u51b3\u5782\u76f4\u722c\u5347\u5931\u8d25\u548cT\u578b\u5206\u652f\u7ba1\u9053\u901a\u8fc7\u6027\u5dee\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u57ce\u5e02\u4e2d\u4f4e\u538b\u71c3\u6c14\u7ba1\u9053\u68c0\u6d4b\u63d0\u4f9b\u6280\u672f\u53ef\u884c\u6027\u53c2\u8003\u3002", "motivation": "\u4f20\u7edf\u6709\u7ebf\u7ba1\u9053\u68c0\u6d4b\u673a\u5668\u4eba\u53d7\u7535\u7f06\u957f\u5ea6\u548c\u91cd\u91cf\u9650\u5236\u4e25\u91cd\uff0c\u6781\u5927\u9650\u5236\u4e86\u5176\u79fb\u52a8\u8303\u56f4\u548c\u53ef\u8fbe\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u53d7\u7535\u7f06\u7ea6\u675f\u7684\u81ea\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f6e\u5f0f\u914d\u7f6e\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u9996\u5148\u4f7f\u7528SolidWorks\u5b8c\u6210\u673a\u5668\u4eba3D\u5efa\u6a21\uff0c\u7136\u540e\u5bfc\u5165ADAMS\u8fdb\u884c\u52a8\u6001\u4eff\u771f\uff0c\u4f18\u5316\u9a71\u52a8\u6a21\u5757\u548c\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u6700\u540e\u642d\u5efa\u4e9a\u514b\u529b\u7ba1\u9053\u5b9e\u9a8c\u5e73\u53f0\u9a8c\u8bc1\u52a8\u6001\u6027\u80fd\u3002", "result": "\u673a\u5668\u4eba\u901a\u8fc7\u8c03\u6574\u8eab\u4f53\u59ff\u6001\u514b\u670d\u969c\u788d\u548c\u9009\u62e9\u65b9\u5411\uff0c\u80fd\u591f\u7a33\u5b9a\u7a7f\u8d8a\u5404\u79cd\u590d\u6742\u7ba1\u9053\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u5782\u76f4\u722c\u5347\u548cT\u578b\u5206\u652f\u7ba1\u9053\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u901a\u8fc7\u6027\u3002", "conclusion": "\u8be5\u81ea\u9a71\u52a8\u7ba1\u9053\u673a\u5668\u4eba\u8bbe\u8ba1\u4e3a\u89e3\u51b3\u4f20\u7edf\u6709\u7ebf\u673a\u5668\u4eba\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u4e3a\u57ce\u5e02\u4e2d\u4f4e\u538b\u71c3\u6c14\u7ba1\u9053\u68c0\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u6280\u672f\u53ef\u884c\u6027\u53c2\u8003\u3002"}}
{"id": "2512.17215", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17215", "abs": "https://arxiv.org/abs/2512.17215", "authors": ["Yan Gao", "Jiliang Wang", "Minghan Wang", "Xiaohua Chen", "Demin Chen", "Zhiyong Ren", "Tian-Yun Huang"], "title": "Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines", "comment": "8 pages, 9 figures", "summary": "In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u7ba1\u9053\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7IMU\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u878d\u5408\u89e3\u51b3\u590d\u6742\u5f2f\u66f2\u7ba1\u9053\u5b9a\u4f4d\u95ee\u9898", "motivation": "\u73b0\u6709\u7ba1\u9053\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u5b9a\u4f4d\u4eea\u5668\uff0c\u5728\u590d\u6742\u5f2f\u66f2\u7ba1\u9053\u573a\u666f\u4e2d\u5e38\u56e0\u7535\u7f06\u7f20\u7ed5\u548c\u8bbe\u5907\u7075\u6d3b\u6027\u4e0d\u8db3\u800c\u5931\u6548\uff1b\u4f20\u7edf\u89c6\u89c9\u548c\u6fc0\u5149\u5efa\u56fe\u65b9\u6cd5\u5728\u7ba1\u9053\u53d7\u9650\u7a7a\u95f4\u5185\u6613\u53d7\u5149\u7167\u6761\u4ef6\u548c\u7279\u5f81\u4e0d\u8db3\u5f71\u54cd\uff0c\u5bfc\u81f4\u5efa\u56fe\u6f02\u79fb\u548c\u53d1\u6563\u95ee\u9898", "method": "\u8bbe\u8ba1\u81ea\u9a71\u5f0f\u7ba1\u9053\u673a\u5668\u4eba\uff0c\u63d0\u51fa\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7IMU\u83b7\u53d6\u521d\u59cb\u59ff\u6001\u89d2\uff0c\u7136\u540e\u7528EKF\u7b97\u6cd5\u63d0\u9ad8\u59ff\u6001\u89d2\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u6700\u540e\u7ed3\u5408\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7ba1\u9053\u5b9a\u4f4d", "result": "\u5728\u77e9\u5f62\u73af\u5f62\u7ba1\u9053\u4e2d\u4f7f\u7528\u81ea\u9a71\u5f0f\u7ba1\u9053\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u822a\u4f4d\u63a8\u7b97\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff1b\u9700\u8981\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u80fd\u529b\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u907f\u514d\u6eda\u8f6e\u8fc7\u7d27\u5bfc\u81f4\u6469\u64e6\u529b\u8fc7\u5927\u5f71\u54cd\u8fd0\u52a8\u63a7\u5236\u7075\u6d3b\u6027", "conclusion": "\u57fa\u4e8e\u60ef\u6027\u5bfc\u822a\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u878d\u5408\u7684\u7ba1\u9053\u5b9a\u4f4d\u65b9\u6cd5\u53d7\u7ba1\u9053\u73af\u5883\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\uff0c\u63d0\u51fa\u7684EKF\u5b9a\u4f4d\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u5f2f\u66f2\u7ba1\u9053\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u7ba1\u9053\u673a\u5668\u4eba\u81ea\u4e3b\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.17241", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.17241", "abs": "https://arxiv.org/abs/2512.17241", "authors": ["Suraj Nukala", "Meera Sushma", "Leimin Tian", "Akansel Cosgun", "Dana Kulic"], "title": "A Service Robot's Guide to Interacting with Busy Customers", "comment": "Presented at ACRA 2025. 10 pages, 4 figures. Includes a user study (N=24) using the Temi robot evaluating speech, visual, and micromotion modalities", "summary": "The growing use of service robots in hospitality highlights the need to understand how to effectively communicate with pre-occupied customers. This study investigates the efficacy of commonly used communication modalities by service robots, namely, acoustic/speech, visual display, and micromotion gestures in capturing attention and communicating intention with a user in a simulated restaurant scenario. We conducted a two-part user study (N=24) using a Temi robot to simulate delivery tasks, with participants engaged in a typing game (MonkeyType) to emulate a state of busyness. The participants' engagement in the typing game is measured by words per minute (WPM) and typing accuracy. In Part 1, we compared non-verbal acoustic cue versus baseline conditions to assess attention capture during a single-cup delivery task. In Part 2, we evaluated the effectiveness of speech, visual display, micromotion and their multimodal combination in conveying specific intentions (correct cup selection) during a two-cup delivery task. The results indicate that, while speech is highly effective in capturing attention, it is less successful in clearly communicating intention. Participants rated visual as the most effective modality for intention clarity, followed by speech, with micromotion being the lowest ranked.These findings provide insights into optimizing communication strategies for service robots, highlighting the distinct roles of attention capture and intention communication in enhancing user experience in dynamic hospitality settings.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u670d\u52a1\u673a\u5668\u4eba\u4e0d\u540c\u6c9f\u901a\u6a21\u5f0f\uff08\u8bed\u97f3\u3001\u89c6\u89c9\u663e\u793a\u3001\u5fae\u52a8\u4f5c\uff09\u5728\u7e41\u5fd9\u9910\u5385\u573a\u666f\u4e2d\u5438\u5f15\u6ce8\u610f\u529b\u548c\u4f20\u8fbe\u610f\u56fe\u7684\u6548\u679c\uff0c\u53d1\u73b0\u8bed\u97f3\u6700\u6709\u6548\u5438\u5f15\u6ce8\u610f\u529b\uff0c\u4f46\u89c6\u89c9\u663e\u793a\u6700\u6e05\u6670\u4f20\u8fbe\u610f\u56fe\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u673a\u5668\u4eba\u5728\u9152\u5e97\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e86\u89e3\u5982\u4f55\u4e0e\u5fd9\u788c\u7684\u987e\u5ba2\u6709\u6548\u6c9f\u901a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u6c9f\u901a\u6a21\u5f0f\u5728\u5438\u5f15\u6ce8\u610f\u529b\u548c\u4f20\u8fbe\u610f\u56fe\u65b9\u9762\u7684\u6548\u679c\uff0c\u4ee5\u4f18\u5316\u670d\u52a1\u673a\u5668\u4eba\u7684\u6c9f\u901a\u7b56\u7565\u3002", "method": "\u91c7\u7528\u4e24\u90e8\u5206\u7528\u6237\u7814\u7a76\uff08N=24\uff09\uff0c\u4f7f\u7528Temi\u673a\u5668\u4eba\u6a21\u62df\u9910\u5385\u9001\u9910\u4efb\u52a1\u3002\u53c2\u4e0e\u8005\u901a\u8fc7\u6253\u5b57\u6e38\u620f\uff08MonkeyType\uff09\u6a21\u62df\u5fd9\u788c\u72b6\u6001\u3002\u7b2c\u4e00\u90e8\u5206\u6bd4\u8f83\u975e\u8bed\u8a00\u58f0\u97f3\u63d0\u793a\u4e0e\u57fa\u7ebf\u6761\u4ef6\u5728\u5355\u676f\u9001\u9910\u4efb\u52a1\u4e2d\u7684\u6ce8\u610f\u529b\u5438\u5f15\u6548\u679c\uff1b\u7b2c\u4e8c\u90e8\u5206\u8bc4\u4f30\u8bed\u97f3\u3001\u89c6\u89c9\u663e\u793a\u3001\u5fae\u52a8\u4f5c\u53ca\u5176\u591a\u6a21\u6001\u7ec4\u5408\u5728\u4e24\u676f\u9001\u9910\u4efb\u52a1\u4e2d\u4f20\u8fbe\u7279\u5b9a\u610f\u56fe\uff08\u6b63\u786e\u9009\u62e9\u676f\u5b50\uff09\u7684\u6548\u679c\u3002", "result": "\u8bed\u97f3\u5728\u5438\u5f15\u6ce8\u610f\u529b\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u6e05\u6670\u4f20\u8fbe\u610f\u56fe\u65b9\u9762\u6548\u679c\u8f83\u5dee\u3002\u53c2\u4e0e\u8005\u8ba4\u4e3a\u89c6\u89c9\u663e\u793a\u662f\u4f20\u8fbe\u610f\u56fe\u6700\u6709\u6548\u7684\u6a21\u5f0f\uff0c\u5176\u6b21\u662f\u8bed\u97f3\uff0c\u5fae\u52a8\u4f5c\u6392\u540d\u6700\u4f4e\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u5438\u5f15\u548c\u610f\u56fe\u4f20\u8fbe\u5728\u670d\u52a1\u673a\u5668\u4eba\u6c9f\u901a\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u670d\u52a1\u673a\u5668\u4eba\u6c9f\u901a\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5728\u52a8\u6001\u9152\u5e97\u73af\u5883\u4e2d\u533a\u5206\u6ce8\u610f\u529b\u5438\u5f15\u548c\u610f\u56fe\u4f20\u8fbe\u7684\u91cd\u8981\u6027\u3002\u89c6\u89c9\u663e\u793a\u5728\u4f20\u8fbe\u660e\u786e\u610f\u56fe\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800c\u8bed\u97f3\u5728\u5438\u5f15\u6ce8\u610f\u529b\u65b9\u9762\u6700\u6709\u6548\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2512.17309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17309", "abs": "https://arxiv.org/abs/2512.17309", "authors": ["Asil Kaan Bozcuoglu", "Ziyuan Liu"], "title": "RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models", "comment": null, "summary": "RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.", "AI": {"tldr": "RecipeMasterLLM\uff1a\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u672c\u4f53\u81ea\u52a8\u751f\u6210\u7cfb\u7edf\uff0c\u5229\u7528\u5fae\u8c03LLM\u6839\u636e\u7528\u6237\u63d0\u793a\u751f\u6210\u7b26\u5408RoboEarth\u6807\u51c6\u5316\u77e5\u8bc6\u56fe\u7684OWL\u52a8\u4f5c\u672c\u4f53\uff0c\u901a\u8fc7RAG\u589e\u5f3a\u73af\u5883\u77e5\u8bc6\u7406\u89e3\u3002", "motivation": "\u4f20\u7edfRoboEarth\u77e5\u8bc6\u56fe\u8c31\u9700\u8981\u5de5\u7a0b\u5e08\u624b\u52a8\u521b\u5efaRDF\u4e09\u5143\u7ec4\u548cOWL\u672c\u4f53\uff0c\u77e5\u8bc6\u83b7\u53d6\u8fc7\u7a0b\u7e41\u7410\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u53ef\u4ee5\u663e\u8457\u81ea\u52a8\u5316\u77e5\u8bc6\u83b7\u53d6\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u77e5\u8bc6\u5171\u4eab\u6548\u7387\u3002", "method": "\u63d0\u51faRecipeMasterLLM\u9ad8\u5c42\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u4e13\u95e8\u9488\u5bf9RoboEarth\u6807\u51c6\u5316\u77e5\u8bc6\u56fe\u5fae\u8c03\u7684LLM\uff0c\u6839\u636e\u7528\u6237\u63d0\u793a\u751f\u6210OWL\u52a8\u4f5c\u672c\u4f53\u3002\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5728RAG\u9636\u6bb5\u63d0\u4f9b\u73af\u5883\u77e5\u8bc6\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u751f\u6210\u51c6\u786e\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7b26\u5408RoboEarth\u6807\u51c6\u5316\u77e5\u8bc6\u56fe\u7684\u52a8\u4f5c\u63cf\u8ff0\uff0c\u76f8\u6bd4\u4f20\u7edf\u624b\u5de5\u521b\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u77e5\u8bc6\u83b7\u53d6\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "LLM\u6280\u672f\u53ef\u4ee5\u6709\u6548\u81ea\u52a8\u5316\u673a\u5668\u4eba\u77e5\u8bc6\u83b7\u53d6\u8fc7\u7a0b\uff0cRecipeMasterLLM\u4e3a\u4e91\u673a\u5668\u4eba\u77e5\u8bc6\u5171\u4eab\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u77e5\u8bc6\u83b7\u53d6\u4ece\u624b\u5de5\u521b\u5efa\u5411AI\u9a71\u52a8\u7684\u8f6c\u53d8\u3002"}}
{"id": "2512.17321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17321", "abs": "https://arxiv.org/abs/2512.17321", "authors": ["Momina Liaqat Ali", "Muhammad Abid"], "title": "Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks", "comment": null, "summary": "Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u4e0e\u795e\u7ecf\u63a7\u5236\u5668\u7684\u4f4e\u5c42\u8fde\u7eed\u8fd0\u52a8\u6267\u884c\u5206\u79bb\uff0c\u89e3\u51b3\u4e86LLM\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u4e0d\u7a33\u5b9a\u3001\u6536\u655b\u6162\u548c\u52a8\u4f5c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u7cfb\u7edf\u7684\u8bed\u8a00\u6761\u4ef6\u63a7\u5236\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u8fde\u7eed\u63a7\u5236\u5e94\u7528\u4e2d\u5b58\u5728\u4e0d\u7a33\u5b9a\u3001\u6536\u655b\u6162\u548c\u4ea7\u751f\u5e7b\u89c9\u52a8\u4f5c\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u795e\u7ecf\u7b26\u53f7\u63a7\u5236\u6846\u67b6\uff1a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u589e\u91cf\u63a7\u5236\u5668\u5728\u8fde\u7eed\u7a7a\u95f4\u6267\u884c\u6709\u754c\u589e\u91cf\u52a8\u4f5c\uff0c\u672c\u5730\u90e8\u7f72\u7684LLM\u89e3\u91ca\u7b26\u53f7\u4efb\u52a1\u3002\u6846\u67b6\u5c06LLM\u9650\u5236\u4e3a\u7b26\u53f7\u8f93\u51fa\uff0c\u5c06\u672a\u89e3\u91ca\u7684\u6267\u884c\u5206\u914d\u7ed9\u5728\u4eba\u5de5\u51e0\u4f55\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u795e\u7ecf\u63a7\u5236\u5668\u3002", "result": "\u5728\u5e73\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u76f8\u6bd4\u7eafLLM\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u548c\u6548\u7387\uff1a\u5e73\u5747\u6b65\u9aa4\u51cf\u5c11\u8d85\u8fc770%\uff0c\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe8.83\u500d\uff0c\u4e14\u5bf9\u8bed\u8a00\u6a21\u578b\u8d28\u91cf\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u5206\u89e3\u4e3a\u8bed\u8a00\u7406\u89e3\u4e0e\u8fde\u7eed\u63a7\u5236\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u65e0\u9700\u5f3a\u5316\u5b66\u4e60\u6216\u6602\u8d35\u63a8\u6f14\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u53ef\u9760\u6709\u6548\u7684\u8bed\u8a00\u5f15\u5bfc\u5177\u8eab\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2512.17349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17349", "abs": "https://arxiv.org/abs/2512.17349", "authors": ["Xijie Huang", "Jinhan Li", "Tianyue Wu", "Xin Zhou", "Zhichao Han", "Fei Gao"], "title": "Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation", "comment": "8 pages, 7 figures", "summary": "Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u5bf9\u6297\u57df\u9002\u5e94\u7684\u6846\u67b6\uff0c\u4f7f\u4ec5\u4f7f\u7528\u5355\u76eeRGB\u56fe\u50cf\u7684\u98de\u884c\u673a\u5668\u4eba\u80fd\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u771f\u5b9e\u4e16\u754c\u5bfc\u822a", "motivation": "\u73b0\u6709\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u6fc0\u5149\u96f7\u8fbe\u548c\u6df1\u5ea6\u76f8\u673a\uff0c\u4f46\u80fd\u5426\u4ec5\u7528\u5355\u76eeRGB\u56fe\u50cf\u5b9e\u73b0\u98de\u884c\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4ecd\u662f\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u7531\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u5728\u4eff\u771f\u4e2d\u5b66\u4e60\u7b56\u7565\u662f\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u5b58\u5728\u663e\u8457\u7684\u4eff\u771f\u5230\u771f\u5b9e\u611f\u77e5\u5dee\u8ddd", "method": "\u63d0\u51fa\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u73af\u5883\u7684\u5149\u771f\u5b9e\u611f\u4e0e\u5bf9\u6297\u57df\u9002\u5e94\u7684\u6846\u67b6\u3002\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u540c\u65f6\u901a\u8fc7\u5bf9\u6297\u57df\u9002\u5e94\u663e\u5f0f\u6700\u5c0f\u5316\u7279\u5f81\u5dee\u5f02\uff0c\u786e\u4fdd\u7b56\u7565\u4f9d\u8d56\u9886\u57df\u4e0d\u53d8\u7684\u7279\u5f81\u7ebf\u7d22", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5230\u7269\u7406\u4e16\u754c\u7684\u9c81\u68d2\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u4f7f\u98de\u884c\u673a\u5668\u4eba\u80fd\u591f\u5728\u5177\u6709\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u548c\u654f\u6377\u7684\u98de\u884c", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5355\u76eeRGB\u56fe\u50cf\u5bfc\u822a\u4e2d\u7684\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\u5b9e\u73b0\u98de\u884c\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4f4e\u6210\u672c\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2512.17370", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17370", "abs": "https://arxiv.org/abs/2512.17370", "authors": ["Deqing Liu", "Yinfeng Gao", "Deheng Qian", "Qichao Zhang", "Xiaoqing Ye", "Junyu Han", "Yupeng Zheng", "Xueyi Liu", "Zhongpu Xia", "Dawei Ding", "Yifeng Pan", "Dongbin Zhao"], "title": "TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data", "comment": null, "summary": "Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.", "AI": {"tldr": "TakeAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u504f\u597d\u7684\u540e\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u63a5\u7ba1\u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u95ed\u73af\u9a7e\u9a76\u6027\u80fd\u5e76\u51cf\u5c11\u7cfb\u7edf\u8131\u79bb\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u9762\u4e34\u5f00\u73af\u8bad\u7ec3\u4e0e\u95ed\u73af\u90e8\u7f72\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f4\u9a7e\u9a76\u5458\u63a5\u7ba1\u548c\u7cfb\u7edf\u8131\u79bb\u3002\u5982\u4f55\u5229\u7528\u8131\u79bb\u573a\u666f\u4e2d\u7684\u4e13\u5bb6\u63a5\u7ba1\u6570\u636e\u6269\u5c55IL\u7b56\u7565\u80fd\u529b\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u4f46\u672a\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faTakeAD\u6846\u67b6\uff1a1) \u8bbe\u8ba1\u53d7\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4eba\u7c7b\u63a5\u7ba1\u673a\u5236\u542f\u53d1\u7684\u4e13\u5bb6\u63a5\u7ba1\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff1b2) \u96c6\u6210\u8fed\u4ee3DAgger\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u548cDPO\u8fdb\u884c\u504f\u597d\u5bf9\u9f50\u7684\u540e\u4f18\u5316\u6846\u67b6\u3002DAgger\u9636\u6bb5\u901a\u8fc7\u76f4\u63a5\u6a21\u4eff\u4e13\u5bb6\u5e72\u9884\u4f7f\u7b56\u7565\u5177\u5907\u5904\u7406\u8131\u79bb\u72b6\u6001\u7684\u57fa\u672c\u80fd\u529b\uff0cDPO\u9636\u6bb5\u5219\u4f18\u5316\u7b56\u7565\u884c\u4e3a\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50\u4e13\u5bb6\u5728\u8131\u79bb\u573a\u666f\u4e2d\u7684\u504f\u597d\u3002", "result": "\u5728\u95ed\u73afBench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u7eafIL\u65b9\u6cd5\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u7efc\u5408\u6d88\u878d\u5b9e\u9a8c\u786e\u8ba4\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "TakeAD\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u8131\u79bb\u72b6\u6001\u7684\u6062\u590d\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5f00\u73af\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u95ed\u73af\u9a7e\u9a76\u6027\u80fd\u3002"}}
{"id": "2512.17425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17425", "abs": "https://arxiv.org/abs/2512.17425", "authors": ["Beatrice Luciani", "Katherine Lin Poggensee", "Heike Vallery", "Alex van den Berg", "Severin David Woernle", "Mostafa Mogharabi", "Stefano Dalla Gasperina", "Laura Marchal-Crespo"], "title": "Personalized Gait Patterns During Exoskeleton-Aided Training May Have Minimal Effect on User Experience. Insights from a Pilot Study", "comment": null, "summary": "Robot-aided gait rehabilitation facilitates high-intensity and repeatable therapy. However, most exoskeletons rely on pre-recorded, non-personalized gait trajectories constrained to the sagittal plane, potentially limiting movement naturalness and user comfort. We present a data-driven gait personalization framework for an exoskeleton that supports multi-planar motion, including hip abduction/adduction and pelvic translation and rotation. Personalized trajectories to individual participants were generated using regression models trained on anthropometric, demographic, and walking speed data from a normative database. In a within-subject experiment involving ten unimpaired participants, these personalized trajectories were evaluated in regard to comfort, naturalness, and overall experience and compared against two standard patterns from the same database: one averaging all the trajectories, and one randomly selected. We did not find relevant differences across pattern conditions, despite all trajectories being executed with high accuracy thanks to a stiff position-derivative controller. We found, however, that pattern conditions in later trials were rated as more comfortable and natural than those in the first trial, suggesting that participants might have adapted to walking within the exoskeleton, regardless of the enforced gait pattern. Our findings highlight the importance of integrating subjective feedback when designing personalized gait controllers and accounting for user adaptation during experimentation.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6b65\u6001\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u591a\u5e73\u9762\u8fd0\u52a8\u7684\u5eb7\u590d\u5916\u9aa8\u9abc\uff0c\u4f46\u5b9e\u9a8c\u53d1\u73b0\u4e2a\u6027\u5316\u8f68\u8ff9\u4e0e\u6807\u51c6\u8f68\u8ff9\u5728\u8212\u9002\u5ea6\u548c\u81ea\u7136\u5ea6\u4e0a\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u7528\u6237\u9002\u5e94\u8fc7\u7a0b\u66f4\u4e3a\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u5eb7\u590d\u5916\u9aa8\u9abc\u5927\u591a\u4f9d\u8d56\u9884\u5f55\u5236\u3001\u975e\u4e2a\u6027\u5316\u7684\u6b65\u6001\u8f68\u8ff9\uff0c\u4e14\u5c40\u9650\u4e8e\u77e2\u72b6\u9762\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u8fd0\u52a8\u7684\u81ea\u7136\u6027\u548c\u7528\u6237\u8212\u9002\u5ea6\u3002\u9700\u8981\u5f00\u53d1\u652f\u6301\u591a\u5e73\u9762\u8fd0\u52a8\u5e76\u80fd\u63d0\u4f9b\u4e2a\u6027\u5316\u6b65\u6001\u8f68\u8ff9\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6b65\u6001\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u57fa\u4e8e\u4eba\u4f53\u6d4b\u91cf\u5b66\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u6b65\u884c\u901f\u5ea6\u6570\u636e\u4ece\u89c4\u8303\u6570\u636e\u5e93\u4e2d\u751f\u6210\u4e2a\u6027\u5316\u8f68\u8ff9\u3002\u572810\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u4e2d\u8fdb\u884c\u53d7\u8bd5\u8005\u5185\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e2a\u6027\u5316\u8f68\u8ff9\u4e0e\u4e24\u79cd\u6807\u51c6\u6a21\u5f0f\uff08\u5e73\u5747\u8f68\u8ff9\u548c\u968f\u673a\u9009\u62e9\u8f68\u8ff9\uff09\u3002", "result": "\u5c3d\u7ba1\u6240\u6709\u8f68\u8ff9\u90fd\u80fd\u901a\u8fc7\u521a\u6027\u4f4d\u7f6e\u5bfc\u6570\u63a7\u5236\u5668\u9ad8\u7cbe\u5ea6\u6267\u884c\uff0c\u4f46\u4e0d\u540c\u6a21\u5f0f\u6761\u4ef6\u4e4b\u95f4\u6ca1\u6709\u53d1\u73b0\u663e\u8457\u5dee\u5f02\u3002\u7136\u800c\uff0c\u540e\u671f\u8bd5\u9a8c\u4e2d\u7684\u6a21\u5f0f\u6761\u4ef6\u6bd4\u7b2c\u4e00\u6b21\u8bd5\u9a8c\u88ab\u8bc4\u4e3a\u66f4\u8212\u9002\u548c\u81ea\u7136\uff0c\u8868\u660e\u53c2\u4e0e\u8005\u53ef\u80fd\u9002\u5e94\u4e86\u5728\u5916\u9aa8\u9abc\u4e2d\u884c\u8d70\uff0c\u4e0e\u5f3a\u5236\u6267\u884c\u7684\u6b65\u6001\u6a21\u5f0f\u65e0\u5173\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u4e2a\u6027\u5316\u6b65\u6001\u63a7\u5236\u5668\u65f6\u6574\u5408\u4e3b\u89c2\u53cd\u9988\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5728\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\u8003\u8651\u7528\u6237\u9002\u5e94\u6027\u7684\u5fc5\u8981\u6027\u3002\u7528\u6237\u5bf9\u5916\u9aa8\u9abc\u7684\u9002\u5e94\u8fc7\u7a0b\u53ef\u80fd\u6bd4\u8f68\u8ff9\u4e2a\u6027\u5316\u672c\u8eab\u5bf9\u8212\u9002\u5ea6\u548c\u81ea\u7136\u5ea6\u611f\u77e5\u6709\u66f4\u5927\u5f71\u54cd\u3002"}}
{"id": "2512.17435", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17435", "abs": "https://arxiv.org/abs/2512.17435", "authors": ["Teng Wang", "Xinxin Zhao", "Wenzhe Cai", "Changyin Sun"], "title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination", "comment": "17 pages, 10 figures. arXiv admin note: text overlap with arXiv:2410.09874", "summary": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.", "AI": {"tldr": "ImagineNav++\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u5730\u56fe\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u60f3\u8c61\u672a\u6765\u89c2\u6d4b\u56fe\u50cf\u548c\u9009\u62e9\u6027\u6ce8\u89c6\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06\u5bfc\u822a\u89c4\u5212\u8f6c\u5316\u4e3a\u7b80\u5355\u7684\"\u6700\u4f73\u89c6\u89d2\u56fe\u50cf\u9009\u62e9\"\u95ee\u9898\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u5bfc\u822a\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bfc\u822a\u65b9\u6cd5\u53d7\u9650\u4e8e\u6587\u672c\u8868\u793a\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7a7a\u95f4\u5360\u7528\u548c\u573a\u666f\u51e0\u4f55\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u5bfc\u822a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u80fd\u5426\u4ec5\u4f7f\u7528\u673a\u8f7dRGB/RGB-D\u6d41\u5b9e\u73b0\u65e0\u5730\u56fe\u89c6\u89c9\u5bfc\u822a\u3002", "method": "\u63d0\u51faImagineNav++\u6846\u67b6\uff1a1\uff09\u672a\u6765\u89c6\u89d2\u60f3\u8c61\u6a21\u5757\uff0c\u901a\u8fc7\u84b8\u998f\u4eba\u7c7b\u5bfc\u822a\u504f\u597d\u751f\u6210\u5177\u6709\u9ad8\u63a2\u7d22\u6f5c\u529b\u7684\u8bed\u4e49\u5316\u89c6\u70b9\uff1b2\uff09\u5c06\u5bfc\u822a\u89c4\u5212\u8f6c\u5316\u4e3aVLM\u7684\u6700\u4f73\u89c6\u89d2\u56fe\u50cf\u9009\u62e9\u95ee\u9898\uff1b3\uff09\u9009\u62e9\u6027\u6ce8\u89c6\u8bb0\u5fc6\u673a\u5236\uff0c\u901a\u8fc7\u7a00\u758f\u5230\u5bc6\u96c6\u7684\u5c42\u6b21\u5316\u96c6\u6210\u6784\u5efa\u7d27\u51d1\u800c\u5168\u9762\u7684\u957f\u671f\u7a7a\u95f4\u8bb0\u5fc6\uff1b4\uff09\u5c06\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u53ef\u5904\u7406\u7684\u70b9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u548c\u5b9e\u4f8b\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cImagineNav++\u5728\u65e0\u5730\u56fe\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u5927\u591a\u6570\u57fa\u4e8e\u5730\u56fe\u7684\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u573a\u666f\u60f3\u8c61\u548c\u8bb0\u5fc6\u5728VLM\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u60f3\u8c61\u672a\u6765\u89c2\u6d4b\u548c\u9009\u62e9\u6027\u8bb0\u5fc6\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u7684\u65e0\u5730\u56fe\u89c6\u89c9\u5bfc\u822a\uff0c\u4e3a\u81ea\u4e3b\u5bb6\u5ead\u8f85\u52a9\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u5bfc\u822a\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u573a\u666f\u60f3\u8c61\u548c\u7a7a\u95f4\u8bb0\u5fc6\u5728VLM\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2512.17505", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.17505", "abs": "https://arxiv.org/abs/2512.17505", "authors": ["Ufuk Asil", "Efendi Nasibov"], "title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry", "comment": null, "summary": "This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u7684\u6df7\u5408\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bef\u5dee\u72b6\u6001EKF\u548cUKF\uff0c\u901a\u8fc7\u52a8\u6001\u4f20\u611f\u5668\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u73af\u5883\u6311\u6218\u548c\u4f20\u611f\u5668\u53ef\u9760\u6027\u53d8\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u53c8\u80fd\u63d0\u4f9b\u51c6\u786e\u59ff\u6001\u4f30\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u677e\u8026\u5408\u4f20\u611f\u5668\u878d\u5408\u67b6\u6784\uff0c\u63d0\u51fa\u6df7\u5408\u56db\u5143\u6570\u8bef\u5dee\u72b6\u6001EKF/UKF\u65b9\u6cd5\uff1a\u5148\u7528\u8bef\u5dee\u72b6\u6001EKF\u4f20\u64ad\u6574\u4e2a\u72b6\u6001\uff0c\u518d\u7528\u7f29\u653eUKF\u4e13\u95e8\u4f18\u5316\u59ff\u6001\u4f30\u8ba1\uff1b\u901a\u8fc7\u56fe\u50cf\u71b5\u3001\u5f3a\u5ea6\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u7b49\u6307\u6807\u52a8\u6001\u8bc4\u4f30\u89c6\u89c9\u6d4b\u91cf\u53ef\u9760\u6027\u3002", "result": "\u5728EuRoC MAV\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u663e\u793a\uff1a\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u4f4d\u7f6e\u7cbe\u5ea6\u5e73\u5747\u63d0\u534749%\uff0c\u65cb\u8f6c\u7cbe\u5ea6\u6bd4ESKF\u65b9\u6cd5\u5e73\u5747\u63d0\u534757%\uff0c\u8ba1\u7b97\u6210\u672c\u6bd4\u5b8c\u6574SUKF\u5b9e\u73b0\u964d\u4f4e\u7ea648%\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u4f30\u8ba1\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u4f20\u611f\u5668\u53ef\u9760\u6027\u53d8\u5316\u7684\u590d\u6742\u73af\u5883\u4e2d\u7684\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2512.17553", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17553", "abs": "https://arxiv.org/abs/2512.17553", "authors": ["Guglielmo Del Col", "V\u00e4in\u00f6 Karjalainen", "Teemu Hakala", "Yibo Zhang", "Eija Honkavaara"], "title": "Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests", "comment": null, "summary": "Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.\n  The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u5bfc\u822a\u6846\u67b6\uff0c\u96c6\u6210\u8bed\u4e49\u589e\u5f3a\u6df1\u5ea6\u7f16\u7801\u4e0e\u795e\u7ecf\u8fd0\u52a8\u57fa\u5143\u8bc4\u4f30\uff0c\u7528\u4e8e\u5bc6\u96c6\u68ee\u6797\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\uff0c\u901a\u8fc7\u591a\u4e2a\u6a21\u5757\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u81ea\u7136\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5305\u62ec\u80fd\u89c1\u5ea6\u6709\u9650\u3001\u7ec6\u8584\u4e0d\u89c4\u5219\u969c\u788d\u7269\u3001GNSS\u4fe1\u53f7\u7f3a\u5931\u4ee5\u53ca\u9891\u7e41\u7684\u611f\u77e5\u9000\u5316\u95ee\u9898\u3002", "method": "\u5728\u539f\u59cbsevae-ORACLE\u7b97\u6cd5\u57fa\u7840\u4e0a\u96c6\u6210\u591a\u4e2a\u6a21\u5757\uff1a\u8bed\u4e49\u589e\u5f3a\u6df1\u5ea6\u7f16\u7801\u3001\u795e\u7ecf\u8fd0\u52a8\u57fa\u5143\u8bc4\u4f30\u3001\u6a2a\u5411\u63a7\u5236\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u673a\u5236\u3001\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u3001\u5b89\u5168\u76d1\u7763\u5c42\u3001\u6df1\u5ea6\u7ec6\u5316\u9636\u6bb5\u548cGPU\u4f18\u5316\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u73af\u5883\u6761\u4ef6\u548c\u786c\u4ef6\u7ea6\u675f\u4e0b\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u7a33\u5b9a\u7684\u8f68\u8ff9\u548c\u66f4\u597d\u7684\u78b0\u649e\u907f\u514d\u80fd\u529b\uff1b\u5728\u4e09\u79cd\u5317\u65b9\u68ee\u6797\u73af\u5883\u4e2d\uff0c\u4e2d\u7b49\u548c\u5bc6\u96c6\u690d\u88ab\u6761\u4ef6\u4e0b\u6240\u6709\u98de\u884c\u5747\u6210\u529f\u5b8c\u6210\uff0c\u9ad8\u5ea6\u5bc6\u96c6\u704c\u6728\u4e1b\u4e2d15\u6b21\u98de\u884c\u6210\u529f12\u6b21\u3002", "conclusion": "\u8be5\u5bfc\u822a\u6846\u67b6\u5728\u590d\u6742\u81ea\u7136\u73af\u5883\u4e2d\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\uff0c\u5b9e\u73b0\u4e86\u5bc6\u96c6\u68ee\u6797\u73af\u5883\u4e2d\u7684\u7a33\u5065\u81ea\u4e3b\u98de\u884c\u3002"}}
{"id": "2512.17560", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17560", "abs": "https://arxiv.org/abs/2512.17560", "authors": ["M. Faroni", "A. Spano", "A. M. Zanchettin", "P. Rocco"], "title": "Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration", "comment": "8 pages", "summary": "Ensuring human safety in collaborative robotics can compromise efficiency because traditional safety measures increase robot cycle time when human interaction is frequent. This paper proposes a safety-aware approach to mitigate efficiency losses without assuming prior knowledge of safety logic. Using a deep-learning model, the robot learns the relationship between system state and safety-induced speed reductions based on execution data. Our framework does not explicitly predict human motions but directly models the interaction effects on robot speed, simplifying implementation and enhancing generalizability to different safety logics. At runtime, the learned model optimizes task selection to minimize cycle time while adhering to safety requirements. Experiments on a pick-and-packaging scenario demonstrated significant reductions in cycle times.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b89\u5168\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b66\u4e60\u7cfb\u7edf\u72b6\u6001\u4e0e\u5b89\u5168\u51cf\u901f\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f18\u5316\u4efb\u52a1\u9009\u62e9\u4ee5\u51cf\u5c11\u534f\u4f5c\u673a\u5668\u4eba\u5faa\u73af\u65f6\u95f4", "motivation": "\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u5728\u9891\u7e41\u4eba\u673a\u4ea4\u4e92\u65f6\u4f1a\u589e\u52a0\u673a\u5668\u4eba\u5faa\u73af\u65f6\u95f4\uff0c\u5f71\u54cd\u6548\u7387\uff0c\u9700\u8981\u5728\u4e0d\u5047\u8bbe\u5b89\u5168\u903b\u8f91\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u6548\u7387\u635f\u5931", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u57fa\u4e8e\u6267\u884c\u6570\u636e\u5b66\u4e60\u7cfb\u7edf\u72b6\u6001\u4e0e\u5b89\u5168\u51cf\u901f\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e0d\u663e\u5f0f\u9884\u6d4b\u4eba\u4f53\u8fd0\u52a8\uff0c\u76f4\u63a5\u5efa\u6a21\u4ea4\u4e92\u5bf9\u673a\u5668\u4eba\u901f\u5ea6\u7684\u5f71\u54cd\uff0c\u8fd0\u884c\u65f6\u4f18\u5316\u4efb\u52a1\u9009\u62e9", "result": "\u5728\u62fe\u53d6\u5305\u88c5\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u5faa\u73af\u65f6\u95f4\u663e\u8457\u51cf\u5c11", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u5b9e\u73b0\uff0c\u589e\u5f3a\u4e86\u5bf9\u4e0d\u540c\u5b89\u5168\u903b\u8f91\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u5728\u9075\u5b88\u5b89\u5168\u8981\u6c42\u7684\u540c\u65f6\u6700\u5c0f\u5316\u5faa\u73af\u65f6\u95f4"}}
{"id": "2512.17568", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17568", "abs": "https://arxiv.org/abs/2512.17568", "authors": ["Kangchen Lv", "Mingrui Yu", "Yongyi Jia", "Chenyu Zhang", "Xiang Li"], "title": "Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation", "comment": "The first two authors contributed equally. Project Website: https://kinematics-aware-diffusion-policy.github.io", "summary": "Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation policy learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e3D\u7a7a\u95f4\u4e00\u81f4\u8868\u793a\u7684\u8fd0\u52a8\u5b66\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u68b0\u81c2\u5168\u8eab\u63a7\u5236\uff0c\u901a\u8fc7\u70b9\u4e91\u8868\u793a\u548c\u6269\u6563\u7b56\u7565\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u5173\u8282\u7a7a\u95f4\u7b56\u7565\u5b66\u4e60\u5b58\u5728\u5173\u8282\u7a7a\u95f4\u4e0e\u4efb\u52a1\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u589e\u52a0\u4e86\u7b56\u7565\u5b66\u4e60\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u5185\u5728\u7406\u89e3\u975e\u7ebf\u6027\u624b\u81c2\u8fd0\u52a8\u5b66\uff0c\u96be\u4ee5\u4ece\u6709\u9650\u6f14\u793a\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u8fd0\u52a8\u5b66\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u75283D\u7a7a\u95f4\u4e2d\u7684\u70b9\u96c6\u8868\u793a\u673a\u5668\u4eba\u72b6\u6001\u548c\u52a8\u4f5c\uff0c\u4e0e3D\u70b9\u4e91\u89c2\u6d4b\u81ea\u7136\u5bf9\u9f50\uff1b\u57fa\u4e8e\u6269\u6563\u7b56\u7565\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u878d\u5165\u8fd0\u52a8\u5b66\u5148\u9a8c\u4fdd\u8bc1\u8f93\u51fa\u52a8\u4f5c\u7684\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\uff1b\u6700\u540e\u901a\u8fc7\u4f18\u5316\u5f0f\u5168\u8eab\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u8ba1\u7b97\u5173\u8282\u89d2\u5ea6\u547d\u4ee4\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8eab\u4f53\u611f\u77e5\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u5f3a\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc73D\u7a7a\u95f4\u4e00\u81f4\u8868\u793a\u548c\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u6269\u6563\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5168\u8eab\u63a7\u5236\u4e2d\u5173\u8282\u7a7a\u95f4\u4e0e\u4efb\u52a1\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7b56\u7565\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u7a7a\u95f4\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.17584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17584", "abs": "https://arxiv.org/abs/2512.17584", "authors": ["Christian Cella", "Sole Ester Sonnino", "Marco Faroni", "Andrea Zanchettin", "Paolo Rocco"], "title": "Optimized Scheduling and Positioning of Mobile Manipulators in Collaborative Applications", "comment": "Accepted at The IFAC Joint Conference on Computers, Cognition and Communication (J3C) 2025", "summary": "The growing integration of mobile robots in shared workspaces requires efficient path planning and coordination between the agents, accounting for safety and productivity. In this work, we propose a digital model-based optimization framework for mobile manipulators in human-robot collaborative environments, in order to determine the sequence of robot base poses and the task scheduling for the robot. The complete problem is treated as black-box, and Particle Swarm Optimization (PSO) is employed to balance conflicting Key-Performance Indicators (KPIs). We demonstrate improvements in cycle time, task sequencing, and adaptation to human presence in a collaborative box-packing scenario.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u6a21\u578b\u7684\u79fb\u52a8\u673a\u68b0\u81c2\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u7c92\u5b50\u7fa4\u7b97\u6cd5\u534f\u8c03\u8def\u5f84\u89c4\u5212\u4e0e\u4efb\u52a1\u8c03\u5ea6\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u88c5\u7bb1\u573a\u666f\u7684\u6548\u7387", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u96c6\u6210\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u548c\u534f\u8c03\u673a\u5236\uff0c\u540c\u65f6\u517c\u987e\u5b89\u5168\u6027\u548c\u751f\u4ea7\u6548\u7387", "method": "\u91c7\u7528\u57fa\u4e8e\u6570\u5b57\u6a21\u578b\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u5b8c\u6574\u95ee\u9898\u89c6\u4e3a\u9ed1\u76d2\uff0c\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u5e73\u8861\u51b2\u7a81\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u786e\u5b9a\u673a\u5668\u4eba\u57fa\u5ea7\u59ff\u6001\u5e8f\u5217\u548c\u4efb\u52a1\u8c03\u5ea6", "result": "\u5728\u534f\u4f5c\u88c5\u7bb1\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5728\u5468\u671f\u65f6\u95f4\u3001\u4efb\u52a1\u6392\u5e8f\u548c\u9002\u5e94\u4eba\u7c7b\u5b58\u5728\u65b9\u9762\u7684\u6539\u8fdb", "conclusion": "\u63d0\u51fa\u7684\u6570\u5b57\u6a21\u578b\u4f18\u5316\u6846\u67b6\u80fd\u6709\u6548\u534f\u8c03\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u4eba\u673a\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u548c\u4efb\u52a1\u8c03\u5ea6\uff0c\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u6027\u80fd"}}
{"id": "2512.17661", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17661", "abs": "https://arxiv.org/abs/2512.17661", "authors": ["Yao Feng", "Chendong Xiang", "Xinyi Mao", "Hengkai Tan", "Zuyue Zhang", "Shuhe Huang", "Kaiwen Zheng", "Haitian Liu", "Hang Su", "Jun Zhu"], "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control", "comment": null, "summary": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.", "AI": {"tldr": "Vidarc\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u9006\u52a8\u529b\u5b66\u6a21\u578b\u548c\u81ea\u56de\u5f52\u751f\u6210\u5b9e\u73b0\u5feb\u901f\u95ed\u73af\u63a7\u5236\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\uff0c\u673a\u5668\u4eba\u624b\u81c2\u64cd\u4f5c\u9762\u4e34\u590d\u6742\u52a8\u529b\u5b66\u548c\u591a\u6837\u5316\u573a\u666f\u7684\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u9891\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u65f6\u7a7a\u4ea4\u4e92\uff0c\u4f46\u901a\u5e38\u672a\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u7684\u95ed\u73af\u63a7\u5236\u8fdb\u884c\u4f18\u5316\uff0c\u5b58\u5728\u5ef6\u8fdf\u9ad8\u548c\u57fa\u7840\u4e0d\u7262\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVidarc\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u81ea\u56de\u5f52\u4f53\u73b0\u63a7\u5236\u6846\u67b6\uff1b2\uff09\u4f7f\u7528\u63a9\u7801\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u4f5c\u76f8\u5173\u63a9\u7801\u6765\u57fa\u7840\u5316\u89c6\u9891\u9884\u6d4b\uff1b3\uff09\u901a\u8fc7\u7f13\u5b58\u81ea\u56de\u5f52\u751f\u6210\u5b9e\u73b0\u5b9e\u65f6\u53cd\u9988\uff1b4\uff09\u5728100\u4e07\u8de8\u4f53\u73b0\u7247\u6bb5\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "Vidarc\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u81f3\u5c11\u63d0\u9ad815%\u7684\u6210\u529f\u7387\uff0c\u5ef6\u8fdf\u964d\u4f4e91%\u3002\u540c\u65f6\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u548c\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002", "conclusion": "Vidarc\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u6269\u6563\u3001\u63a9\u7801\u9006\u52a8\u529b\u5b66\u548c\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u51c6\u786e\u7684\u95ed\u73af\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.17680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17680", "abs": "https://arxiv.org/abs/2512.17680", "authors": ["Ana Stankovic", "Mohamed Khalil Ben-Larbi", "Wolfgang H. M\u00fcller"], "title": "A Dual Quaternion based RRT* Path Planning Approach for Satellite Rendezvous and Docking", "comment": "6 pages, CAMSAT 2025, This work has been accepted to IFAC", "summary": "This paper proposes a sampling-based motion planner that employs a dual quaternion representation to generate smooth, collision-free six-degree-of-freedom pose trajectories for satellite rendezvous and docking under keep-out zone constraints. The proposed planner integrates the dual quaternion algebra directly into an RRT* framework, thereby enabling natural screw motion interpolation in SE(3). The dual quaternion-based RRT* has been implemented in Python and demonstrated on a representative multi-obstacle scenario. A comparison with a standard RRT* using separate translation and quaternion steering highlights the enhanced pose continuity and obstacle avoidance of the proposed method. The present approach is purely kinematic in nature and does not take into account relative orbital dynamics. Consequently, the resulting path provides a preliminary estimate for a subsequent optimisation-based trajectory planner, which will refine the motion with dynamic constraints for the purpose of practical satellite rendezvous and docking missions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u5076\u56db\u5143\u6570\u7684RRT*\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u751f\u6210\u536b\u661f\u4ea4\u4f1a\u5bf9\u63a5\u7684\u5e73\u6ed1\u516d\u81ea\u7531\u5ea6\u4f4d\u59ff\u8f68\u8ff9\uff0c\u5728\u907f\u969c\u7ea6\u675f\u4e0b\u5b9e\u73b0\u81ea\u7136\u87ba\u65cb\u8fd0\u52a8\u63d2\u503c\u3002", "motivation": "\u536b\u661f\u4ea4\u4f1a\u5bf9\u63a5\u9700\u8981\u751f\u6210\u5e73\u6ed1\u7684\u516d\u81ea\u7531\u5ea6\u4f4d\u59ff\u8f68\u8ff9\uff0c\u540c\u65f6\u6ee1\u8db3\u907f\u969c\u7ea6\u675f\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u5e73\u79fb\u548c\u65cb\u8f6c\u5206\u5f00\u5904\u7406\uff0c\u96be\u4ee5\u4fdd\u8bc1\u4f4d\u59ff\u8fde\u7eed\u6027\uff0c\u9700\u8981\u66f4\u81ea\u7136\u7684SE(3)\u7a7a\u95f4\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5c06\u5bf9\u5076\u56db\u5143\u6570\u4ee3\u6570\u76f4\u63a5\u96c6\u6210\u5230RRT*\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u5bf9\u5076\u56db\u5143\u6570\u5728SE(3)\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u81ea\u7136\u87ba\u65cb\u8fd0\u52a8\u63d2\u503c\uff0c\u751f\u6210\u5e73\u6ed1\u7684\u516d\u81ea\u7531\u5ea6\u4f4d\u59ff\u8f68\u8ff9\u3002", "result": "\u4e0e\u4f7f\u7528\u5206\u79bb\u5e73\u79fb\u548c\u56db\u5143\u6570\u8f6c\u5411\u7684\u6807\u51c6RRT*\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728Python\u5b9e\u73b0\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u4f4d\u59ff\u8fde\u7eed\u6027\u548c\u907f\u969c\u80fd\u529b\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u591a\u969c\u788d\u7269\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7eaf\u8fd0\u52a8\u5b66\u65b9\u6cd5\uff0c\u4e0d\u8003\u8651\u76f8\u5bf9\u8f68\u9053\u52a8\u529b\u5b66\uff0c\u751f\u6210\u7684\u8def\u5f84\u53ef\u4f5c\u4e3a\u540e\u7eed\u57fa\u4e8e\u4f18\u5316\u7684\u8f68\u8ff9\u89c4\u5212\u5668\u7684\u521d\u6b65\u4f30\u8ba1\uff0c\u4e3a\u5b9e\u9645\u536b\u661f\u4ea4\u4f1a\u5bf9\u63a5\u4efb\u52a1\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2512.17764", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17764", "abs": "https://arxiv.org/abs/2512.17764", "authors": ["Kangchen Lv", "Mingrui Yu", "Shihefeng Wang", "Xiangyang Ji", "Xiang Li"], "title": "UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation", "comment": "The first two authors contributed equally. Project page: https://unistatedlo.github.io", "summary": "Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.", "AI": {"tldr": "UniStateDLO\uff1a\u9996\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5b8c\u6574\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u611f\u77e5\u7ba1\u9053\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u4e25\u91cd\u906e\u6321\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u4e0e\u8ddf\u8e2a\uff0c\u4ec5\u9700\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5bf9\u906e\u6321\u9ad8\u5ea6\u654f\u611f\uff0c\u5728\u53d7\u9650\u64cd\u4f5c\u73af\u5883\u4e2d\u9762\u4e34\u81ea\u906e\u6321\u3001\u591a\u7269\u4f53\u906e\u6321\u7b49\u6311\u6218\uff0c\u4e14\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u3001\u7f3a\u4e4f\u89c6\u89c9\u7279\u5f81\u548c\u4f20\u611f\u5668\u566a\u58f0\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86\u53ef\u9760\u611f\u77e5\u7684\u96be\u5ea6\u3002", "method": "\u5c06\u5355\u5e27\u72b6\u6001\u4f30\u8ba1\u548c\u8de8\u5e27\u72b6\u6001\u8ddf\u8e2a\u90fd\u6784\u5efa\u4e3a\u6761\u4ef6\u751f\u6210\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5f3a\u5927\u7684\u6620\u5c04\u80fd\u529b\uff0c\u4ece\u90e8\u5206\u70b9\u4e91\u5230\u9ad8\u7ef4DLO\u72b6\u6001\u3002\u4ec5\u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u6570\u636e\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5373\u4f7f\u5728\u4e25\u91cd\u906e\u6321\u4e0b\u4e5f\u80fd\u5b9e\u65f6\u751f\u6210\u5168\u5c40\u5e73\u6ed1\u4e14\u5c40\u90e8\u7cbe\u786e\u7684DLO\u72b6\u6001\u9884\u6d4b\uff0c\u652f\u6301\u95ed\u73afDLO\u64cd\u4f5c\u7cfb\u7edf\u7684\u7a33\u5b9a\u53cd\u9988\u63a7\u5236\u3002", "conclusion": "UniStateDLO\u662f\u9996\u4e2a\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60DLO\u611f\u77e5\u7ba1\u9053\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u79cd\u906e\u6321\u6a21\u5f0f\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6570\u636e\u6548\u7387\u548c\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u53d7\u9650\u73af\u5883\u4e2d\u7684DLO\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u524d\u7aef\u6a21\u5757\u3002"}}
{"id": "2512.17846", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17846", "abs": "https://arxiv.org/abs/2512.17846", "authors": ["Carlos V\u00e9lez Garc\u00eda", "Miguel Cazorla", "Jorge Pomares"], "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes", "comment": null, "summary": "We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.\n  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.\n  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.", "AI": {"tldr": "PaD\u662f\u4e00\u4e2a\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u9a71\u52a8\u8f68\u8ff9\u5408\u6210\uff0c\u5b66\u4e60\u76ee\u6807\u6761\u4ef6\u80fd\u91cf\u51fd\u6570\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u89c4\u5212\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u4f7f\u7528\u76f8\u540c\u8ba1\u7b97\u51cf\u5c11\u4e0d\u5339\u914d\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3-\u6d4b\u8bd5\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u89e3\u8026\u5efa\u6a21\u7ba1\u9053\u4e2d\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u4e0d\u540c\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u5e0c\u671b\u901a\u8fc7\u9a8c\u8bc1\u9a71\u52a8\u7684\u89c4\u5212\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5b66\u4e60\u76ee\u6807\u6761\u4ef6\u80fd\u91cf\u51fd\u6570\uff0c\u4e3a\u53ef\u884c\u4e14\u76ee\u6807\u4e00\u81f4\u7684\u672a\u6765\u5206\u914d\u4f4e\u80fd\u91cf\u3002\u89c4\u5212\u901a\u8fc7\u5728\u8be5\u80fd\u91cf\u666f\u89c2\u4e2d\u8fdb\u884c\u68af\u5ea6\u4f18\u5316\u5b9e\u73b0\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u4f7f\u7528\u76f8\u540c\u8ba1\u7b97\u3002\u901a\u8fc7\u81ea\u76d1\u7763\u540e\u89c6\u76ee\u6807\u91cd\u6807\u8bb0\u8bad\u7ec3\uff0c\u5728\u89c4\u5212\u52a8\u6001\u5468\u56f4\u5851\u9020\u80fd\u91cf\u666f\u89c2\u3002\u63a8\u7406\u65f6\u5728\u4e0d\u540c\u65f6\u95f4\u5047\u8bbe\u4e0b\u4f18\u5316\u591a\u4e2a\u8f68\u8ff9\u5019\u9009\uff0c\u9009\u62e9\u5e73\u8861\u53ef\u884c\u6027\u548c\u6548\u7387\u7684\u4f4e\u80fd\u91cf\u8ba1\u5212\u3002", "result": "\u5728OGBench\u7acb\u65b9\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8bc4\u4f30\u3002\u5728\u72ed\u7a84\u4e13\u5bb6\u6f14\u793a\u4e0a\u8bad\u7ec3\u65f6\u8fbe\u523095%\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d68%\u7684\u65b9\u6cd5\u3002\u5728\u566a\u58f0\u6b21\u4f18\u6570\u636e\u4e0a\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u8ba1\u5212\u6548\u7387\u3002", "conclusion": "\u5b66\u4e60\u8bc4\u4f30\u548c\u4f18\u5316\u8f68\u8ff9\u4e3a\u79bb\u7ebf\u3001\u65e0\u5956\u52b1\u89c4\u5212\u63d0\u4f9b\u4e86\u6bd4\u76f4\u63a5\u7b56\u7565\u5b66\u4e60\u66f4\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9a8c\u8bc1\u9a71\u52a8\u7684\u89c4\u5212\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
