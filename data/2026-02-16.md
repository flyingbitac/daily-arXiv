<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning](https://arxiv.org/abs/2602.12314)
*Junwoon Lee,Yulun Tian*

Main category: cs.RO

TL;DR: LatentAM是一个在线3D高斯泼溅建图框架，通过流式RGB-D观测构建可扩展的潜在特征地图，用于开放词汇机器人感知。它采用模型无关、无需预训练的在线字典学习方法，实现与不同视觉语言模型的即插即用集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要蒸馏高维视觉语言模型嵌入并使用模型特定的解码器，这限制了灵活性和可扩展性。需要一种能够在线适应场景语义变化、支持不同视觉语言模型、并能扩展到长轨迹和大环境的建图框架。

Method: 1. 为每个高斯基元关联紧凑查询向量，通过注意力机制与可学习字典转换为近似视觉语言模型嵌入；2. 从流式观测高效初始化字典，在信任域正则化下在线优化以适应场景语义变化；3. 基于体素哈希的高效地图管理策略，GPU上优化活动局部地图，CPU上存储和索引全局地图以控制内存使用。

Result: 在公共基准测试和大规模自定义数据集上的实验表明，LatentAM相比最先进方法显著提高了特征重建保真度，同时在评估数据集上达到接近实时的速度（12-35 FPS）。

Conclusion: LatentAM提出了一种模型无关、无需预训练的在线字典学习方法，能够构建可扩展的潜在特征地图，支持开放词汇机器人感知，在保持高保真度的同时实现接近实时的性能。

Abstract: We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM

</details>


### [2] [ForeAct: Steering Your VLA with Efficient Visual Foresight Planning](https://arxiv.org/abs/2602.12322)
*Zhuoyang Zhang,Shang Yang,Qinghao Hu,Luke J. Huang,James Hou,Yufei Sun,Yao Lu,Song Han*

Main category: cs.RO

TL;DR: ForeAct是一个视觉前瞻规划器，通过生成未来观测图像来指导VLA模型逐步执行任务，在开放世界环境中显著提升多步任务的成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在开放世界环境中将高级语言指令转换为具体可执行动作具有挑战性，需要更好的规划机制来提升准确性和泛化能力。

Method: 提出Visual Foresight Planning框架，包含高效的前瞻图像生成模块（0.33秒内生成640×480未来观测）和视觉语言模型推理模块。前瞻生成器在超过100万跨具身多任务片段上预训练，学习鲁棒的具身动力学。

Result: 在11个多样化多步真实世界任务基准测试中，平均成功率87.4%，比基线π0（46.5%）提升40.9%，比带文本子任务指导的π0（57.1%）提升30.3%。

Conclusion: ForeAct通过视觉前瞻规划显著提升了VLA模型在复杂开放世界任务中的性能，且无需修改现有VLA架构即可无缝集成。

Abstract: Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).

</details>


### [3] [Schur-MI: Fast Mutual Information for Robotic Information Gathering](https://arxiv.org/abs/2602.12346)
*Kalvik Jakkala,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: Schur-MI是一种高斯过程互信息计算方法，通过预计算和Schur补分解将计算复杂度从O(|V|³)降低到O(|A|³)，实现12.7倍加速，使互信息目标能用于机器人实时信息采集规划。


<details>
  <summary>Details</summary>
Motivation: 互信息(MI)是机器人信息采集(RIG)的理论基础，但计算成本高（主要来自重复的对数行列式计算），限制了其在实时规划中的应用。

Method: 提出Schur-MI方法：(1)利用RIG的迭代结构预计算和重用昂贵的中间量；(2)使用Schur补分解避免大型行列式计算。

Result: 在真实世界海底地形数据集上，Schur-MI比标准MI实现最高12.7倍加速；通过自主水面车辆(ASV)的自适应信息路径规划(IPP)现场试验验证了实用性。

Conclusion: Schur-MI使MI计算在在线规划中变得可行，有助于弥合信息论目标与实时机器人探索之间的差距。

Abstract: Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\mathcal{O}(|\mathcal{V}|^3)$ to $\mathcal{O}(|\mathcal{A}|^3)$, where $\mathcal{V}$ and $\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.

</details>


### [4] [LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation](https://arxiv.org/abs/2602.12351)
*Yue Hu,Avery Xi,Qixin Xiao,Seth Isaacson,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: LongNav-R1：一个端到端多轮强化学习框架，通过将导航决策过程重新定义为VLA策略与环境之间的连续多轮对话，优化视觉-语言-动作模型的长时程导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有单轮范式无法让智能体推理历史交互的因果效应和序列未来结果，且依赖人类演示导致行为僵化。需要开发能够从在线交互中直接学习、支持多样化轨迹生成的多轮RL框架。

Method: 1) 提出LongNav-R1多轮RL框架，将导航决策重构为VLA策略与环境间的连续对话；2) 引入Horizon-Adaptive Policy Optimization机制，在优势估计中显式考虑不同时程长度，实现扩展序列上的准确时间信用分配。

Result: 在物体导航基准测试中，仅用4,000条轨迹，LongNav-R1将Qwen3-VL-2B的成功率从64.3%提升至73.0%。在长时程真实世界导航设置中表现出零样本泛化能力和鲁棒性，显著优于现有最先进方法。

Conclusion: LongNav-R1通过多轮RL框架和时程自适应策略优化，使智能体能够发展多样化导航行为，避免长时程任务中的崩溃，展示了卓越的样本效率和泛化能力。所有源代码将在发表后开源。

Abstract: This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework's efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model's generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.

</details>


### [5] [Predicting Dynamic Map States from Limited Field-of-View Sensor Data](https://arxiv.org/abs/2602.12360)
*Knut Peterson,David Han*

Main category: cs.RO

TL;DR: 该研究探索了在有限视场约束下，使用深度学习进行动态地图状态预测的有效性，通过将时空信息编码为单图像格式，利用图像到图像学习模型实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，自主系统的传感器常受限于有限的视场约束，这可能是系统设计本身造成的，也可能是由意外遮挡或传感器故障导致的。当无法获得大视场时，需要基于可用数据推断环境信息并预测附近状态，以维持安全和准确的操作。

Method: 将动态传感器数据表示为简单的单图像格式，该格式同时捕捉空间和时间信息，然后利用各种现有的图像到图像学习模型来预测地图状态。

Result: 该方法能够在多种传感场景下，以高精度预测地图状态，证明了深度学习在有限视场时间序列数据上进行动态地图状态预测的有效性。

Conclusion: 通过将时空信息编码为单图像表示，可以利用现有的图像到图像学习模型有效地预测动态地图状态，为有限视场约束下的自主系统提供了实用的解决方案。

Abstract: When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.

</details>


### [6] [Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling](https://arxiv.org/abs/2602.12385)
*Anuj Pokhrel,Aniket Datar,Mohammad Nazeri,Francesco Cancelliere,Xuesu Xiao*

Main category: cs.RO

TL;DR: 提出ZLIK方法，使用自监督学习将损伤描述语义信息与运动动力学行为关联，实现零样本适应不同结构损伤的移动机器人运动动力学建模


<details>
  <summary>Details</summary>
Motivation: 野外作业的自主移动机器人承受显著机械应力，结构损伤不可避免，但异质性损伤难以量化并融入运动动力学模型。自然语言能描述各种损伤，因此可用于捕获损伤多样性

Method: 提出零样本语言信息运动动力学（ZLIK），采用自监督学习将损伤描述语义信息与运动动力学行为关联，以数据驱动方式学习前向运动动力学模型。使用BeamNG.tech高保真软体物理模拟器收集各种结构损伤车辆数据

Result: 学习模型实现零样本适应不同损伤，运动动力学误差减少高达81%，并能跨模拟到现实以及全尺寸到1/10比例尺度的泛化

Conclusion: 自然语言能有效描述机器人结构损伤，ZLIK方法通过将语义信息与运动动力学行为关联，实现了对异质性损伤的零样本适应和跨尺度泛化

Abstract: High-performance autonomous mobile robots endure significant mechanical stress during in-the-wild operations, e.g., driving at high speeds or over rugged terrain. Although these platforms are engineered to withstand such conditions, mechanical degradation is inevitable. Structural damage manifests as consistent and notable changes in kinodynamic behavior compared to a healthy vehicle. Given the heterogeneous nature of structural failures, quantifying various damages to inform kinodynamics is challenging. We posit that natural language can describe and thus capture this variety of damages. Therefore, we propose Zero-shot Language Informed Kinodynamics (ZLIK), which employs self-supervised learning to ground semantic information of damage descriptions in kinodynamic behaviors to learn a forward kinodynamics model in a data-driven manner. Using the high-fidelity soft-body physics simulator BeamNG.tech, we collect data from a variety of structurally compromised vehicles. Our learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error and generalizes across the sim-to-real and full-to-1/10$^{\text{th}}$ scale gaps.

</details>


### [7] [Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning](https://arxiv.org/abs/2602.12405)
*Carl Qi,Xiaojie Wang,Silong Yong,Stephen Sheng,Huitan Mao,Sriram Srinivasan,Manikantan Nambi,Amy Zhang,Yesh Dattatreya*

Main category: cs.RO

TL;DR: ARMOR是一个用于机器人故障检测和推理的自适应多任务模型，通过迭代自精炼过程处理开放式故障，利用异构监督学习，在检测率和推理准确性上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人故障通常是微妙、组合且难以枚举的，而丰富的推理标注成本高昂。现有方法要么将故障推理视为封闭集分类问题，要么假设有充足的人工标注，无法有效处理开放式故障场景。

Method: ARMOR将检测和推理制定为多任务自精炼过程，模型迭代预测检测结果和自然语言推理。训练时结合大规模稀疏二元标签和小规模丰富推理标注，通过离线和在线模仿学习优化。推理时生成多个精炼轨迹并通过自确定性指标选择最置信的预测。

Result: 在多样化环境中的实验表明，ARMOR实现了最先进的性能：故障检测率比先前方法提升高达30%，通过LLM模糊匹配分数衡量的推理能力提升高达100%，对异构监督和超出预定义故障模式的开放式推理表现出鲁棒性。

Conclusion: ARMOR通过自适应多任务自精炼框架有效解决了机器人故障检测和推理问题，能够处理现实世界中微妙、组合的开放式故障，在检测准确性和推理质量上都显著超越了现有方法。

Abstract: Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor

</details>


### [8] [MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery](https://arxiv.org/abs/2602.12407)
*Keshara Weerasinghe,Seyed Hamid Reza Roodabeh,Andrew Hawkins,Zhaomeng Zhang,Zachary Schrader,Homa Alemzadeh*

Main category: cs.RO

TL;DR: MiDAS是一个开源、平台无关的系统，用于机器人辅助微创手术的非侵入式多模态数据采集，无需依赖专有机器人接口。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术研究依赖多模态数据，但获取专有机器人遥测数据存在障碍，需要一种开放、平台无关的解决方案。

Method: 集成电磁和RGB-D手部追踪、脚踏板传感和手术视频捕获，不依赖专有机器人接口。在Raven-II和da Vinci Xi系统上验证，收集了缝合任务的多模态数据集。

Result: 外部手部和脚部传感能近似内部机器人运动学，非侵入式运动信号在手势识别性能上与专有遥测数据相当。

Conclusion: MiDAS实现了可重复的多模态RMIS数据采集，发布了带标注的数据集，包括首个在高保真模拟模型上捕获疝气修复缝合的多模态数据集。

Abstract: Background: Robot-assisted minimally invasive surgery (RMIS) research increasingly relies on multimodal data, yet access to proprietary robot telemetry remains a major barrier. We introduce MiDAS, an open-source, platform-agnostic system enabling time-synchronized, non-invasive multimodal data acquisition across surgical robotic platforms.
  Methods: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary robot interfaces. We validated MiDAS on the open-source Raven-II and the clinical da Vinci Xi by collecting multimodal datasets of peg transfer and hernia repair suturing tasks performed by surgical residents. Correlation analysis and downstream gesture recognition experiments were conducted.
  Results: External hand and foot sensing closely approximated internal robot kinematics and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry.
  Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.

</details>


### [9] [Control Barrier Functions with Audio Risk Awareness for Robot Safe Navigation on Construction Sites](https://arxiv.org/abs/2602.12416)
*Johannes Mootz,Reza Akhavian*

Main category: cs.RO

TL;DR: 提出一种基于控制屏障函数的安全滤波器，利用音频检测（电镐）作为风险提示来调整安全边界，为建筑工地自主移动机器人提供安全保证


<details>
  <summary>Details</summary>
Motivation: 建筑工地环境动态且视觉遮挡严重，现有自主系统未充分利用音频信息，需要更丰富的多模态安全推理方法

Method: 基于控制屏障函数的安全滤波器，结合轻量级实时电镐检测器（基于信号包络和周期性分析），将音频风险直接纳入控制器

Result: 在模拟实验中，CBF安全滤波器在所有试验中消除了安全违规；圆形CBF达到目标40.2%，椭圆CBF达到76.5%（后者能更好避免死锁）

Conclusion: 将音频感知集成到CBF控制器中，为安全关键动态环境中的自主机器人提供了更丰富的多模态安全推理途径

Abstract: Construction automation increasingly requires autonomous mobile robots, yet robust autonomy remains challenging on construction sites. These environments are dynamic and often visually occluded, which complicates perception and navigation. In this context, valuable information from audio sources remains underutilized in most autonomy stacks. This work presents a control barrier function (CBF)-based safety filter that provides safety guarantees for obstacle avoidance while adapting safety margins during navigation using an audio-derived risk cue. The proposed framework augments the CBF with a lightweight, real-time jackhammer detector based on signal envelope and periodicity. Its output serves as an exogenous risk that is directly enforced in the controller by modulating the barrier function. The approach is evaluated in simulation with two CBF formulations (circular and goal-aligned elliptical) with a unicycle robot navigating a cluttered construction environment. Results show that the CBF safety filter eliminates safety violations across all trials while reaching the target in 40.2% (circular) vs. 76.5% (elliptical), as the elliptical formulation better avoids deadlock. This integration of audio perception into a CBF-based controller demonstrates a pathway toward richer multimodal safety reasoning in autonomous robots for safety-critical and dynamic environments.

</details>


### [10] [An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation](https://arxiv.org/abs/2602.12421)
*Minduli C. Wijayatunga,Julian Guinane,Nathan D. Wallace,Xiaofeng Wu*

Main category: cs.RO

TL;DR: CORTEX是一个用于近距离交会任务的自主、感知启发的实时轨迹设计与制导框架，结合深度学习感知与凸优化轨迹设计，在软件仿真和硬件在环实验中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 自主卫星服务任务需要在严格的安全和操作约束下执行近距离交会，同时保持计算可行性以适应机载使用，并对感知、执行和动力学的不确定性具有鲁棒性。

Method: CORTEX框架集成了深度学习感知管道与基于凸优化的轨迹设计和制导，包括参考轨迹再生和紧急安全轨道中止逻辑，以应对传感器故障和发动机故障导致的大偏差。

Result: 在高保真软件仿真中，蒙特卡洛实验在最严苛情况下实现终端对接误差：相对位置36.85±44.46毫米，相对速度1.25±2.26毫米/秒。在平面气浮测试台上，18个测试案例（10个正常，8个异常）实现位置误差8.09±5.29毫米，速度误差2.23±1.72毫米/秒。

Conclusion: CORTEX框架成功展示了在近距离交会任务中集成感知、轨迹设计和制导的能力，能够在传感器故障和发动机故障等异常情况下保持安全性和性能，为自主卫星服务提供了可行的解决方案。

Abstract: Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures.
  CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \pm 44.46$ mm in relative position and $1.25 \pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \pm 5.29$ mm in position and $2.23 \pm 1.72$ mm/s in velocity.

</details>


### [11] [Gradient-Enhanced Partitioned Gaussian Processes for Real-Time Quadrotor Dynamics Modeling](https://arxiv.org/abs/2602.12487)
*Xinhuan Sang,Adam Rozman,Sheryl Grace,Roberto Tron*

Main category: cs.RO

TL;DR: 提出一种带梯度信息的四旋翼动力学高斯过程模型，通过状态空间分区和近似实现实时推理，利用中等精度势流模拟数据捕捉空气动力学效应。


<details>
  <summary>Details</summary>
Motivation: 传统基于高斯过程的方法虽然能提供可靠的贝叶斯预测和不确定性量化，但计算成本高，不适合实时仿真。需要一种既能保持精度又能实现实时推理的方法来处理四旋翼空气动力学效应。

Method: 1) 集成梯度信息提高精度；2) 提出新颖的分区和近似策略降低在线计算成本：将状态空间划分为非重叠区域，每个区域关联一个局部高斯过程，通过将训练数据分为局部近场和远场子集，利用Schur补实现大部分矩阵求逆的离线计算；3) 使用CHARM中等精度空气动力学求解器生成包含转子-转子相互作用和表观风向等空气动力学效应的训练数据集。

Result: 提出的带梯度条件的分区高斯过程比不带梯度信息的标准分区高斯过程精度更高，同时大幅减少计算时间，在标准桌面硬件上实现超过30Hz的实时推理频率。

Conclusion: 该框架为复杂非稳态环境中的实时空气动力学预测和控制算法提供了高效基础，平衡了计算效率与预测精度。

Abstract: We present a quadrotor dynamics Gaussian Process (GP) with gradient information that achieves real-time inference via state-space partitioning and approximation, and that includes aerodynamic effects using data from mid-fidelity potential flow simulations. While traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification, they are computationally expensive and thus unsuitable for real-time simulations. To address this challenge, we integrate gradient information to improve accuracy and introduce a novel partitioning and approximation strategy to reduce online computational cost. In particular, for the latter, we associate a local GP with each non-overlapping region; by splitting the training data into local near and far subsets, and by using Schur complements, we show that a large part of the matrix inversions required for inference can be performed offline, enabling real-time inference at frequencies above 30 Hz on standard desktop hardware. To generate a training dataset that captures aerodynamic effects, such as rotor-rotor interactions and apparent wind direction, we use the CHARM code, which is a mid-fidelity aerodynamic solver. It is applied to the SUI Endurance quadrotor to predict force and torque, along with noise at three specified locations. The derivative information is obtained via finite differences. Experimental results demonstrate that the proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while greatly reducing computational time. This framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.

</details>


### [12] [Composable Model-Free RL for Navigation with Input-Affine Systems](https://arxiv.org/abs/2602.12492)
*Xinhuan Sang,Abdelrahman Abdelgawad,Roberto Tron*

Main category: cs.RO

TL;DR: 提出一种可组合的无模型强化学习方法，通过为每个环境元素（目标或障碍物）学习独立的价值函数和最优策略，在线组合实现目标到达和避障


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂动态环境中需要实时安全导航，但预测所有可能行为不可行，需要一种能够适应未知非线性动力学的安全导航方法

Method: 基于连续时间HJB方程推导价值函数，利用优势函数的二次结构，提出无模型actor-critic算法学习静态或移动障碍物的策略和价值函数，通过QCQP组合多个到达/避障模型

Result: 方法在仿真中表现优于应用于离散时间近似的PPO基线，提供了基于价值函数水平集的形式化避障保证

Conclusion: 该方法为CLF/CBF控制提供了无模型替代方案，能够在未知非线性动力学环境中实现安全导航，具有形式化保证

Abstract: As autonomous robots move into complex, dynamic real-world environments, they must learn to navigate safely in real time, yet anticipating all possible behaviors is infeasible. We propose a composable, model-free reinforcement learning method that learns a value function and an optimal policy for each individual environment element (e.g., goal or obstacle) and composes them online to achieve goal reaching and collision avoidance. Assuming unknown nonlinear dynamics that evolve in continuous time and are input-affine, we derive a continuous-time Hamilton-Jacobi-Bellman (HJB) equation for the value function and show that the corresponding advantage function is quadratic in the action and optimal policy. Based on this structure, we introduce a model-free actor-critic algorithm that learns policies and value functions for static or moving obstacles using gradient descent. We then compose multiple reach/avoid models via a quadratically constrained quadratic program (QCQP), yielding formal obstacle-avoidance guarantees in terms of value-function level sets, providing a model-free alternative to CLF/CBF-based controllers. Simulations demonstrate improved performance over a PPO baseline applied to a discrete-time approximation.

</details>


### [13] [Monocular Reconstruction of Neural Tactile Fields](https://arxiv.org/abs/2602.12508)
*Pavan Mantripragada,Siddhanth Deshmukh,Eadom Dessalene,Manas Desai,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 提出神经触觉场，一种从单目RGB图像预测接触时触觉响应的3D表示，用于机器人路径规划，能区分不同物体的可通行性


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人需要在可变形、可屈服和可重构的环境中规划路径，需要超越静态几何占用的交互感知3D表示

Method: 引入神经触觉场，从单目RGB图像预测空间位置在接触时的预期触觉响应，并与现成路径规划器集成

Result: 相比最先进的单目3D重建方法（LRM和Direct3D），体积3D重建提升85.8%，表面重建提升26.7%

Conclusion: 神经触觉场使机器人能够生成避免高阻力物体、有意识通过低阻力区域的路径，而不是将所有占用空间视为同等不可通行

Abstract: Robots operating in the real world must plan through environments that deform, yield, and reconfigure under contact, requiring interaction-aware 3D representations that extend beyond static geometric occupancy. To address this, we introduce neural tactile fields, a novel 3D representation that maps spatial locations to the expected tactile response upon contact. Our model predicts these neural tactile fields from a single monocular RGB image -- the first method to do so. When integrated with off-the-shelf path planners, neural tactile fields enable robots to generate paths that avoid high-resistance objects while deliberately routing through low-resistance regions (e.g. foliage), rather than treating all occupied space as equally impassable. Empirically, our learning framework improves volumetric 3D reconstruction by $85.8\%$ and surface reconstruction by $26.7\%$ compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D).

</details>


### [14] [CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning](https://arxiv.org/abs/2602.12532)
*Yike Zhang,Yaonan Wang,Xinxin Sun,Kaizhen Huang,Zhiyuan Xu,Junjie Ji,Zhengping Che,Jian Tang,Jingtao Sun*

Main category: cs.RO

TL;DR: CRAFT是一个力感知课程微调框架，通过变分信息瓶颈模块调节视觉和语言嵌入，解决VLA模型在接触丰富操作任务中的力信号不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在执行通用指令方面表现出色，但在接触丰富的操作任务中存在困难，因为需要精确对齐、稳定接触维持和有效处理可变形物体。根本挑战在于高熵视觉语言输入与低熵但关键的力信号之间的不平衡，导致过度依赖感知和不稳定控制。

Method: 1. 提出CRAFT力感知课程微调框架；2. 集成变分信息瓶颈模块来调节早期训练中的视觉和语言嵌入；3. 采用课程策略：先优先考虑力信号，然后逐步恢复对完整多模态信息的访问；4. 设计同源领导者-跟随者遥操作系统，收集跨多种接触丰富任务的同步视觉、语言和力数据。

Result: 真实世界实验表明，CRAFT持续提高任务成功率，能够泛化到未见过的物体和新任务变体，并有效适应不同的VLA架构，实现鲁棒且可泛化的接触丰富操作。

Conclusion: CRAFT框架通过力感知课程微调和变分信息瓶颈，成功解决了VLA模型在接触丰富操作任务中的力信号不平衡问题，实现了更鲁棒和可泛化的机器人操作能力。

Abstract: Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

</details>


### [15] [Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking](https://arxiv.org/abs/2602.12549)
*Yue Lin,Yang Liu,Dong Wang,Huchuan Lu*

Main category: cs.RO

TL;DR: Eva-Tracker：一种用于空中跟踪的可见性感知轨迹规划框架，通过预计算的FoV-ESDF消除ESDF更新开销，结合目标轨迹预测和恢复能力路径生成，实现高效、鲁棒的跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统的ESDF（欧几里得符号距离场）在可见性评估中广泛应用，但频繁更新带来显著计算开销。为了解决这一问题，需要开发一种无需ESDF更新的高效跟踪框架。

Method: 1. 设计目标轨迹预测方法和可见性感知初始路径生成算法，保持适当观测距离、避免遮挡，并在目标丢失时快速重新规划；2. 提出FoV-ESDF（视场ESDF），为跟踪器视场预计算的ESDF，无需更新即可快速评估可见性；3. 使用可微分的FoV-ESDF目标函数优化轨迹，确保整个跟踪过程的连续可见性。

Result: 大量仿真和真实世界实验表明，该方法比现有最先进方法提供更鲁棒的跟踪结果，同时计算开销更低。

Conclusion: Eva-Tracker通过消除ESDF更新需求并引入FoV-ESDF和恢复能力路径生成，实现了高效、鲁棒的空中跟踪，在计算效率和跟踪性能方面均优于现有方法。

Abstract: The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.

</details>


### [16] [Hemispherical Angular Power Mapping of Installed mmWave Radar Modules Under Realistic Deployment Constraints](https://arxiv.org/abs/2602.12584)
*Maaz Qureshi,Mohammad Omid Bagheri,William Melek,George Shaker*

Main category: cs.RO

TL;DR: 提出了一种用于现场验证已安装毫米波模块的半球形角度接收功率映射方法，可在实际部署约束下进行电磁验证


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达模块在实际传感平台中安装后，其封装、安装硬件和附近结构会显著改变有效辐射特性，而传统的天线测量方法在这种情况下往往不实用

Method: 采用半球形角度接收功率映射方法，通过将校准接收探头放置在规定的位置（phi, theta, r），使用几何一致定位和准静态采集，记录仅振幅的接收功率，生成捕捉安装相关辐射特性的半球形角度功率图

Result: 在60GHz雷达模块上的概念验证测量展示了可重复的半球形映射，其角度趋势与全波仿真结果良好吻合

Conclusion: 该方法支持对嵌入式毫米波发射器进行实用的现场表征，为已安装毫米波模块的电磁验证提供了可行的解决方案

Abstract: Characterizing the angular radiation behavior of installed millimeter-wave (mmWave) radar modules is increasingly important in practical sensing platforms, where packaging, mounting hardware, and nearby structures can significantly alter the effective emission profile. However, once a device is embedded in its host environment, conventional chamber- and turntable-based antenna measurements are often impractical. This paper presents a hemispherical angular received-power mapping methodology for in-situ EM validation of installed mmWave modules under realistic deployment constraints. The approach samples the accessible half-space around a stationary device-under-test by placing a calibrated receiving probe at prescribed (phi, theta, r) locations using geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate hemispherical angular power maps that capture installation-dependent radiation characteristics. Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemi-spherical mapping with angular trends in good agreement with full-wave simulation, supporting practical on-site characterization of embedded mmWave transmitters.

</details>


### [17] [PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People](https://arxiv.org/abs/2602.12597)
*Mahdi Haghighat Joo,Maryam Karimi Jafari,Alireza Taheri*

Main category: cs.RO

TL;DR: PISHYAR是一款结合社交导航与多模态人机交互的智能手杖，通过实时感知、动态路径规划和AI交互系统，为视障用户提供物理移动支持和交互辅助。


<details>
  <summary>Details</summary>
Motivation: 传统导航辅助设备主要关注物理障碍物避让，缺乏社交意识和交互能力。PISHYAR旨在开发一款既能实现社交合规导航，又能提供自然多模态交互的智能辅助设备，满足视障用户在复杂社交环境中的综合需求。

Method: 系统包含两个核心组件：1) 基于Raspberry Pi 5的社交导航框架，集成OAK-D Lite摄像头、YOLOv8目标检测、COMPOSER群体活动识别、D* Lite动态路径规划和触觉反馈；2) 基于多模态LLM-VLM的交互框架，整合语音识别、视觉语言模型、大语言模型和文本转语音，支持语音和视觉模式动态切换。

Result: 在模拟和真实室内环境测试中，系统实现了约80%的整体准确率，可靠地进行障碍物避让和社交合规导航。群体活动识别在不同人群场景中表现稳健。初步用户研究显示，8名视障参与者对系统的可用性、信任度和社交感知度评价积极。

Conclusion: PISHYAR展示了作为多模态辅助移动设备的潜力，不仅提供导航功能，还能为视障用户提供社交交互支持，是传统导航设备的重要扩展。

Abstract: This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.

</details>


### [18] [RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models](https://arxiv.org/abs/2602.12628)
*Liangzhi Shi,Shuaihang Chen,Feng Gao,Yinuo Chen,Kang Chen,Tonghe Zhang,Hongzhi Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 提出RL-Co框架，通过强化学习在模拟环境中进行闭环训练，同时用真实数据监督防止遗忘，显著提升VLA模型在真实机器人任务上的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调（SFT）的模拟-真实协同训练方法将模拟视为静态演示源，无法利用大规模闭环交互，导致真实世界性能提升和泛化能力有限。

Method: 提出两阶段RL-Co框架：1）用真实和模拟演示混合数据对策略进行监督微调预热；2）在模拟环境中进行强化学习微调，同时添加真实数据的辅助监督损失来锚定策略并防止灾难性遗忘。

Result: 在四个真实桌面操作任务上，使用OpenVLA和π0.5两种VLA架构，相比仅真实数据微调和SFT协同训练，分别获得+24%和+20%的真实世界成功率提升，同时展现出更好的泛化能力和数据效率。

Conclusion: RL-Co框架通过结合模拟环境的交互式训练和真实数据的监督锚定，为利用模拟增强真实机器人部署提供了实用且可扩展的途径，显著提升了VLA模型的真实世界性能和泛化能力。

Abstract: Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.

</details>


### [19] [Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning](https://arxiv.org/abs/2602.12633)
*Tianyi Xiang,Jiahang Cao,Sikai Guo,Guoyang Zhao,Andrew F. Luo,Jun Ma*

Main category: cs.RO

TL;DR: 提出了一种基于物理约束的Real-to-Sim流水线，从单视角RGB-D数据重建物理一致的3D场景，通过可微分优化和接触图建模解决机器人操作中的物理有效性问题。


<details>
  <summary>Details</summary>
Motivation: 从单视角观测重建物理有效的3D场景是连接视觉感知与机器人控制的关键，但在需要精确接触推理的场景中（如高度杂乱环境中的机器人操作），仅几何保真度不足。标准感知流水线常忽略物理约束，导致无效状态（如漂浮物体或严重穿透），使下游仿真不可靠。

Method: 提出物理约束的Real-to-Sim流水线，核心是可微分优化管道，通过接触图显式建模空间依赖关系，联合优化物体姿态和物理属性，使用可微分刚体仿真进行细化。

Result: 在仿真和真实环境中的广泛评估表明，重建的场景实现了高物理保真度，能准确复现真实世界的接触动力学，支持稳定可靠的接触密集型操作。

Conclusion: 该方法通过物理约束的Real-to-Sim流水线，成功解决了单视角RGB-D重建中的物理一致性问题，为机器人操作提供了可靠的仿真基础。

Abstract: Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.

</details>


### [20] [PMG: Parameterized Motion Generator for Human-like Locomotion Control](https://arxiv.org/abs/2602.12656)
*Chenxi Han,Yuheng Min,Zihao Huang,Ao Hong,Hang Liu,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: 提出参数化运动生成器（PMG），通过分析人体运动结构，使用少量参数化运动数据和高维控制命令合成参考轨迹，结合模仿学习和优化式仿真到现实的电机参数识别，在ZERITH Z1人形机器人上验证了自然、类人的运动控制。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的强化学习和运动跟踪技术虽然改善了人形机器人运动，但仍存在实际挑战：基于全身参考引导的方法难以适应高级命令接口和多样化任务场景，需要大量高质量数据集，在不同速度和姿态下表现脆弱，且对机器人特定校准敏感。

Method: 提出参数化运动生成器（PMG），基于人体运动结构分析，使用紧凑的参数化运动数据和高维控制命令合成参考轨迹；结合模仿学习流程和基于优化的仿真到现实电机参数识别模块，在ZERITH Z1人形机器人上实现完整系统验证。

Result: PMG在单一集成系统中产生自然、类人的运动，精确响应高维控制输入（包括VR远程操作），实现高效、可验证的仿真到现实迁移，建立了实用且经过实验验证的人形机器人控制路径。

Conclusion: 该方法解决了当前人形机器人控制中的实际限制，通过参数化运动生成和系统集成，为自然、可部署的人形机器人控制提供了实用且经过实验验证的解决方案。

Abstract: Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.

</details>


### [21] [Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution](https://arxiv.org/abs/2602.12684)
*Rui Cai,Jun Guo,Xinze He,Piaopiao Jin,Jie Li,Bingxuan Lin,Futeng Liu,Wei Liu,Fei Ma,Kun Ma,Feng Qiu,Heng Qu,Yifei Su,Qiao Sun,Dong Wang,Donghao Wang,Yunhong Wang,Rujie Wu,Diyun Xiang,Yu Yang,Hangjun Ye,Yuan Zhang,Quanyun Zhou*

Main category: cs.RO

TL;DR: Xiaomi-Robotics-0是一个优化的视觉-语言-动作模型，通过精心设计的训练方案和部署策略，实现高性能、快速平滑的实时执行，在仿真基准和真实机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在真实机器人上快速平滑执行的视觉-语言-动作模型，解决推理延迟问题，实现连续无缝的实时部署，同时保持视觉语义知识不丢失。

Method: 1) 在大规模跨体现机器人轨迹和视觉语言数据上进行预训练，获得广泛可泛化的动作生成能力；2) 提出异步执行训练技术解决实时机器人部署中的推理延迟；3) 部署时对齐连续预测动作块的时间步，确保连续无缝的实时执行。

Result: 在所有仿真基准测试中达到最先进性能，在需要精确灵巧双手操作的两个真实机器人任务上，使用消费级GPU能够快速平滑执行，实现高成功率和吞吐量。

Conclusion: Xiaomi-Robotics-0通过创新的训练和部署策略，成功实现了高性能、低延迟的实时机器人控制，为未来研究提供了有价值的开源代码和模型检查点。

Abstract: In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io

</details>


### [22] [SignScene: Visual Sign Grounding for Mapless Navigation](https://arxiv.org/abs/2602.12686)
*Nicky Zimmerman,Joel Loo,Benjamin Koh,Zishuo Wang,David Hsu*

Main category: cs.RO

TL;DR: 机器人利用现实世界中的导航标志进行无地图导航，通过SignScene空间语义表示方法，将标志的语义指令与3D场景元素对应，实现88%的准确率，并在Spot机器人上成功演示。


<details>
  <summary>Details</summary>
Motivation: 人类能够利用导航标志在不熟悉环境中导航而无需地图，但机器人难以实现类似能力。主要挑战在于标志的多样性、复杂性，以及需要将抽象语义内容与局部3D场景进行关联。

Method: 提出SignScene方法，这是一种以标志为中心的空间语义表示方法，捕捉与导航相关的场景元素和标志信息，并以适合视觉语言模型推理的形式呈现。将问题形式化为标志接地问题，即把标志上的语义指令映射到相应的场景元素和导航动作。

Result: 在9种不同环境类型收集的114个查询数据集上评估，实现了88%的标志接地准确率，显著优于基线方法。最终在Spot机器人上成功演示了仅使用标志的现实世界无地图导航。

Conclusion: SignScene方法通过有效的空间语义表示，使机器人能够像人类一样利用现实世界中的导航标志进行无地图导航，为解决开放世界机器人导航问题提供了新途径。

Abstract: Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.

</details>


### [23] [ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training](https://arxiv.org/abs/2602.12691)
*Rushuai Yang,Hecheng Wang,Chiming Liu,Xiaohan Yan,Yunlong Wang,Xuan Du,Shuoyu Yue,Yongcheng Liu,Chuheng Zhang,Lizhe Qi,Yi Chen,Wei Shan,Maoqing Yao*

Main category: cs.RO

TL;DR: ALOE框架通过动作级别的离策略评估，使用基于分块的时序差分引导来评估单个动作序列，改进了大型视觉语言动作系统的在线强化学习效果


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言动作系统在真实世界在线强化学习中，通常采用保守的同策略估计来保证稳定性，但这避免了对当前高容量策略的直接评估，限制了学习效果。需要解决从混合数据中评估当前行为质量的离策略评估问题

Method: 提出ALOE框架，采用基于分块的时序差分引导方法，评估单个动作序列而非预测最终任务结果。这种设计改进了稀疏奖励下对关键动作分块的信用分配，支持稳定的策略改进

Result: 在三个真实世界操作任务上评估：智能手机包装（高精度任务）、衣物折叠（长时程可变形物体任务）、双手抓取放置（多物体感知任务）。在所有任务中，ALOE都提高了学习效率而不影响执行速度

Conclusion: ALOE表明离策略强化学习可以以可靠的方式重新引入真实世界VLA后训练中，改进了学习效率并保持了执行速度

Abstract: We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.

</details>


### [24] [Constrained PSO Six-Parameter Fuzzy PID Tuning Method for Balanced Optimization of Depth Tracking Performance in Underwater Vehicles](https://arxiv.org/abs/2602.12700)
*Yanxi Ding,Tingyue Jia*

Main category: cs.RO

TL;DR: 提出一种基于约束粒子群优化算法的六参数模糊PID控制器调参方法，用于水下航行器深度控制，在满足控制能量约束的同时显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 水下航行器深度控制需要同时满足快速跟踪、低超调和执行器约束要求。传统模糊PID调参依赖经验方法，难以在性能提升和控制成本之间获得稳定可复现的平衡解。

Method: 采用约束粒子群优化算法对六参数模糊PID控制器进行联合调优：调整基准PID参数以及模糊控制器的输入量化因子和输出比例增益，实现模糊PID系统整体调优强度与动态响应特性的协同优化。引入时间加权绝对误差积分、调整时间、相对超调、控制能量和饱和占用率构建约束驱动的综合评价体系。

Result: 在保持控制能量和饱和水平一致的情况下，显著提升深度跟踪性能：时间加权绝对误差积分从0.2631降至0.1473，调整时间从2.301秒缩短至1.613秒，相对超调从0.1494降至0.01839。控制能量从7980变为7935，满足能量约束，饱和占用率从0.004降至0.003。

Conclusion: 验证了所提出的约束六参数联合调优策略在水下航行器深度控制场景中的有效性和工程意义，实现了性能提升与控制约束的良好平衡。

Abstract: Depth control of underwater vehicles in engineering applications must simultaneously satisfy requirements for rapid tracking, low overshoot, and actuator constraints. Traditional fuzzy PID tuning often relies on empirical methods, making it difficult to achieve a stable and reproducible equilibrium solution between performance enhancement and control cost. This paper proposes a constrained particle swarm optimization (PSO) method for tuning six-parameter fuzzy PID controllers. By adjusting the benchmark PID parameters alongside the fuzzy controller's input quantization factor and output proportional gain, it achieves synergistic optimization of the overall tuning strength and dynamic response characteristics of the fuzzy PID system. To ensure engineering feasibility of the optimization results, a time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate are introduced. Control energy constraints are applied to construct a constraint-driven comprehensive evaluation system, suppressing pseudo-improvements achieved solely by increasing control inputs. Simulation results demonstrate that, while maintaining consistent control energy and saturation levels, the proposed method significantly enhances deep tracking performance: the time-weighted absolute error integral decreases from 0.2631 to 0.1473, the settling time shortens from 2.301 s to 1.613 s, and the relative overshoot reduces from 0.1494 to 0.01839. Control energy varied from 7980 to 7935, satisfying the energy constraint, while saturation occupancy decreased from 0.004 to 0.003. These results validate the effectiveness and engineering significance of the proposed constrained six-parameter joint tuning strategy for depth control in underwater vehicle navigation scenarios.

</details>


### [25] [TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions](https://arxiv.org/abs/2602.12724)
*Wei Zhu,Irfan Tito Kurniawan,Ye Zhao,Mistuhiro Hayashibe*

Main category: cs.RO

TL;DR: TRANS是一个用于四足机器人在非结构化地形上进行社交导航的深度强化学习框架，通过两阶段训练和三个DRL管道实现地形感知的敏捷导航。


<details>
  <summary>Details</summary>
Motivation: 传统四足机器人导航通常将运动规划与运动控制分离，忽略了全身约束和地形感知。端到端方法虽然更集成，但需要高频传感且计算成本高。现有方法大多假设静态环境，限制了在人群环境中的应用。

Method: 提出两阶段训练框架包含三个DRL管道：TRANS-Loco使用非对称actor-critic模型进行四足运动，无需显式地形或接触观测；TRANS-Nav应用对称actor-critic框架进行社交导航，将转换的LiDAR数据直接映射到差速运动学下的动作；TRANS统一管道整合前两者，支持地形感知的四足导航。

Result: 与运动控制和社交导航基线相比的综合基准测试证明了TRANS的有效性。硬件实验进一步证实了其从仿真到现实的迁移潜力。

Conclusion: TRANS框架成功解决了四足机器人在非结构化地形和社交环境中的导航问题，通过整合地形感知和社交交互能力，实现了更自然、安全的导航性能。

Abstract: This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.

</details>


### [26] [SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies](https://arxiv.org/abs/2602.12794)
*Thies Oelerich,Gerald Ebmer,Christian Hartl-Nesic,Andreas Kugi*

Main category: cs.RO

TL;DR: SafeFlowMPC结合流匹配和在线优化，在保证安全性的同时实现实时机器人控制


<details>
  <summary>Details</summary>
Motivation: 机器人融入日常生活需要灵活性和实时反应能力。基于学习的方法虽然能训练强大策略，但缺乏可解释性和严格安全保证；基于优化的方法有安全保证但缺乏所需的灵活性和泛化能力。

Method: 提出SafeFlowMPC方法，结合流匹配和在线优化，使用次优模型预测控制公式满足实时执行需求，保证所有时间的安全性。

Result: 在KUKA 7自由度机械臂上进行了三个真实世界实验（两个抓取实验和一个动态人机物体交接实验），表现出色。

Conclusion: SafeFlowMPC成功结合了学习和优化的优势，在保证安全性的同时实现了实时机器人控制。

Abstract: The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.

</details>


### [27] [SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders](https://arxiv.org/abs/2602.12838)
*Houssem Eddine Mohamadi,Nadjia Kara*

Main category: cs.RO

TL;DR: 提出一种面向具备滑翔能力的无人机的自主部署方法，采用局部-全局行为管理与决策框架，通过协同无人机建模、任务规划、路径跟踪优化等策略，显著提升监视持久性和目标检测能力。


<details>
  <summary>Details</summary>
Motivation: 小型无人机在监视应用中的成功受限于有限的机载电源续航时间。为应对这一挑战，需要寻找替代的可再生升力来源，其中从上升的浮力空气中提取能量是一个有前景的解决方案。

Method: 1) 将协同无人机建模为非确定性有限状态理性智能体；2) 开发任务规划模块分配任务并发布动态导航航点；3) 应用可见性和预测概念的新路径规划方案避免碰撞；4) 采用延迟学习和调谐策略优化路径跟踪控制器增益。

Result: 与三个基准线和15种进化算法进行严格比较分析表明：1) 显著提升监视持久性（更长时间保持飞行不降落）；2) 目标检测能力是非协同和半协同方法的两倍；3) 功耗更低（6小时仅消耗约6%电池电量）。

Conclusion: 所提出的局部-全局行为管理与决策方法能有效维持无人机监视持久性，最大化目标检测能力，同时显著降低功耗，为滑翔能力无人机的自主部署提供了有效解决方案。

Abstract: The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).

</details>


### [28] [Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips](https://arxiv.org/abs/2602.12918)
*Iris Andrussow,Jans Solano,Benjamin A. Richardson,Georg Martius,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: 机器人通过结合视觉触觉传感器（Minsight）和音频触觉传感器（Minsound）来感知布料纹理，使用Transformer模型在20种常见布料上达到97%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 人类指尖能同时感知空间力模式和纹理诱导振动来识别布料，但机器人触觉传感器难以同时实现高空间分辨率和高时间采样率。需要开发能同时感知这两种触觉信息的系统。

Method: 开发了Minsight（50Hz视觉传感器）和Minsound（50Hz-15kHz MEMS麦克风）两种传感器，机器人模仿人类动作夹持和摩擦布料样本，使用Transformer模型进行布料分类。

Result: 音频传感器对分类性能贡献显著，在20种常见布料上达到97%的最高分类准确率。外部麦克风增强了在嘈杂环境下的鲁棒性，并能学习布料拉伸性、厚度和粗糙度的通用表示。

Conclusion: 音频-视觉触觉传感方法能有效识别布料纹理，音频模态在触觉感知中具有重要价值，该方法能推广到训练数据之外的布料属性表征。

Abstract: Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.

</details>


### [29] [INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval](https://arxiv.org/abs/2602.12971)
*YukTungSamuel Fang,Zhikang Shi,Jiabin Qiu,Zixuan Chen,Jieqi Shi,Hao Xu,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: INHerit-SG提出了一种面向机器人导航的语义场景图新范式，通过结构化知识库、异步双进程架构和事件触发更新机制，解决现有方法在可解释性和实时性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语义场景图方法主要依赖离线批处理或隐式特征嵌入，难以支持复杂环境中可解释的人类意图推理，与具身任务需求存在根本性不匹配。

Method: 1) 将地图重新定义为结构化、RAG就绪的知识库，引入自然语言描述作为显式语义锚点；2) 采用异步双进程架构和Floor-Room-Area-Object层次结构，解耦几何分割与语义推理；3) 事件触发的地图更新机制仅在发生有意义语义事件时重组图结构；4) 部署多角色LLM分解查询并处理逻辑否定，采用硬到软过滤策略确保鲁棒推理。

Result: 在HM3DSem-SQR新构建数据集和真实环境中评估，INHerit-SG在复杂查询上达到最先进性能，并展示了下游导航任务的可扩展性。

Conclusion: INHerit-SG通过显式可解释性提高了复杂检索的成功率和可靠性，使系统能够适应更广泛的人类交互任务，为机器人导航中的语义场景理解提供了新解决方案。

Abstract: Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/

</details>


### [30] [Learning Native Continuation for Action Chunking Flow Policies](https://arxiv.org/abs/2602.12978)
*Yufeng Liu,Hang Yu,Juntu Zhao,Bocheng Li,Di Zhang,Mingzhu Li,Wenxuan Wu,Yingdong Hu,Junyuan Xie,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: Legato是一种用于动作分块视觉语言动作模型的训练时延续方法，通过改进去噪过程和调度条件，解决分块边界不连续问题，提升轨迹平滑度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 实时分块（RTC）虽然能缓解动作分块执行中的边界不连续问题，但它是外部策略，会导致虚假的多模态切换和轨迹不自然平滑。需要一种内在的解决方案来改善分块边界处的连续性。

Method: Legato采用训练时延续方法：1）从已知动作和噪声的调度形状混合初始化去噪，让模型接触部分动作信息；2）重塑学习到的流动态，确保训练和推理在每步指导下的去噪过程一致；3）使用随机化调度条件训练，支持变化的推理延迟并实现可控平滑度。

Result: 实验表明Legato能产生更平滑的轨迹，减少执行中的虚假多模态切换，降低犹豫并缩短任务完成时间。在五个操作任务的真实世界实验中，Legato在轨迹平滑度和任务完成时间上均比RTC提升约10%。

Conclusion: Legato通过训练时延续方法有效解决了动作分块VLA模型中的边界不连续问题，实现了内在的轨迹平滑，在真实世界操作任务中显著优于现有的实时分块方法。

Abstract: Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.

</details>


### [31] [How Swarms Differ: Challenges in Collective Behaviour Comparison](https://arxiv.org/abs/2602.13016)
*André Fialho Jesus,Jonas Kuckling*

Main category: cs.RO

TL;DR: 研究集体行为特征集对行为相似性度量的影响，评估现有特征集的鲁棒性，提出基于自组织映射的方法识别难以区分的特征空间区域


<details>
  <summary>Details</summary>
Motivation: 集体行为通常需要通过数值特征来表达（如分类或模仿学习），但现有方法多为特定场景设计的临时特征集，缺乏对鲁棒性的考虑。自动设计集体行为的方法依赖于定量测量行为相似性的能力，因此需要研究特征集对集体行为相似性度量的影响。

Method: 1) 从先前群体机器人工作中选择群体特征集和相似性度量方法；2) 评估这些特征集在狭窄行为上下文之外的鲁棒性；3) 分析特征集与相似性度量的相互作用；4) 提出基于自组织映射的方法来识别特征空间中难以区分行为的区域。

Result: 研究发现特征集与相似性度量的相互作用使得某些组合更适合区分相似行为群体。同时，提出的自组织映射方法能够有效识别特征空间中行为难以区分的区域。

Conclusion: 特征集的选择对集体行为相似性度量至关重要，某些特征集与相似性度量的组合能更好地区分行为。提出的自组织映射方法为识别特征空间中的模糊区域提供了有效工具，有助于改进集体行为的自动设计和评估。

Abstract: Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.

</details>


### [32] [SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot](https://arxiv.org/abs/2602.13078)
*Storm de Kam,Ebrahim Shahabi,Cosimo Della Santina*

Main category: cs.RO

TL;DR: 该论文提出了一种基于学习的控制框架，用于配备触觉吸盘脚的气动软体四足机器人，通过仿真训练和实验验证，实现了在平坦和倾斜表面的闭环运动控制。


<details>
  <summary>Details</summary>
Motivation: 软体四足机器人的鲁棒闭环运动控制面临挑战，包括高维动力学、执行器迟滞、难以建模的接触交互，以及传统本体感知提供的地面接触信息有限。需要开发能够利用触觉反馈的控制方法来解决这些问题。

Method: 提出基于学习的控制框架，为配备触觉吸盘脚的气动软体四足机器人设计控制策略。通过分阶段学习过程在仿真中训练控制策略：从参考步态开始，在随机化环境条件下逐步优化。控制器将本体感知和触觉反馈映射到协调的气动执行和吸盘命令。

Result: 在真实机器人上部署时，闭环策略优于开环基线：在平坦表面上向前速度提高41%，在5度斜坡上提高91%。消融研究进一步表明触觉力估计和惯性反馈在稳定运动中的作用，与无传感器反馈配置相比，性能提升高达56%。

Conclusion: 该学习控制框架成功实现了软体四足机器人的鲁棒闭环运动，证明了触觉反馈和惯性反馈在提升运动性能方面的重要性，为软体机器人控制提供了有效的解决方案。

Abstract: Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.

</details>


### [33] [Agentic AI for Robot Control: Flexible but still Fragile](https://arxiv.org/abs/2602.13081)
*Oscar Lima,Marc Vinci,Martin Günther,Marian Renz,Alexander Sung,Sebastian Stock,Johannes Brust,Lennart Niecksch,Zongyao Yi,Felix Igelbrink,Benjamin Kisliuk,Martin Atzmueller,Joachim Hertzberg*

Main category: cs.RO

TL;DR: 该研究提出了一个基于推理能力语言模型的机器人控制系统，通过迭代规划-执行循环选择和调用机器人技能，在移动操作和农业导航两个平台上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 利用生成模型的能力和常识先验进行机器人控制，处理不确定性、部分可观测性、传感器噪声和模糊自然语言指令等复杂环境挑战。

Method: 构建一个基于推理能力语言模型的智能控制系统，采用迭代规划-执行循环，通过选择和执行机器人技能完成任务，支持结构化内省、显式事件检查和操作员干预。

Result: 在两个物理机器人平台上部署成功，但发现系统存在显著脆弱性：非确定性次优行为、指令跟随错误、对提示规范高度敏感；同时架构具有灵活性，转移到不同机器人和任务领域主要只需更新系统提示和重新绑定工具接口。

Conclusion: 该基于语言模型的机器人控制系统展示了在复杂环境中处理不确定性和自然语言指令的潜力，但当前实现存在显著脆弱性，需要在鲁棒性和可靠性方面进一步改进。

Abstract: Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.

</details>


### [34] [Temporally-Sampled Efficiently Adaptive State Lattices for Autonomous Ground Robot Navigation in Partially Observed Environments](https://arxiv.org/abs/2602.13159)
*Ashwin Satish Menon,Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: 提出TSEASL架构解决越野机器人导航中因环境部分可观测导致区域规划频繁变化引发的安全问题


<details>
  <summary>Details</summary>
Motivation: 越野移动机器人在部分可观测环境中运行时，传感器不断感知新信息导致最优路径持续修订。传统导航架构中，区域运动规划器输出的参考轨迹在连续规划周期中可能差异显著，这种快速变化的指导会导致不安全的导航行为，经常需要人工干预。

Method: 提出TSEASL（时间采样高效自适应状态网格）区域规划器仲裁架构，该架构考虑先前生成轨迹的更新优化版本与当前生成轨迹的对比，通过仲裁机制选择更稳定的参考轨迹。

Result: 在Clearpath Robotics Warthog无人地面车辆及其实测地图数据上的测试表明，使用TSEASL时，机器人在基线规划器需要人工干预的相同位置不再需要干预。此外，TSEASL相比基线规划器记录了更高的规划器稳定性。

Conclusion: TSEASL能有效提高越野自主导航的安全性，减少人工干预需求。论文最后讨论了进一步改进TSEASL以使其更适用于各种越野自主场景的方法。

Abstract: Due to sensor limitations, environments that off-road mobile robots operate in are often only partially observable. As the robots move throughout the environment and towards their goal, the optimal route is continuously revised as the sensors perceive new information. In traditional autonomous navigation architectures, a regional motion planner will consume the environment map and output a trajectory for the local motion planner to use as a reference. Due to the continuous revision of the regional plan guidance as a result of changing map information, the reference trajectories which are passed down to the local planner can differ significantly across sequential planning cycles. This rapidly changing guidance can result in unsafe navigation behavior, often requiring manual safety interventions during autonomous traversals in off-road environments. To remedy this problem, we propose Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), which is a regional planner arbitration architecture that considers updated and optimized versions of previously generated trajectories against the currently generated trajectory. When tested on a Clearpath Robotics Warthog Unmanned Ground Vehicle as well as real map data collected from the Warthog, results indicate that when running TSEASL, the robot did not require manual interventions in the same locations where the robot was running the baseline planner. Additionally, higher levels of planner stability were recorded with TSEASL over the baseline. The paper concludes with a discussion of further improvements to TSEASL in order to make it more generalizable to various off-road autonomy scenarios.

</details>


### [35] [Human Emotion-Mediated Soft Robotic Arts: Exploring the Intersection of Human Emotions, Soft Robotics and Arts](https://arxiv.org/abs/2602.13163)
*Saitarun Nadipineni,Chenhao Hong,Tanishtha Ramlall,Chapa Sirithunge,Kaspar Althoefer,Fumiya Iida,Thilina Dulantha Lalitharatne*

Main category: cs.RO

TL;DR: 该研究探索了人类情感、软体机器人与艺术的交叉领域，通过脑电α波测量情感并映射到软体艺术装置的运动中，创建了情感介导的软体机器人艺术新形式。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有灵活性、适应性和安全性，适合需要细腻、有机、逼真运动的艺术应用。研究旨在探索人类情感如何通过软体机器人艺术化表达，创造沉浸式互动体验。

Method: 通过脑电图(EEG)测量α波来量化人类情感状态，将α波信号映射到两个软体装置（软体角色和软体花朵）的动态运动中，通过实验验证概念。

Result: 成功展示了软体机器人能够体现人类情感状态，α波信号可以有效地驱动软体艺术装置产生相应的动态运动，实现了情感介导的艺术表达。

Conclusion: 软体机器人可以作为人类情感状态的艺术化载体，为艺术表达和互动提供了新媒介，展示了艺术装置如何通过情感信号实现具身化表达。

Abstract: Soft robotics has emerged as a versatile field with applications across various domains, from healthcare to industrial automation, and more recently, art and interactive installations. The inherent flexibility, adaptability, and safety of soft robots make them ideal for applications that require delicate, organic, and lifelike movement, allowing for immersive and responsive interactions. This study explores the intersection of human emotions, soft robotics, and art to establish and create new forms of human emotion-mediated soft robotic art. In this paper, we introduce two soft embodiments: a soft character and a soft flower as an art display that dynamically responds to brain signals based on alpha waves, reflecting different emotion levels. We present how human emotions can be measured as alpha waves based on brain/EEG signals, how we map the alpha waves to the dynamic movements of the two soft embodiments, and demonstrate our proposed concept using experiments. The findings of this study highlight how soft robotics can embody human emotional states, offering a new medium for insightful artistic expression and interaction, and demonstrating how art displays can be embodied.

</details>


### [36] [Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control](https://arxiv.org/abs/2602.13193)
*William Chen,Jagdeep Singh Bhatia,Catherine Glossop,Nikhil Mathihalli,Ria Doshi,Andy Tang,Danny Driess,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出Steerable Policies方法，通过在不同抽象层次（子任务、动作、像素坐标）上训练视觉语言动作模型，提升低层控制能力，从而更好地利用预训练视觉语言模型的常识知识，实现机器人任务泛化。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型（VLMs）具有跨领域的语义和视觉推理能力，能为机器人控制提供有价值的常识先验。然而，如何有效地将这些知识落地到机器人行为中仍然是一个开放挑战。现有方法通常采用分层架构，VLMs推理高层指令，再由独立的低层策略执行，这种自然语言接口限制了VLM推理对低层行为的指导能力。

Method: 提出Steerable Policies方法：训练视觉语言动作模型（VLAs）处理不同抽象层次的丰富合成指令，包括子任务、动作和基于像素坐标的接地指令。通过提升低层可控性，使预训练的VLM知识能够更好地指导机器人行为。展示了两种控制方式：1）学习的高层具身推理器；2）通过上下文学习提示现成VLM推理指令抽象。

Result: 在大量真实世界操作实验中，这两种新方法都优于现有的具身推理VLAs和基于VLM的分层基线，包括在具有挑战性的泛化任务和长时程任务上表现更优。

Conclusion: Steerable Policies通过在不同抽象层次上训练VLAs，显著提升了低层控制能力，从而能够解锁预训练VLMs中的知识，实现更好的任务泛化性能。这种方法为将视觉语言模型的常识推理能力有效落地到机器人控制提供了新途径。

Abstract: Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io

</details>


### [37] [Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos](https://arxiv.org/abs/2602.13197)
*Albert J. Zhai,Kuo-Hao Zeng,Jiasen Lu,Ali Farhadi,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.RO

TL;DR: PSI框架利用人类视频数据训练模块化机器人操作策略，通过仿真中的抓取-轨迹配对过滤解决任务兼容性抓取问题，无需真实机器人数据即可高效学习精确操作技能。


<details>
  <summary>Details</summary>
Motivation: 人类视频为机器人学习提供了可扩展的数据源，但视频主要提供抓取后动作信号，对抓取行为学习帮助有限，特别是机器人手部与人类不同。现有模块化策略使用专用抓取生成器，但生成的稳定抓取往往与任务不兼容，影响后续动作执行。

Method: 提出Perceive-Simulate-Imitate (PSI)框架：1) 使用人类视频运动数据；2) 在仿真中进行抓取-轨迹配对过滤处理数据；3) 为轨迹数据添加抓取适宜性标签；4) 通过监督学习训练任务导向的抓取能力。

Result: 真实世界实验表明，该框架无需任何机器人数据即可高效学习精确操作技能，相比简单使用抓取生成器，性能显著更鲁棒。

Conclusion: PSI框架通过仿真中的抓取-轨迹配对过滤，有效解决了任务兼容性抓取问题，使机器人能够仅从人类视频数据中学习复杂的操作技能，为机器人学习提供了新的数据源。

Abstract: The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.

</details>
