<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Revisiting Continuous-Time Trajectory Estimation via Gaussian Processes and the Magnus Expansion](https://arxiv.org/abs/2601.03360)
*Timothy Barfoot,Cedric Le Gentil,Sven Lilge*

Main category: cs.RO

TL;DR: 本文提出了一种基于Magnus展开的全局高斯过程先验方法，用于李群上的连续时间状态估计，相比现有的局部线性时不变SDE核方法更具理论优雅性和通用性。


<details>
  <summary>Details</summary>
Motivation: 连续时间状态估计在处理异步高频率测量、引入平滑性、事后查询估计值以及解决扫描移动传感器可观测性问题方面具有优势。现有方法使用局部高斯过程配合线性时不变SDE核，虽然实用但缺乏理论优雅性。

Method: 采用完整的线性时变高斯过程方法，通过Magnus展开推导出李群上的全局高斯过程先验，提供更优雅和通用的解决方案。

Result: 提供了两种方法的数值比较，并讨论了各自的相对优势。

Conclusion: 基于Magnus展开的全局高斯过程先验方法为李群上的连续时间轨迹估计提供了更优雅和通用的理论框架。

Abstract: Continuous-time state estimation has been shown to be an effective means of (i) handling asynchronous and high-rate measurements, (ii) introducing smoothness to the estimate, (iii) post hoc querying the estimate at times other than those of the measurements, and (iv) addressing certain observability issues related to scanning-while-moving sensors. A popular means of representing the trajectory in continuous time is via a Gaussian process (GP) prior, with the prior's mean and covariance functions generated by a linear time-varying (LTV) stochastic differential equation (SDE) driven by white noise. When the state comprises elements of Lie groups, previous works have resorted to a patchwork of local GPs each with a linear time-invariant SDE kernel, which while effective in practice, lacks theoretical elegance. Here we revisit the full LTV GP approach to continuous-time trajectory estimation, deriving a global GP prior on Lie groups via the Magnus expansion, which offers a more elegant and general solution. We provide a numerical comparison between the two approaches and discuss their relative merits.

</details>


### [2] [Cost-Effective Radar Sensors for Field-Based Water Level Monitoring with Sub-Centimeter Accuracy](https://arxiv.org/abs/2601.03447)
*Anna Zavei-Boroda,J. Toby Minear,Kyle Harlow,Dusty Woods,Christoffer Heckman*

Main category: cs.RO

TL;DR: 该研究探索使用商用雷达传感器进行低成本水位监测，通过统计滤波技术提高精度，在实地测试中实现了厘米级精度，适用于无人机和机器人平台的自主水监测。


<details>
  <summary>Details</summary>
Motivation: 水位监测对洪水管理、水资源分配和生态评估至关重要，但传统方法成本高且覆盖范围有限。需要寻找低成本、非接触式且对环境条件鲁棒的替代方案。

Method: 采用雷达传感技术作为水位估计的低成本替代方案，评估商用雷达传感器在真实环境中的性能，应用统计滤波技术来提高测量精度。

Result: 单个雷达传感器在最小化校准的情况下能够实现厘米级精度，表明雷达传感是水位监测的实用解决方案。

Conclusion: 雷达传感技术为自主水监测提供了低成本、高精度的解决方案，特别适合与无人机和机器人平台集成，具有广阔的应用前景。

Abstract: Water level monitoring is critical for flood management, water resource allocation, and ecological assessment, yet traditional methods remain costly and limited in coverage. This work explores radar-based sensing as a low-cost alternative for water level estimation, leveraging its non-contact nature and robustness to environmental conditions. Commercial radar sensors are evaluated in real-world field tests, applying statistical filtering techniques to improve accuracy. Results show that a single radar sensor can achieve centimeter-scale precision with minimal calibration, making it a practical solution for autonomous water monitoring using drones and robotic platforms.

</details>


### [3] [FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin](https://arxiv.org/abs/2601.03449)
*Chris Webb,Mobin Habibpour,Mayamin Hamid Raha,Ali Reza Tavakkoli,Janice Coen,Fatemeh Afghah*

Main category: cs.RO

TL;DR: FIRE-VLM：首个端到端视觉语言模型引导的强化学习框架，用于在物理真实的野火数字孪生环境中训练无人机自主监测野火


<details>
  <summary>Details</summary>
Motivation: 现有无人机导航方法依赖简化的模拟器和监督感知流程，缺乏与物理真实火灾环境交互的具身智能体，无法应对极端视觉退化、快速演变的物理动态和真实训练数据稀缺的野火监测挑战

Method: 1) 基于USGS数字高程模型地形、LANDFIRE燃料清单和半物理火灾蔓延求解器构建高保真野火数字孪生；2) 使用PPO算法训练具有双视角无人机感知的智能体；3) 通过CLIP风格VLM引导，将野火特定语义对齐分数作为基于势能的奖励塑形信号

Result: 在五个数字孪生评估任务中，VLM引导策略将检测时间减少高达6倍，增加了视野内停留时间，是首个在公里级物理真实数字孪生火灾中展示的基于RL的无人机野火监测系统

Conclusion: FIRE-VLM框架成功整合了GIS到模拟流程、VLM引导的RL智能体和野火感知奖励设计，为自主野火监测提供了有效的端到端解决方案，在物理真实环境中显著提升了无人机性能

Abstract: Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-end vision-language model (VLM) guided reinforcement learning (RL) framework trained entirely within a high-fidelity, physics-grounded wildfire digital twin. Built from USGS Digital Elevation Model (DEM) terrain, LANDFIRE fuel inventories, and semi-physical fire-spread solvers, this twin captures terrain-induced runs, wind-driven acceleration, smoke plume occlusion, and dynamic fuel consumption. Within this environment, a PPO agent with dual-view UAV sensing is guided by a CLIP-style VLM. Wildfire-specific semantic alignment scores, derived from a single prompt describing active fire and smoke plumes, are integrated as potential-based reward shaping signals. Our contributions are: (1) a GIS-to-simulation pipeline for constructing wildfire digital twins; (2) a VLM-guided RL agent for UAV firefront tracking; and (3) a wildfire-aware reward design that combines physical terms with VLM semantics. Across five digital-twin evaluation tasks, our VLM-guided policy reduces time-to-detection by up to 6 times, increases time-in-FOV, and is, to our knowledge, the first RL-based UAV wildfire monitoring system demonstrated in kilometer-scale, physics-grounded digital-twin fires.

</details>


### [4] [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
*Liangdong Zhang,Yiming Nie,Haoyang Li,Fanjie Kong,Baobao Zhang,Shunxin Huang,Kai Fu,Chen Min,Liang Xiao*

Main category: cs.RO

TL;DR: OFF-EMMA是一个用于越野自动驾驶的端到端多模态框架，通过视觉提示块和链式思维一致性推理策略，显著提升了轨迹规划的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 越野环境中的高效轨迹规划对自动驾驶车辆具有挑战性，传统方法在动态环境中适应性有限。现有的视觉-语言-动作模型在越野自动驾驶场景中存在空间感知不足和推理不稳定的缺陷。

Method: 提出OFF-EMMA框架：1）设计视觉提示块，使用语义分割掩码作为视觉提示，增强预训练视觉-语言模型对复杂地形的空间理解能力；2）引入链式思维一致性推理策略，通过多路径推理机制减轻异常值对规划性能的错误影响。

Result: 在RELLIS-3D越野数据集上的实验表明，OFF-EMMA显著优于现有方法：将Qwen骨干模型的平均L2误差降低了13.3%，故障率从16.52%降至6.56%。

Conclusion: OFF-EMMA通过增强空间感知和稳定推理能力，有效解决了越野自动驾驶中轨迹规划的挑战，为端到端多模态规划提供了有前景的解决方案。

Abstract: Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.

</details>


### [5] [From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance](https://arxiv.org/abs/2601.03562)
*Samantha Sudhoff,Pranesh Velmurugan,Jiashu Liu,Vincent Zhao,Yung-Hsiang Lu,Kristen Yeon-Ji Yun*

Main category: cs.RO

TL;DR: 提出了一种端到端的MIDI乐谱到机器人运动管道，使UR5e机器人能够演奏大提琴，无需动作捕捉，通过音乐图灵测试验证性能，并建立了首个机器人演奏基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有机器人演奏家主要关注精确的弓法轨迹，但依赖昂贵的动作捕捉技术，且无法像人类一样视奏乐谱。需要一种更自然、成本更低的方法来实现机器人演奏弦乐器。

Method: 1. 端到端MIDI乐谱到机器人运动管道，直接将音乐输入转换为碰撞感知的弓法运动；2. 利用Universal Robot的Freedrive功能实现人类化声音，无需动作捕捉；3. 通过RTDE实时记录关节数据，建立标注数据集；4. 引入音乐图灵测试评估机器人演奏质量；5. 提出残差强化学习方法改进控制性能。

Result: 1. 成功开发了能够演奏大提琴的UR5e机器人系统；2. 收集了5首标准曲目的标注机器人演奏数据并公开；3. 通过132名参与者的音乐图灵测试，验证了机器人演奏与人类演奏的可比性；4. 建立了首个机器人演奏基准数据集和评估方法。

Conclusion: 该方法实现了无需动作捕捉的机器人演奏，通过端到端管道将MIDI乐谱直接转换为机器人运动，建立了首个机器人演奏基准，并通过音乐图灵测试验证了其有效性，为未来改进弦乐演奏效率和音质提供了基础。

Abstract: Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.
  We propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality.

</details>


### [6] [Locomotion Beyond Feet](https://arxiv.org/abs/2601.03607)
*Tae Hoon Yang,Haochen Shi,Jiacheng Hu,Zhicong Zhang,Daniel Jiang,Weizhuo Wang,Yao He,Zhen Wu,Yuming Chen,Yifan Hou,Monroe Kennedy,Shuran Song,C. Karen Liu*

Main category: cs.RO

TL;DR: 该论文提出了"Locomotion Beyond Feet"系统，让人形机器人能够使用全身（包括手、膝盖、肘部）进行接触式运动，以应对极端复杂地形。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人运动方法主要关注基于腿部的步态，但自然双足生物在复杂环境中经常依赖手、膝盖和肘部建立额外接触点以获得稳定性和支撑。需要开发能够应对极端挑战性地形的全身运动系统。

Method: 结合物理基础的关键帧动画和强化学习。关键帧编码人类运动技能知识，强化学习将这些参考转化为鲁棒的物理精确运动。采用分层框架，包括特定地形运动跟踪策略、故障恢复机制和基于视觉的技能规划器。

Result: 真实世界实验表明，Locomotion Beyond Feet系统实现了鲁棒的全身运动，能够泛化到不同障碍物尺寸、障碍物实例和地形序列。

Conclusion: 通过结合关键帧动画和强化学习的分层方法，成功开发出能够在极端挑战性地形中实现全身接触式运动的人形机器人系统，超越了传统基于足部的运动限制。

Abstract: Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains. To this end, we combine physics-grounded keyframe animation with reinforcement learning. Keyframes encode human knowledge of motor skills, are embodiment-specific, and can be readily validated in simulation or on hardware, while reinforcement learning transforms these references into robust, physically accurate motions. We further employ a hierarchical framework consisting of terrain-specific motion-tracking policies, failure recovery mechanisms, and a vision-based skill planner. Real-world experiments demonstrate that Locomotion Beyond Feet achieves robust whole-body locomotion and generalizes across obstacle sizes, obstacle instances, and terrain sequences.

</details>


### [7] [PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation](https://arxiv.org/abs/2601.03782)
*Wenlong Huang,Yu-Wei Chao,Arsalan Mousavian,Ming-Yu Liu,Dieter Fox,Kaichun Mo,Li Fei-Fei*

Main category: cs.RO

TL;DR: PointWorld是一个大型预训练的3D世界模型，通过将状态和动作统一表示为3D点流，能够根据RGB-D图像和机器人动作命令预测3D空间中的像素位移响应，实现跨具身的通用操作能力。


<details>
  <summary>Details</summary>
Motivation: 人类能够从一瞥和身体动作中预测3D世界的响应，这种能力对机器人操作同样至关重要。现有方法通常使用特定于具体机器人的动作空间，限制了跨不同机器人平台的学习和泛化能力。

Method: 提出PointWorld模型，将状态和动作统一在共享3D空间中表示为3D点流。给定一个或多个RGB-D图像和低级机器人动作命令，模型预测响应给定动作的每像素3D位移。通过大规模数据集（约200万轨迹，500小时）训练，涵盖真实和模拟环境中的单臂Franka和双手机器人操作。

Result: PointWorld实现了实时推理（0.1秒），可集成到模型预测控制框架中。单个预训练检查点使真实Franka机器人能够执行刚体推动、可变形和关节物体操作以及工具使用，无需演示或后训练，仅需单张野外捕获图像。

Conclusion: PointWorld通过将动作表示为3D点流而非特定于具体机器人的动作空间，实现了跨具身的统一学习，为大规模3D世界建模提供了设计原则，展示了在真实世界机器人操作中的强大泛化能力。

Abstract: Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.

</details>


### [8] [Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots](https://arxiv.org/abs/2601.03807)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 结合代际替换和生命周期内学习可以在保持性能的同时增加进化机器人形态和控制器协同优化中的多样性


<details>
  <summary>Details</summary>
Motivation: 进化机器人学通过协同优化机器人的形态和控制器来自动设计机器人解决特定任务，但存在两个主要问题：1）控制器需要时间适应不断演化的形态，导致有前景的新设计难以进入种群；2）进化种群缺乏多样性，进化过程过早收敛到少数设计

Method: 提出结合代际替换（整个种群被后代机器人替换）和生命周期内学习（每个个体增加额外的控制器优化循环）的方法，并强调根据函数评估与进化代数评估性能指标的重要性

Result: 结合代际替换和生命周期内学习可以增加多样性同时保持性能，性能评估方式（按函数评估vs按进化代数）会影响研究结论

Conclusion: 代际替换与生命周期内学习的结合是解决进化机器人学中多样性-性能权衡的有效方法，同时需要谨慎选择性能评估指标来准确评估学习效果

Abstract: Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.

</details>


### [9] [Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics](https://arxiv.org/abs/2601.03813)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 该研究在进化机器人学中，针对形态-控制器协同优化问题，提出使用贝叶斯优化结合样本继承机制，在有限控制器学习预算下提升进化效率。


<details>
  <summary>Details</summary>
Motivation: 在进化机器人学中，自动设计机器人形态会产生形态-控制器协同优化问题。传统方法为每个新形态从头优化控制器需要大量学习预算，研究者希望找到在有限预算下提高效率的方法。

Method: 使用贝叶斯优化进行控制器优化，利用其样本效率和强探索能力，并采用拉马克式继承的样本继承机制。在故意设置的低控制器学习预算下，研究两种样本继承类型：(1)将父代所有样本转移给子代作为先验而不重新评估；(2)在子代上重新评估父代的最佳样本。与无继承基线进行比较。

Result: 重新评估方法表现最佳，基于先验的继承也优于无继承。分析表明，虽然单个形态的学习预算过低，但代际继承通过积累跨代学习适应来弥补这一点。继承主要使与父代相似的子代形态受益。研究还证明环境的关键作用，更具挑战性的环境会产生更稳定的行走步态。

Conclusion: 继承机制可以在不需要大量学习预算的情况下提升进化机器人学的性能，为设计更强大机器人提供高效路径。样本继承通过积累跨代学习适应，有效弥补了单个形态学习预算不足的问题。

Abstract: In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design.

</details>


### [10] [Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware](https://arxiv.org/abs/2601.03904)
*Korbinian Moller,Glenn Johannes Tungka,Lucas Jürgens,Johannes Betz*

Main category: cs.RO

TL;DR: 该论文提出了一种用于自动驾驶的实时运动规划方法，在嵌入式RTOS平台上部署轻量级采样轨迹规划器，为故障操作安全系统提供主动安全保障。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶安全验证方法（如在线验证）只能检测不可行的规划输出，缺乏在主规划器故障时确保安全运行的主动机制。需要开发能够在嵌入式实时系统上运行的轻量级规划器，为下一代故障操作安全框架提供基础。

Method: 在汽车级嵌入式平台上部署轻量级采样轨迹规划器，使用实时操作系统（RTOS）确保确定性时序行为。该方法在受限计算资源下持续计算轨迹，为未来紧急规划架构奠定基础。

Result: 实验结果表明，该方法具有确定性时序行为、有界延迟和最小抖动，验证了在安全可认证硬件上进行轨迹规划的可行性。代码已在GitHub开源。

Conclusion: 该研究展示了将主动后备机制整合到下一代安全框架中的潜力，同时指出了剩余挑战。这是实现故障操作自动驾驶主动安全扩展的重要第一步。

Abstract: Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning

</details>


### [11] [An Event-Based Opto-Tactile Skin](https://arxiv.org/abs/2601.03907)
*Mohammadreza Koolani,Simeon Bamford,Petr Trunin,Simon F. Müller-Cleve,Matteo Lo Preti,Fulvio Mastrogiovanni,Lucia Beccai,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 基于动态视觉传感器和柔性光学波导的神经形态触觉传感系统，通过立体视觉和DBSCAN聚类实现大面积软皮肤上的按压定位，即使在事件数据大幅减少时仍能保持功能


<details>
  <summary>Details</summary>
Motivation: 开发一种适用于大面积柔性皮肤的神经形态、事件驱动的触觉传感系统，为软机器人和交互环境提供灵活、响应迅速的触觉传感器

Method: 采用动态视觉传感器与柔性硅胶光学波导皮肤集成，通过立体视觉设置（两个DVS相机从侧面观察皮肤），利用三角测量定位按压位置，并使用DBSCAN聚类算法找到接触事件的质量中心

Result: 在4620 mm²的探测区域内，95%的按压可见时定位RMSE为4.66 mm；即使事件数据减少到原始大小的1/1024，平均定位误差仅增加到9.33 mm，85%的试验仍能产生有效定位；系统检测延迟分布特征宽度为31 ms

Conclusion: 该传感方法即使在事件数据非常稀疏的情况下仍能保持功能，为未来实现降低功耗和计算负载提供了有前景的途径，展示了在软机器人和交互环境中应用大面积柔性响应触觉传感器的潜力

Abstract: This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.

</details>


### [12] [CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM](https://arxiv.org/abs/2601.03956)
*Kangjie Zhou,Zhejia Wen,Zhiyong Zhuo,Zike Yan,Pengying Wu,Ieng Hou U,Shuaiyang Li,Han Gao,Kang Ding,Wenhan Cao,Wei Pan,Chang Liu*

Main category: cs.RO

TL;DR: CoINS是一个用于机器人交互导航的分层框架，通过技能感知的视觉语言模型进行反事实推理，结合强化学习技能库执行，显著提升了在复杂环境中的导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在机器人规划中主要作为语义推理器，缺乏对机器人具体物理能力的理解。在交互导航中，机器人需要主动修改杂乱环境来创建可通行路径，而现有VLM导航器主要局限于被动避障，无法推理何时以及如何与物体交互来清理阻塞路径。

Method: 提出了CoINS分层框架：1）微调InterNav-VLM模型，将技能可用性和具体约束参数融入输入上下文，并将其映射到度量尺度环境表示中；2）通过在InterNav数据集上微调，模型学习隐式评估物体移除对导航连通性的因果效应，确定交互必要性和目标选择；3）通过强化学习开发全面的技能库，引入面向可通行性的策略来操纵多样化物体以清理路径。

Result: 在Isaac Sim中建立了系统性基准测试，评估交互导航的推理和执行两方面。广泛的模拟和真实世界实验表明，CoINS显著优于代表性基线方法，总体成功率提高了17%，在复杂长视野场景中相比最佳基线有超过80%的改进。

Conclusion: CoINS通过集成技能感知推理和鲁棒低级执行，成功解决了VLM在交互导航中的局限性，使机器人能够主动修改环境以创建可通行路径，显著提升了导航性能。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\% higher overall success rate and over 80\% improvement in complex long-horizon scenarios compared to the best-performing baseline

</details>


### [13] [Stable Language Guidance for Vision-Language-Action Models](https://arxiv.org/abs/2601.04052)
*Zhihao Zhan,Yuhao Chen,Jiaying Zhou,Qinhan Lv,Hao Liu,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 该论文提出Residual Semantic Steering (RSS)框架，解决VLA模型中语言扰动脆弱性问题，通过解耦物理可供性与语义执行来增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action (VLA)模型在广义机器人控制方面表现出色，但对语言扰动极其脆弱。研究发现存在"模态坍塌"现象：强烈的视觉先验压倒稀疏的语言信号，导致智能体过度拟合特定指令措辞而忽略底层语义意图。

Method: 提出Residual Semantic Steering (RSS)概率框架，包含两个理论创新：1) Monte Carlo Syntactic Integration：通过密集的LLM驱动的分布扩展来近似真实语义后验；2) Residual Affordance Steering：双流解码机制，通过减去视觉可供性先验来显式隔离语言因果影响。

Result: 理论分析表明RSS能有效最大化动作与意图之间的互信息，同时抑制视觉干扰。在多样化操作基准测试中，RSS实现了最先进的鲁棒性，即使在对抗性语言扰动下也能保持性能。

Conclusion: RSS框架成功解决了VLA模型中的模态坍塌问题，通过解耦物理可供性和语义执行，显著提升了模型对语言扰动的鲁棒性，为更可靠的机器人控制提供了新方法。

Abstract: Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.

</details>


### [14] [CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos](https://arxiv.org/abs/2601.04061)
*Chubin Zhang,Jianan Wang,Zifeng Gao,Yue Su,Tianru Dai,Cai Zhou,Jiwen Lu,Yansong Tang*

Main category: cs.RO

TL;DR: CLAP提出了一种对比潜在动作预训练框架，通过将视频视觉潜在空间与机器人本体感知潜在空间对齐，利用人类视频数据提升机器人操作技能，解决了现有方法中的视觉纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 当前通用视觉-语言-动作模型面临机器人数据稀缺而人类视频数据丰富的困境。现有的潜在动作模型试图利用视频数据，但常常遭受视觉纠缠问题，捕捉到的是噪声而非操作技能。

Method: 提出对比潜在动作预训练（CLAP）框架，通过对比学习将视频转换映射到量化的、物理可执行的码本上。基于此表示，引入双重VLA框架：CLAP-NTP（自回归模型）擅长指令跟随和对象泛化；CLAP-RF（整流流策略）设计用于高频精确操作。还提出知识匹配正则化策略来缓解微调时的灾难性遗忘。

Result: 大量实验表明，CLAP显著优于强基线方法，能够有效地将人类视频中的技能转移到机器人执行中。

Conclusion: CLAP通过对比学习对齐视觉和本体感知空间，成功解决了视觉纠缠问题，实现了从人类视频到机器人操作的有效技能迁移，为通用VLA模型提供了有效的预训练框架。

Abstract: Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.

</details>


### [15] [Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test](https://arxiv.org/abs/2601.04137)
*Chun-Kai Fan,Xiaowei Chi,Xiaozhu Ju,Hao Li,Yong Bao,Yu-Kai Wang,Lizhang Chen,Zhiyuan Jiang,Kuangzhi Ge,Ying Li,Weishi Mi,Qingpo Wuwu,Peidong Jia,Yulin Luo,Kevin Zhang,Zhiyuan Qin,Yong Dai,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 论文提出了WoW-World-Eval基准测试，用于评估视频基础模型在具身AI中作为世界模型的性能，发现现有模型在长期规划和物理一致性方面表现有限，与现实世界存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 随着世界模型在具身AI中的兴起，视频基础模型被用作预测世界模型，但存在两个关键问题未解决：1) 生成泛化能力是否足以保持人类观察者的感知保真度；2) 是否足够稳健以作为现实世界具身智能体的通用先验。需要标准化框架来评估这些问题。

Method: 基于609个机器人操作数据构建了Embodied Turing Test基准测试：WoW-World-Eval，评估五个核心能力（感知、规划、预测、泛化和执行）。提出了包含22个指标的综合评估协议，建立了与人类偏好高度相关（>0.93）的可靠评估基础。

Result: 在WoW-World-Eval上，模型在长期规划方面仅得17.27分，物理一致性最好为68.02分，表明时空一致性和物理推理能力有限。在逆动态模型图灵测试中，大多数模型成功率接近0%，而WoW模型保持40.74%的成功率。

Conclusion: 生成视频与现实世界之间存在明显差距，凸显了在具身AI中基准测试世界模型的紧迫性和必要性。现有视频基础模型作为世界模型的能力仍有很大提升空间。

Abstract: As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.

</details>


### [16] [Embedding Autonomous Agents in Resource-Constrained Robotic Platforms](https://arxiv.org/abs/2601.04191)
*Negar Halakou,Juan F. Gutierrez,Ye Sun,Han Jiang,Xueming Wu,Yilun Song,Andres Gomez*

Main category: cs.RO

TL;DR: 将AgentSpeak智能体集成到双轮机器人中，使其在迷宫中自主决策导航，59秒内完成迷宫探索，推理过程高效适合资源受限硬件


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备在资源受限和动态环境中运行，需要本地决策能力。让设备独立决策可以提高系统响应性，减少对外部控制的依赖。

Method: 将使用AgentSpeak编程的自主智能体集成到小型双轮机器人中，机器人利用自身决策和传感器数据探索迷宫。

Result: 智能体在59秒内成功解决迷宫问题，使用了287个推理周期，决策阶段耗时不到1毫秒。推理过程效率足够在资源受限硬件上实时执行。

Conclusion: 该集成展示了高层基于智能体的控制如何应用于资源受限嵌入式系统以实现自主操作，证明了智能体推理在实时嵌入式环境中的可行性。

Abstract: Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.

</details>
