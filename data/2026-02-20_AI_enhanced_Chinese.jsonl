{"id": "2602.16744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16744", "abs": "https://arxiv.org/abs/2602.16744", "authors": ["Takuro Kato", "Mitsuharu Morisawa"], "title": "ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts", "comment": "Accepted and published in IEEE/SICE SII 2024", "summary": "This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\u4e0a\u5378\u8f7d\u6258\u76d8\u7684\u9632\u62d6\u62fd\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7ICP\u7b97\u6cd5\u5b9e\u65f6\u8ddf\u8e2a\u6258\u76d8\u4e0e\u8d27\u53c9\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\uff0c\u5b9e\u73b0\u5e73\u884c\u5bf9\u9f50\u540e\u6cbf\u503e\u659c\u65b9\u5411\u64a4\u56de\u8d27\u53c9", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\uff08\u5982\u5361\u8f66\u8d27\u7bb1\uff09\u4e0a\u5378\u8f7d\u6258\u76d8\u65f6\uff0c\u8d27\u53c9\u64a4\u56de\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u62d6\u62fd\u6258\u76d8\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u5378\u8f7d\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387", "method": "\u4f7f\u7528\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u7b97\u6cd5\u5904\u7406\u6258\u76d8\u4e0a\u90e8\u533a\u57df\u7684\u70b9\u4e91\u6570\u636e\uff0c\u5b9e\u65f6\u8ddf\u8e2a\u6258\u76d8\u4e0e\u8d27\u53c9\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\u89d2\u5ea6\u5dee\u5f02\uff0c\u6839\u636e\u8ddf\u8e2a\u7ed3\u679c\u5c06\u8d27\u53c9\u4e0e\u76ee\u6807\u8868\u9762\u5e73\u884c\u5bf9\u9f50\uff0c\u7136\u540e\u6cbf\u503e\u659c\u65b9\u5411\u64a4\u56de\u8d27\u53c9\u5b8c\u6210\u5378\u8f7d", "result": "\u901a\u8fc7\u52a8\u6001\u4eff\u771f\u548c\u771f\u5b9e\u53c9\u8f66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5728\u5361\u8f66\u503e\u659c\u8d27\u7bb1\u4e0a\u5b8c\u6210\u5378\u8f7d\u64cd\u4f5c\uff0c\u907f\u514d\u4e86\u6258\u76d8\u62d6\u62fd", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eICP\u7b97\u6cd5\u7684\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u81ea\u4e3b\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\u4e0a\u7684\u65e0\u62d6\u62fd\u6258\u76d8\u5378\u8f7d\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u7269\u6599\u642c\u8fd0\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027"}}
{"id": "2602.16758", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16758", "abs": "https://arxiv.org/abs/2602.16758", "authors": ["Sina Akhbari", "Mehran Mahboubkhah"], "title": "Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot", "comment": "30 pages, 17 figures, published in Elsevier Precision Engineering (https://www.sciencedirect.com/science/article/abs/pii/S0141635925001266)", "summary": "This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u56db\u81ea\u7531\u5ea6\u5e76\u8054\u94e3\u524a\u673a\u5668\u4eba\u7684\u5e73\u6ed1\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408B\u6837\u6761\u548c\u56db\u5143\u6570\u63d2\u503c\u6280\u672f\u5904\u7406\u89e3\u8026\u7684\u4f4d\u7f6e\u548c\u59ff\u6001\u6570\u636e\u70b9\uff0c\u901a\u8fc7\u5206\u6bb5\u8d1d\u585e\u5c14\u66f2\u7ebf\u540c\u6b65\u59ff\u6001\u548c\u5f27\u957f\u53c2\u6570\u5316\u4f4d\u7f6e\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u5728\u5e76\u8054\u94e3\u524a\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u4e2d\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u3001\u901f\u5ea6\u6ce2\u52a8\u5927\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u7a7a\u95f4\u7ea6\u675f\u548c\u65f6\u95f4\u4f18\u5316\u3001\u907f\u514d\u4e07\u5411\u8282\u9501\u6b7b\u7684\u9ad8\u6548\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528B\u6837\u6761\u548c\u56db\u5143\u6570\u63d2\u503c\u6280\u672f\u5904\u7406\u89e3\u8026\u7684\u4f4d\u7f6e\u548c\u59ff\u6001\u6570\u636e\uff1b\u901a\u8fc7\u5206\u6bb5\u8d1d\u585e\u5c14\u66f2\u7ebf\u62df\u5408\u8def\u5f84\u957f\u5ea6\u4e0e\u5de5\u5177\u59ff\u6001\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u5229\u7528\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u51f8\u5305\u7279\u6027\u786e\u4fdd\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u79bb\u7ea6\u675f\uff1b\u4f7f\u7528\u56db\u5143\u6570\u8fdb\u884c\u59ff\u6001\u63d2\u503c\u907f\u514d\u4e07\u5411\u8282\u9501\u6b7b\uff1b\u91c7\u7528\u4fee\u6b63\u591a\u9879\u5f0f\u8fdb\u884c\u4f4d\u7f6e\u63d2\u503c\uff1b\u901a\u8fc7\u6700\u5c0f\u52a0\u52a0\u901f\u5ea6\u548c\u65f6\u95f4\u6700\u4f18\u7684\u5206\u6bb5\u8d1d\u585e\u5c14\u66f2\u7ebf\u8fdb\u884c\u4e24\u9636\u6bb5\u4f18\u5316\uff08\u4efb\u52a1\u7a7a\u95f4\u540e\u5173\u8282\u7a7a\u95f4\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3001\u66f4\u5c0f\u7684\u901f\u5ea6\u6ce2\u52a8\u548c\u66f4\u597d\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u5728\u4f4e\u6210\u672c\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u81ea\u7531\u5ea6\u5e76\u8054\u94e3\u524a\u673a\u5668\u4eba\u7684\u5e73\u6ed1\u8f68\u8ff9\u751f\u6210\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u540c\u6b65\u6280\u672f\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u8fd0\u52a8\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16825", "abs": "https://arxiv.org/abs/2602.16825", "authors": ["Ahmad Ahmad", "Shuo Liu", "Roberto Tron", "Calin Belta"], "title": "RRT$^\u03b7$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness", "comment": null, "summary": "Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.\n  We propose RRT$^\u03b7$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.", "AI": {"tldr": "RRT^\u03b7\uff1a\u4e00\u79cd\u7ed3\u5408\u7b97\u672f\u51e0\u4f55\u5e73\u5747\u9c81\u68d2\u6027\u5ea6\u91cf\u7684\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u6ee1\u8db3\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u65f6\u7a7a\u7ea6\u675f\u4e0b\u8868\u73b0\u66f4\u4f18", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u7ed3\u5408\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u7684\u65b9\u6cd5\u4f7f\u7528\u6700\u5c0f-\u6700\u5927\u9c81\u68d2\u6027\u5ea6\u91cf\uff0c\u53ea\u5173\u6ce8\u5173\u952e\u65f6\u95f4\u70b9\u548c\u5b50\u516c\u5f0f\uff0c\u5bfc\u81f4\u975e\u5e73\u6ed1\u4f18\u5316\u666f\u89c2\u548c\u5c16\u9510\u51b3\u7b56\u8fb9\u754c\uff0c\u963b\u788d\u4e86\u9ad8\u6548\u7684\u6811\u63a2\u7d22", "method": "\u63d0\u51faRRT^\u03b7\u6846\u67b6\uff0c\u96c6\u6210\u7b97\u672f\u51e0\u4f55\u5e73\u5747\u9c81\u68d2\u6027\u5ea6\u91cf\u8bc4\u4f30\u6240\u6709\u65f6\u95f4\u70b9\u548c\u5b50\u516c\u5f0f\u7684\u6ee1\u8db3\u7a0b\u5ea6\uff1b\u5305\u62ecAGM\u9c81\u68d2\u6027\u533a\u95f4\u8bed\u4e49\u3001\u9ad8\u6548\u589e\u91cf\u76d1\u63a7\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5229\u7528\u6ee1\u8db3\u4f18\u5148\u7ea7\u903b\u8f91\u589e\u5f3a\u7684\u6ee1\u610f\u5ea6\u589e\u52a0\u65b9\u5411\u5411\u91cf", "result": "\u6846\u67b6\u80fd\u591f\u5408\u6210\u6ee1\u8db3STL\u89c4\u8303\u7684\u9ad8\u9c81\u68d2\u6027\u52a8\u6001\u53ef\u884c\u63a7\u5236\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u6301RRT*\u7684\u6982\u7387\u5b8c\u5907\u6027\u548c\u6e10\u8fd1\u6700\u4f18\u6027\uff1b\u5728\u53cc\u79ef\u5206\u70b9\u673a\u5668\u4eba\u3001\u72ec\u8f6e\u8f66\u79fb\u52a8\u673a\u5668\u4eba\u548c7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\uff0c\u5728\u591a\u91cd\u7ea6\u675f\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edfSTL\u9c81\u68d2\u6027\u89c4\u5212\u5668", "conclusion": "RRT^\u03b7\u6846\u67b6\u901a\u8fc7AGM\u9c81\u68d2\u6027\u5ea6\u91cf\u89e3\u51b3\u4e86\u4f20\u7edfSTL\u89c4\u5212\u4e2d\u7684\u975e\u5e73\u6ed1\u4f18\u5316\u95ee\u9898\uff0c\u5728\u590d\u6742\u65f6\u7a7a\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u8fd0\u52a8\u89c4\u5212"}}
{"id": "2602.16846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16846", "abs": "https://arxiv.org/abs/2602.16846", "authors": ["Xili Yi", "Ying Xing", "Zachary Manchester", "Nima Fazeli"], "title": "Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations", "comment": "12 pages, 10 figures", "summary": "Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.", "AI": {"tldr": "Sound of Touch\uff1a\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u5f26\u7684\u4e3b\u52a8\u58f0\u5b66\u89e6\u89c9\u4f20\u611f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u62fe\u97f3\u5668\u68c0\u6d4b\u63a5\u89e6\u5f15\u8d77\u7684\u9891\u8c31\u53d8\u5316\uff0c\u5b9e\u73b0\u6beb\u7c73\u7ea7\u5b9a\u4f4d\u3001\u529b\u4f30\u8ba1\u548c\u5b9e\u65f6\u6ed1\u79fb\u68c0\u6d4b\u3002", "motivation": "\u5206\u5e03\u5f0f\u89e6\u89c9\u4f20\u611f\u5728\u5927\u9762\u79ef\u5e94\u7528\u65f6\u9762\u4e34\u6311\u6218\uff1a\u5bc6\u96c6\u4f20\u611f\u5668\u9635\u5217\u589e\u52a0\u5e03\u7ebf\u3001\u6210\u672c\u548c\u8106\u5f31\u6027\uff0c\u800c\u8bb8\u591a\u66ff\u4ee3\u65b9\u6848\u8986\u76d6\u6709\u9650\u6216\u65e0\u6cd5\u6355\u6349\u5feb\u901f\u4ea4\u4e92\u52a8\u6001\u3002", "method": "\u4f7f\u7528\u632f\u52a8\u5f20\u7d27\u5f26\u4f5c\u4e3a\u4f20\u611f\u5143\u4ef6\uff0c\u901a\u8fc7\u7535\u78c1\u8fde\u7eed\u6fc0\u52b1\u5f26\uff0c\u5c11\u91cf\u63a5\u89e6\u5f0f\u9ea6\u514b\u98ce\u89c2\u5bdf\u63a5\u89e6\u5f15\u8d77\u7684\u9891\u8c31\u53d8\u5316\u3002\u5f00\u53d1\u57fa\u4e8e\u7269\u7406\u7684\u5f26\u632f\u52a8\u6a21\u62df\u5668\u9884\u6d4b\u63a5\u89e6\u4f4d\u7f6e\u548c\u529b\u5982\u4f55\u6539\u53d8\u632f\u52a8\u6a21\u5f0f\uff0c\u5e76\u5efa\u7acb\u5b9e\u65f6\u63a8\u7406\u7ba1\u9053\u5c06\u632f\u52a8\u6d4b\u91cf\u6620\u5c04\u5230\u63a5\u89e6\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6beb\u7c73\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u53ef\u9760\u7684\u529b\u4f30\u8ba1\u548c\u5b9e\u65f6\u6ed1\u79fb\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u5f26\u57fa\u89e6\u89c9\u4f20\u611f\u786c\u4ef6\u6982\u5ff5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8868\u9762\u7684\u5927\u9762\u79ef\u4f20\u611f\uff1b\u5f00\u53d1\u4e86\u7269\u7406\u57fa\u7840\u7684\u6a21\u62df\u5206\u6790\u5de5\u5177\u548c\u5b9e\u65f6\u63a8\u7406\u7ba1\u9053\u3002"}}
{"id": "2602.16861", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16861", "abs": "https://arxiv.org/abs/2602.16861", "authors": ["EunJeong Cheon", "Do Yeon Shin"], "title": "\"Hello, I'm Delivering. Let Me Pass By\": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets", "comment": null, "summary": "As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.", "AI": {"tldr": "\u63d0\u51fa\"Walk-Along with Robots\"\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u516c\u5171\u7a7a\u95f4\u4e2d\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u771f\u5b9e\u4ea4\u4e92", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u65e5\u76ca\u666e\u53ca\uff0c\u73b0\u6709HRI\u7814\u7a76\u65b9\u6cd5\uff08\u5982\u53d7\u63a7\u5b9e\u9a8c\u3001\u7ed3\u6784\u5316\u89c2\u5bdf\uff09\u96be\u4ee5\u7814\u7a76\u771f\u5b9e\u73af\u5883\u4e2d\u4e0d\u53d7\u7814\u7a76\u8005\u63a7\u5236\u7684\u81ea\u4e3b\u673a\u5668\u4eba\uff08\u5982\u914d\u9001\u673a\u5668\u4eba\uff09\uff0c\u9700\u8981\u65b0\u7684\u7814\u7a76\u65b9\u6cd5", "method": "\u501f\u9274\u57ce\u5e02\u7814\u7a76\u3001\u5730\u7406\u5b66\u548c\u793e\u4f1a\u5b66\u4e2d\u7684\u516c\u5171\u9886\u57df\u6c11\u65cf\u5fd7\u65b9\u6cd5\uff0c\u63d0\u51faWalk-Along with Robots\u65b9\u6cd5\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u7279\u5f81\u3001\u5b9e\u65bd\u6b65\u9aa4\u3001\u72ec\u7279\u89c1\u89e3\u548c\u8bc4\u4f30\u65b9\u5f0f", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u6355\u6349\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u52a8\u6001\u8def\u7ebf\u548c\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u4ea4\u4e92\uff0c\u4e3a\u7814\u7a76\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u5de5\u5177", "conclusion": "Walk-Along with Robots\u65b9\u6cd5\u4e3a\u7814\u7a76\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e0c\u671b\u6fc0\u53d1\u5173\u4e8e\u81ea\u4e3b\u673a\u5668\u4eba\u7814\u7a76\u65b9\u6cd5\u8bba\u7684\u8fdb\u4e00\u6b65\u8ba8\u8bba"}}
{"id": "2602.16863", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16863", "abs": "https://arxiv.org/abs/2602.16863", "authors": ["Kushal Kedia", "Tyler Ga Wei Lum", "Jeannette Bohg", "C. Karen Liu"], "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation", "comment": null, "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.", "AI": {"tldr": "SimToolReal\uff1a\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u5927\u91cf\u5de5\u5177\u72b6\u7269\u4f53\u57fa\u5143\u5e76\u8bad\u7ec3\u5355\u4e00\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u7684\u901a\u7528\u5de5\u5177\u64cd\u4f5c\uff0c\u65e0\u9700\u7279\u5b9a\u7269\u4f53\u6216\u4efb\u52a1\u8bad\u7ec3", "motivation": "\u5de5\u5177\u64cd\u4f5c\u662f\u673a\u5668\u4eba\u7075\u5de7\u6027\u7684\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u6293\u53d6\u8584\u7269\u4f53\u3001\u624b\u5185\u65cb\u8f6c\u548c\u5f3a\u529b\u4ea4\u4e92\u3002\u7531\u4e8e\u6536\u96c6\u9065\u64cd\u4f5c\u6570\u636e\u56f0\u96be\uff0c\u6a21\u62df\u5230\u771f\u5b9e\u7684\u5f3a\u5316\u5b66\u4e60\u662f\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u52aa\u529b\u6765\u5efa\u6a21\u7269\u4f53\u548c\u8c03\u6574\u6bcf\u4e2a\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u3002", "method": "\u63d0\u51faSimToolReal\u65b9\u6cd5\uff1a\u5728\u6a21\u62df\u4e2d\u7a0b\u5e8f\u5316\u751f\u6210\u5927\u91cf\u5de5\u5177\u72b6\u7269\u4f53\u57fa\u5143\uff0c\u8bad\u7ec3\u5355\u4e00\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u5c06\u6bcf\u4e2a\u7269\u4f53\u64cd\u7eb5\u5230\u968f\u673a\u76ee\u6807\u59ff\u6001\u4e3a\u901a\u7528\u76ee\u6807\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u6267\u884c\u901a\u7528\u7075\u5de7\u5de5\u5177\u64cd\u4f5c\uff0c\u65e0\u9700\u4efb\u4f55\u7269\u4f53\u6216\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "result": "SimToolReal\u6bd4\u4e4b\u524d\u7684\u91cd\u5b9a\u5411\u548c\u56fa\u5b9a\u6293\u53d6\u65b9\u6cd5\u6027\u80fd\u9ad8\u51fa37%\uff0c\u540c\u65f6\u4e0e\u5728\u7279\u5b9a\u76ee\u6807\u7269\u4f53\u548c\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u4e13\u5bb6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6027\u80fd\u76f8\u5f53\u3002\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff1a\u8d85\u8fc7120\u6b21\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\uff0c\u6db5\u76d624\u4e2a\u4efb\u52a1\u300112\u4e2a\u7269\u4f53\u5b9e\u4f8b\u548c6\u4e2a\u5de5\u5177\u7c7b\u522b\u3002", "conclusion": "SimToolReal\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u591a\u6837\u5316\u5de5\u5177\u72b6\u7269\u4f53\u548c\u8bad\u7ec3\u901a\u7528\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5de5\u5177\u64cd\u4f5c\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u6a21\u62df\u5230\u771f\u5b9e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u5de5\u5177\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.16870", "categories": ["cs.RO", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.16870", "abs": "https://arxiv.org/abs/2602.16870", "authors": ["Daniil Lisus", "Katya M. Papais", "Cedric Le Gentil", "Elliot Preston-Krebs", "Andrew Lambert", "Keith Y. K. Leung", "Timothy D. Barfoot"], "title": "Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads", "comment": "23 pages, 15 figures, 12 tables, submitted to The International Journal of Robotics Research (IJRR)", "summary": "The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.", "AI": {"tldr": "Boreas-RT\u6570\u636e\u96c6\u6269\u5c55\u4e86\u539f\u6709\u7684Boreas\u6570\u636e\u96c6\uff0c\u5305\u542b9\u6761\u4e0d\u540c\u8def\u7ebf\u7684643\u516c\u91cc\u9a7e\u9a76\u6570\u636e\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u5728\u591a\u6837\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8\u591a\u6a21\u6001\u611f\u77e5\u548c\u5b9a\u4f4d\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u901a\u5e38\u5728\u7b80\u5355\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u6837\u5316\u3001\u590d\u6742\u7684\u771f\u5b9e\u9053\u8def\u6761\u4ef6\u4e0b\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u3001\u591a\u6837\u5316\u8def\u7ebf\u548c\u591a\u5b63\u8282\u53d8\u5316\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u914d\u5907\u591a\u6a21\u6001\u4f20\u611f\u5668\u7684\u8f66\u8f86\u5e73\u53f0\uff0c\u57289\u6761\u4e0d\u540c\u8def\u7ebf\u4e0a\u6536\u96c660\u4e2a\u5e8f\u5217\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u5305\u62ec\u6444\u50cf\u5934\u3001\u96f7\u8fbe\u3001\u6fc0\u5149\u96f7\u8fbe\u3001IMU\u548c\u8f6e\u901f\u7f16\u7801\u5668\u7b49\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u3002\u63d0\u4f9b\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684GNSS-INS\u5730\u9762\u771f\u503c\uff0c\u4ee5\u53ca\u7cbe\u786e\u7684\u5916\u53c2\u548c\u5185\u53c2\u6807\u5b9a\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u91cc\u7a0b\u8ba1\u548c\u5b9a\u4f4d\u7b97\u6cd5\u5728\u7b80\u5355\u7684\u9a7e\u9a76\u73af\u5883\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684Boreas-RT\u8def\u7ebf\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u7b97\u6cd5\u5728\u4e0d\u540c\u9053\u8def\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u3002", "conclusion": "Boreas-RT\u6570\u636e\u96c6\u586b\u8865\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u8bc4\u4f30\u5728\u591a\u6837\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u3001\u7cbe\u786e\u6807\u5b9a\u548c\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u53d1\u5c55\u3002\u6570\u636e\u96c6\u3001\u5f00\u53d1\u5de5\u5177\u5305\u548c\u6392\u884c\u699c\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.16898", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16898", "abs": "https://arxiv.org/abs/2602.16898", "authors": ["Iman Ahmadi", "Mehrshad Taji", "Arad Mahdinezhad Kashani", "AmirHossein Jadidi", "Saina Kashani", "Babak Khalaj"], "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation", "comment": null, "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.", "AI": {"tldr": "MALLVi\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u89c6\u89c9\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u9a71\u52a8\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u534f\u8c03\u5b8c\u6210\u611f\u77e5\u3001\u5b9a\u4f4d\u3001\u63a8\u7406\u548c\u89c4\u5212\uff0c\u63d0\u9ad8\u96f6\u6837\u672c\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e13\u95e8\u6a21\u578b\u3001\u5fae\u8c03\u6216\u63d0\u793a\u8c03\u6574\uff0c\u4e14\u4ee5\u5f00\u73af\u65b9\u5f0f\u8fd0\u884c\uff0c\u7f3a\u4e4f\u9c81\u68d2\u7684\u73af\u5883\u53cd\u9988\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u8106\u5f31\u3002", "method": "MALLVi\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u5305\u542b\u5206\u89e3\u5668\u3001\u5b9a\u4f4d\u5668\u3001\u601d\u8003\u5668\u548c\u53cd\u5c04\u5668\u7b49\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u73af\u5883\u53cd\u9988\uff0c\u5b9e\u73b0\u95ed\u73af\u63a7\u5236\u3002\u53cd\u5c04\u5668\u652f\u6301\u9488\u5bf9\u6027\u9519\u8bef\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u907f\u514d\u5b8c\u5168\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fed\u4ee3\u95ed\u73af\u591a\u667a\u80fd\u4f53\u534f\u8c03\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u8457\u589e\u52a0\u4e86\u96f6\u6837\u672c\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "conclusion": "MALLVi\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u8c03\u548c\u95ed\u73af\u53cd\u9988\u673a\u5236\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u3001\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.16911", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16911", "abs": "https://arxiv.org/abs/2602.16911", "authors": ["Adrian R\u00f6fer", "Nick Heppert", "Abhinav Valada"], "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations", "comment": "9 pages, 6 figures, under review", "summary": "Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u5f62\u5316\u5bf9\u8c61\u5173\u7cfb\u8868\u793a\u573a\u666f\u72b6\u6001\u6f14\u5316\uff0c\u4f7f\u7528\u6f14\u793a\u5206\u5272\u548c\u6c60\u5316\u6280\u672f\u63d0\u53d6\u64cd\u4f5c\u56fe\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u5bf9\u8c61\u5339\u914d\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u9ad8\u6548\u5b66\u4e60\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u662f\u673a\u5668\u4eba\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5728\u52a8\u4f5c\u57df\u76f4\u63a5\u5b66\u4e60\u4efb\u52a1\uff0c\u800c\u672c\u6587\u5173\u6ce8\u63a8\u65ad\u673a\u5668\u4eba\u5e94\u8be5\u5b9e\u73b0\u4ec0\u4e48\uff08\u76ee\u6807\uff09\uff0c\u800c\u4e0d\u662f\u5982\u4f55\u5b9e\u73b0\uff08\u65b9\u6cd5\uff09\u3002", "method": "\u4f7f\u7528\u56fe\u5f62\u5316\u5bf9\u8c61\u5173\u7cfb\u8868\u793a\u6f14\u5316\u573a\u666f\u72b6\u6001\uff1b\u63d0\u51fa\u6f14\u793a\u5206\u5272\u548c\u6c60\u5316\u65b9\u6cd5\u63d0\u53d6\u64cd\u4f5c\u56fe\u5e76\u4f30\u8ba1\u4efb\u52a1\u5404\u9636\u6bb5\u7684\u5bf9\u8c61\u72b6\u6001\u5206\u5e03\uff1b\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u5bf9\u8c61\u5339\u914d\u4ee5\u63d0\u9ad8\u591a\u6f14\u793a\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8bc4\u4f30\u4e86\u6f14\u793a\u5206\u5272\u51c6\u786e\u6027\u4ee5\u53ca\u4ece\u591a\u6f14\u793a\u4e2d\u5b66\u4e60\u5bfb\u627e\u671f\u671b\u6700\u5c0f\u4efb\u52a1\u6a21\u578b\u7684\u6548\u7528\uff1b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u62df\u5408\u6a21\u578b\uff0c\u8bc1\u660e\u6240\u5f97\u4efb\u52a1\u8868\u793a\u652f\u6301\u8de8\u73af\u5883\u7684\u53ef\u9760\u6267\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u56fe\u5f62\u5316\u8868\u793a\u548c\u5bf9\u8c61\u5339\u914d\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4ece\u6f14\u793a\u4e2d\u63d0\u53d6\u9c81\u68d2\u7684\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u53ef\u9760\u6267\u884c\u3002"}}
{"id": "2602.17101", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17101", "abs": "https://arxiv.org/abs/2602.17101", "authors": ["Varun Burde", "Pavel Burget", "Torsten Sattler"], "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success", "comment": null, "summary": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f306D\u59ff\u6001\u4f30\u8ba1\u5668\u548c3D\u7f51\u683c\u6a21\u578b\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u91cd\u5efa\u8d28\u91cf\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d3D\u91cd\u5efa\u65b9\u6cd5\u867d\u7136\u80fd\u4ea7\u751f\u89c6\u89c9\u548c\u51e0\u4f55\u4e0a\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7f51\u683c\uff0c\u4f46\u6807\u51c6\u51e0\u4f55\u8bc4\u4f30\u65e0\u6cd5\u53cd\u6620\u91cd\u5efa\u8d28\u91cf\u5982\u4f55\u5f71\u54cd\u673a\u5668\u4eba\u6293\u53d6\u7b49\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u5927\u89c4\u6a21\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u4e0d\u540c\u91cd\u5efa\u76843D\u7f51\u683c\u4e0a\u751f\u6210\u6293\u53d6\u59ff\u6001\uff0c\u7136\u540e\u5728\u771f\u5b9e\u6a21\u578b\u4e0a\u6267\u884c\u8fd9\u4e9b\u59ff\u6001\uff0c\u6a21\u62df\u4f7f\u7528\u4e0d\u5b8c\u7f8e\u6a21\u578b\u751f\u6210\u7684\u6293\u53d6\u59ff\u6001\u5982\u4f55\u5f71\u54cd\u4e0e\u771f\u5b9e\u7269\u4f53\u7684\u4ea4\u4e92\u3002", "result": "\u91cd\u5efa\u4f2a\u5f71\u663e\u8457\u51cf\u5c11\u4e86\u6293\u53d6\u59ff\u6001\u5019\u9009\u6570\u91cf\uff0c\u4f46\u5728\u59ff\u6001\u4f30\u8ba1\u51c6\u786e\u7684\u60c5\u51b5\u4e0b\u5bf9\u6293\u53d6\u6027\u80fd\u5f71\u54cd\u53ef\u5ffd\u7565\uff1b\u6293\u53d6\u6210\u529f\u4e0e\u59ff\u6001\u8bef\u5dee\u7684\u5173\u7cfb\u4e3b\u8981\u7531\u7a7a\u95f4\u8bef\u5dee\u4e3b\u5bfc\uff0c\u5373\u4f7f\u7b80\u5355\u7684\u5e73\u79fb\u8bef\u5dee\u4e5f\u80fd\u4e3a\u5bf9\u79f0\u7269\u4f53\u7684\u6293\u53d6\u6210\u529f\u63d0\u4f9b\u6d1e\u5bdf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3\u611f\u77e5\u7cfb\u7edf\u5982\u4f55\u4e0e\u673a\u5668\u4eba\u7269\u4f53\u64cd\u4f5c\u76f8\u5173\u8054\u63d0\u4f9b\u4e86\u6d1e\u5bdf\uff0c\u5efa\u7acb\u4e86\u91cd\u5efa\u8d28\u91cf\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u529f\u80fd\u8054\u7cfb\u3002"}}
{"id": "2602.17110", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17110", "abs": "https://arxiv.org/abs/2602.17110", "authors": ["Tanisha Parulekar", "Ge Shi", "Josh Pinskier", "David Howard", "Jen Jen Chung"], "title": "Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching", "comment": null, "summary": "A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d(CFM)\u5c06\u521a\u6027\u5939\u722a\u7684\u6293\u53d6\u59ff\u6001\u6620\u5c04\u5230\u8f6f\u4f53Fin-ray\u5939\u722a\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8f6f\u4f53\u5939\u722a\u7684\u6293\u53d6\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u6293\u53d6\u5408\u6210\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u5e73\u884c\u5939\u722a\u8bbe\u8ba1\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u8f6f\u4f53\u5939\u722a\u65f6\u65e0\u6cd5\u6355\u6349\u5176\u72ec\u7279\u7684\u987a\u5e94\u6027\u884c\u4e3a\uff0c\u5bfc\u81f4\u6570\u636e\u9700\u6c42\u5927\u4e14\u51c6\u786e\u6027\u4f4e\uff0c\u9700\u8981\u5728\u521a\u6027\u5939\u722a\u548c\u8f6f\u4f53\u5939\u722a\u4e4b\u95f4\u5efa\u7acb\u6709\u6548\u7684\u6293\u53d6\u59ff\u6001\u6620\u5c04", "method": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6d41\u5339\u914d(CFM)\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u6570\u636e\u6536\u96c6\u7ba1\u9053\u751f\u6210\u914d\u5bf9\u7684\u521a\u6027-\u8f6f\u4f53\u6293\u53d6\u59ff\u6001\uff1b2) \u4f7f\u7528U-Net\u81ea\u7f16\u7801\u5668\u4ece\u6df1\u5ea6\u56fe\u50cf\u4e2d\u63d0\u53d6\u7269\u4f53\u51e0\u4f55\u7279\u5f81\u4f5c\u4e3aCFM\u7684\u6761\u4ef6\uff1b3) \u5b66\u4e60\u4ece\u521d\u59cbAnygrasp\u59ff\u6001\u5230\u7a33\u5b9aFin-ray\u5939\u722a\u59ff\u6001\u7684\u8fde\u7eed\u6620\u5c04", "result": "\u57287-DOF\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cCFM\u751f\u6210\u7684\u59ff\u6001\u76f8\u6bd4\u57fa\u7ebf\u521a\u6027\u59ff\u6001\u663e\u8457\u63d0\u5347\uff1a\u6574\u4f53\u6210\u529f\u7387\u4ece6%/25%\u63d0\u5347\u523034%/46%\uff08\u5df2\u89c1/\u672a\u89c1\u7269\u4f53\uff09\uff1b\u5706\u67f1\u4f53\u6210\u529f\u7387\u4ece50%/100%\u63d0\u5347\u523050%/100%\uff1b\u7403\u4f53\u6210\u529f\u7387\u4ece25%/31%\u63d0\u5347\u523025%/31%\uff1b\u6a21\u578b\u80fd\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u7269\u4f53", "conclusion": "CFM\u662f\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5728\u521a\u6027\u5939\u722a\u548c\u8f6f\u4f53\u5939\u722a\u4e4b\u95f4\u8f6c\u79fb\u6293\u53d6\u7b56\u7565\uff0c\u4e3a\u5176\u4ed6\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u8bba"}}
{"id": "2602.17128", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17128", "abs": "https://arxiv.org/abs/2602.17128", "authors": ["Huishi Huang", "Jack Klusmann", "Haozhe Wang", "Shuchen Ji", "Fengkang Ying", "Yiyuan Zhang", "John Nassour", "Gordon Cheng", "Daniela Rus", "Jun Liu", "Marcelo H Ang", "Cecilia Laschi"], "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy", "comment": "Camera-ready version for RoboSoft 2026. 8 pages, 6 figures", "summary": "Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u589e\u5f3a\u73b0\u5b9e\u7684\u6df7\u5408\u521a\u67d4\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7AR\u5934\u663e\u5b9e\u73b0\u865a\u62df\u4eff\u771f\u4e0e\u7269\u7406\u7cfb\u7edf\u7684\u53e0\u52a0\uff0c\u5f15\u5165\u5b9e-\u4eff\u53c2\u6570\u8bc6\u522b\u786e\u4fdd\u4e00\u81f4\u6027", "motivation": "\u6df7\u5408\u521a\u67d4\u673a\u5668\u4eba\u7ed3\u5408\u4e86\u521a\u6027\u673a\u68b0\u81c2\u7684\u7cbe\u786e\u6027\u548c\u8f6f\u4f53\u624b\u81c2\u7684\u987a\u5e94\u6027\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u534f\u8c03\u63a7\u5236\u9762\u4e34\u5efa\u6a21\u3001\u611f\u77e5\u548c\u8de8\u57df\u8fd0\u52a8\u5b66\u7684\u6311\u6218", "method": "\u5f00\u53d1AR\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u7528\u6237\u901a\u8fc7AR\u5934\u663e\u4e0e\u96c6\u6210\u5728\u901a\u7528\u7269\u7406\u5f15\u64ce\u4e2d\u7684\u673a\u5668\u4eba\u4eff\u771f\u6a21\u578b\u4ea4\u4e92\uff0c\u6a21\u578b\u53e0\u52a0\u5728\u771f\u5b9e\u7cfb\u7edf\u4e0a\uff1b\u5f15\u5165\u57fa\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u51e0\u4f55\u7279\u6027\u7684\u5b9e-\u4eff\u53c2\u6570\u8bc6\u522b\u7ba1\u9053\uff0c\u51c6\u786e\u5efa\u6a21\u9759\u6001\u52a8\u6001\u884c\u4e3a\u548c\u63a7\u5236\u54cd\u5e94", "result": "\u5b9e\u73b0\u4e86\u6df7\u5408\u521a\u67d4\u673a\u5668\u4eba\u7684\u76f4\u63a5\u9065\u64cd\u4f5c\uff0c\u652f\u6301\u7b80\u5355\u7684\u5230\u8fbe\u548c\u6293\u53d6\u4efb\u52a1\uff0c\u786e\u4fdd\u865a\u62df\u673a\u5668\u4eba\u4e0e\u7269\u7406\u673a\u5668\u4eba\u884c\u4e3a\u4e00\u81f4", "conclusion": "AR\u6846\u67b6\u4e3a\u6df7\u5408\u521a\u67d4\u673a\u5668\u4eba\u63d0\u4f9b\u76f4\u89c2\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u53c2\u6570\u8bc6\u522b\u7ba1\u9053\u89e3\u51b3\u4e86\u5efa\u6a21\u6311\u6218\uff0c\u4e3a\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u6b64\u7c7b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2602.17166", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17166", "abs": "https://arxiv.org/abs/2602.17166", "authors": ["Antonio Franchi", "Chiara Gabellieri"], "title": "Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft", "comment": null, "summary": "We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fa\u5b9a\u7ffc\u98de\u673a\u7684\u65e0\u5750\u6807\u9006\u98de\u884c\u52a8\u529b\u5b66\u516c\u5f0f\uff0c\u5728SO(3)\u4e0a\u5efa\u7acb\u51e0\u4f55\u6846\u67b6\uff0c\u907f\u514d\u5c40\u90e8\u59ff\u6001\u5750\u6807\uff0c\u63a8\u5bfc\u51fa\u8f68\u8ff9\u5230\u8f93\u5165\u7684\u95ed\u5f0f\u6620\u5c04\uff0c\u5e76\u5e94\u7528\u4e8e\u7cfb\u7559\u98de\u884c\u5206\u6790\u3002", "motivation": "\u5c06\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u51e0\u4f55\u5efa\u6a21\u65b9\u6cd5\u4e0e\u822a\u7a7a\u9886\u57df\u7684\u9006\u4eff\u771f\u76f8\u7ed3\u5408\uff0c\u4e3a\u8f68\u8ff9\u8bbe\u8ba1\u548c\u53ef\u884c\u6027\u68c0\u67e5\u63d0\u4f9b\u4e25\u683c\u7684\u6570\u5b66\u57fa\u7840\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4f9d\u8d56\u5c40\u90e8\u5750\u6807\u7684\u5c40\u9650\u6027\u3002", "method": "\u5728SO(3)\u4e0a\u5efa\u7acb\u65e0\u5750\u6807\u516c\u5f0f\uff1a\u5e73\u79fb\u529b\u5e73\u8861\u5728\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\u8868\u8fbe\uff0c\u65cb\u8f6c\u52a8\u529b\u5b66\u5728\u673a\u4f53\u5750\u6807\u7cfb\u4e2d\u8868\u8fbe\uff1b\u51e0\u4f55\u5b9a\u4e49\u6c14\u52a8\u65b9\u5411\uff08\u963b\u529b\u3001\u5347\u529b\u3001\u4fa7\u529b\uff09\uff1b\u5f3a\u5236\u534f\u8c03\u98de\u884c\uff08\u65e0\u4fa7\u6ed1\uff09\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u7684\u8f68\u8ff9\u5230\u8f93\u5165\u6620\u5c04\u3002", "result": "\u5f97\u5230\u4e86\u59ff\u6001\u3001\u89d2\u901f\u5ea6\u3001\u63a8\u529b-\u653b\u89d2\u5bf9\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u6062\u590d\u6c14\u52a8\u529b\u77e9\u7cfb\u6570\u5206\u91cf\uff1b\u5e94\u7528\u4e8e\u7403\u5f62\u5e73\u884c\u7ebf\u4e0a\u7684\u7cfb\u7559\u98de\u884c\uff0c\u83b7\u5f97\u4e86\u6240\u9700\u6eda\u8f6c\u89d2\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u8bc6\u522b\u51fa\u96f6\u6eda\u8f6c\u8f68\u8ff9\uff1b\u5728\u7b80\u5355\u5347\u529b/\u963b\u529b\u5b9a\u5f8b\u4e0b\uff0c\u6700\u5c0f\u63a8\u529b\u653b\u89d2\u6709\u95ed\u5f0f\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u8fde\u63a5\u4e86\u822a\u7a7a\u9006\u4eff\u771f\u4e0e\u673a\u5668\u4eba\u51e0\u4f55\u5efa\u6a21\uff0c\u4e3a\u8f68\u8ff9\u8bbe\u8ba1\u548c\u53ef\u884c\u6027\u68c0\u67e5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6784\u5efa\u6a21\u5757\uff1b\u70b9\u6001\u51c6\u7a33\u6001\u9006\u89e3\u5728\u8f68\u8ff9\u548c\u65cb\u8f6c\u52a8\u529b\u5b66\u65f6\u4e0d\u53d8\u65f6\u6210\u4e3a\u7a33\u6001\u98de\u884c\u914d\u5e73\u89e3\u3002"}}
{"id": "2602.17199", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17199", "abs": "https://arxiv.org/abs/2602.17199", "authors": ["Antonio Rapuano", "Yaolei Shen", "Federico Califano", "Chiara Gabellieri", "Antonio Franchi"], "title": "Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7a7a\u4e2d\u64cd\u7eb5\u53ef\u4f38\u7f29\u7535\u7f06\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u9ad8\u4fdd\u771f\u6a21\u578b\u548c\u9002\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u7684\u964d\u9636\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u643a\u5e26\u67d4\u6027\u7535\u7f06\u7684\u5b9e\u65f6\u52a8\u6001\u611f\u77e5\u63a7\u5236\u3002", "motivation": "\u65e0\u4eba\u673a\u643a\u5e26\u67d4\u6027\u7535\u7f06\u8fdb\u884c\u7a7a\u4e2d\u64cd\u7eb5\u65f6\u9762\u4e34\u590d\u6742\u7684\u52a8\u529b\u5b66\u6311\u6218\uff0c\u9700\u8981\u9ad8\u4fdd\u771f\u6a21\u578b\u6765\u51c6\u786e\u63cf\u8ff0\u7535\u7f06\u53d8\u5f62\uff0c\u540c\u65f6\u9700\u8981\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u6a21\u578b\u6765\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u91c7\u7528\u504f\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21\u7535\u7f06\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u6709\u9650\u5dee\u5206\u6cd5\u79bb\u6563\u5316\uff0c\u901a\u8fc7\u672c\u5f81\u6b63\u4ea4\u5206\u89e3\u63d0\u53d6\u964d\u9636\u6a21\u578b\u4ee5\u4fdd\u7559\u4e3b\u8981\u53d8\u5f62\u6a21\u5f0f\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u57fa\u4e8e\u964d\u9636\u6a21\u578b\u6784\u5efa\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6848\uff0c\u80fd\u591f\u7a33\u5b9a\u7535\u7f06\u632f\u8361\u5e76\u5904\u7406\u6709\u6548\u8f7d\u8377\u9644\u7740/\u5206\u79bb\u7b49\u6df7\u5408\u8fc7\u6e21\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u964d\u9636\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u63a7\u5236\u5668\u5728\u5404\u79cd\u64cd\u4f5c\u6761\u4ef6\u4e0b\u90fd\u80fd\u6709\u6548\u8c03\u8282\u7535\u7f06\u52a8\u529b\u5b66\u3002\u989d\u5916\u4eff\u771f\u5c55\u793a\u4e86\u964d\u9636\u6a21\u578b\u5728\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u643a\u5e26\u60ac\u6302\u67d4\u6027\u7535\u7f06\u7684\u5b9e\u65f6\u52a8\u6001\u611f\u77e5\u63a7\u5236\uff0c\u7ed3\u5408\u4e86\u9ad8\u4fdd\u771f\u5efa\u6a21\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u7a7a\u4e2d\u7535\u7f06\u64cd\u7eb5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17226", "abs": "https://arxiv.org/abs/2602.17226", "authors": ["Lorenzo Montano-Olivan", "Julio A. Placed", "Luis Montano", "Maria T. Lazaro"], "title": "Multi-session Localization and Mapping Exploiting Topological Information", "comment": null, "summary": "Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4f1a\u8bdd\u5efa\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u5730\u56fe\u7684\u5b9a\u4f4d\u548c\u62d3\u6251\u611f\u77e5\u7684\u51b3\u7b56\u673a\u5236\uff0c\u5728\u91cd\u590d\u8bbf\u95ee\u73af\u5883\u4e2d\u4f18\u5316\u5efa\u56fe\u548c\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u5728\u91cd\u590d\u8bbf\u95ee\u76f8\u540c\u73af\u5883\u65f6\u9762\u4e34\u5efa\u56fe\u548c\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u8d2a\u5a6a\u5730\u8fd0\u884c\u5b8c\u6574SLAM\u4f1a\u8bdd\u5e76\u5c1d\u8bd5\u5728\u7ed3\u679c\u5730\u56fe\u95f4\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5730\u56fe\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u51b3\u7b56\u673a\u5236\uff0c\u5206\u6790\u4f4d\u59ff\u56fe\u7ed3\u6784\u4ee5\u68c0\u6d4b\u4f4e\u8fde\u901a\u6027\u533a\u57df\uff0c\u9009\u62e9\u6027\u89e6\u53d1\u5efa\u56fe\u548c\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\u3002", "result": "\u5728\u91cd\u53e0\u5e8f\u5217\u6570\u636e\u96c6\u548c\u771f\u5b9e\u77ff\u4e95\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51cf\u5c11\u7d2f\u79ef\u8bef\u5dee\u5e76\u589e\u5f3a\u5168\u5c40\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u4f1a\u8bdd\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u51b3\u7b56\u673a\u5236\u4f18\u5316\u4e86\u91cd\u590d\u73af\u5883\u4e2d\u7684\u5efa\u56fe\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u957f\u671f\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17259", "abs": "https://arxiv.org/abs/2602.17259", "authors": ["Han Zhao", "Jingbo Wang", "Wenxuan Song", "Shuai Chen", "Yang Liu", "Yan Wang", "Haoang Li", "Donglin Wang"], "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment", "comment": "Project Website: https://h-zhao1997.github.io/frappe", "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.", "AI": {"tldr": "FRAPPE\u65b9\u6cd5\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u89e3\u51b3VLA\u6a21\u578b\u5728\u73af\u5883\u52a8\u6001\u9884\u6d4b\u4e2d\u7684\u50cf\u7d20\u91cd\u5efa\u8fc7\u5ea6\u5f3a\u8c03\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u4e16\u754c\u611f\u77e5\u80fd\u529b", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u73af\u5883\u52a8\u6001\u9884\u6d4b\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1.\u8bad\u7ec3\u76ee\u6807\u8feb\u4f7f\u6a21\u578b\u8fc7\u5ea6\u5f3a\u8c03\u50cf\u7d20\u7ea7\u91cd\u5efa\uff0c\u9650\u5236\u4e86\u8bed\u4e49\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\uff1b2.\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f9d\u8d56\u9884\u6d4b\u7684\u672a\u6765\u89c2\u6d4b\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef", "method": "\u63d0\u51faFRAPPE\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff1a\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u9884\u6d4b\u672a\u6765\u89c2\u6d4b\u7684\u6f5c\u5728\u8868\u793a\uff1b\u540e\u671f\u8bad\u7ec3\u9636\u6bb5\u5e76\u884c\u6269\u5c55\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u540c\u65f6\u4e0e\u591a\u4e2a\u4e0d\u540c\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u8868\u793a", "result": "\u5728RoboTwin\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cFRAPPE\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u957f\u65f6\u7a0b\u548c\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "FRAPPE\u901a\u8fc7\u663e\u8457\u63d0\u9ad8\u5fae\u8c03\u6548\u7387\u548c\u51cf\u5c11\u5bf9\u52a8\u4f5c\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u589e\u5f3a\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u4e16\u754c\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u9014\u5f84"}}
{"id": "2602.17393", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.17393", "abs": "https://arxiv.org/abs/2602.17393", "authors": ["Minxing Sun", "Yao Mao"], "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots", "comment": "28 pages, 30 figures", "summary": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528IMU\u548c\u7535\u673a\u6d4b\u91cf\u7684\u7eaf\u672c\u4f53\u611f\u77e5\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u63a5\u89e6\u817f\u4f5c\u4e3a\u8fd0\u52a8\u5b66\u951a\u70b9\u6765\u6291\u5236\u957f\u671f\u6f02\u79fb\uff0c\u9002\u7528\u4e8e\u53cc\u8db3\u3001\u56db\u8db3\u548c\u8f6e\u817f\u673a\u5668\u4eba", "motivation": "\u89e3\u51b3\u65e0\u6444\u50cf\u5934\u6216\u6fc0\u5149\u96f7\u8fbe\u7684\u817f\u5f0f\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u95ee\u9898\uff0c\u514b\u670dIMU\u6f02\u79fb\u548c\u5173\u8282\u901f\u5ea6\u566a\u58f0\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u672c\u4f53\u611f\u77e5\u72b6\u6001\u4f30\u8ba1", "method": "1) \u5c06\u63a5\u89e6\u817f\u4f5c\u4e3a\u8fd0\u52a8\u5b66\u951a\u70b9\uff0c\u57fa\u4e8e\u5173\u8282\u529b\u77e9\u7684\u8db3\u90e8\u529b\u4f30\u8ba1\u9009\u62e9\u53ef\u9760\u63a5\u89e6\u70b9\uff1b2) \u5f15\u5165\u8f7b\u91cf\u7ea7\u9ad8\u5ea6\u805a\u7c7b\u548c\u65f6\u95f4\u8870\u51cf\u6821\u6b63\u9632\u6b62\u9ad8\u5ea6\u6f02\u79fb\uff1b3) \u4f7f\u7528\u9006\u8fd0\u52a8\u5b66\u7acb\u65b9\u4f53\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6539\u5584\u8db3\u7aef\u901f\u5ea6\u89c2\u6d4b\uff1b4) \u901a\u8fc7\u591a\u63a5\u89e6\u51e0\u4f55\u4e00\u81f4\u6027\u51cf\u5c11\u504f\u822a\u6f02\u79fb", "result": "\u5728\u56db\u4e2a\u56db\u8db3\u5e73\u53f0\u4e0a\u6d4b\u8bd5\uff1aAstrall\u70b9\u8db3\u673a\u5668\u4ebaA\u5b8c\u6210\u7ea6200\u7c73\u6c34\u5e73\u56de\u8def\u8bef\u5dee0.1638\u7c73\uff0c\u7ea615\u7c73\u5782\u76f4\u56de\u8def\u8bef\u5dee0.219\u7c73\uff1b\u8f6e\u817f\u673a\u5668\u4ebaB\u76f8\u5e94\u8bef\u5dee\u4e3a0.2264\u7c73\u548c0.199\u7c73\uff1b\u8f6e\u817f\u673a\u5668\u4ebaC\u7ea6700\u7c73\u6c34\u5e73\u56de\u8def\u8bef\u5dee7.68\u7c73\uff0c\u7ea620\u7c73\u5782\u76f4\u56de\u8def\u8bef\u5dee0.540\u7c73\uff1bUnitree Go2 EDU\u7ea6120\u7c73\u6c34\u5e73\u56de\u8def\u8bef\u5dee2.2138\u7c73\uff0c\u7ea68\u7c73\u5782\u76f4\u56de\u8def\u5782\u76f4\u8bef\u5dee\u5c0f\u4e8e0.1\u7c73", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528IMU\u548c\u7535\u673a\u6d4b\u91cf\u5c31\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u4f4d\u59ff\u548c\u901f\u5ea6\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u817f\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6709\u6548\u6291\u5236\u4e86\u957f\u671f\u6f02\u79fb\u95ee\u9898"}}
{"id": "2602.17421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17421", "abs": "https://arxiv.org/abs/2602.17421", "authors": ["Diana Cafiso", "Petr Trunin", "Carolina Gay", "Lucia Beccai"], "title": "3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing", "comment": "11 pages, 5 figures, submitted to Materials & Design", "summary": "Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSOLen\u76843D\u6253\u5370\u8f6f\u5149\u5b66\u4f20\u611f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6253\u5370\u900f\u955c\u5728Y\u5f62\u6ce2\u5bfc\u4e2d\u5b9e\u73b0\u53d8\u5f62\u4f20\u611f\uff0c\u5229\u7528\u900f\u955c\u65cb\u8f6c\u548c\u7126\u70b9\u5e73\u79fb\u6765\u91cd\u65b0\u5206\u914d\u4e24\u4e2a\u5206\u652f\u7684\u5149\u529f\u7387\uff0c\u4ece\u800c\u7f16\u7801\u8fd0\u52a8\u65b9\u5411\u548c\u5e45\u5ea6\u3002", "motivation": "\u589e\u6750\u5236\u9020\u4f7f\u5f97\u8f6f\u673a\u5668\u4eba\u5177\u6709\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u9700\u8981\u4e0e\u5355\u6750\u6599\u3001\u4e00\u6b65\u5236\u9020\u517c\u5bb9\u7684\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u3002\u5149\u5b66\u8f6f\u4f20\u611f\u5668\u9002\u5408\u6574\u4f53\u6253\u5370\uff0c\u4f46\u5176\u6027\u80fd\u5e38\u53d7\u73af\u5883\u8026\u5408\u3001\u6cc4\u6f0f\u3001\u6563\u5c04\u7b49\u4e0d\u53d7\u63a7\u5149\u4f20\u64ad\u5f71\u54cd\uff0c\u800c\u5e38\u89c1\u7684\u7f13\u89e3\u7b56\u7565\u901a\u5e38\u9700\u8981\u591a\u6750\u6599\u754c\u9762\u3002", "method": "\u5728Y\u5f62\u6ce2\u5bfc\u7684\u53d1\u5c04\u5668\u524d\u653e\u7f6e\u6253\u5370\u900f\u955c\uff0c\u4f20\u611f\u673a\u5236\u4f9d\u8d56\u4e8e\u53d8\u5f62\u5f15\u8d77\u7684\u900f\u955c\u65cb\u8f6c\u548c\u7126\u70b9\u5e73\u79fb\uff0c\u91cd\u65b0\u5206\u914d\u4e24\u4e2a\u5206\u652f\u4e4b\u95f4\u7684\u5149\u529f\u7387\uff0c\u4ea7\u751f\u7f16\u7801\u8fd0\u52a8\u65b9\u5411\u548c\u5e45\u5ea6\u7684\u5dee\u5206\u8f93\u51fa\u3002\u4f7f\u7528\u4e19\u70ef\u9178\u805a\u6c28\u916f\u6811\u8102\u6539\u6027\u63d0\u9ad8\u67d4\u987a\u6027\u548c\u5149\u5b66\u900f\u5c04\u7387\uff0c\u901a\u8fc7\u5355\u5c42\u5149\u5b66\u8868\u5f81\u83b7\u5f97\u6ce2\u957f\u76f8\u5173\u7684\u6298\u5c04\u7387\u548c\u900f\u5c04\u7387\uff0c\u6a21\u62df\u8bbe\u8ba1\u900f\u955c\u8f6e\u5ed3\u5e76\u5b9e\u73b0\u4e9a\u6beb\u7c73\u7cbe\u5ea6\u6253\u5370\u3002", "result": "\u65cb\u8f6c\u6d4b\u8bd5\u8bc1\u660e\u5728\u591a\u4e2a\u5468\u671f\u5185\u53ef\u91cd\u590d\u7684\u5206\u652f\u9009\u62e9\u6027\u4fe1\u53f7\u5207\u6362\u3002\u5efa\u7acb\u4e86\u4ece\u6750\u6599\u5230\u5149\u5b66\u7684\u53ef\u8f6c\u79fb\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8f6f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5177\u6709\u65b0\u529f\u80fd\u7684\u5149\u5b66\u4f20\u611f\u5668\u3002", "conclusion": "SOLen\u65b9\u6cd5\u4e3a\u8f6f\u5149\u5b66\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8f6c\u79fb\u7684\u6750\u6599\u5230\u5149\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u6253\u5370\u900f\u955c\u5b9e\u73b0\u4e86\u65b0\u7684\u529f\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8f6f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u517c\u5bb9\u5355\u6750\u6599\u3001\u4e00\u6b65\u5236\u9020\u7684\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17472", "abs": "https://arxiv.org/abs/2602.17472", "authors": ["Mohamed Sabry", "Joseba Gorospe", "Cristina Olaverri-Monreal"], "title": "A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras", "comment": null, "summary": "Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.\n  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.\n  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u96e8\u5929\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8f66\u8f86\u7f16\u961f\u7b49\u5173\u952e\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u611f\u77e5\u7b97\u6cd5\u6539\u8fdb\uff0c\u800c\u7269\u7406\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u7814\u7a76\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u4eb2\u6c34/\u758f\u6c34\u955c\u5934\u548c\u55b7\u96fe\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u95ee\u9898\uff0c\u5de5\u4e1a\u7ea7\u4fdd\u62a4\u7cfb\u7edf\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5728\u6c7d\u8f66\u9886\u57df\u89c4\u6a21\u5316\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u95e8\u9488\u5bf9\u96e8\u5929\u6761\u4ef6\u8bbe\u8ba1\uff0c\u80fd\u591f\u540c\u65f6\u517c\u5bb9\u591a\u4e2a\u6444\u50cf\u5934\u3002\u8be5\u7cfb\u7edf\u4e0e\u73b0\u6709\u6444\u50cf\u5934\u4f20\u611f\u5e73\u53f0\u517c\u5bb9\uff0c\u65e0\u9700\u989d\u5916\u9ad8\u6210\u672c\u4f20\u611f\u5668\u6216\u786c\u4ef6\u66f4\u6362\u3002", "result": "\u8be5\u7cfb\u7edf\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u884c\u4eba\u68c0\u6d4b\u51c6\u786e\u7387\u4ece8.3%\u63d0\u5347\u81f341.6%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u96e8\u5929\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "\u8be5\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u8fd8\u652f\u6301\u4ea4\u901a\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u901a\u8fc7\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3001\u652f\u6301\u6a21\u5757\u5316\u5347\u7ea7\uff0c\u4fc3\u8fdb\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u90e8\u7f72\u3002"}}
{"id": "2602.17474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17474", "abs": "https://arxiv.org/abs/2602.17474", "authors": ["Carolina Gay", "Petr Trunin", "Diana Cafiso", "Yuejun Xu", "Majid Taghavi", "Lucia Beccai"], "title": "Optically Sensorized Electro-Ribbon Actuator (OS-ERA)", "comment": "6 pages, 5 figures, accepted for 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.", "AI": {"tldr": "OS-ERA\uff1a\u4e00\u79cd\u5149\u5b66\u4f20\u611f\u7684Electro-Ribbon\u6267\u884c\u5668\uff0c\u901a\u8fc7\u5d4c\u5165\u8f6f\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u5668\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5f2f\u66f2\u72b6\u6001\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7535\u5bb9\u4f20\u611f\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u95ed\u73af\u63a7\u5236\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u4f20\u7edfElectro-Ribbon\u6267\u884c\u5668\uff08ERAs\uff09\u867d\u7136\u5177\u6709\u8d85\u9ad8\u7684\u4f4d\u79fb\u548c\u5feb\u901f\u8fd0\u52a8\u80fd\u529b\uff0c\u4f46\u5176\u5d4c\u5165\u5f0f\u4f20\u611f\u4f9d\u8d56\u4e8e\u7cbe\u5ea6\u6709\u9650\u7684\u7535\u5bb9\u4f20\u611f\u5668\uff0c\u8fd9\u963b\u788d\u4e86\u7cbe\u786e\u63a7\u5236\u3002\u9700\u8981\u4e00\u79cd\u4e0d\u5f71\u54cd\u9a71\u52a8\u6027\u80fd\u7684\u53ef\u9760\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86OS-ERA\uff0c\u5728ERA\u4e2d\u5d4c\u5165\u4e24\u4e2a\u8f6f\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u5668\u6765\u5206\u6790\u590d\u6742\u66f2\u7387\u8fd0\u52a8\u3002\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u4f20\u611f\u4fe1\u53f7\u6620\u5c04\u5230\u516b\u4e2a\u5f2f\u66f2\u72b6\u6001\u3002\u901a\u8fc7\u516d\u4e2a\u4fdd\u7559\u8bd5\u9a8c\u9a8c\u8bc1\u6a21\u578b\uff0c\u5e76\u4e0e\u8bad\u7ec3\u8fd0\u884c\u4e2d\u5b66\u4e60\u7684\u4fe1\u53f7\u8f68\u8ff9\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\uff0c\u4f20\u611f\u8f93\u51fa\u4fe1\u53f7\u9075\u5faa\u8bad\u7ec3\u6d41\u5f62\uff0c\u9884\u6d4b\u5e8f\u5217\u53cd\u6620\u5b9e\u9645\u6027\u80fd\u5e76\u786e\u8ba4\u53ef\u91cd\u590d\u6027\u3002\u5373\u4f7f\u5728\u9a71\u52a8\u901f\u5ea6\u6545\u610f\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0c\u4fe1\u53f7\u8f68\u8ff9\u4ecd\u4fdd\u6301\u5f62\u72b6\uff0c\u5206\u7c7b\u4fdd\u6301\u51c6\u786e\uff0c\u5c55\u793a\u4e86\u7535\u538b\u548c\u901f\u5ea6\u4e0d\u53d8\u6027\u3002", "conclusion": "OS-ERA\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u5206\u7c7b\u5f2f\u66f2\u72b6\u6001\uff0c\u5feb\u901f\u4e14\u53ef\u91cd\u590d\uff0c\u89e3\u51b3\u4e86ERA\u957f\u671f\u5b58\u5728\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u95ed\u73af\u63a7\u5236\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.17502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17502", "abs": "https://arxiv.org/abs/2602.17502", "authors": ["Kyle R. Embry", "Lorenzo Vianello", "Jim Lipsey", "Frank Ursetta", "Michael Stephens", "Zhi Wang", "Ann M. Simon", "Andrea J. Ikeda", "Suzanne B. Finucane", "Shawana Anarwala", "Levi J. Hargrove"], "title": "Proximal powered knee placement: a case study", "comment": "Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)", "summary": "Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u52a8\u529b\u5047\u80a2\u819d\u5173\u8282\u7684\u7535\u673a\u653e\u7f6e\u4f4d\u7f6e\uff08\u819d\u4e0avs\u819d\u4e0b\uff09\uff0c\u53d1\u73b0\u819d\u4e0a\u914d\u7f6e\u80fd\u6539\u5584\u6b65\u884c\u901f\u5ea6\u548c\u6b65\u9891\uff0c\u8868\u660e\u4f18\u5316\u8d28\u91cf\u5206\u5e03\u800c\u975e\u5355\u7eaf\u51cf\u91cd\u53ef\u80fd\u66f4\u6709\u6548\u3002", "motivation": "\u4e0b\u80a2\u622a\u80a2\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u4eba\uff0c\u5bfc\u81f4\u884c\u52a8\u80fd\u529b\u53d7\u635f\u3001\u6b65\u884c\u901f\u5ea6\u4e0b\u964d\u548c\u65e5\u5e38\u793e\u4ea4\u6d3b\u52a8\u53d7\u9650\u3002\u52a8\u529b\u5047\u80a2\u819d\u5173\u8282\u80fd\u4e3b\u52a8\u8f85\u52a9\u819d\u5173\u8282\u626d\u77e9\uff0c\u6539\u5584\u6b65\u6001\u5bf9\u79f0\u6027\u3001\u5750\u7ad9\u8f6c\u6362\u548c\u6b65\u884c\u901f\u5ea6\uff0c\u4f46\u589e\u52a0\u7684\u91cd\u91cf\u53ef\u80fd\u62b5\u6d88\u8fd9\u4e9b\u76ca\u5904\uff0c\u5f71\u54cd\u6b65\u6001\u529b\u5b66\u5e76\u589e\u52a0\u4ee3\u8c22\u6210\u672c\u3002", "method": "\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u5728\u5c0f\u89c4\u6a21\u961f\u5217\u4e2d\u8bc4\u4f30\u52a8\u529b\u5047\u80a2\u819d\u5173\u8282\u819d\u4e0a\u653e\u7f6e\u7684\u53ef\u884c\u6027\u3002\u4e0e\u819d\u4e0b\u914d\u7f6e\u5bf9\u6bd4\uff0c\u6d4b\u91cf\u6b65\u884c\u901f\u5ea6\u3001\u6b65\u9891\u3001\u6b65\u6001\u5bf9\u79f0\u6027\u3001\u819d\u5173\u8282\u6d3b\u52a8\u8303\u56f4\u548c\u5cf0\u503c\u901f\u5ea6\u7b49\u53c2\u6570\u3002\u5728\u5761\u9053\u548c\u697c\u68af\u4e0a\u8fdb\u884c\u989d\u5916\u6d4b\u8bd5\u9a8c\u8bc1\u63a7\u5236\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "result": "\u819d\u4e0a\u914d\u7f6e\u76f8\u6bd4\u819d\u4e0b\u914d\u7f6e\uff1a\u4e00\u540d\u53c2\u4e0e\u8005\u7684\u6b65\u884c\u901f\u5ea6\u63d0\u9ad89.2%\uff0c\u6b65\u9891\u63d0\u9ad83.6%\uff0c\u6b65\u6001\u5bf9\u79f0\u6027\u6548\u679c\u4e0d\u4e00\u3002\u8fd0\u52a8\u5b66\u6d4b\u91cf\u663e\u793a\u819d\u5173\u8282\u6d3b\u52a8\u8303\u56f4\u548c\u5cf0\u503c\u901f\u5ea6\u5728\u4e24\u79cd\u914d\u7f6e\u4e2d\u76f8\u4f3c\u3002\u5761\u9053\u548c\u697c\u68af\u6d4b\u8bd5\u8bc1\u5b9e\u63a7\u5236\u7b56\u7565\u5728\u591a\u79cd\u8fd0\u52a8\u4efb\u52a1\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u819d\u4e0a\u653e\u7f6e\u529f\u80fd\u4e0a\u53ef\u884c\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8d28\u91cf\u5206\u5e03\u53ef\u4ee5\u5728\u4fdd\u7559\u52a8\u529b\u8f85\u52a9\u76ca\u5904\u7684\u540c\u65f6\u51cf\u8f7b\u989d\u5916\u91cd\u91cf\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u786e\u8ba4\u8fd9\u4e9b\u8d8b\u52bf\u5e76\u4e3a\u8bbe\u8ba1\u548c\u4e34\u5e8a\u5efa\u8bae\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2602.17537", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17537", "abs": "https://arxiv.org/abs/2602.17537", "authors": ["Qilong Cheng", "Matthew Mackay", "Ali Bereyhi"], "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control", "comment": null, "summary": "Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.", "AI": {"tldr": "IRIS\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u57fa\u4e8e\u5b66\u4e60\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u6444\u50cf\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u548cTransformer\u67b6\u6784\u5b9e\u73b0\u81ea\u4e3b\u7535\u5f71\u7ea7\u8fd0\u52a8\u63a7\u5236", "motivation": "\u5de5\u4e1a\u7ea7\u673a\u5668\u4eba\u6444\u50cf\u7cfb\u7edf\u6210\u672c\u9ad8\u3001\u64cd\u4f5c\u590d\u6742\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u6613\u7528\u3001\u80fd\u81ea\u4e3b\u6267\u884c\u7535\u5f71\u7ea7\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u6444\u50cf\u7cfb\u7edf\u3002", "method": "\u91c7\u75283D\u6253\u5370\u8f7b\u91cf\u5316\u786c\u4ef6\u8bbe\u8ba1\uff0c\u7ed3\u5408\u57fa\u4e8eAction Chunking with Transformers (ACT)\u7684\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u7269\u4f53\u611f\u77e5\u548c\u611f\u77e5\u5e73\u6ed1\u7684\u76f8\u673a\u8f68\u8ff9\u3002", "result": "\u7cfb\u7edf\u6210\u672c\u4f4e\u4e8e1000\u7f8e\u5143\uff0c\u652f\u63011.5\u516c\u65a4\u8d1f\u8f7d\uff0c\u91cd\u590d\u7cbe\u5ea6\u7ea61\u6beb\u7c73\u3002\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3001\u53ef\u9760\u7684\u81ea\u4e3b\u6267\u884c\u80fd\u529b\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u591a\u79cd\u7535\u5f71\u7ea7\u8fd0\u52a8\u3002", "conclusion": "IRIS\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u673a\u5668\u4eba\u6444\u50cf\u5e73\u53f0\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u7f16\u7a0b\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7535\u5f71\u7ea7\u8fd0\u52a8\uff0c\u4e3a\u673a\u5668\u4eba\u6444\u50cf\u7cfb\u7edf\u7684\u666e\u53ca\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.17573", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17573", "abs": "https://arxiv.org/abs/2602.17573", "authors": ["Konstantinos Foteinos", "Georgios Angelidis", "Aggelos Psiris", "Vasileios Argyriou", "Panagiotis Sarigiannidis", "Georgios Th. Papadopoulos"], "title": "FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations", "comment": null, "summary": "The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FR-GESTURE\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u901a\u8fc7\u624b\u52bf\u63a7\u5236\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff0c\u5305\u542b12\u79cd\u624b\u52bf\u547d\u4ee4\u30013312\u4e2aRGBD\u56fe\u50cf\u5bf9\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u5b9e\u9a8c\u3002", "motivation": "\u707e\u96be\u5f3a\u5ea6\u548c\u9891\u7387\u4e0d\u65ad\u589e\u52a0\uff0c\u4f7f\u5f97\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u7684\u5de5\u4f5c\u66f4\u52a0\u56f0\u96be\u3002\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u534f\u52a9\u4ed6\u4eec\u7684\u64cd\u4f5c\uff0c\u5f25\u8865\u8fd9\u4e9b\u56f0\u96be\u3002\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u624b\u52bf\u63a7\u5236\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u3002", "method": "\u8bbe\u8ba1\u4e8612\u79cd\u624b\u52bf\u547d\u4ee4\uff0c\u7075\u611f\u6765\u81ea\u73b0\u6709\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u624b\u52bf\u548c\u6218\u672f\u624b\u8bed\uff0c\u5e76\u7ecf\u8fc7\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u53cd\u9988\u5b8c\u5584\u3002\u4ece2\u4e2a\u89c6\u89d2\u548c7\u4e2a\u8ddd\u79bb\u6536\u96c6\u4e863312\u4e2aRGBD\u56fe\u50cf\u5bf9\u3002\u5b9a\u4e49\u4e86\u8bc4\u4f30\u534f\u8bae\u5e76\u8fdb\u884c\u57fa\u51c6\u5b9e\u9a8c\u3002", "result": "\u521b\u5efa\u4e86FR-GESTURE\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u624b\u52bf\u63a7\u5236\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u5b9e\u9a8c\u7ed3\u679c\u4f9b\u540e\u7eed\u6539\u8fdb\u3002", "conclusion": "FR-GESTURE\u6570\u636e\u96c6\u586b\u8865\u4e86\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u624b\u52bf\u63a7\u5236\u673a\u5668\u4eba\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u671b\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u5728\u707e\u96be\u54cd\u5e94\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.17574", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17574", "abs": "https://arxiv.org/abs/2602.17574", "authors": ["Joshua A. Robbins", "Andrew F. Thompson", "Jonah J. Glunt", "Herschel C. Pangborn"], "title": "Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes", "comment": null, "summary": "Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408Zonotope\u548cADMM\u6df7\u5408\u6574\u6570\u89c4\u5212\u542f\u53d1\u5f0f\u7684\u6df7\u5408\u7cfb\u7edf\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u964d\u4f4e\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6", "motivation": "\u5d4c\u5165\u5f0f\u6df7\u5408\u7cfb\u7edf\u4f18\u5316\u89c4\u5212\u9762\u4e34\u6df7\u5408\u6574\u6570\u89c4\u5212\u8ba1\u7b97\u91cf\u5927\u3001\u5bf9\u6570\u503c\u516c\u5f0f\u654f\u611f\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89c4\u5212\u65b9\u6cd5", "method": "\u7ed3\u5408\u6df7\u5408Zonotope\u96c6\u5408\u8868\u793a\u548cADMM\u6df7\u5408\u6574\u6570\u89c4\u5212\u542f\u53d1\u5f0f\uff0c\u63d0\u51fa\u5206\u6bb5\u4eff\u5c04\u7cfb\u7edf\u53ef\u8fbe\u6027\u5206\u6790\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u6700\u4f18\u89c4\u5212\u95ee\u9898", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u4ea7\u751f\u66f4\u4f4e\u5185\u5b58\u590d\u6742\u5ea6\u548c\u66f4\u7d27\u51f8\u677e\u5f1b\u7684\u96c6\u5408\uff0cADMM\u542f\u53d1\u5f0f\u5728\u6df7\u5408Zonotope\u7ed3\u6784\u4e0a\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u73b0\u6709\u6df7\u5408\u6574\u6570\u89c4\u5212\u542f\u53d1\u5f0f", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\u4e0e\u8fd0\u52a8\u89c4\u5212\u573a\u666f\u4e2d\u5f97\u5230\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e3a\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u7684\u6df7\u5408\u7cfb\u7edf\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17586", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17586", "abs": "https://arxiv.org/abs/2602.17586", "authors": ["Antonio Guillen-Perez"], "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space", "comment": null, "summary": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.", "AI": {"tldr": "Deep-Flow\uff1a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u6761\u4ef6\u6d41\u5339\u914d\u7684\u65e0\u76d1\u7763\u5b89\u5168\u5173\u952e\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4f4e\u79e9\u8c31\u6d41\u5f62\u7ea6\u675f\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u7a33\u5b9a\u3001\u786e\u5b9a\u6027\u7684\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u3002", "motivation": "L4\u7ea7\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u9a8c\u8bc1\u76ee\u524d\u53d7\u9650\u4e8e\u65e0\u6cd5\u901a\u8fc7\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6269\u5c55\u68c0\u6d4b\u7f55\u89c1\u3001\u9ad8\u98ce\u9669\u7684\u957f\u5c3e\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5b89\u5168\u5173\u952e\u5f02\u5e38\u884c\u4e3a\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "1. \u4f7f\u7528\u6700\u4f18\u4f20\u8f93\u6761\u4ef6\u6d41\u5339\u914d\uff08OT-CFM\uff09\u6765\u8868\u5f81\u4e13\u5bb6\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u8fde\u7eed\u6982\u7387\u5bc6\u5ea6\uff1b2. \u901a\u8fc7PCA\u74f6\u9888\u5c06\u751f\u6210\u8fc7\u7a0b\u7ea6\u675f\u5230\u4f4e\u79e9\u8c31\u6d41\u5f62\uff0c\u786e\u4fdd\u8fd0\u52a8\u5b66\u5e73\u6ed1\u6027\uff1b3. \u91c7\u7528\u5177\u6709\u8f66\u9053\u611f\u77e5\u76ee\u6807\u6761\u4ef6\u7684\u65e9\u671f\u878d\u5408Transformer\u7f16\u7801\u5668\uff0c\u5904\u7406\u590d\u6742\u8def\u53e3\u7684\u591a\u6a21\u6001\u6a21\u7cca\u6027\uff1b4. \u5f15\u5165\u8fd0\u52a8\u5b66\u590d\u6742\u6027\u52a0\u6743\u65b9\u6848\uff0c\u5728\u65e0\u6a21\u62df\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5148\u5904\u7406\u9ad8\u80fd\u91cf\u673a\u52a8\u3002", "result": "\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\uff08WOMD\uff09\u4e0a\u8bc4\u4f30\uff0c\u6846\u67b6\u5b9e\u73b0\u4e860.766\u7684AUC-ROC\uff08\u76f8\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7684\u542f\u53d1\u5f0f\u9ec4\u91d1\u96c6\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5206\u6790\u63ed\u793a\u4e86\u8fd0\u52a8\u5b66\u5371\u9669\u548c\u8bed\u4e49\u8fdd\u89c4\u4e4b\u95f4\u7684\u6839\u672c\u533a\u522b\uff0c\u8bc6\u522b\u51fa\u4f20\u7edf\u5b89\u5168\u8fc7\u6ee4\u5668\u5ffd\u7565\u7684\u5206\u5e03\u5916\u884c\u4e3a\u3002", "conclusion": "Deep-Flow\u4e3a\u5b9a\u4e49\u7edf\u8ba1\u5b89\u5168\u95e8\u63d0\u4f9b\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u57fa\u7840\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u7684\u5b89\u5168\u90e8\u7f72\u80fd\u591f\u8fdb\u884c\u5ba2\u89c2\u3001\u6570\u636e\u9a71\u52a8\u7684\u9a8c\u8bc1\u3002\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u5173\u952e\u53ef\u9884\u6d4b\u6027\u5dee\u8ddd\uff0c\u5982\u8f66\u9053\u8fb9\u754c\u8fdd\u89c4\u548c\u975e\u89c4\u8303\u6027\u8def\u53e3\u673a\u52a8\u3002"}}
