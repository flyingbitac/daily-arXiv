{"id": "2601.20968", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20968", "abs": "https://arxiv.org/abs/2601.20968", "authors": ["Yulie Arad", "Stav Ashur", "Nancy M. Amato"], "title": "Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs", "comment": null, "summary": "In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the \"Red-Green-Gray\" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u7ea2-\u7eff-\u7070\"\u8303\u5f0f\uff0c\u6539\u8fdbSPITE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ec9\u4ef7\u542f\u53d1\u5f0f\u68c0\u67e5\u5feb\u901f\u66f4\u65b0\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u8def\u7ebf\u56fe\uff0c\u9002\u5e94\u975e\u9759\u6001\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u8def\u7ebf\u56fe\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u73af\u5883\u53d8\u5316\uff0c\u9700\u8981\u5feb\u901f\u66f4\u65b0\u8def\u7ebf\u56fe\u7684\u6709\u6548\u6027\u72b6\u6001\u3002", "method": "\u5f15\u5165\"\u7ea2-\u7eff-\u7070\"\u5206\u7c7b\u8303\u5f0f\uff0c\u4f7f\u7528\u7b80\u5355\u8ba1\u7b97\u51e0\u4f55\u65b9\u6cd5\u8fd1\u4f3c\u673a\u5668\u4eba\u626b\u63a0\u4f53\u79ef\uff0c\u8fdb\u884c\u60f0\u6027\u78b0\u649e\u68c0\u67e5\uff0c\u5c06\u8fb9\u6807\u8bb0\u4e3a\u65e0\u6548(\u7ea2)\u3001\u6709\u6548(\u7eff)\u6216\u672a\u77e5(\u7070)\uff0c\u5b9e\u73b0\u534a\u60f0\u6027\u8def\u7ebf\u56fe\u5feb\u901f\u66f4\u65b0\u3002", "result": "\u4e0eLeven\u548cHutchinson\u7684\u6210\u719f\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u80fd\u591f\u6b63\u786e\u6807\u8bb0\u65e0\u6548\u8fb9\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u66f4\u65b0\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\"\u7ea2-\u7eff-\u7070\"\u8303\u5f0f\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8def\u7ebf\u56fe\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ec9\u4ef7\u542f\u53d1\u5f0f\u68c0\u67e5\u5b9e\u73b0\u5feb\u901f\u72b6\u6001\u5206\u7c7b\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.21011", "categories": ["cs.RO", "cs.MA", "cs.OS", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21011", "abs": "https://arxiv.org/abs/2601.21011", "authors": ["Anshul Ranjan", "Anoosh Damodar", "Neha Chougule", "Dhruva S Nayak", "Anantharaman P. N", "Shylaja S S"], "title": "Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems", "comment": "Checkout the Python Library - https://pypi.org/project/metaros/ To be Submitted in ACM Transactions on Autonomous and Adaptive Systems (TAAS) Journal", "summary": "The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.", "AI": {"tldr": "Meta-ROS\u662f\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3ROS2\u7b49\u73b0\u6709\u6846\u67b6\u7684\u590d\u6742\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7b80\u5316\u96c6\u6210\u3001\u63d0\u5347\u6027\u80fd\u3001\u786e\u4fdd\u8de8\u5e73\u53f0\u517c\u5bb9\u6027\u6765\u4f18\u5316\u673a\u5668\u4eba\u5f00\u53d1\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u6846\u67b6\uff08\u5982ROS2\uff09\u5b58\u5728\u590d\u6742\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u5bf9\u65b0\u624b\u5f00\u53d1\u8005\u4e0d\u53cb\u597d\uff0c\u963b\u788d\u4e86\u673a\u5668\u4eba\u6280\u672f\u7684\u5e7f\u6cdb\u91c7\u7528\u548c\u53d1\u5c55\u3002", "method": "Meta-ROS\u91c7\u7528\u73b0\u4ee3\u901a\u4fe1\u534f\u8bae\uff08\u5982Zenoh\u548cZeroMQ\uff09\u5b9e\u73b0\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\uff08\u97f3\u9891\u3001\u56fe\u50cf\u3001\u89c6\u9891\uff09\uff0c\u5e76\u5177\u6709\u8de8\u786c\u4ef6\u5e73\u53f0\u517c\u5bb9\u6027\u548c\u5f00\u53d1\u8005\u53cb\u597d\u7684\u8bbe\u8ba1\u3002", "result": "Meta-ROS\u5728\u6027\u80fd\u6d4b\u8bd5\u4e2d\u4f18\u4e8eROS2\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u663e\u8457\u964d\u4f4e\u6d88\u606f\u5ef6\u8fdf\uff0c\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u786c\u4ef6\u652f\u6301\u548c\u6613\u7528\u6027\u3002", "conclusion": "Meta-ROS\u901a\u8fc7\u7b80\u5316\u96c6\u6210\u3001\u63d0\u5347\u6027\u80fd\u548c\u786e\u4fdd\u8de8\u5e73\u53f0\u517c\u5bb9\u6027\uff0c\u6210\u4e3a\u73b0\u4ee3\u5b9e\u65f6\u673a\u5668\u4ebaAI\u5e94\u7528\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e2d\u95f4\u4ef6\u6846\u67b6\u7684\u4e3b\u8981\u75db\u70b9\u3002"}}
{"id": "2601.21027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21027", "abs": "https://arxiv.org/abs/2601.21027", "authors": ["Youngim Nam", "Jungbin Kim", "Kyungtae Kang", "Cheolhyeon Kwon"], "title": "Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing", "comment": null, "summary": "This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7684\u6700\u5c0f\u5316\u5708\u65f6\u5168\u5c40\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u5728\u4e0d\u786e\u5b9a\u8f66\u8f86\u52a8\u529b\u5b66\u6761\u4ef6\u4e0b\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u76f4\u63a5\u4f18\u5316\u5168\u65f6\u57df\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5728\u8ddf\u8e2a\u5c42\u9762\u5b66\u4e60\u52a8\u529b\u5b66\uff0c\u800c\u4e0d\u66f4\u65b0\u8f68\u8ff9\u672c\u8eab\u4ee5\u9002\u5e94\u5b66\u4e60\u5230\u7684\u52a8\u529b\u5b66\uff0c\u4e14\u5168\u65f6\u57df\u8f68\u8ff9\u4f18\u5316\u8ba1\u7b97\u6602\u8d35\uff0c\u5728\u4e0d\u786e\u5b9a\u52a8\u529b\u5b66\u6761\u4ef6\u4e0b\u96be\u4ee5\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u6027\u3002", "method": "\u91c7\u7528\u8d5b\u9053\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5728\u8d5b\u9053\u65e0\u5173\u53c2\u6570\u7a7a\u95f4\u4e2d\u8868\u793a\u8f68\u8ff9\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u9ad8\u6548\u63a2\u7d22\u8be5\u7a7a\u95f4\uff0c\u5e76\u5c06\u4f18\u5316\u5d4c\u5165\u8fed\u4ee3\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u5b9e\u9645\u6570\u636e\u66f4\u65b0\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe20.7%\u7684\u5708\u65f6\u63d0\u5347\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u8f66\u8f86\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u548c\u8f68\u8ff9\u4f18\u5316\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7684\u5708\u65f6\u6027\u80fd\u3002"}}
{"id": "2601.21129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21129", "abs": "https://arxiv.org/abs/2601.21129", "authors": ["Guangping Liu", "Tipu Sultan", "Vittorio Di Giorgio", "Nick Hawkins", "Flavio Esposito", "Madi Babaiasl"], "title": "WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics", "comment": "Accepted to IEEE International Symposium on Medical Robotics (ISMR) 2026", "summary": "Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WheelArm-Sim\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u6536\u96c6\u8f6e\u6905\u4e0e\u673a\u68b0\u81c2\u96c6\u6210\u63a7\u5236\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e3a\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u867d\u7136\u8f85\u52a9\u673a\u5668\u4eba\u6280\u672f\u5df2\u6709\u8fdb\u6b65\uff0c\u4f46\u9488\u5bf9\u8f6e\u6905\u4e0e\u673a\u68b0\u81c2\u7ec4\u5408\u7684\u96c6\u6210\u7edf\u4e00\u63a7\u5236\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u7684\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eIsaac Sim\u7684WheelArm-Sim\u4eff\u771f\u6846\u67b6\uff0c\u6536\u96c6\u4e86\u5305\u542b13\u4e2a\u4efb\u52a1\u3001232\u6761\u8f68\u8ff9\u300167,783\u4e2a\u6837\u672c\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u73b0\u4e86\u82a5\u672b\u62fe\u53d6\u4efb\u52a1\u7684\u57fa\u7ebf\u52a8\u4f5c\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u6536\u96c6\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u57fa\u7ebf\u6a21\u578b\u9a8c\u8bc1\u4e86\u4eceWheelArm-Sim\u6536\u96c6\u7684\u6570\u636e\u53ef\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u96c6\u6210\u63a7\u5236\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "WheelArm-Sim\u6846\u67b6\u4e3a\u8f6e\u6905\u4e0e\u673a\u68b0\u81c2\u96c6\u6210\u63a7\u5236\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6570\u636e\u6536\u96c6\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2601.21173", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21173", "abs": "https://arxiv.org/abs/2601.21173", "authors": ["Zeyi Liu", "Shuang Liu", "Jihai Min", "Zhaoheng Zhang", "Jun Cen", "Pengyu Han", "Songqiao Hu", "Zihan Meng", "Xiao He", "Donghua Zhou"], "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios", "comment": "15 pages, 7 figures", "summary": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.", "AI": {"tldr": "InspecSafe-V1\u662f\u9996\u4e2a\u7528\u4e8e\u5de5\u4e1a\u5de1\u68c0\u5b89\u5168\u8bc4\u4f30\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u5de1\u68c0\u673a\u5668\u4eba\u91c7\u96c6\u7684\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u8986\u76d65\u79cd\u5de5\u4e1a\u573a\u666f\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\u548c\u5b89\u5168\u7b49\u7ea7\u6807\u7b7e\u3002", "motivation": "\u5de5\u4e1a\u667a\u80fd\u5316\u548c\u65e0\u4eba\u5de1\u68c0\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u6a21\u62df\u6570\u636e\u3001\u5355\u6a21\u6001\u611f\u77e5\u6216\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5de5\u4e1a\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u573a\u666f\u7406\u89e3\u548c\u591a\u6a21\u6001\u5b89\u5168\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ece\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u8fd0\u884c\u768441\u53f0\u8f6e\u5f0f\u548c\u8f68\u9053\u5f0f\u5de1\u68c0\u673a\u5668\u4eba\u6536\u96c6\u6570\u636e\uff0c\u8986\u76d6\u96a7\u9053\u3001\u7535\u529b\u8bbe\u65bd\u3001\u70e7\u7ed3\u8bbe\u5907\u3001\u77f3\u6cb9\u5316\u5de5\u548c\u7164\u70ad\u8f93\u9001\u6808\u68655\u79cd\u573a\u666f\uff0c\u5305\u542b7\u79cd\u540c\u6b65\u611f\u77e5\u6a21\u6001\uff08\u53ef\u89c1\u5149\u3001\u7ea2\u5916\u89c6\u9891\u3001\u97f3\u9891\u3001\u6df1\u5ea6\u70b9\u4e91\u3001\u96f7\u8fbe\u70b9\u4e91\u3001\u6c14\u4f53\u6d4b\u91cf\u3001\u6e29\u6e7f\u5ea6\uff09\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b2,239\u4e2a\u6709\u6548\u5de1\u68c0\u7ad9\u70b9\u30015,013\u4e2a\u5de1\u68c0\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u63d0\u4f9b\u53ef\u89c1\u5149\u8c31\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\u3001\u8bed\u4e49\u573a\u666f\u63cf\u8ff0\u548c\u5b89\u5168\u7b49\u7ea7\u6807\u7b7e\u3002", "conclusion": "InspecSafe-V1\u586b\u8865\u4e86\u5de5\u4e1a\u5de1\u68c0\u5b89\u5168\u8bc4\u4f30\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u652f\u6301\u591a\u6a21\u6001\u5f02\u5e38\u8bc6\u522b\u3001\u8de8\u6a21\u6001\u878d\u5408\u548c\u5de5\u4e1a\u73af\u5883\u7efc\u5408\u5b89\u5168\u8bc4\u4f30\uff0c\u4e3a\u5de5\u4e1a\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2601.21188", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21188", "abs": "https://arxiv.org/abs/2601.21188", "authors": ["Hao Cheng", "Feitian Zhang"], "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation", "comment": null, "summary": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u98de\u8247\u7684\u6270\u52a8\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u79fb\u52a8\u8d28\u91cf\u6267\u884c\u5668\u7ed3\u5408\u79fb\u52a8\u6c34\u5e73\u4f30\u8ba1\u5668\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u98ce\u6270\u52a8\u73af\u5883\u4e0b\u7684\u98de\u884c\u7a33\u5b9a\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u98de\u8247\u4f5c\u4e3a\u8f7b\u4e8e\u7a7a\u6c14\u7684\u822a\u7a7a\u7cfb\u7edf\uff0c\u5177\u6709\u957f\u7eed\u822a\u548c\u672c\u8d28\u5b89\u5168\u7684\u7279\u70b9\uff0c\u4f46\u5bf9\u98ce\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9LTA\u5e73\u53f0\u7684\u6270\u52a8\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u9700\u8981\u80fd\u591f\u660e\u786e\u5efa\u6a21\u548c\u8865\u507f\u98ce\u81f4\u6548\u5e94\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u79fb\u52a8\u6c34\u5e73\u4f30\u8ba1\u5668\u5b9e\u65f6\u63a8\u65ad\u98ce\u6270\u52a8\uff0c\u5c06\u4f30\u8ba1\u7ed3\u679c\u63d0\u4f9b\u7ed9\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u3002\u5229\u7528\u4e8c\u81ea\u7531\u5ea6\u79fb\u52a8\u8d28\u91cf\u673a\u5236\u4ea7\u751f\u60ef\u6027\u548c\u6c14\u52a8\u529b\u77e9\uff0c\u5b9e\u73b0\u59ff\u6001\u548c\u822a\u5411\u63a7\u5236\uff0c\u5f62\u6210\u96c6\u6210\u7684MHE-MPC\u63a7\u5236\u6846\u67b6\u3002", "result": "\u5728\u9876\u98ce\u548c\u4fa7\u98ce\u6761\u4ef6\u4e0b\u7684\u5e7f\u6cdb\u98de\u884c\u5b9e\u9a8c\u8868\u660e\uff0c\u96c6\u6210\u7684MHE-MPC\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u51c6PID\u63a7\u5236\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6270\u52a8\u611f\u77e5LTA\u98de\u884c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u6270\u52a8\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u79fb\u52a8\u8d28\u91cf\u6267\u884c\u5668\u7ed3\u5408MHE-MPC\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u98de\u8247\u5728\u98ce\u6270\u52a8\u73af\u5883\u4e0b\u7684\u8f68\u8ff9\u548c\u822a\u5411\u8c03\u8282\u80fd\u529b\uff0c\u4e3aLTA\u5e73\u53f0\u5728\u6270\u52a8\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u98de\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21346", "abs": "https://arxiv.org/abs/2601.21346", "authors": ["Wei Zuo", "Chengyang Li", "Yikun Wang", "Bingyang Cheng", "Zeyi Ren", "Shuai Wang", "Derrick Wing Kwan Ng", "Yik-Chung Wu"], "title": "HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u4e3b\u52a8\u8c03\u4f18\u6846\u67b6HPTune\uff0c\u901a\u8fc7\u8bc4\u4f30\u672a\u6267\u884c\u52a8\u4f5c\u6765\u6539\u8fdbMPC\u53c2\u6570\u8c03\u4f18\uff0c\u7ed3\u5408\u5feb\u901f\u5c42\u548c\u6162\u901f\u5c42\u8c03\u4f18\uff0c\u96c6\u6210\u591a\u666e\u52d2\u6fc0\u5149\u96f7\u8fbe\u589e\u5f3a\u8fd0\u52a8\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709MPC\u8fd0\u52a8\u89c4\u5212\u5668\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u901a\u5e38\u53ea\u8bc4\u4f30\u5df2\u6267\u884c\u52a8\u4f5c\uff0c\u7531\u4e8e\u5931\u8d25\u4e8b\u4ef6\uff08\u5982\u969c\u788d\u7269\u63a5\u8fd1\u6216\u78b0\u649e\uff09\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u53c2\u6570\u66f4\u65b0\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u4e3b\u52a8\u8c03\u4f18\u6846\u67b6HPTune\uff1a1\uff09\u5feb\u901f\u5c42\u8c03\u4f18\u91c7\u7528\u9884\u6d4b\u63a5\u8fd1\u901f\u5ea6\u548c\u9884\u6d4b\u63a5\u8fd1\u8ddd\u79bb\u7684\u98ce\u9669\u6307\u6807\uff1b2\uff09\u6162\u901f\u5c42\u8c03\u4f18\u5229\u7528\u6269\u5c55\u8bc4\u4f30\u635f\u5931\u8fdb\u884c\u95ed\u73af\u53cd\u5411\u4f20\u64ad\uff1b3\uff09\u96c6\u6210\u591a\u666e\u52d2\u6fc0\u5149\u96f7\u8fbe\u83b7\u53d6\u969c\u788d\u7269\u901f\u5ea6\u4fe1\u606f\u4ee5\u589e\u5f3a\u8fd0\u52a8\u9884\u6d4b\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHPTune\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684MPC\u8c03\u4f18\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6848\uff0c\u80fd\u591f\u5236\u5b9a\u5b89\u5168\u3001\u654f\u6377\u7684\u907f\u78b0\u7b56\u7565\u3002", "conclusion": "HPTune\u901a\u8fc7\u8bc4\u4f30\u672a\u6267\u884c\u52a8\u4f5c\u6269\u5c55\u4e86\u53c2\u6570\u8c03\u4f18\u8303\u56f4\uff0c\u5b9e\u73b0\u4e86\u60c5\u5883\u5b9a\u5236\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u9ad8\u4e86MPC\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2601.21363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21363", "abs": "https://arxiv.org/abs/2601.21363", "authors": ["Weidong Huang", "Zhehan Li", "Hangxin Liu", "Biao Hou", "Yao Su", "Jingwen Zhang"], "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control", "comment": "ICLR 2026", "summary": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u89c4\u6a21\u79bb\u7ebf\u7b56\u7565\u9884\u8bad\u7ec3\u548c\u57fa\u4e8e\u6a21\u578b\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u548c\u9ad8\u6548\u9002\u5e94\u65b0\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4e2d\uff0c\u5728\u7ebf\u7b56\u7565\u65b9\u6cd5\uff08\u5982PPO\uff09\u867d\u7136\u80fd\u5b9e\u73b0\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u548c\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u4f46\u6837\u672c\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5b89\u5168\u9002\u5e94\u65b0\u73af\u5883\u3002\u79bb\u7ebf\u7b56\u7565\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u6837\u672c\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u9ad8\u6548\u5fae\u8c03\u4e4b\u95f4\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u7b56\u7565Soft Actor-Critic (SAC)\u7b97\u6cd5\uff0c\u914d\u5408\u5927\u6279\u91cf\u66f4\u65b0\u548c\u9ad8\u66f4\u65b0\u6570\u636e\u6bd4(UTD)\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002\u5728\u9002\u5e94\u65b0\u73af\u5883\u65f6\uff0c\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5fae\u8c03\u9884\u8bad\u7ec3\u7b56\u7565\uff1a\u5728\u65b0\u73af\u5883\u4e2d\u6267\u884c\u786e\u5b9a\u6027\u7b56\u7565\u6536\u96c6\u6570\u636e\uff0c\u800c\u968f\u673a\u63a2\u7d22\u5219\u9650\u5236\u5728\u57fa\u4e8e\u7269\u7406\u7684\u4e16\u754c\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u964d\u4f4e\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u63a2\u7d22\u98ce\u9669\u3002", "result": "SAC\u9884\u8bad\u7ec3\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u96f6\u6837\u672c\u90e8\u7f72\u5230\u771f\u5b9e\u673a\u5668\u4eba\u3002\u9884\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u5728\u65b0\u73af\u5883\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u5927\u89c4\u6a21\u4eff\u771f\u6548\u7387\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u57fa\u4e8e\u6a21\u578b\u5b66\u4e60\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u5927\u89c4\u6a21\u79bb\u7ebf\u7b56\u7565\u9884\u8bad\u7ec3\u4e0e\u57fa\u4e8e\u6a21\u578b\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u65e2\u80fd\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\u53c8\u80fd\u9ad8\u6548\u9002\u5e94\u65b0\u73af\u5883\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u9002\u5e94\u5b89\u5168\u6027\u3002"}}
{"id": "2601.21394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21394", "abs": "https://arxiv.org/abs/2601.21394", "authors": ["Leonidas Askianakis", "Aleksandr Artemov"], "title": "Towards Space-Based Environmentally-Adaptive Grasping", "comment": null, "summary": "Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u6d41\u5f62\u4e2d\u76f4\u63a5\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a7a\u95f4\u73af\u5883\u4e2d\u7684\u6293\u53d6\u4efb\u52a1\uff0c\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u9762\u4e34\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u3001\u7a00\u758f\u5956\u52b1\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u73af\u5883\u7b49\u6781\u7aef\u6761\u4ef6\u4e0b\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u6293\u53d6\u80fd\u529b\u3002", "method": "\u4f7f\u7528GPU\u52a0\u901f\u7269\u7406\u4eff\u771f\u521b\u5efa\u5355\u6b21\u6293\u53d6\u4efb\u52a1\uff0c\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u6d41\u5f62\u4e2d\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5f62\u6210\u7ed3\u6784\u5316\u8868\u793a\uff0c\u91c7\u7528Soft Actor-Critic\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u76f4\u63a5\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728\u4e0d\u5230100\u4e07\u73af\u5883\u6b65\u6570\u5185\u5b9e\u73b0\u8d85\u8fc795%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5728\u8fde\u7eed\u53d8\u5316\u7684\u6293\u53d6\u6761\u4ef6\u4e0b\u4ece\u7b2c\u4e00\u6b65\u5f00\u59cb\u5c31\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u89c6\u89c9\u57fa\u7ebf\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5bf9\u65b0\u9896\u7269\u4f53\u3001\u5939\u722a\u51e0\u4f55\u3001\u73af\u5883\u6742\u4e71\u548c\u4f20\u611f\u5668\u914d\u7f6e\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u80fd\u591f\u5b9e\u73b0\u66f4\u6837\u672c\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6781\u7aef\u7a7a\u95f4\u6761\u4ef6\u4e0b\u7684\u5b8c\u5168\u81ea\u9002\u5e94\u548c\u53ef\u6cdb\u5316\u6293\u53d6\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.21413", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21413", "abs": "https://arxiv.org/abs/2601.21413", "authors": ["Andreas Mueller"], "title": "Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates", "comment": "10 pages", "summary": "A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u674e\u7fa4\u79ef\u5206\u5668\u4e0e\u6807\u51c6\u591a\u4f53\u7cfb\u7edf\u5efa\u6a21\u6846\u67b6\u5bf9\u63a5\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u674e\u7fa4\u79ef\u5206\u4e0e\u4f20\u7edf\u7edd\u5bf9\u5750\u6807\u65b9\u7a0b\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u521a\u4f53\u8fd0\u52a8\u51e0\u4f55\u7279\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u4f53\u7cfb\u7edf\u5efa\u6a21\u4f7f\u7528\u7edd\u5bf9\u5750\u6807\uff0c\u4f46\u7a7a\u95f4\u8fd0\u52a8\u7684\u65e0\u5947\u5f02\u6027\u53c2\u6570\u5316\u5b58\u5728\u95ee\u9898\u3002\u674e\u7fa4\u79ef\u5206\u65b9\u6cd5\u80fd\u4fdd\u6301\u8fd0\u52a8\u51e0\u4f55\u7279\u6027\uff0c\u4f46\u4e0e\u73b0\u6709\u6807\u51c6\u65b9\u7a0b\u4e0d\u517c\u5bb9\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709\u4eff\u771f\u4ee3\u7801\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u4e3b\u8981\u65b9\u6cd5\uff1a1\uff09\u5efa\u7acb\u674e\u7fa4\u79ef\u5206\u5668\u4e0e\u6807\u51c6\u8fd0\u52a8\u65b9\u7a0b\u6846\u67b6\u7684\u63a5\u53e3\uff1b2\uff09\u5728\u7edd\u5bf9\u5750\u6807\u4e2d\u4e00\u81f4\u5730\u878d\u5165\u521a\u4f53\u8fd0\u52a8\u51e0\u4f55\u7279\u6027\u3002\u4f7f\u7528SO(3)\u00d7R3\u548cSE(3)\u7fa4\u8868\u793a\u521a\u4f53\u8fd0\u52a8\uff0c\u5173\u952e\u662f\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u8f6c\u6362\u6620\u5c04\u66f4\u65b0\u5168\u5c40\u7edd\u5bf9\u5750\u6807\u3002", "result": "\u5b9e\u73b0\u4e86\u674e\u7fa4\u79ef\u5206\u5668\u4e0e\u6807\u51c6\u591a\u4f53\u7cfb\u7edf\u5efa\u6a21\u6846\u67b6\u7684\u517c\u5bb9\uff0c\u5141\u8bb8\u4f7f\u7528\u5404\u79cd\u7edd\u5bf9\u5750\u6807\u63cf\u8ff0\u7cfb\u7edf\uff0c\u540c\u65f6\u5e94\u7528\u674e\u7fa4\u79ef\u5206\u65b9\u6848\uff0c\u4fdd\u6301\u4e86\u521a\u4f53\u8fd0\u52a8\u7684\u51e0\u4f55\u7279\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u674e\u7fa4\u79ef\u5206\u4e0e\u6807\u51c6\u591a\u4f53\u7cfb\u7edf\u5efa\u6a21\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u4e3a\u5728\u73b0\u6709\u4eff\u771f\u4ee3\u7801\u4e2d\u4f7f\u7528\u674e\u7fa4\u79ef\u5206\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fd0\u52a8\u51e0\u4f55\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2601.21416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21416", "abs": "https://arxiv.org/abs/2601.21416", "authors": ["Alexandre Chapin", "Bruno Machado", "Emmanuel Dellandr\u00e9a", "Liming Chen"], "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation", "comment": null, "summary": "The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4e2d\u4e09\u79cd\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\uff1a\u5168\u5c40\u7279\u5f81\u3001\u5bc6\u96c6\u7279\u5f81\u548c\u57fa\u4e8e\u69fd\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff08SBOCR\uff09\uff0c\u53d1\u73b0SBOCR\u5728\u5149\u7167\u3001\u7eb9\u7406\u53d8\u5316\u548c\u5e72\u6270\u7269\u5b58\u5728\u7b49\u5206\u5e03\u504f\u79fb\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d0\u53d6\u7684\u5168\u5c40\u7279\u5f81\u6216\u5bc6\u96c6\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u6df7\u5408\u4e86\u4efb\u52a1\u76f8\u5173\u548c\u65e0\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u5149\u7167\u3001\u7eb9\u7406\u53d8\u5316\u6216\u5b58\u5728\u5e72\u6270\u7269\u7b49\u5206\u5e03\u504f\u79fb\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u69fd\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff08SBOCR\uff09\u4f5c\u4e3a\u4e2d\u95f4\u7ed3\u6784\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u5bc6\u96c6\u7279\u5f81\u5206\u7ec4\u4e3a\u6709\u9650\u7684\u5bf9\u8c61\u7c7b\u5b9e\u4f53\u3002\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5bf9\u5168\u5c40\u7279\u5f81\u3001\u5bc6\u96c6\u7279\u5f81\u548cSBOCR\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u5149\u7167\u3001\u7eb9\u7406\u53d8\u5316\u548c\u5e72\u6270\u7269\u5b58\u5728\u7b49\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "SBOCR\u57fa\u4e8e\u7b56\u7565\u5728\u6cdb\u5316\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5bc6\u96c6\u7279\u5f81\u548c\u5168\u5c40\u7279\u5f81\u57fa\u4e8e\u7b56\u7565\uff0c\u5373\u4f7f\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u3002SBOCR\u80fd\u591f\u81ea\u7136\u5730\u51cf\u5c11\u63d0\u4f9b\u7ed9\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u8db3\u591f\u4fe1\u606f\u6765\u9ad8\u6548\u6267\u884c\u4efb\u52a1\u3002", "conclusion": "SBOCR\u662f\u8bbe\u8ba1\u5728\u52a8\u6001\u3001\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u73af\u5883\u4e2d\u6709\u6548\u6cdb\u5316\u7684\u89c6\u89c9\u7cfb\u7edf\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u5176\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u6709\u52a9\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.21449", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21449", "abs": "https://arxiv.org/abs/2601.21449", "authors": ["Zeyu He", "Yuchang Zhang", "Yuanzhen Zhou", "Miao Tao", "Hengjie Li", "Yang Tian", "Jia Zeng", "Tai Wang", "Wenzhe Cai", "Yilun Chen", "Ning Gao", "Jiangmiao Pang"], "title": "Nimbus: A Unified Embodied Synthetic Data Generation Framework", "comment": null, "summary": "Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.", "AI": {"tldr": "Nimbus\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u56db\u5c42\u67b6\u6784\u548c\u5f02\u6b65\u6267\u884c\u6a21\u578b\uff0c\u5c06\u5f02\u6784\u5bfc\u822a\u548c\u64cd\u4f5c\u7ba1\u9053\u96c6\u6210\uff0c\u5b9e\u73b0\u4e862-3\u500d\u7684\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u788e\u7247\u5316\u4e14\u4efb\u52a1\u7279\u5b9a\uff0c\u5bfc\u81f4\u5de5\u7a0b\u6548\u7387\u4f4e\u4e0b\u548c\u7cfb\u7edf\u4e0d\u7a33\u5b9a\uff0c\u65e0\u6cd5\u652f\u6301\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u7684\u9ad8\u541e\u5410\u91cf\u6570\u636e\u751f\u6210\u3002", "method": "\u63d0\u51faNimbus\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u56db\u5c42\u67b6\u6784\uff0c\u5c06\u8f68\u8ff9\u89c4\u5212\u3001\u6e32\u67d3\u548c\u5b58\u50a8\u89e3\u8026\u4e3a\u5f02\u6b65\u9636\u6bb5\uff0c\u5b9e\u73b0\u52a8\u6001\u7ba1\u9053\u8c03\u5ea6\u3001\u5168\u5c40\u8d1f\u8f7d\u5747\u8861\u3001\u5206\u5e03\u5f0f\u5bb9\u9519\u548c\u540e\u7aef\u7279\u5b9a\u6e32\u67d3\u4f18\u5316\u3002", "result": "\u76f8\u6bd4\u672a\u4f18\u5316\u7684\u57fa\u7ebf\uff0cNimbus\u5b9e\u73b0\u4e862-3\u500d\u7684\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\uff0c\u786e\u4fdd\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7a33\u5065\u957f\u671f\u8fd0\u884c\uff0c\u4f5c\u4e3aInternData\u5957\u4ef6\u7684\u751f\u4ea7\u9aa8\u5e72\u3002", "conclusion": "Nimbus\u6846\u67b6\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u8de8\u9886\u57df\u6570\u636e\u5408\u6210\uff0c\u652f\u6301\u5177\u8eab\u667a\u80fd\u7684\u89c4\u6a21\u5316\u53d1\u5c55\u3002"}}
{"id": "2601.21454", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21454", "abs": "https://arxiv.org/abs/2601.21454", "authors": ["Shanliang Yao", "Zhuoxiao Li", "Runwei Guan", "Kebin Cao", "Meng Xia", "Fuping Hu", "Sen Xu", "Yong Yue", "Xiaohui Zhu", "Weiping Ding", "Ryan Wen Liu"], "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving", "comment": null, "summary": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.", "AI": {"tldr": "4D-CAAL\u662f\u4e00\u4e2a\u7edf\u4e00\u76844D\u96f7\u8fbe-\u76f8\u673a\u6807\u5b9a\u548c\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7528\u9014\u6807\u5b9a\u9776\u8bbe\u8ba1\u548c\u9c81\u68d2\u7684\u5bf9\u5e94\u5339\u914d\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5916\u53c2\u6807\u5b9a\uff0c\u5e76\u5229\u7528\u6807\u5b9a\u5173\u7cfb\u5c06\u76f8\u673a\u6807\u6ce8\u8f6c\u79fb\u5230\u96f7\u8fbe\u70b9\u4e91\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002", "motivation": "4D\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u96f7\u8fbe-\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5206\u79bb\u7684\u6807\u5b9a\u9776\uff0c\u96be\u4ee5\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\uff1b\u540c\u65f6\uff0c\u624b\u52a8\u6807\u6ce8\u7a00\u758f\u96f7\u8fbe\u6570\u636e\u65e2\u8d39\u65f6\u53c8\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7b80\u5316\u6807\u5b9a\u5e76\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002", "method": "\u63d0\u51fa\u53cc\u7528\u9014\u6807\u5b9a\u9776\u8bbe\u8ba1\uff1a\u6b63\u9762\u91c7\u7528\u68cb\u76d8\u683c\u56fe\u6848\u7528\u4e8e\u76f8\u673a\u68c0\u6d4b\uff0c\u80cc\u9762\u4e2d\u5fc3\u653e\u7f6e\u89d2\u53cd\u5c04\u5668\u7528\u4e8e\u96f7\u8fbe\u68c0\u6d4b\u3002\u5f00\u53d1\u9c81\u68d2\u7684\u5bf9\u5e94\u5339\u914d\u7b97\u6cd5\uff0c\u5c06\u68cb\u76d8\u683c\u4e2d\u5fc3\u4e0e\u6700\u5f3a\u96f7\u8fbe\u53cd\u5c04\u70b9\u5bf9\u9f50\uff0c\u5b9e\u73b0\u7cbe\u786e\u5916\u53c2\u6807\u5b9a\u3002\u968f\u540e\u8bbe\u8ba1\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u5229\u7528\u6807\u5b9a\u5173\u7cfb\u901a\u8fc7\u51e0\u4f55\u6295\u5f71\u548c\u591a\u7279\u5f81\u4f18\u5316\u5c06\u76f8\u673a\u5206\u5272\u6807\u6ce8\u8f6c\u79fb\u5230\u96f7\u8fbe\u70b9\u4e91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6807\u5b9a\u6548\u679c\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u52a0\u901f\u4e86\u81ea\u52a8\u9a7e\u9a76\u9c81\u68d2\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "conclusion": "4D-CAAL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e864D\u96f7\u8fbe-\u76f8\u673a\u6807\u5b9a\u548c\u81ea\u52a8\u6807\u6ce8\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21506", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21506", "abs": "https://arxiv.org/abs/2601.21506", "authors": ["Joonhee Lee", "Hyunseung Shin", "Jeonggil Ko"], "title": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation", "comment": null, "summary": "Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.", "AI": {"tldr": "IROS\u6846\u67b6\u7ed3\u5408VLM\u7ea7\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u8f7b\u91cf\u611f\u77e5\u6a21\u5757\uff0c\u5728\u4f4e\u6210\u672c\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5ba4\u5185\u5bfc\u822a\uff0c\u51b3\u7b56\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5ef6\u8fdf\u964d\u4f4e66%", "motivation": "\u73b0\u6709\u5ba4\u5185\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u5feb\u901f\u54cd\u5e94\u548c\u9c81\u68d2\u8bed\u4e49\u7406\u89e3\u9700\u6c42\u3002\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u4f9d\u8d56\u8be6\u7ec6\u5730\u56fe\u4e14\u65e0\u6cd5\u89e3\u91ca\u4eba\u7c7b\u5bfc\u5411\u7ebf\u7d22\uff1bVLA\u6a21\u578b\u4ec5\u57fa\u4e8e\u53ef\u89c1\u5e27\u53cd\u5e94\uff0c\u65e0\u6cd5\u9884\u89c1\u672a\u89c1\u7684\u4ea4\u53c9\u53e3\uff1bVLM\u8ba1\u7b97\u5ef6\u8fdf\u9ad8\uff0c\u4e0d\u9002\u5408\u5d4c\u5165\u5f0f\u5e73\u53f0\u5b9e\u65f6\u64cd\u4f5c", "method": "\u63d0\u51faIROS\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\uff0c\u53d7\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u5c06\u5feb\u901f\u53cd\u5c04\u51b3\u7b56\uff08\u7cfb\u7edf\u4e00\uff09\u4e0e\u6162\u901f\u5ba1\u614e\u63a8\u7406\uff08\u7cfb\u7edf\u4e8c\uff09\u5206\u79bb\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8c03\u7528VLM\u3002\u901a\u8fc7\u4e3a\u7d27\u51d1VLM\u589e\u5f3a\u7a7a\u95f4\u548c\u6587\u672c\u7ebf\u7d22\uff0c\u5b9e\u73b0\u9c81\u68d2\u3001\u7c7b\u4eba\u5bfc\u822a", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u5efa\u7b51\u4e2d\uff0cIROS\u76f8\u6bd4\u8fde\u7eedVLM\u5bfc\u822a\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u5ef6\u8fdf\u964d\u4f4e\u4e8666%", "conclusion": "IROS\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86VLM\u7ea7\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u8f7b\u91cf\u611f\u77e5\u6a21\u5757\u7684\u6548\u7387\uff0c\u5728\u4f4e\u6210\u672c\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9c81\u68d2\u7684\u5ba4\u5185\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u54cd\u5e94\u901f\u5ea6\u548c\u8bed\u4e49\u7406\u89e3\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898"}}
{"id": "2601.21602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21602", "abs": "https://arxiv.org/abs/2601.21602", "authors": ["Jianli Sun", "Bin Tian", "Qiyao Zhang", "Chengxiang Li", "Zihan Song", "Zhiyong Cui", "Yisheng Lv", "Yonglin Tian"], "title": "AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation", "comment": null, "summary": "While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u7a7a\u4e2d\u64cd\u4f5c\u7cfb\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u51c6AIR-VLA\uff0c\u5305\u542b\u7269\u7406\u4eff\u771f\u73af\u5883\u548c3000\u4e2a\u624b\u52a8\u9065\u64cd\u4f5c\u6f14\u793a\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e3b\u6d41VLA\u548cVLM\u6a21\u578b\u5728\u65e0\u4eba\u673a\u79fb\u52a8\u3001\u673a\u68b0\u81c2\u63a7\u5236\u548c\u9ad8\u5c42\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5730\u9762\u5b9e\u4f53\u667a\u80fd\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u7a7a\u4e2d\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002\u7a7a\u4e2d\u64cd\u4f5c\u7cfb\u7edf\u7684\u6d6e\u52a8\u57fa\u5ea7\u52a8\u529b\u5b66\u3001\u65e0\u4eba\u673a\u4e0e\u673a\u68b0\u81c2\u7684\u5f3a\u8026\u5408\u6027\u4ee5\u53ca\u591a\u6b65\u9aa4\u957f\u65f6\u7a0b\u7684\u64cd\u4f5c\u4efb\u52a1\u7279\u6027\uff0c\u5bf9\u73b0\u6709\u4e3a\u9759\u6001\u62162D\u79fb\u52a8\u57fa\u5ea7\u8bbe\u8ba1\u7684VLA\u8303\u5f0f\u6784\u6210\u4e86\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u73af\u5883\uff0c\u53d1\u5e03\u4e86\u5305\u542b3000\u4e2a\u624b\u52a8\u9065\u64cd\u4f5c\u6f14\u793a\u7684\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u57fa\u5ea7\u64cd\u4f5c\u3001\u7269\u4f53\u4e0e\u7a7a\u95f4\u7406\u89e3\u3001\u8bed\u4e49\u63a8\u7406\u548c\u957f\u65f6\u7a0b\u89c4\u5212\u3002\u5229\u7528\u8be5\u5e73\u53f0\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e3b\u6d41VLA\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u5c06VLA\u8303\u5f0f\u8fc1\u79fb\u5230\u7a7a\u4e2d\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u8fd8\u901a\u8fc7\u9488\u5bf9\u7a7a\u4e2d\u4efb\u52a1\u7684\u591a\u7ef4\u5ea6\u6307\u6807\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u65e0\u4eba\u673a\u79fb\u52a8\u6027\u3001\u673a\u68b0\u81c2\u63a7\u5236\u548c\u9ad8\u5c42\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "AIR-VLA\u4e3a\u901a\u7528\u7a7a\u4e2d\u673a\u5668\u4eba\u7814\u7a76\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\u548c\u6570\u636e\u57fa\u7840\uff0c\u586b\u8865\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u7a7a\u4e2d\u64cd\u4f5c\u7cfb\u7edf\u5e94\u7528\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.21667", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21667", "abs": "https://arxiv.org/abs/2601.21667", "authors": ["Hao Ju", "Shaofei Huang", "Hongyu Li", "Zihan Ding", "Si Liu", "Meng Wang", "Zhedong Zheng"], "title": "From Instruction to Event: Sound-Triggered Mobile Manipulation", "comment": null, "summary": "Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u58f0\u97f3\u89e6\u53d1\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u8ba9\u667a\u80fd\u4f53\u4e3b\u52a8\u611f\u77e5\u58f0\u97f3\u6e90\u5e76\u4e0e\u4e4b\u4ea4\u4e92\uff0c\u65e0\u9700\u660e\u786e\u7684\u884c\u52a8\u6307\u4ee4\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6307\u4ee4\u9a71\u52a8\u8303\u5f0f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u4e3b\u8981\u9075\u5faa\u6307\u4ee4\u9a71\u52a8\u8303\u5f0f\uff0c\u667a\u80fd\u4f53\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u6587\u672c\u6307\u4ee4\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u79cd\u8bbe\u7f6e\u5c06\u667a\u80fd\u4f53\u9650\u5236\u5728\u88ab\u52a8\u89d2\u8272\uff0c\u9650\u5236\u4e86\u5176\u81ea\u4e3b\u6027\u548c\u5bf9\u52a8\u6001\u73af\u5883\u4e8b\u4ef6\u7684\u53cd\u5e94\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86Habitat-Echo\u6570\u636e\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u58f0\u5b66\u6e32\u67d3\u4e0e\u7269\u7406\u4ea4\u4e92\uff1b\u63d0\u51fa\u4e86\u5305\u542b\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u5668\u548c\u4f4e\u7ea7\u7b56\u7565\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4e3b\u52a8\u68c0\u6d4b\u548c\u54cd\u5e94\u542c\u89c9\u4e8b\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u57fa\u7ebf\u65b9\u6cd5\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4e3b\u52a8\u68c0\u6d4b\u548c\u54cd\u5e94\u542c\u89c9\u4e8b\u4ef6\uff0c\u65e0\u9700\u9010\u6848\u6307\u4ee4\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u53cc\u58f0\u6e90\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u6210\u529f\u4ece\u91cd\u53e0\u7684\u58f0\u5b66\u5e72\u6270\u4e2d\u5206\u79bb\u51fa\u4e3b\u8981\u58f0\u6e90\u6267\u884c\u9996\u6b21\u4ea4\u4e92\uff0c\u968f\u540e\u7ee7\u7eed\u64cd\u4f5c\u6b21\u8981\u5bf9\u8c61\u3002", "conclusion": "\u58f0\u97f3\u89e6\u53d1\u7684\u79fb\u52a8\u64cd\u4f5c\u80fd\u591f\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u81ea\u4e3b\u6027\u548c\u73af\u5883\u9002\u5e94\u80fd\u529b\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4e3a\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.21712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21712", "abs": "https://arxiv.org/abs/2601.21712", "authors": ["Xuanran Zhai", "Binkai Ou", "Yemin Wang", "Hui Yi Leong", "Qiaojun Yu", "Ce Hao", "Yaohua Liu"], "title": "CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation", "comment": null, "summary": "Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.", "AI": {"tldr": "CoFreeVLA\u901a\u8fc7\u6dfb\u52a0\u77ed\u65f6\u7a0b\u81ea\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\u5668\u6765\u589e\u5f3a\u7aef\u5230\u7aefVLA\u6a21\u578b\uff0c\u51cf\u5c11\u53cc\u81c2\u64cd\u4f5c\u4e2d\u7684\u81ea\u78b0\u649e\u98ce\u9669\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u53cc\u81c2\u64cd\u4f5c\u4e2d\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u56e0\u4e3a\u6a21\u578b\u672a\u80fd\u5145\u5206\u5efa\u6a21\u53cc\u81c2\u4e4b\u95f4\u4ee5\u53ca\u6293\u53d6\u7269\u4f53\u4e4b\u95f4\u7684\u81ea\u78b0\u649e\u98ce\u9669\u3002", "method": "\u5728\u7aef\u5230\u7aefVLA\u6a21\u578b\u57fa\u7840\u4e0a\u589e\u52a0\u77ed\u65f6\u7a0b\u81ea\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\u5668\uff0c\u8be5\u4f30\u8ba1\u5668\u4ece\u672c\u4f53\u611f\u77e5\u3001\u89c6\u89c9\u5d4c\u5165\u548c\u8ba1\u5212\u52a8\u4f5c\u4e2d\u9884\u6d4b\u78b0\u649e\u53ef\u80fd\u6027\u3002\u901a\u8fc7\u98ce\u9669\u95e8\u63a7\u673a\u5236\u62e6\u622a\u5371\u9669\u6307\u4ee4\uff0c\u901a\u8fc7\u98ce\u9669\u5f15\u5bfc\u8c03\u6574\u6062\u590d\u5b89\u5168\u72b6\u6001\uff0c\u5e76\u5229\u7528\u98ce\u9669\u4fe1\u606f\u4f18\u5316\u7b56\u7565\u8bad\u7ec3\u3002\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u78b0\u649e\u6807\u7b7e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u901a\u8fc7rollout\u6570\u636e\u8fdb\u884c\u540e\u8bad\u7ec3\u6821\u51c6\u3002", "result": "\u5728PiPER\u673a\u5668\u4eba\u624b\u81c2\u7684\u4e94\u4e2a\u53cc\u624b\u4efb\u52a1\u4e2d\uff0cCoFreeVLA\u76f8\u6bd4RDT\u548cAPEX\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u81ea\u78b0\u649e\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "CoFreeVLA\u901a\u8fc7\u96c6\u6210\u81ea\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\u5668\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u53cc\u81c2\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21713", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21713", "abs": "https://arxiv.org/abs/2601.21713", "authors": ["Donatien Delehelle", "Fei Chen", "Darwin Caldwell"], "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations", "comment": "6 pages, 4 figures,", "summary": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.", "AI": {"tldr": "\u672c\u6587\u8d28\u7591\u4e86\u5e03\u6599\u64cd\u4f5c\u4e2d\u5e38\u89c1\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6a21\u5757\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c0f\u4e86\u6a21\u578b\u89c4\u6a21\u5e76\u7f29\u77ed\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8f6c\u79fb\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5e03\u6599\u64cd\u4f5c\u5728\u673a\u5668\u4eba\u9886\u57df\u9762\u4e34\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u3001\u590d\u6742\u52a8\u529b\u5b66\u548c\u81ea\u906e\u6321\u7b49\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u6570\u636e\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u548c\u957f\u65f6\u95f4\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u963b\u788d\u4e86\u65b9\u6cd5\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002\u6b64\u5916\uff0c\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u4fbf\u4e8e\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8f6c\u79fb\uff0c\u4f46\u4f7f\u7528\u9ad8\u5ea6\u4fe1\u606f\u635f\u5931\u7684\u73af\u5883\u72b6\u6001\u8868\u793a\u4e5f\u5e26\u6765\u4e86\u663e\u8457\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6a21\u5757\u5316\u7684\u5e03\u6599\u64cd\u4f5c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u9009\u62e9\uff0c\u5728\u4eff\u771f\u5b66\u4e60\u4e2d\u663e\u8457\u51cf\u5c0f\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u65f6\u95f4\u3002\u540c\u65f6\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u4eff\u771f\u8bad\u7ec3\u7684\u6a21\u578b\u8f6c\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u3002", "result": "\u5728SoftGym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u660e\u663e\u66f4\u5c0f\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u8d28\u7591\u5e03\u6599\u64cd\u4f5c\u4e2d\u5e38\u89c1\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u8bbe\u8ba1\u9009\u62e9\uff0c\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6a21\u5757\u5316\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e3a\u5e03\u6599\u64cd\u4f5c\u7b56\u7565\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.21772", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21772", "abs": "https://arxiv.org/abs/2601.21772", "authors": ["Carmen D. R. Pita-Romero", "Pedro Arias-Perez", "Miguel Fernandez-Cortizas", "Rafael Perez-Segui", "Pascual Campoy"], "title": "Flocking behavior for dynamic and complex swarm structures", "comment": null, "summary": "Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u865a\u62df\u8d28\u5fc3\u6982\u5ff5\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u7f16\u961f\u7b97\u6cd5\uff0c\u7b80\u5316\u590d\u6742\u7ed3\u6784\u5f62\u6210\u4e0e\u8f68\u8ff9\u8ddf\u8e2a", "motivation": "\u591a\u65e0\u4eba\u673a\u4fdd\u6301\u590d\u6742\u7f16\u961f\u5e76\u5b9e\u73b0\u590d\u6742\u8f68\u8ff9\u8ddf\u8e2a\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u63a7\u5236\u65b9\u6cd5", "method": "\u57fa\u4e8e\u865a\u62df\u8d28\u5fc3\u6982\u5ff5\u8bbe\u8ba1\u96c6\u7fa4\u884c\u4e3a\u7b97\u6cd5\uff0c\u6269\u5c55\u7ecf\u5178\u865a\u62df\u884c\u4e3a\u65b9\u6cd5\uff0c\u63d0\u4f9b\u52a8\u6001\u63a7\u5236\u65e0\u4eba\u673a\u6570\u91cf\u548c\u7f16\u961f\u7ed3\u6784\u7684\u7406\u8bba\u6846\u67b6", "result": "\u901a\u8fc7\u4eff\u771f\u6d4b\u8bd5\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7b97\u6cd5\u5373\u4f7f\u5728\u590d\u6742\u7f16\u961f\u548c\u590d\u6742\u8f68\u8ff9\u4e0b\u4e5f\u8868\u73b0\u51fa\u7b80\u5355\u6709\u6548\u7684\u7279\u6027", "conclusion": "\u63d0\u51fa\u7684\u865a\u62df\u8d28\u5fc3\u65b9\u6cd5\u4e3a\u65e0\u4eba\u673a\u96c6\u7fa4\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7ed3\u6784\u548c\u8f68\u8ff9"}}
{"id": "2601.21829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21829", "abs": "https://arxiv.org/abs/2601.21829", "authors": ["Bsher Karbouj", "Baha Eddin Gaaloul", "Jorg Kruger"], "title": "GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration", "comment": null, "summary": "This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.", "AI": {"tldr": "GAZELOAD\u662f\u4e00\u4e2a\u7528\u4e8e\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u4e2d\u8111\u529b\u8d1f\u8377\u4f30\u8ba1\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b26\u540d\u53c2\u4e0e\u8005\u5728\u4e0e\u534f\u4f5c\u673a\u5668\u4eba\u4ea4\u4e92\u65f6\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u3001\u73af\u5883\u6d4b\u91cf\u6570\u636e\u548c\u4efb\u52a1\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u5728\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u4f30\u8ba1\u64cd\u4f5c\u5458\u7684\u8111\u529b\u8d1f\u8377\u5bf9\u4e8e\u4f18\u5316\u534f\u4f5c\u6548\u7387\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6570\u636e\u96c6\u5f80\u5f80\u7f3a\u4e4f\u591a\u6a21\u6001\u540c\u6b65\u6570\u636e\uff0c\u7279\u522b\u662f\u773c\u52a8\u4fe1\u53f7\u4e0e\u73af\u5883\u56e0\u7d20\u7684\u7ed3\u5408\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u5f00\u53d1\u8111\u529b\u8d1f\u8377\u4f30\u8ba1\u7b97\u6cd5\u7684\u80fd\u529b\u3002", "method": "\u5728\u5b9e\u9a8c\u5ba4\u88c5\u914d\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c26\u540d\u53c2\u4e0e\u8005\u4f69\u6234Meta ARIA\u667a\u80fd\u773c\u955c\u4e0e\u4e24\u53f0\u534f\u4f5c\u673a\u5668\u4eba\uff08UR5\u548cFranka Emika Panda\uff09\u4ea4\u4e92\u3002\u6570\u636e\u96c6\u65f6\u95f4\u540c\u6b65\u4e86\u591a\u79cd\u6570\u636e\uff1a\u773c\u52a8\u8ffd\u8e2a\u4fe1\u53f7\uff08\u77b3\u5b54\u76f4\u5f84\u3001\u6ce8\u89c6\u70b9\u3001\u626b\u89c6\u3001\u773c\u52a8\u8f68\u8ff9\u3001\u6ce8\u89c6\u8f6c\u79fb\u71b5\u3001\u6ce8\u89c6\u5206\u6563\u6307\u6570\uff09\u3001\u73af\u5883\u5b9e\u65f6\u8fde\u7eed\u6d4b\u91cf\uff08\u7167\u5ea6\uff09\u3001\u4efb\u52a1\u548c\u673a\u5668\u4eba\u4e0a\u4e0b\u6587\uff08\u5de5\u4f5c\u53f0\u3001\u4efb\u52a1\u5757\u3001\u8bf1\u5bfc\u6545\u969c\uff09\uff0c\u5e76\u5728\u4efb\u52a1\u96be\u5ea6\u548c\u73af\u5883\u6761\u4ef6\u7684\u53d7\u63a7\u64cd\u4f5c\u4e0b\u6536\u96c6\u6570\u636e\u3002", "result": "\u4e3a\u6bcf\u4e2a\u53c2\u4e0e\u8005\u548c\u6309\u8111\u529b\u8d1f\u8377\u5206\u7ea7\u7684\u4efb\u52a1\u5757\u63d0\u4f9b\u4e86CSV\u6587\u4ef6\uff0c\u5305\u542b\u4ee5250\u6beb\u79d2\u7a97\u53e3\u805a\u5408\u7684\u773c\u52a8\u6307\u6807\u3001\u73af\u5883\u65e5\u5fd7\u4ee5\u53ca1-10\u674e\u514b\u7279\u91cf\u8868\u7684\u81ea\u6211\u62a5\u544a\u8111\u529b\u8d1f\u8377\u8bc4\u5206\u3002\u6570\u636e\u6309\u53c2\u4e0e\u8005\u7279\u5b9a\u6587\u4ef6\u5939\u7ec4\u7ec7\uff0c\u5e76\u9644\u5e26\u6587\u6863\u8bf4\u660e\u3002", "conclusion": "GAZELOAD\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u8111\u529b\u8d1f\u8377\u4f30\u8ba1\u7b97\u6cd5\u3001\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u7814\u7a76\u5149\u7167\u7b49\u73af\u5883\u56e0\u7d20\u5bf9\u57fa\u4e8e\u773c\u52a8\u7684\u8111\u529b\u8d1f\u8377\u6807\u8bb0\u7269\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.21876", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21876", "abs": "https://arxiv.org/abs/2601.21876", "authors": ["He Li", "Zhaowei Chen", "Rui Gao", "Guoliang Li", "Qi Hao", "Shuai Wang", "Chengzhong Xu"], "title": "LLM-Driven Scenario-Aware Planning for Autonomous Driving", "comment": null, "summary": "Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLAP\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6a21\u5f0f\u914d\u7f6e\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u9ad8\u901f\u9a7e\u9a76\u6548\u7387\u548c\u5bc6\u96c6\u4ea4\u901a\u4e2d\u5b89\u5168\u64cd\u63a7\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u89c4\u5212\u5668\u5207\u6362\u6846\u67b6\u5728\u5bc6\u96c6\u4ea4\u901a\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u53ef\u9760\u6a21\u5f0f\u8f6c\u6362\u548c\u6301\u7eed\u9ad8\u6548\u9a7e\u9a76\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u542f\u53d1\u5f0f\u573a\u666f\u8bc6\u522b\u548c\u4f4e\u9891\u63a7\u5236\u66f4\u65b0\u3002", "method": "\u63d0\u51faLAP\u65b9\u6cd5\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\uff0c\u5c06\u63a8\u7406\u7ed3\u679c\u96c6\u6210\u5230\u6a21\u5f0f\u914d\u7f6e\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u8054\u5408\u4f18\u5316\u4e2d\uff0c\u91c7\u7528\u6811\u641c\u7d22\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u4ea4\u66ff\u6700\u5c0f\u5316\u6c42\u89e3\u3002", "result": "\u9ad8\u4fdd\u771f\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cLAP\u5728\u9a7e\u9a76\u65f6\u95f4\u548c\u6210\u529f\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "LAP\u65b9\u6cd5\u901a\u8fc7LLM\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u89c4\u5212\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9ad8\u901f\u9a7e\u9a76\u6548\u7387\u4e0e\u5bc6\u96c6\u4ea4\u901a\u5b89\u5168\u64cd\u63a7\u7684\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2601.21884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21884", "abs": "https://arxiv.org/abs/2601.21884", "authors": ["Pratik Ingle", "J\u00f8rn Lambertsen", "Kasper St\u00f8y", "Andres Faina"], "title": "Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation", "comment": "8 pages", "summary": "Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.", "AI": {"tldr": "\u591a\u6a21\u5757MANTA-RAY\u5e73\u53f0\u901a\u8fc7\u5206\u5e03\u5f0f\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5728\u964d\u4f4e\u81f4\u52a8\u5668\u5bc6\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u64cd\u4f5c\u6027\u80fd\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u67d4\u6027\u7269\u4f53\u64cd\u63a7", "motivation": "\u4f20\u7edf\u5bc6\u96c6\u81f4\u52a8\u5668\u9635\u5217\u867d\u7136\u80fd\u4ea7\u751f\u590d\u6742\u53d8\u5f62\uff0c\u4f46\u5e26\u6765\u9ad8\u81ea\u7531\u5ea6\u3001\u7cfb\u7edf\u590d\u6742\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u6a21\u5757\u56db\u81f4\u52a8\u5668\u914d\u7f6e\uff0c\u591a\u6a21\u5757\u914d\u7f6e\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u5c1a\u672a\u63a2\u7d22", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u3001\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684MANTA-RAY\u5e73\u53f0\u53d8\u4f53\uff0c\u91c7\u7528\u6a21\u5757\u95f4\u7269\u4f53\u4f20\u9012\u7b56\u7565\u548c\u51e0\u4f55\u53d8\u6362\u9a71\u52a8\u7684PID\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u5c06\u503e\u659c\u89d2\u5ea6\u63a7\u5236\u8f93\u51fa\u6620\u5c04\u5230\u81f4\u52a8\u5668\u547d\u4ee4\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u9a71\u52a8\u6216\u9ed1\u76d2\u8bad\u7ec3", "result": "\u57283x3\u548c4x4\u6a21\u5757\u914d\u7f6e\u7684\u4eff\u771f\u4e2d\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u901a\u8fc72x2\u786c\u4ef6\u539f\u578b\u9a8c\u8bc1\u53ef\u884c\u6027\u3002\u7cfb\u7edf\u6210\u529f\u64cd\u63a7\u5177\u6709\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u3001\u8d28\u91cf\u548c\u7eb9\u7406\u7684\u7269\u4f53\uff0c\u5305\u62ec\u9e21\u86cb\u3001\u82f9\u679c\u7b49\u6613\u788e\u7269\u54c1\uff0c\u5e76\u652f\u6301\u5e76\u884c\u64cd\u4f5c", "conclusion": "\u591a\u6a21\u5757MANTA-RAY\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5728\u66f4\u5927\u533a\u57df\u5185\u534f\u8c03\u64cd\u4f5c\u591a\u4e2a\u7269\u4f53\uff0c\u5c55\u73b0\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b"}}
{"id": "2601.21926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21926", "abs": "https://arxiv.org/abs/2601.21926", "authors": ["Jinhao Zhang", "Wenlong Xia", "Yaojia Wang", "Zhexuan Zhou", "Huizhe Li", "Yichen Lai", "Haoming Song", "Youmin Gong", "Jie Me"], "title": "Information Filtering via Variational Regularization for Robot Manipulation", "comment": null, "summary": "Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53d8\u5206\u6b63\u5219\u5316\uff08VR\uff09\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u5927\u578b\u53bb\u566a\u89e3\u7801\u5668\u5e26\u6765\u7684\u4e2d\u95f4\u7279\u5f81\u5197\u4f59\u548c\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u9aa8\u5e72\u7279\u5f81\u4e0a\u65bd\u52a0\u65f6\u95f4\u6b65\u6761\u4ef6\u9ad8\u65af\u5206\u5e03\u5e76\u5e94\u7528KL\u6563\u5ea6\u6b63\u5219\u5316\u5668\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u4fe1\u606f\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u57fa\u51c6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4f7f\u7528\u8fc7\u5927\u7684\u53bb\u566a\u89e3\u7801\u5668\uff0c\u867d\u7136\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u53ef\u4ee5\u6539\u5584\u53bb\u566a\u6548\u679c\uff0c\u4f46\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u8fd9\u4f1a\u5f15\u5165\u4e2d\u95f4\u7279\u5f81\u5757\u7684\u5197\u4f59\u548c\u566a\u58f0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u63a8\u7406\u65f6\u968f\u673a\u63a9\u7801\u9aa8\u5e72\u7279\u5f81\uff08\u4e0d\u6539\u53d8\u8bad\u7ec3\uff09\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u8fd9\u8bc1\u5b9e\u4e86\u4e2d\u95f4\u7279\u5f81\u4e2d\u5b58\u5728\u4efb\u52a1\u65e0\u5173\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u6b63\u5219\u5316\uff08VR\uff09\u6a21\u5757\uff0c\u5bf9\u9aa8\u5e72\u7279\u5f81\u65bd\u52a0\u65f6\u95f4\u6b65\u6761\u4ef6\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5e94\u7528KL\u6563\u5ea6\u6b63\u5219\u5316\u5668\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u4fe1\u606f\u74f6\u9888\u3002\u8be5\u6a21\u5757\u8f7b\u91cf\u7ea7\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u4e2d\u95f4\u7279\u5f81\u6765\u51cf\u5c11\u5197\u4f59\u548c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5bb9\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\uff08RoboTwin2.0\u3001Adroit\u548cMetaWorld\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebfDP3\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728RoboTwin2.0\u4e0a\u6210\u529f\u7387\u63d0\u9ad8\u4e866.1%\uff0c\u5728Adroit\u548cMetaWorld\u4e0a\u63d0\u9ad8\u4e864.1%\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u53d8\u5206\u6b63\u5219\u5316\uff08VR\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u80fd\u591f\u51cf\u5c11\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u4e2d\u95f4\u7279\u5f81\u7684\u5197\u4f59\u548c\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u6027\u80fd\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.21971", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21971", "abs": "https://arxiv.org/abs/2601.21971", "authors": ["Lorenzo Mazza", "Ariel Rodriguez", "Rayan Younis", "Martin Lelis", "Ortrun Hellig", "Chenpan Li", "Sebastian Bodenstedt", "Martin Wagner", "Stefanie Speidel"], "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts", "comment": null, "summary": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u624b\u672f\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u76d1\u7763\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u80fd\u5728\u5c11\u91cf\u6f14\u793a\u6570\u636e\u4e0b\u5b66\u4e60\u590d\u6742\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5728\u79bb\u5206\u5e03\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5df2\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u624b\u672f\u673a\u5668\u4eba\u9886\u57df\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u5de5\u4f5c\u7a7a\u95f4\u53d7\u9650\u4ee5\u53ca\u9700\u8981\u6781\u9ad8\u5b89\u5168\u6027\u548c\u53ef\u9884\u6d4b\u6027\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\u6216\u6570\u5343\u6b21\u6f14\u793a\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u76d1\u7763\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u53ef\u6dfb\u52a0\u5230\u4efb\u4f55\u81ea\u4e3b\u7b56\u7565\u4e4b\u4e0a\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u8f7b\u91cf\u7ea7\u52a8\u4f5c\u89e3\u7801\u5668\u7b56\u7565\uff08\u5982ACT\uff09\uff0c\u4ec5\u9700\u4e0d\u5230150\u6b21\u6f14\u793a\u548c\u7acb\u4f53\u5185\u7aa5\u955c\u56fe\u50cf\u5373\u53ef\u5b66\u4e60\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u3002\u5728\u80a0\u9053\u6293\u53d6\u548c\u7275\u62c9\u4efb\u52a1\u4e2d\uff0c\u673a\u5668\u4eba\u52a9\u624b\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u7406\u89e3\u5916\u79d1\u533b\u751f\u610f\u56fe\uff0c\u6267\u884c\u5bf9\u53ef\u53d8\u5f62\u7ec4\u7ec7\u7684\u7cbe\u786e\u6293\u53d6\u548c\u6301\u7eed\u7275\u62c9\u3002", "result": "\u901a\u7528VLA\u6a21\u578b\u5b8c\u5168\u65e0\u6cd5\u5b66\u4e60\u8be5\u4efb\u52a1\uff1b\u6807\u51c6ACT\u5728\u5206\u5e03\u5185\u6761\u4ef6\u4e0b\u53d6\u5f97\u4e2d\u7b49\u6210\u529f\u7387\uff1b\u800c\u91c7\u7528\u76d1\u7763MoE\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u5206\u5e03\u5185\u83b7\u5f97\u66f4\u9ad8\u6210\u529f\u7387\uff0c\u5e76\u5728\u79bb\u5206\u5e03\u573a\u666f\uff08\u65b0\u6293\u53d6\u4f4d\u7f6e\u3001\u5149\u7167\u51cf\u5f31\u3001\u90e8\u5206\u906e\u6321\uff09\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u89c6\u89d2\uff0c\u5e76\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u79bb\u4f53\u732a\u7ec4\u7ec7\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u76d1\u7763MoE\u67b6\u6784\u4e3a\u624b\u672f\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5b9e\u73b0\u590d\u6742\u64cd\u4f5c\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4f53\u5185\u624b\u672f\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u521d\u6b65\u4f53\u5185\u732a\u624b\u672f\u7b56\u7565\u5c55\u793a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.21976", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.21976", "abs": "https://arxiv.org/abs/2601.21976", "authors": ["Alex S. Miller", "Leo McElroy", "Jeffrey H. Lang"], "title": "Macro-Scale Electrostatic Origami Motor", "comment": null, "summary": "Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9996\u4e2a\u5b8f\u89c2\u5c3a\u5ea6\u7684\u53ef\u6298\u53e0\u6298\u7eb8\u65cb\u8f6c\u7535\u673a\uff0c\u4f7f\u7528\u7535\u6655\u653e\u7535\u4ea7\u751f\u626d\u77e9\uff0c\u53ef\u6298\u53e0\u5e73\u653e\u540e\u5c55\u5f00\u5de5\u4f5c", "motivation": "\u73b0\u6709\u53ef\u6298\u53e0\u673a\u5668\u4eba\u7684\u6267\u884c\u5668\u8981\u4e48\u5d4c\u5165\u7ebf\u6027\u6267\u884c\u5668\uff0c\u8981\u4e48\u9644\u52a0\u975e\u6298\u53e0\u65cb\u8f6c\u7535\u673a\uff0c\u4e14\u90fd\u53ea\u80fd\u4ea7\u751f\u7ebf\u6027\u6216\u6298\u53e0\u8fd0\u52a8\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8fde\u7eed\u65cb\u8f6c\u8fd0\u52a8\u3002\u5b8f\u89c2\u5c3a\u5ea6\u4e0a\u5c1a\u672a\u6709\u53ef\u6298\u53e0\u7684\u8fde\u7eed\u65cb\u8f6c\u6267\u884c\u5668\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u6298\u7eb8\u7ed3\u6784\u7684\u65cb\u8f6c\u7535\u673a\uff0c\u91c7\u7528\u7535\u6655\u653e\u7535\u4ea7\u751f\u626d\u77e9\u3002\u7535\u673a\u53ef\u4ee5\u6298\u53e0\u5e73\u653e\uff0c\u7136\u540e\u5c55\u5f00\u5de5\u4f5c\u3002", "result": "\u539f\u578b\u7535\u673a\u5b9e\u73b0\u4e862.5:1\u7684\u5c55\u5f00\u6bd4\uff0c\u5728-29 kV\u9a71\u52a8\u4e0b\u8fbe\u52301440 rpm\u7684\u6700\u9ad8\u8f6c\u901f\uff0c\u6700\u5927\u8f93\u51fa\u626d\u77e9\u8d85\u8fc70.15 mN\u00b7m\uff0c\u4e3b\u52a8\u90e8\u4ef6\u626d\u77e9\u5bc6\u5ea6\u4e3a0.04 Nm/kg\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u5b8f\u89c2\u5c3a\u5ea6\u7684\u53ef\u6298\u53e0\u6298\u7eb8\u65cb\u8f6c\u7535\u673a\uff0c\u4e3a\u53ef\u6298\u53e0\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8fde\u7eed\u65cb\u8f6c\u8fd0\u52a8\u80fd\u529b\uff0c\u5177\u6709\u9ad8\u4f53\u79ef\u8d28\u91cf\u6bd4\u548c\u5f62\u72b6\u9002\u5e94\u6027\u4f18\u52bf\u3002"}}
{"id": "2601.22018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22018", "abs": "https://arxiv.org/abs/2601.22018", "authors": ["Jinhao Zhang", "Zhexuan Zhou", "Huizhe Li", "Yichen Lai", "Wenlong Xia", "Haoming Song", "Youmin Gong", "Jie Me"], "title": "PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy", "comment": null, "summary": "Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.", "AI": {"tldr": "PocketDP3\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea73D\u6269\u6563\u7b56\u7565\uff0c\u7528\u57fa\u4e8eMLP-Mixer\u7684Diffusion Mixer\u66ff\u6362\u4f20\u7edfU-Net\u89e3\u7801\u5668\uff0c\u53c2\u6570\u51cf\u5c1199%\u4ee5\u4e0a\uff0c\u652f\u6301\u4e24\u6b65\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u4eff\u771f\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u67093D\u89c6\u89c9\u6269\u6563\u7b56\u7565\u5b58\u5728\u67b6\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u9ad8\u6548\u76843D\u70b9\u4e91\u7f16\u7801\u5668\u4e0e\u5e9e\u5927\u7684\u89e3\u7801\u5668\u914d\u5bf9\uff0c\u5bfc\u81f4\u89e3\u7801\u5668\u53c2\u6570\u6d6a\u8d39\u4e25\u91cd\u3002\u4f5c\u8005\u8ba4\u4e3a\u7d27\u51d1\u7684\u573a\u666f\u8868\u793a\u4e0d\u9700\u8981\u5982\u6b64\u5e9e\u5927\u7684\u89e3\u7801\u5668\u3002", "method": "\u63d0\u51faPocketDP3\uff0c\u7528\u8f7b\u91cf\u7ea7Diffusion Mixer\uff08\u57fa\u4e8eMLP-Mixer\u5757\uff09\u66ff\u6362\u4f20\u7edf\u6761\u4ef6U-Net\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u65f6\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\u7684\u9ad8\u6548\u878d\u5408\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u3002\u65e0\u9700\u4e00\u81f4\u6027\u84b8\u998f\u6280\u672f\u5373\u53ef\u652f\u6301\u4e24\u6b65\u63a8\u7406\u3002", "result": "\u5728RoboTwin2.0\u3001Adroit\u548cMetaWorld\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u53c2\u6570\u5c11\u4e8e\u5148\u524d\u65b9\u6cd5\u76841%\uff0c\u540c\u65f6\u52a0\u901f\u63a8\u7406\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "PocketDP3\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u8bbe\u8ba1\u89e3\u51b3\u4e863D\u6269\u6563\u7b56\u7565\u4e2d\u7684\u53c2\u6570\u6d6a\u8d39\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u5b66\u4e60\uff0c\u4e3a\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.22074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22074", "abs": "https://arxiv.org/abs/2601.22074", "authors": ["Kevin Zakka", "Qiayuan Liao", "Brent Yi", "Louis Le Lay", "Koushil Sreenath", "Pieter Abbeel"], "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning", "comment": "Code is available at https://github.com/mujocolab/mjlab", "summary": "We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.", "AI": {"tldr": "mjlab\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408GPU\u52a0\u901f\u4eff\u771f\u3001\u53ef\u7ec4\u5408\u73af\u5883\u548c\u6700\u5c0f\u5316\u8bbe\u7f6e\u590d\u6742\u5ea6\uff0c\u63d0\u4f9b\u5355\u547d\u4ee4\u5b89\u88c5\u548c\u539f\u751fMuJoCo\u6570\u636e\u7ed3\u6784\u8bbf\u95ee\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\u4e2d\u5b58\u5728\u7684\u8bbe\u7f6e\u590d\u6742\u3001\u4f9d\u8d56\u8fc7\u591a\u3001\u4eff\u771f\u901f\u5ea6\u6162\u7b49\u95ee\u9898\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6613\u4e8e\u4f7f\u7528\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528Isaac Lab\u5f15\u5165\u7684\u57fa\u4e8e\u7ba1\u7406\u5668\u7684API\uff0c\u7ed3\u5408MuJoCo Warp\u8fdb\u884cGPU\u52a0\u901f\u7269\u7406\u4eff\u771f\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u6784\u5efa\u5757\u7528\u4e8e\u89c2\u6d4b\u3001\u5956\u52b1\u548c\u4e8b\u4ef6\u7ec4\u5408\u3002", "result": "mjlab\u5b9e\u73b0\u4e86\u5355\u547d\u4ee4\u5b89\u88c5\u3001\u6700\u5c0f\u5316\u4f9d\u8d56\u3001\u76f4\u63a5\u8bbf\u95ee\u539f\u751fMuJoCo\u6570\u636e\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u901f\u5ea6\u8ddf\u8e2a\u3001\u8fd0\u52a8\u6a21\u4eff\u548c\u64cd\u4f5c\u4efb\u52a1\u7684\u53c2\u8003\u5b9e\u73b0\u3002", "conclusion": "mjlab\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u6613\u7528\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7GPU\u52a0\u901f\u4eff\u771f\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u7814\u7a76\u95e8\u69db\u3002"}}
{"id": "2601.22090", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22090", "abs": "https://arxiv.org/abs/2601.22090", "authors": ["Runsheng Wang", "Katelyn Lee", "Xinyue Zhu", "Lauren Winterbottom", "Dawn M. Nilsen", "Joel Stein", "Matei Ciocarlie"], "title": "ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection", "comment": null, "summary": "Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.", "AI": {"tldr": "\u63d0\u51fa\u5065\u5eb7\u5230\u4e2d\u98ce\u7684\u9002\u5e94\u7ba1\u9053\uff0c\u5229\u7528\u5927\u89c4\u6a21\u5065\u5eb7\u4ebasEMG\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4ec5\u9700\u5c11\u91cf\u4e2d\u98ce\u60a3\u8005\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e2d\u98ce\u610f\u56fe\u68c0\u6d4b\u6027\u80fd", "motivation": "\u8868\u9762\u808c\u7535\u4fe1\u53f7(sEMG)\u662f\u4e2d\u98ce\u540e\u624b\u90e8\u5eb7\u590d\u7684\u6f5c\u5728\u63a7\u5236\u4fe1\u53f7\uff0c\u4f46\u4ece\u4e2d\u98ce\u762b\u75ea\u808c\u8089\u68c0\u6d4b\u610f\u56fe\u901a\u5e38\u9700\u8981\u5197\u957f\u7684\u53d7\u8bd5\u8005\u7279\u5b9a\u6821\u51c6\uff0c\u4e14\u5bf9\u53d8\u5f02\u6027\u654f\u611f", "method": "\u5065\u5eb7\u5230\u4e2d\u98ce\u9002\u5e94\u7ba1\u9053\uff1a\u9996\u5148\u5728\u5927\u578b\u5065\u5eb7\u4ebasEMG\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u4ec5\u4f7f\u7528\u5c11\u91cf\u4e2d\u98ce\u60a3\u8005\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002\u6bd4\u8f83\u4e86\u4e09\u79cd\u9002\u5e94\u7b56\u7565\uff1a\u4ec5\u5934\u90e8\u8c03\u8c10\u3001\u53c2\u6570\u9ad8\u6548\u7684LoRA\u9002\u914d\u5668\u548c\u5b8c\u6574\u7aef\u5230\u7aef\u5fae\u8c03", "result": "\u5065\u5eb7\u9884\u8bad\u7ec3\u9002\u5e94\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u96f6\u6837\u672c\u8fc1\u79fb\u548c\u540c\u7b49\u6570\u636e\u9884\u7b97\u4e0b\u7684\u4ec5\u4e2d\u98ce\u8bad\u7ec3\u3002\u6700\u4f73\u9002\u5e94\u65b9\u6cd5\u5c06\u5e73\u5747\u8fc7\u6e21\u51c6\u786e\u7387\u4ece0.42\u63d0\u5347\u81f30.61\uff0c\u539f\u59cb\u51c6\u786e\u7387\u4ece0.69\u63d0\u5347\u81f30.78", "conclusion": "\u8f6c\u79fb\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u5065\u5eb7\u57dfEMG\u8868\u793a\u53ef\u4ee5\u51cf\u5c11\u6821\u51c6\u8d1f\u62c5\uff0c\u540c\u65f6\u63d0\u9ad8\u5b9e\u65f6\u4e2d\u98ce\u540e\u610f\u56fe\u68c0\u6d4b\u7684\u9c81\u68d2\u6027"}}
{"id": "2601.22153", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22153", "abs": "https://arxiv.org/abs/2601.22153", "authors": ["Haozhe Xie", "Beichen Wen", "Jiarui Zheng", "Zhaoxi Chen", "Fangzhou Hong", "Haiwen Diao", "Ziwei Liu"], "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "comment": "Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "AI": {"tldr": "DynamicVLA\u662f\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u51d1\u6a21\u578b\u67b6\u6784\u3001\u8fde\u7eed\u63a8\u7406\u548c\u6f5c\u5728\u611f\u77e5\u52a8\u4f5c\u6d41\u5f0f\u5904\u7406\u89e3\u51b3\u52a8\u6001\u573a\u666f\u4e2d\u7684\u611f\u77e5\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u4e86DOM\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u9759\u6001\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u5feb\u901f\u611f\u77e5\u3001\u65f6\u95f4\u9884\u6d4b\u548c\u8fde\u7eed\u63a7\u5236\u7684\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "1) \u4f7f\u7528\u5377\u79ef\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7d27\u51d10.4B VLA\u6a21\u578b\u5b9e\u73b0\u7a7a\u95f4\u9ad8\u6548\u7f16\u7801\uff1b2) \u8fde\u7eed\u63a8\u7406\u5b9e\u73b0\u91cd\u53e0\u63a8\u7406\u548c\u6267\u884c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\uff1b3) \u6f5c\u5728\u611f\u77e5\u52a8\u4f5c\u6d41\u5f0f\u5904\u7406\u786e\u4fdd\u65f6\u95f4\u5bf9\u9f50\u7684\u52a8\u4f5c\u6267\u884c\u3002\u540c\u65f6\u521b\u5efa\u4e86DOM\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\u5728\u54cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0cDynamicVLA\u6210\u4e3a\u8de8\u5b9e\u73b0\u65b9\u5f0f\u7684\u901a\u7528\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "DynamicVLA\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u63a8\u7406\u548c\u95ed\u73af\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
