{"id": "2601.04334", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04334", "abs": "https://arxiv.org/abs/2601.04334", "authors": ["Amit Jain", "Richard Linares"], "title": "Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization", "comment": null, "summary": "This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5b66\u4e60\u578b\u5236\u5bfc\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5728\u591a\u79cd\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5c06\u63a8\u7406\u80fd\u529b\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u751f\u6210\u63a7\u5236\u5e8f\u5217\uff0c\u8fd8\u80fd\u63d0\u4f9b\u4eba\u7c7b\u53ef\u8bfb\u7684\u51b3\u7b56\u89e3\u91ca\uff0c\u7279\u522b\u662f\u5728\u822a\u7a7a\u822a\u5929\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5b66\u4e60\u683c\u5f0f\u5316\u548c\u63a7\u5236\u539f\u8bed\uff0c\u7136\u540e\u4f7f\u7528GRPO\u8fdb\u884c\u4ea4\u4e92\u9a71\u52a8\u7684\u7b56\u7565\u6539\u8fdb\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u79cd\u4e0d\u540c\u52a8\u529b\u5b66\u590d\u6742\u5ea6\u7684\u63a7\u5236\u95ee\u9898\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u7684\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684LLM\u80fd\u591f\u5728\u4e00\u81f4\u7684\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\uff0c\u4e3a\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u5408\u6210\u53ef\u884c\u7684\u7a33\u5b9a\u7b56\u7565\uff0c\u5e76\u80fd\u751f\u6210\u63a7\u5236\u5e8f\u5217\u5e76\u63d0\u4f9b\u51b3\u7b56\u8fc7\u7a0b\u7684\u89e3\u91ca\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u57fa\u4e8eGRPO\u7684\u63a8\u7406\u5e94\u7528\u4e8e\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728\u822a\u7a7a\u822a\u5929\u548c\u5176\u4ed6\u5b89\u5168\u5173\u952e\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u5c55\u793a\u4e86LLM\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.04356", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04356", "abs": "https://arxiv.org/abs/2601.04356", "authors": ["Zhengtong Xu", "Yuki Shirai"], "title": "UNIC: Learning Unified Multimodal Extrinsic Contact Estimation", "comment": null, "summary": "Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.", "AI": {"tldr": "UNIC\u662f\u4e00\u4e2a\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u6216\u76f8\u673a\u6807\u5b9a\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u5916\u90e8\u63a5\u89e6\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u63a5\u89e6\u4f4d\u7f6e\u3001\u7269\u4f53\u548c\u52a8\u6001\u76f8\u673a\u89c6\u89d2\u4e0b\u8868\u73b0\u53ef\u9760\u3002", "motivation": "\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u9700\u8981\u53ef\u9760\u7684\u5916\u90e8\u63a5\u89e6\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u63a5\u89e6\u7c7b\u578b\u3001\u56fa\u5b9a\u6293\u53d6\u914d\u7f6e\u6216\u76f8\u673a\u6807\u5b9a\u7b49\u9650\u5236\u6027\u5047\u8bbe\uff0c\u963b\u788d\u4e86\u5728\u65b0\u7269\u4f53\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "UNIC\u76f4\u63a5\u7f16\u7801\u76f8\u673a\u5750\u6807\u7cfb\u4e2d\u7684\u89c6\u89c9\u89c2\u6d4b\uff0c\u4ee5\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u6574\u5408\u672c\u4f53\u611f\u89c9\u548c\u89e6\u89c9\u6a21\u6001\uff0c\u5f15\u5165\u57fa\u4e8e\u573a\u666f\u53ef\u4f9b\u6027\u56fe\u7684\u7edf\u4e00\u63a5\u89e6\u8868\u793a\uff0c\u5e76\u91c7\u7528\u5e26\u968f\u673a\u63a9\u7801\u7684\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\u8fdb\u884c\u9c81\u68d2\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u63a5\u89e6\u4f4d\u7f6e\u5b9e\u73b09.6\u6beb\u7c73\u7684\u5e73\u5747Chamfer\u8ddd\u79bb\u8bef\u5dee\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5728\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\uff0c\u5e76\u80fd\u9002\u5e94\u52a8\u6001\u76f8\u673a\u89c6\u89d2\u3002", "conclusion": "UNIC\u5c06\u5916\u90e8\u63a5\u89e6\u4f30\u8ba1\u786e\u7acb\u4e3a\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4e2d\u5b9e\u7528\u4e14\u591a\u529f\u80fd\u7684\u80fd\u2f12\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04493", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04493", "abs": "https://arxiv.org/abs/2601.04493", "authors": ["James M. Ferguson", "Alan Kuntz", "Tucker Hermans"], "title": "Fast Continuum Robot Shape and External Load State Estimation on SE(3)", "comment": "Public preprint for ICRA 2026", "summary": "Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \\textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u65f6\u7a7a\u72b6\u6001\u4f30\u8ba1\u901a\u7528\u6846\u67b6\uff0c\u7ed3\u5408\u9a71\u52a8\u8f93\u5165\u3001\u5916\u90e8\u8f7d\u8377\u3001\u8fc7\u7a0b\u566a\u58f0\u7b49\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u8054\u5408\u4f30\u8ba1\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u52a8\u5b66\u3001\u672b\u7aef\u529b\u611f\u77e5\u548c\u5206\u5e03\u5f0f\u8f7d\u8377\u4f30\u8ba1", "motivation": "\u4f20\u7edf\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u91c7\u7528\u7b80\u5316\u7684Cosserat\u6746\u6a21\u578b\uff0c\u65e0\u6cd5\u76f4\u63a5\u8003\u8651\u9a71\u52a8\u8f93\u5165\u548c\u5916\u90e8\u8f7d\u8377\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6846\u67b6\u6765\u6574\u5408\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20", "method": "\u5efa\u7acb\u5305\u542b\u9a71\u52a8\u4e0d\u786e\u5b9a\u6027\u3001\u4f5c\u7528\u529b/\u529b\u77e9\u3001\u8fc7\u7a0b\u566a\u58f0\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u4efb\u610f\u80cc\u90e8\u6d4b\u91cf\u7684\u901a\u7528\u6846\u67b6\uff0c\u6dfb\u52a0\u65f6\u95f4\u5148\u9a8c\u5b9e\u73b0\u65f6\u7a7a\u8054\u5408\u4f30\u8ba1\uff0c\u901a\u8fc7\u5f27\u957f\u57df\u79bb\u6563\u5316\u83b7\u5f97\u56e0\u5b50\u56fe\u8868\u793a\uff0c\u5229\u7528SLAM\u98ce\u683c\u7684\u7a00\u758f\u975e\u7ebf\u6027\u4f18\u5316", "result": "\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u808c\u8171\u9a71\u52a8\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u8fd0\u52a8\u5b66\uff08\u542b\u4e0d\u786e\u5b9a\u6027\uff09\u3001\u57fa\u4e8e\u4f4d\u7f6e\u53cd\u9988\u7684\u672b\u7aef\u529b\u611f\u77e5\u3001\u57fa\u4e8e\u80cc\u90e8\u5e94\u53d8\u7684\u5206\u5e03\u5f0f\u8f7d\u8377\u4f30\u8ba1\uff1b\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u540c\u5fc3\u7ba1\u624b\u672f\u673a\u5668\u4eba\u7684\u7cbe\u786e\u8fd0\u52a8\u5b66\u548c\u672b\u7aef\u529b\u4f30\u8ba1\uff0c\u5c55\u793a\u624b\u672f\u89e6\u8bca\u6f5c\u529b", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7c7b\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u8054\u5408\u4f30\u8ba1\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u72b6\u6001\u4f30\u8ba1\u80fd\u529b\uff0c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u8fd0\u52a8\u5b66\u548c\u529b\u611f\u77e5\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.04541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04541", "abs": "https://arxiv.org/abs/2601.04541", "authors": ["Gustavo H. Diaz", "A. Sejal Jain", "Matteo Brugnera", "Elian Neppel", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Design and Development of Modular Limbs for Reconfigurable Robots on the Moon", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u540d\u4e3aMoonbots\u76844\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u80a2\u4f53\u6a21\u5757\uff0c\u53ef\u4e0e\u8f6e\u5f0f\u6a21\u5757\u8fde\u63a5\u7ec4\u6210\u591a\u79cd\u914d\u7f6e\uff0c\u7528\u4e8e\u6708\u7403\u63a2\u7d22\u548c\u5efa\u8bbe\u4efb\u52a1\u3002\u91c7\u7528\u7edf\u4e00\u7684\u9ad8\u626d\u77e9-\u901f\u5ea6\u6bd4\u6267\u884c\u5668\u8bbe\u8ba1\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u548c\u7ef4\u62a4\u3002\u5c55\u793a\u4e869\u79cd\u529f\u80fd\u914d\u7f6e\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u53ef\u91cd\u6784\u6027\u3002", "motivation": "\u4e3a\u592a\u7a7a\u63a2\u7d22\u548c\u6708\u7403\u5efa\u8bbe\u4efb\u52a1\u5f00\u53d1\u7075\u6d3b\u3001\u591a\u529f\u80fd\u7684\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u3002\u5728\u8d44\u6e90\u53d7\u9650\u7684\u592a\u7a7a\u4efb\u52a1\u4e2d\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u80fd\u63d0\u4f9b\u9002\u5e94\u4e0d\u540c\u73af\u5883\u548c\u4efb\u52a1\u7684\u7075\u6d3b\u6027\uff0c\u6ee1\u8db3Moonshot\u9879\u76ee\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e864\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u80a2\u4f53\u6a21\u5757(Moonbots)\u548c\u8f6e\u5f0f\u6a21\u5757\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u9ad8\u626d\u77e9-\u901f\u5ea6\u6bd4\u6267\u884c\u5668\u9a71\u52a8\u3002\u5f00\u53d1\u4e86\u786c\u4ef6\u5b9e\u73b0\u3001\u673a\u68b0\u8bbe\u8ba1\u548c\u6574\u4f53\u8f6f\u4ef6\u67b6\u6784\uff0c\u7528\u4e8e\u63a7\u5236\u548c\u534f\u8c03\u6a21\u5757\u3002\u5728\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u6267\u884c\u5668\u7684\u63a7\u5236\u6027\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5c55\u793a\u4e869\u79cd\u529f\u80fd\u914d\u7f6e\uff1a4\u81ea\u7531\u5ea6\u80a2\u4f53\u30018\u81ea\u7531\u5ea6\u80a2\u4f53\u3001\u8f66\u8f86\u3001\u9f99\u5f62\u3001\u6700\u5c0f\u914d\u7f6e\u3001\u56db\u8db3\u3001\u8d27\u7269\u3001\u6700\u5c0f\u8d27\u7269\u548c\u81ea\u884c\u8f66\u914d\u7f6e\u3002\u8fd9\u4e9b\u914d\u7f6e\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u8fd0\u52a8\u7b56\u7565\u548c\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u53ef\u91cd\u6784\u6027\u3002", "conclusion": "Moonbots\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e3a\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002\u7edf\u4e00\u6267\u884c\u5668\u8bbe\u8ba1\u7b80\u5316\u4e86\u5f00\u53d1\u548c\u7ef4\u62a4\uff0c\u591a\u79cd\u914d\u7f6e\u5c55\u793a\u4e86\u7cfb\u7edf\u9002\u5e94\u4e0d\u540c\u73af\u5883\u548c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u592a\u7a7a\u63a2\u7d22\u4efb\u52a1\u3002"}}
{"id": "2601.04547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04547", "abs": "https://arxiv.org/abs/2601.04547", "authors": ["Jakob M. Kern", "James M. Hurrell", "Shreya Santra", "Keisuke Takehana", "Kentaro Uno", "Kazuya Yoshida"], "title": "Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u51c6\u786e\u6027\u7684\u6708\u7403\u8868\u9762\u6a21\u62df\u5668\uff0c\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684\u56de\u5f52\u6a21\u578b\u6765\u6a21\u62df\u8f6e-\u571f\u58e4\u76f8\u4e92\u4f5c\u7528\uff0c\u652f\u6301\u5b9e\u65f6\u5e94\u7528", "motivation": "\u5f53\u524d\u6708\u7403\u8868\u9762\u6a21\u62df\u5668\u8981\u4e48\u6ce8\u91cd\u89c6\u89c9\u771f\u5b9e\u611f\uff0c\u8981\u4e48\u6ce8\u91cd\u7269\u7406\u51c6\u786e\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u590d\u5236\u6708\u7403\u6761\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u5728\u6f2b\u6e38\u8f66\u64cd\u4f5c\u548c\u4efb\u52a1\u89c4\u5212\u6d4b\u8bd5\u4e2d\u7684\u80fd\u529b", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f7f\u7528\u4ece\u5168\u8f66\u548c\u5355\u8f6e\u5b9e\u9a8c\u53ca\u6a21\u62df\u4e2d\u6536\u96c6\u7684\u6570\u636e\u6784\u5efa\u56de\u5f52\u6a21\u578b\uff0c\u9884\u6d4b\u6ed1\u79fb\u548c\u4e0b\u6c89\u91cf\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u5730\u5f62\u53d8\u5f62\u548c\u8f66\u8f6e\u8f68\u8ff9\u53ef\u89c6\u5316\u7684\u771f\u5b9e\u611f", "result": "\u56de\u5f52\u578b\u5730\u9762\u529b\u5b66\u6a21\u578b\u80fd\u51c6\u786e\u518d\u73b0\u5e73\u5766\u5730\u5f62\u548c20\u5ea6\u659c\u5761\u4e0a\u7684\u7a33\u6001\u548c\u52a8\u6001\u6ed1\u79fb\u4ee5\u53ca\u4e0b\u6c89\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u4e86\u73b0\u573a\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u652f\u6301\u9700\u8981\u7269\u7406\u5408\u7406\u5730\u5f62\u54cd\u5e94\u548c\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u5b9e\u65f6\u5e94\u7528", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u586b\u8865\u4e86\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u7269\u7406\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6708\u7403\u6f2b\u6e38\u8f66\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u5e94\u7528\u573a\u666f"}}
{"id": "2601.04551", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04551", "abs": "https://arxiv.org/abs/2601.04551", "authors": ["Riku Suzuki", "Ayumi Umemura", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain", "comment": "Author's version of a manuscript accepted at the 11th International Conference on Automation, Robotics, and Applications (ICARA). (c) IEEE", "summary": "Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u7684\u70b9\u4e91\u5730\u56fe\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u8f6c\u6362\u4e3a\u9891\u57df\u56fe\u50cf\u5e76\u7701\u7565\u9ad8\u9891\u5206\u91cf\uff0c\u5b9e\u73b0\u9488\u5bf9\u6e10\u53d8\u5730\u5f62\uff08\u5982\u884c\u661f\u3001\u6c99\u6f20\uff09\u7684\u6709\u6548\u538b\u7f29\u3002", "motivation": "\u5728\u65e0\u4eba\u673a\u673a\u5668\u4eba\u63a2\u7d22\u4efb\u52a1\u4e2d\uff0cSLAM\u6280\u672f\u5bf9\u6548\u7387\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u673a\u8f7d\u8ba1\u7b97\u80fd\u529b\u548c\u901a\u4fe1\u5e26\u5bbd\u6709\u9650\uff0c\u800cSLAM\u5904\u7406\u7684\u70b9\u4e91\u6570\u636e\u91cf\u5e9e\u5927\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u70b9\u4e91\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u7684\u70b9\u4e91\u5730\u56fe\u538b\u7f29\u65b9\u6cd5\uff1a\u5c06\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u8f6c\u6362\u4e3a\u9891\u57df2D\u56fe\u50cf\uff0c\u5e76\u7701\u7565\u5176\u9ad8\u9891\u5206\u91cf\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u6e10\u53d8\u5730\u5f62\uff08\u5982\u884c\u661f\u3001\u6c99\u6f20\uff09\uff0c\u56e0\u4e3a\u4e0e\u4eba\u5de5\u73af\u5883\u7b49\u5177\u6709\u8be6\u7ec6\u7ed3\u6784\u7684\u5730\u5f62\u4e0d\u540c\uff0c\u9ad8\u9891\u5206\u91cf\u5bf9\u6e10\u53d8\u5730\u5f62\u7684\u8868\u793a\u8d21\u732e\u5f88\u5c0f\u3002", "result": "\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u9ad8\u7a0b\u5256\u9762\u7684\u5730\u5f62\u76f8\u673a\u5e8f\u5217\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5728\u538b\u7f29\u7387\u548c\u7cbe\u5ea6\u65b9\u9762\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u70b9\u4e91\u8d28\u91cf\u6ca1\u6709\u663e\u8457\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u538b\u7f29\u6570\u636e\u5927\u5c0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6e10\u53d8\u5730\u5f62\u73af\u5883\u3002"}}
{"id": "2601.04629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04629", "abs": "https://arxiv.org/abs/2601.04629", "authors": ["Zhongxuan Li", "Zeliang Guo", "Jun Hu", "David Navarro-Alarcon", "Jia Pan", "Hongmin Wu", "Peng Zhou"], "title": "UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation", "comment": null, "summary": "We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.", "AI": {"tldr": "UniBiDex\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u652f\u6301VR\u548c\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u4e24\u79cd\u8f93\u5165\u6a21\u5f0f\uff0c\u901a\u8fc7\u96f6\u7a7a\u95f4\u63a7\u5236\u4f18\u5316\u53cc\u624b\u914d\u7f6e\uff0c\u5728\u53a8\u623f\u6574\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u5e73\u6ed1\u8f68\u8ff9\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u6536\u96c6\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u96c6\u7684\u969c\u788d\uff0c\u5e76\u52a0\u901f\u673a\u5668\u4eba\u5b66\u4e60\u8fdb\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u9065\u64cd\u4f5c\u6846\u67b6\u6765\u652f\u6301\u591a\u79cd\u8f93\u5165\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u63a5\u89e6\u4e30\u5bcc\u7684\u53cc\u624b\u64cd\u4f5c\u3002", "method": "UniBiDex\u5c06\u5f02\u6784\u8f93\u5165\u8bbe\u5907\u96c6\u6210\u5230\u5171\u4eab\u63a7\u5236\u5806\u6808\u4e2d\uff0c\u91c7\u7528\u4e00\u81f4\u7684\u52a8\u529b\u5b66\u5904\u7406\u548c\u5b89\u5168\u6027\u4fdd\u8bc1\uff0c\u4f7f\u7528\u96f6\u7a7a\u95f4\u63a7\u5236\u6765\u4f18\u5316\u53cc\u624b\u914d\u7f6e\uff0c\u786e\u4fdd\u5e73\u6ed1\u3001\u65e0\u78b0\u649e\u4e14\u80fd\u611f\u77e5\u5947\u5f02\u70b9\u7684\u8fd0\u52a8\u3002", "result": "\u5728\u6d89\u53ca\u4e94\u4e2a\u987a\u5e8f\u64cd\u4f5c\u5b50\u4efb\u52a1\u7684\u957f\u65f6\u7a0b\u53a8\u623f\u6574\u7406\u4efb\u52a1\u4e2d\uff0cUniBiDex\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\u548c\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u6240\u6709\u786c\u4ef6\u548c\u8f6f\u4ef6\u7ec4\u4ef6\uff0cUniBiDex\u65e8\u5728\u964d\u4f4e\u6536\u96c6\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u96c6\u7684\u969c\u788d\uff0c\u52a0\u901f\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2601.04657", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04657", "abs": "https://arxiv.org/abs/2601.04657", "authors": ["Takafumi Sakamoto", "Yugo Takeuchi"], "title": "Model of Spatial Human-Agent Interaction with Consideration for Others", "comment": null, "summary": "Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u8003\u8651\u4ed6\u4eba\u901a\u4fe1\u610f\u613f\u7684\u8ba1\u7b97\u7a7a\u95f4\u4ea4\u4e92\u6a21\u578b\uff0c\u901a\u8fc7VR\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8003\u8651\u53c2\u6570\u5bf9\u673a\u5668\u4eba\u4ea4\u4e92\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u901a\u4fe1\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u9700\u8981\u4e3b\u52a8\u4e0e\u4eba\u4ea4\u6d41\uff0c\u4f46\u540c\u65f6\u4e0d\u80fd\u6253\u6270\u884c\u4eba\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\uff0c\u9700\u8981\u57fa\u4e8e\u4ed6\u4eba\u884c\u4e3a\u4f30\u8ba1\u5176\u901a\u4fe1\u610f\u613f\uff0c\u5e76\u76f8\u5e94\u8c03\u6574\u673a\u5668\u4eba\u7684\u901a\u4fe1\u6d3b\u52a8\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u7a7a\u95f4\u4ea4\u4e92\u6a21\u578b\uff0c\u5c06\"\u8003\u8651\u4ed6\u4eba\"\u8868\u8fbe\u4e3a\u5b9a\u91cf\u53c2\u6570\uff1a\u6839\u636e\u4f30\u8ba1\u7684\u4ed6\u4eba\u5185\u90e8\u72b6\u6001\u8c03\u6574\u81ea\u8eab\u5185\u90e8\u72b6\u6001\u7684\u7a0b\u5ea6\u3002\u901a\u8fc7VR\u73af\u5883\u4e2d\u4eba\u4e0e\u865a\u62df\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5f53\u53c2\u4e0e\u8005\u5411\u76ee\u6807\u79fb\u52a8\u65f6\uff0c\u4f4e\u8003\u8651\u503c\u7684\u865a\u62df\u673a\u5668\u4eba\u4f1a\u6291\u5236\u53c2\u4e0e\u8005\u7684\u79fb\u52a8\uff0c\u800c\u9ad8\u8003\u8651\u503c\u7684\u673a\u5668\u4eba\u4e0d\u4f1a\u6291\u5236\u3002\u5f53\u53c2\u4e0e\u8005\u63a5\u8fd1\u673a\u5668\u4eba\u65f6\uff0c\u65e0\u8bba\u8003\u8651\u503c\u9ad8\u4f4e\uff0c\u673a\u5668\u4eba\u90fd\u4f1a\u8868\u73b0\u51fa\u63a5\u8fd1\u884c\u4e3a\uff0c\u4ece\u800c\u51cf\u5c11\u53c2\u4e0e\u8005\u7684\u79fb\u52a8\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u80fd\u591f\u9610\u660e\u8003\u8651\u4ed6\u4eba\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u4e3a\u901a\u4fe1\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u7684\u884c\u4e3a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.04668", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04668", "abs": "https://arxiv.org/abs/2601.04668", "authors": ["Laukik Patade", "Rohan Rane", "Sandeep Pillai"], "title": "Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture", "comment": null, "summary": "This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u519c\u4e1a\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u8def\u5f84\u89c4\u5212\uff0c\u4ece\u4f20\u7edf\u7f51\u683c\u65b9\u6cd5\u8f6c\u5411\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684DRL\u7b97\u6cd5\uff0c\u6700\u7ec8TD3\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fbe\u523095%\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff08\u5982A*\u548cDijkstra\u7b97\u6cd5\uff09\u5728\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\u6765\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u519c\u7530\u573a\u666f\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4eceDQN\u53ca\u5176\u53d8\u4f53\uff08Double Q-Networks\u3001Dueling Networks\uff09\u5f00\u59cb\uff0c\u7136\u540e\u8f6c\u5411\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u6a21\u578bDDPG\u548cTD3\uff0c\u5728ROS\u548cGazebo\u6784\u5efa\u7684\u4e09\u7ef4\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u9884\u8bad\u7ec3\u7684TD3\u667a\u80fd\u4f53\u5728\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e2d\u8fbe\u523095%\u7684\u6210\u529f\u7387\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u79fb\u52a8\u969c\u788d\u7269\uff0c\u540c\u65f6\u786e\u4fdd\u4f5c\u7269\u548c\u673a\u5668\u4eba\u7684\u5b89\u5168\u3002", "conclusion": "\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u7279\u522b\u662fTD3\uff09\u5728\u519c\u4e1a\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u52a8\u6001\u8def\u5f84\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04881", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04881", "abs": "https://arxiv.org/abs/2601.04881", "authors": ["Kiyoung Choi", "Juwon Jeong", "Sehoon Oh"], "title": "Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly", "comment": null, "summary": "This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u529b\u77e9\u6270\u52a8\u89c2\u6d4b\u5668(DW-DOB)\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u7a7a\u95f4\u60ef\u6027\u5d4c\u5165\u89c2\u6d4b\u5668\u540d\u4e49\u6a21\u578b\uff0c\u5b9e\u73b0\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4e2d\u7684\u9ad8\u7075\u654f\u5ea6\u96f6\u529b\u77e9\u63a7\u5236\uff0c\u5728\u5de5\u4e1a\u516c\u5dee(H7/h6)\u7684\u5b54\u8f74\u88c5\u914d\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u529b\u77e9\u6270\u52a8\u89c2\u6d4b\u5668\u65e0\u6cd5\u8865\u507f\u60ef\u6027\u6548\u5e94\uff0c\u5bfc\u81f4\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u96be\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u96f6\u529b\u77e9\u63a7\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u79bb\u56fa\u6709\u52a8\u6001\u53cd\u5e94\u548c\u771f\u5b9e\u5916\u90e8\u529b\u77e9\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5c0f\u529b\u548c\u529b\u77e9\u7684\u654f\u611f\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u529b\u77e9\u6270\u52a8\u89c2\u6d4b\u5668(DW-DOB)\uff0c\u5c06\u4efb\u52a1\u7a7a\u95f4\u60ef\u6027\u5d4c\u5165\u89c2\u6d4b\u5668\u7684\u540d\u4e49\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u6e05\u6670\u5206\u79bb\u56fa\u6709\u52a8\u6001\u53cd\u5e94\u548c\u771f\u5b9e\u5916\u90e8\u529b\u77e9\u3002\u91c7\u7528\u57fa\u4e8e\u65e0\u6e90\u6027\u7684\u5206\u6790\u65b9\u6cd5\u8bc1\u660eDW-DOB\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u4ea4\u4e92\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u516c\u5dee(H7/h6)\u7684\u5b54\u8f74\u88c5\u914d\u5b9e\u9a8c\u4e2d\uff0cDW-DOB\u5b9e\u73b0\u4e86\u66f4\u6df1\u3001\u66f4\u67d4\u987a\u7684\u63d2\u5165\uff0c\u6b8b\u4f59\u529b\u77e9\u6700\u5c0f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u529b\u77e9\u6270\u52a8\u89c2\u6d4b\u5668\u548cPD\u57fa\u7ebf\u63a7\u5236\u5668\u3002", "conclusion": "DW-DOB\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u65e0\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u96f6\u529b\u77e9\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c2\u6d4b\u5668\u65e0\u6cd5\u8865\u507f\u60ef\u6027\u6548\u5e94\u7684\u7f3a\u9677\u3002"}}
{"id": "2601.04948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04948", "abs": "https://arxiv.org/abs/2601.04948", "authors": ["Junchi Gu", "Feiyang Yuan", "Weize Shi", "Tianchen Huang", "Haopeng Zhang", "Xiaohu Zhang", "Yu Wang", "Wei Gao", "Shiwu Zhang"], "title": "SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles", "comment": null, "summary": "Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\\%$ and $63.34\\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u914d\u5907\u88ab\u52a8\u8f6e\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6ed1\u51b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6846\u67b6\u5b9e\u73b0\u4e86\u6ed1\u51b0\u6b65\u6001\uff0c\u76f8\u6bd4\u4f20\u7edf\u53cc\u8db3\u884c\u8d70\u663e\u8457\u964d\u4f4e\u4e86\u51b2\u51fb\u5f3a\u5ea6\u548c\u8fd0\u8f93\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u4eba\u5f62\u673a\u5668\u4eba\u7684\u884c\u8d70\u548c\u8dd1\u6b65\u6b65\u6001\u4f1a\u4ea7\u751f\u9ad8\u77ac\u65f6\u51b2\u51fb\u529b\uff0c\u5bfc\u81f4\u5173\u8282\u78e8\u635f\u52a0\u5267\u548c\u80fd\u91cf\u5229\u7528\u7387\u5dee\u3002\u6ed1\u51b0\u4f5c\u4e3a\u4e00\u79cd\u5177\u6709\u91cd\u8981\u751f\u7269\u529b\u5b66\u4ef7\u503c\u7684\u8fd0\u52a8\uff0c\u80fd\u591f\u901a\u8fc7\u5408\u7406\u5229\u7528\u8eab\u4f53\u60ef\u6027\u5b9e\u73b0\u5feb\u901f\u8fde\u7eed\u6ed1\u52a8\uff0c\u5177\u6709\u6700\u5c0f\u7684\u52a8\u80fd\u635f\u5931\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u6bcf\u53ea\u811a\u914d\u5907\u4e00\u6392\u56db\u4e2a\u88ab\u52a8\u8f6e\u7528\u4e8e\u6ed1\u51b0\u3002\u5f00\u53d1\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6846\u67b6\uff0c\u9488\u5bf9\u6ed1\u51b0\u6b65\u6001\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u8003\u8651\u4e86\u6ed1\u51b0\u7684\u5185\u5728\u7279\u6027\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5206\u6790\u5b66\u4e60\u7b56\u7565\u540e\uff0c\u90e8\u7f72\u5230\u7269\u7406\u673a\u5668\u4eba\u4e0a\uff0c\u5c55\u793a\u4e86\u6ed1\u51b0\u6b65\u6001\u76f8\u6bd4\u4f20\u7edf\u53cc\u8db3\u884c\u8d70\u5728\u51b2\u51fb\u5f3a\u5ea6\u548c\u8fd0\u8f93\u6210\u672c\u65b9\u9762\u7684\u5e73\u6ed1\u6027\u548c\u6548\u7387\u4f18\u52bf\u3002\u8fd9\u4e24\u4e2a\u6307\u6807\u5206\u522b\u964d\u4f4e\u4e8675.86%\u548c63.34%\u3002", "conclusion": "\u6ed1\u51b0\u4f5c\u4e3a\u4e00\u79cd\u4f18\u8d8a\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\u548c\u5ef6\u957f\u5173\u8282\u5bff\u547d\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u8010\u7528\u7684\u8fd0\u52a8\u65b9\u5f0f\u3002"}}
{"id": "2601.04982", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04982", "abs": "https://arxiv.org/abs/2601.04982", "authors": ["Johannes A. Gaus", "Winfried Ilg", "Daniel Haeufle"], "title": "When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics", "comment": null, "summary": "Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6821\u51c6\u6982\u7387\u7684\u5b89\u5168\u5173\u952e\u89e6\u53d1\u6846\u67b6\uff0c\u7528\u4e8e\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4e2d\u7684\u591a\u6a21\u6001\u4e0b\u4e00\u52a8\u4f5c\u9884\u6d4b\uff0c\u901a\u8fc7\u540e\u5904\u7406\u6821\u51c6\u4f7f\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0e\u7ecf\u9a8c\u53ef\u9760\u6027\u5bf9\u9f50\uff0c\u5e76\u8bbe\u8ba1ACT/HOLD\u89c4\u5219\u786e\u4fdd\u4ec5\u5728\u53ef\u9760\u6027\u9ad8\u65f6\u63d0\u4f9b\u8f85\u52a9", "motivation": "\u8f85\u52a9\u8bbe\u5907\u9700\u8981\u5728\u63d0\u4f9b\u652f\u6301\u524d\u786e\u5b9a\u7528\u6237\u7684\u610f\u56fe\u4ee5\u53ca\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002\u539f\u59cb\u6a21\u578b\u7f6e\u4fe1\u5ea6\u901a\u5e38\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u6b63\u786e\u6027\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u5b89\u5168\u5173\u952e\u7684\u89e6\u53d1\u673a\u5236", "method": "\u5f15\u5165\u57fa\u4e8e\u6821\u51c6\u6982\u7387\u7684\u5b89\u5168\u5173\u952e\u89e6\u53d1\u6846\u67b6\uff0c\u901a\u8fc7\u540e\u5904\u7406\u6821\u51c6\u5c06\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0e\u7ecf\u9a8c\u53ef\u9760\u6027\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u6821\u51c6\u8bef\u5dee\u3002\u4f7f\u7528\u6821\u51c6\u540e\u7684\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7b80\u5355\u7684ACT/HOLD\u89c4\u5219\uff0c\u4ec5\u5728\u53ef\u9760\u6027\u9ad8\u65f6\u91c7\u53d6\u884c\u52a8\uff0c\u5426\u5219\u4fdd\u6301\u7b49\u5f85", "result": "\u540e\u5904\u7406\u6821\u51c6\u5c06\u6821\u51c6\u8bef\u5dee\u964d\u4f4e\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002\u6821\u51c6\u540e\u7684\u7f6e\u4fe1\u5ea6\u5c06\u7f6e\u4fe1\u5ea6\u9608\u503c\u8f6c\u5316\u4e3a\u8f85\u52a9\u52a8\u4f5c\u7684\u5b9a\u91cf\u5b89\u5168\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u8f85\u52a9\u63a7\u5236\u56de\u8def\u4e2d\u7684\u53ef\u9a8c\u8bc1\u884c\u4e3a", "conclusion": "\u57fa\u4e8e\u6821\u51c6\u6982\u7387\u7684\u5b89\u5168\u5173\u952e\u89e6\u53d1\u6846\u67b6\u80fd\u591f\u53ef\u9760\u5730\u786e\u5b9a\u4f55\u65f6\u63d0\u4f9b\u8f85\u52a9\u652f\u6301\uff0c\u5c06\u7f6e\u4fe1\u5ea6\u9608\u503c\u8f6c\u5316\u4e3a\u5b89\u5168\u53c2\u6570\uff0c\u4e3a\u8f85\u52a9\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236"}}
{"id": "2601.05014", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05014", "abs": "https://arxiv.org/abs/2601.05014", "authors": ["Lingdong Kong", "Shaoyuan Xie", "Zeying Gong", "Ye Li", "Meng Chu", "Ao Liang", "Yuhao Dong", "Tianshuai Hu", "Ronghe Qiu", "Rong Li", "Hanjiang Hu", "Dongyue Lu", "Wei Yin", "Wenhao Ding", "Linfeng Li", "Hang Song", "Wenwei Zhang", "Yuexin Ma", "Junwei Liang", "Zhedong Zheng", "Lai Xing Ng", "Benoit R. Cottereau", "Wei Tsang Ooi", "Ziwei Liu", "Zhanpeng Zhang", "Weichao Qiu", "Wei Zhang", "Ji Ao", "Jiangpeng Zheng", "Siyu Wang", "Guang Yang", "Zihao Zhang", "Yu Zhong", "Enzhu Gao", "Xinhan Zheng", "Xueting Wang", "Shouming Li", "Yunkai Gao", "Siming Lan", "Mingfei Han", "Xing Hu", "Dusan Malic", "Christian Fruhwirth-Reisinger", "Alexander Prutsch", "Wei Lin", "Samuel Schulter", "Horst Possegger", "Linfeng Li", "Jian Zhao", "Zepeng Yang", "Yuhang Song", "Bojun Lin", "Tianle Zhang", "Yuchen Yuan", "Chi Zhang", "Xuelong Li", "Youngseok Kim", "Sihwan Hwang", "Hyeonjun Jeong", "Aodi Wu", "Xubo Luo", "Erjia Xiao", "Lingfeng Zhang", "Yingbo Tang", "Hao Cheng", "Renjing Xu", "Wenbo Ding", "Lei Zhou", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao", "Shuangzhi Li", "Junlong Shen", "Xingyu Li", "Hao Ruan", "Jinliang Lin", "Zhiming Luo", "Yu Zang", "Cheng Wang", "Hanshi Wang", "Xijie Gong", "Yixiang Yang", "Qianli Ma", "Zhipeng Zhang", "Wenxiang Shi", "Jingmeng Zhou", "Weijun Zeng", "Kexin Xu", "Yuchen Zhang", "Haoxiang Fu", "Ruibin Hu", "Yanbiao Ma", "Xiyan Feng", "Wenbo Zhang", "Lu Zhang", "Yunzhi Zhuge", "Huchuan Lu", "You He", "Seungjun Yu", "Junsung Park", "Youngsun Lim", "Hyunjung Shim", "Faduo Liang", "Zihang Wang", "Yiming Peng", "Guanyu Zong", "Xu Li", "Binghao Wang", "Hao Wei", "Yongxin Ma", "Yunke Shi", "Shuaipeng Liu", "Dong Kong", "Yongchun Lin", "Huitong Yang", "Liang Lei", "Haoang Li", "Xinliang Zhang", "Zhiyong Wang", "Xiaofeng Wang", "Yuxia Fu", "Yadan Luo", "Djamahl Etchegaray", "Yang Li", "Congfei Li", "Yuxiang Sun", "Wenkai Zhu", "Wang Xu", "Linru Li", "Longjie Liao", "Jun Yan", "Benwu Wang", "Xueliang Ren", "Xiaoyu Yue", "Jixian Zheng", "Jinfeng Wu", "Shurui Qin", "Wei Cong", "Yao He"], "title": "The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms", "comment": "Official IROS 2025 RoboSense Challenge Report; 51 pages, 37 figures, 5 tables; Competition Website at https://robosense2025.github.io/", "summary": "Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.", "AI": {"tldr": "RoboSense 2025\u6311\u6218\u8d5b\u65e8\u5728\u63a8\u8fdb\u673a\u5668\u4eba\u611f\u77e5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u5305\u542b\u4e94\u4e2a\u4e92\u8865\u7814\u7a76\u8d5b\u9053\uff0c\u5438\u5f15\u4e86\u5168\u7403143\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u901a\u8fc7\u5206\u679023\u4e2a\u83b7\u80dc\u65b9\u6848\u603b\u7ed3\u65b9\u6cd5\u8d8b\u52bf\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u5728\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u611f\u77e5\u6a21\u578b\u9700\u8981\u5728\u4f20\u611f\u5668\u566a\u58f0\u3001\u73af\u5883\u53d8\u5316\u548c\u5e73\u53f0\u5dee\u5f02\u4e0b\u4fdd\u6301\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u89c1\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u611f\u77e5\u6280\u672f\u3002", "method": "\u6311\u6218\u8d5b\u8bbe\u8ba1\u4e86\u4e94\u4e2a\u4e92\u8865\u7814\u7a76\u8d5b\u9053\uff1a\u8bed\u8a00\u57fa\u7840\u51b3\u7b56\u3001\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u3001\u4f20\u611f\u5668\u914d\u7f6e\u6cdb\u5316\u3001\u8de8\u89c6\u56fe\u8de8\u6a21\u6001\u5bf9\u5e94\u3001\u8de8\u5e73\u53f03D\u611f\u77e5\u3002\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u6a21\u578b\u548c\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u6bd4\u8f83\u3002", "result": "\u5438\u5f15\u4e86\u6765\u81ea16\u4e2a\u56fd\u5bb685\u4e2a\u673a\u6784\u7684143\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u53cd\u6620\u4e86\u5e7f\u6cdb\u7684\u793e\u533a\u53c2\u4e0e\u3002\u901a\u8fc7\u5206\u679023\u4e2a\u83b7\u80dc\u65b9\u6848\uff0c\u603b\u7ed3\u4e86\u65b0\u5174\u65b9\u6cd5\u8d8b\u52bf\u3001\u5171\u4eab\u8bbe\u8ba1\u539f\u5219\u548c\u5f00\u653e\u6311\u6218\u3002", "conclusion": "RoboSense 2025\u6311\u6218\u8d5b\u4e3a\u8bc4\u4f30\u771f\u5b9e\u4e16\u754c\u611f\u77e5\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u901a\u8fc7\u6574\u5408\u5404\u8d5b\u9053\u89c1\u89e3\uff0c\u671d\u7740\u6784\u5efa\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u53ef\u9760\u611f\u77e5\u3001\u9c81\u68d2\u884c\u52a8\u548c\u8de8\u5e73\u53f0\u9002\u5e94\u7684\u673a\u5668\u4eba\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2601.05074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05074", "abs": "https://arxiv.org/abs/2601.05074", "authors": ["Julian Kulozik", "Nathana\u00ebl Jarrass\u00e9"], "title": "Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses", "comment": null, "summary": "Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8eaf\u5e72\u5c48\u4f38\u7684\u8865\u507f\u6548\u5e94\u653e\u5927\u63a7\u5236(CEAC)\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0a\u80a2\u5047\u80a2\u8098\u5173\u8282\u7684\u8fde\u7eed\u901f\u5ea6\u8c03\u5236\u63a7\u5236\uff0c\u5728\u7ed8\u56fe\u548c\u4f38\u624b\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u81ea\u7136\u624b\u81c2\u8fd0\u52a8\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u80a2\u5047\u80a2\u8bbe\u8ba1\u6709\u8fdb\u6b65\uff0c\u4f46\u5b9e\u73b0\u4e2d\u95f4\u5173\u8282\uff08\u5982\u624b\u8155\u548c\u8098\u90e8\uff09\u7684\u76f4\u89c2\u63a7\u5236\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u548c\u901f\u5ea6\u8c03\u5236\u7684\u8fd0\u52a8\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u7528\u6237\u81ea\u7136\u8865\u507f\u8fd0\u52a8\u6765\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8865\u507f\u6548\u5e94\u653e\u5927\u63a7\u5236(CEAC)\u8303\u5f0f\uff0c\u5229\u7528\u8eaf\u5e72\u5c48\u4f38\u4f5c\u4e3a\u8f93\u5165\u6765\u63a7\u5236\u5047\u80a2\u8098\u5173\u8282\u901f\u5ea6\u3002\u8be5\u65b9\u6cd5\u653e\u5927\u8eaf\u5e72\u4e0e\u5047\u80a2\u4e4b\u95f4\u7684\u81ea\u7136\u8026\u5408\uff0c\u540c\u65f6\u5f15\u5165\u53d7\u63a7\u5ef6\u8fdf\uff0c\u5141\u8bb8\u7528\u6237\u540c\u65f6\u8c03\u5236\u5047\u80a2\u5173\u8282\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u3002", "result": "\u572812\u540d\u5065\u5168\u53c2\u4e0e\u8005\u4f7f\u7528\u5e26\u4e3b\u52a8\u8098\u5173\u8282\u7684\u8d85\u6570\u5047\u80a2\u8fdb\u884c\u7684\u7ed8\u56fe\u4efb\u52a1\u4e2d\uff0c\u4ee5\u53ca10\u540d\u53c2\u4e0e\u8005\u7684\u591a\u76ee\u6807\u4f38\u624b\u4efb\u52a1\u4e2d\uff0cCEAC\u8868\u73b0\u51fa\u4e0e\u81ea\u7136\u624b\u81c2\u8fd0\u52a8\u76f8\u5f53\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5373\u4f7f\u624b\u52bf\u901f\u5ea6\u6216\u7ed8\u56fe\u5c3a\u5bf8\u53d8\u5316\u65f6\u4e5f\u80fd\u4fdd\u6301\u7b26\u5408\u4eba\u4f53\u5de5\u5b66\u7684\u8eaf\u5e72\u59ff\u52bf\u3002", "conclusion": "CEAC\u6709\u6548\u6062\u590d\u4e86\u5173\u8282\u534f\u8c03\u52a8\u4f5c\uff0c\u5728\u8eaf\u5e72\u548c\u8098\u90e8\u4e4b\u95f4\u5206\u914d\u8fd0\u52a8\u52aa\u529b\uff0c\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u8f68\u8ff9\u63a7\u5236\u800c\u65e0\u9700\u6781\u7aef\u8865\u507f\u8fd0\u52a8\uff0c\u4e3a\u9700\u8981\u8fde\u7eed\u7cbe\u786e\u534f\u8c03\u7684\u4efb\u52a1\u4e2d\u7684\u4e0a\u80a2\u5047\u80a2\u4e2d\u95f4\u5173\u8282\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u7b56\u7565\u3002"}}
{"id": "2601.05248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05248", "abs": "https://arxiv.org/abs/2601.05248", "authors": ["Zhuoyang Liu", "Jiaming Liu", "Hao Chen", "Ziyu Guo", "Chengkai Hou", "Chenyang Gu", "Jiale Yu", "Xiangju Mi", "Renrui Zhang", "Zhengping Che", "Jian Tang", "Pheng-Ann Heng", "Shanghang Zhang"], "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0", "AI": {"tldr": "LaST\u2080\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6f5c\u5728\u65f6\u7a7a\u601d\u7ef4\u94fe\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u65b9\u6cd5\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548c\u8bed\u8a00\u8868\u793a\u74f6\u9888\u7684\u95ee\u9898\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u5e76\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709VLA\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u751f\u6210\u8bed\u8a00\u63a8\u7406\u8f68\u8ff9\u6216\u672a\u6765\u89c6\u89c9\u89c2\u5bdf\u6765\u63d0\u5347\u52a8\u4f5c\u51c6\u786e\u6027\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u663e\u5f0f\u63a8\u7406\u5bfc\u81f4\u4e0d\u53ef\u5ffd\u89c6\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u6240\u9700\u7684\u65f6\u95f4\u5206\u8fa8\u7387\uff1b2\uff09\u63a8\u7406\u5c40\u9650\u4e8e\u8bed\u8a00\u7a7a\u95f4\uff0c\u5f62\u6210\u8868\u793a\u74f6\u9888\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u96be\u4ee5\u8a00\u8868\u7684\u7269\u7406\u5c5e\u6027\u3002", "method": "\u63d0\u51faLaST\u2080\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u65f6\u7a7a\u601d\u7ef4\u94fe\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002\u5f15\u5165token\u9ad8\u6548\u7684\u6f5c\u5728CoT\u7a7a\u95f4\uff0c\u5efa\u6a21\u672a\u6765\u89c6\u89c9\u52a8\u6001\u30013D\u7ed3\u6784\u4fe1\u606f\u548c\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\u72b6\u6001\uff0c\u5e76\u8de8\u65f6\u95f4\u6269\u5c55\u8fd9\u4e9b\u8868\u793a\u4ee5\u652f\u6301\u65f6\u95f4\u4e00\u81f4\u7684\u9690\u5f0f\u63a8\u7406\u8f68\u8ff9\u3002\u91c7\u7528\u6df7\u5408Transformer\u7684\u53cc\u7cfb\u7edf\u67b6\u6784\uff1a\u63a8\u7406\u4e13\u5bb6\u8fdb\u884c\u4f4e\u9891\u6f5c\u5728\u63a8\u7406\uff0c\u52a8\u4f5c\u4e13\u5bb6\u57fa\u4e8e\u673a\u5668\u4eba\u5bfc\u5411\u7684\u6f5c\u5728\u8868\u793a\u751f\u6210\u9ad8\u9891\u52a8\u4f5c\u3002\u901a\u8fc7\u5f02\u6784\u64cd\u4f5c\u9891\u7387\u8bad\u7ec3\u5b9e\u73b0\u63a8\u7406\u548c\u52a8\u4f5c\u63a8\u65ad\u7387\u4e4b\u95f4\u7684\u81ea\u9002\u5e94\u5207\u6362\u3002", "result": "\u572810\u4e2a\u6a21\u62df\u4efb\u52a1\u548c6\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cLaST\u2080\u76f8\u6bd4\u5148\u524dVLA\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u4e868%\u548c13%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "LaST\u2080\u901a\u8fc7\u6f5c\u5728\u65f6\u7a7a\u601d\u7ef4\u94fe\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u8868\u793a\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
