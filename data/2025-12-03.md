<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 14]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Reinforcement Learning for Robotic Safe Control with Force Sensing](https://arxiv.org/abs/2512.02022)
*Nan Lin,Linrui Zhang,Yuxuan Chen,Zhenrui Chen,Yujun Zhu,Ruoxi Chen,Peichen Wu,Xiaoping Chen*

Main category: cs.RO

TL;DR: 论文提出将力与触觉感知融入强化学习，以提升机器人在非结构化环境中复杂操作任务的安全性和可靠性，特别是在仿真到现实的迁移中。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中执行复杂操作任务时，传统手工编码方法效果有限，而强化学习虽然能提供更通用的策略，但其稳定性和可靠性难以保证，存在安全隐患，且仿真到现实的迁移也会带来不可预测的情况。

Method: 将力和触觉感知引入强化学习框架，利用力与触觉感知在机器人动态控制和人机交互中的关键作用，开发基于力的强化学习方法。

Result: 实验结果表明，在物体推动任务中，该策略在仿真和现实世界中都更安全、更高效，特别是在仿真到现实的迁移中表现出更好的适应性。

Conclusion: 基于力的强化学习方法能提升机器人的安全性和可靠性，在仿真到现实迁移中更具适应性，具有广泛的机器人应用前景。

Abstract: For the task with complicated manipulation in unstructured environments, traditional hand-coded methods are ineffective, while reinforcement learning can provide more general and useful policy. Although the reinforcement learning is able to obtain impressive results, its stability and reliability is hard to guarantee, which would cause the potential safety threats. Besides, the transfer from simulation to real world also will lead in unpredictable situations. To enhance the safety and reliability of robots, we introduce the force and haptic perception into reinforcement learning. Force and tactual sensation play key roles in robotic dynamic control and human-robot interaction. We demonstrate that the force-based reinforcement learning method can be more adaptive to environment, especially in sim-to-real transfer. Experimental results show in object pushing task, our strategy is safer and more efficient in both simulation and real world, thus it holds prospects for a wide variety of robotic applications.

</details>


### [2] [VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM](https://arxiv.org/abs/2512.02293)
*Zihan Zhu,Wei Zhang,Norbert Haala,Marc Pollefeys,Daniel Barath*

Main category: cs.RO

TL;DR: VIGS-SLAM是一个视觉-惯性3D高斯泼溅SLAM系统，通过紧密耦合视觉和惯性信息，在运动模糊、低纹理和曝光变化等挑战性场景下实现鲁棒的实时跟踪和高保真重建。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-based SLAM方法虽然能实现密集和逼真的建图，但纯视觉设计在运动模糊、低纹理和曝光变化等情况下性能下降。需要引入惯性测量单元（IMU）来增强系统的鲁棒性。

Method: 提出视觉-惯性3D高斯泼溅SLAM系统，在统一优化框架中紧密耦合视觉和惯性信息，联合优化相机位姿、深度和IMU状态。系统包含鲁棒的IMU初始化、时变偏差建模以及具有一致高斯更新的闭环检测。

Result: 在四个具有挑战性的数据集上进行实验，结果表明VIGS-SLAM在鲁棒性和重建质量方面优于现有的最先进方法。

Conclusion: VIGS-SLAM通过视觉-惯性融合有效解决了纯视觉SLAM在挑战性场景下的局限性，实现了鲁棒的实时跟踪和高保真重建，为3DGS-based SLAM系统提供了更强大的解决方案。

Abstract: We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io

</details>


### [3] [Vehicle Dynamics Embedded World Models for Autonomous Driving](https://arxiv.org/abs/2512.02417)
*Huiqian Li,Wei Pan,Haodong Zhang,Jin Huang,Zhihua Zhong*

Main category: cs.RO

TL;DR: 本文提出VDD方法，将自车动力学与环境动力学解耦建模，提升自动驾驶世界模型对不同车辆参数的泛化能力，并通过PAD和PAT策略增强策略鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型通常将自车动力学与环境动力学联合学习，导致效率低下且对车辆动力学变化缺乏鲁棒性。需要一种能够有效泛化到不同车辆参数的方法。

Method: 提出VDD方法，将自车动力学与环境动力学解耦建模。引入两种增强策略：部署时的策略调整(PAD)和训练时的策略增强(PAT)。

Result: 在仿真环境中的综合实验表明，该方法显著提升了驾驶性能和车辆动力学变化的鲁棒性，优于现有方法。

Conclusion: 通过解耦建模自车动力学与环境动力学，VDD方法能够有效泛化到不同车辆参数，结合PAD和PAT策略进一步增强了策略鲁棒性，为自动驾驶世界模型提供了更有效的解决方案。

Abstract: World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.

</details>


### [4] [AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning](https://arxiv.org/abs/2512.02535)
*Jeric Lew,Yuhong Cao,Derek Ming Siang Tan,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: AID：基于扩散模型的去中心化多智能体信息路径规划框架，通过非自回归方式生成长时轨迹，相比现有方法实现4倍加速和17%信息增益提升


<details>
  <summary>Details</summary>
Motivation: 大规模或时间关键场景（如环境监测、搜救）需要多智能体系统在有限时间内实现广泛覆盖。现有基于学习的MAIPP方法使用自回归意图预测器，但存在计算成本高和误差累积问题

Method: 提出AID框架：1）对现有MAIPP规划器生成的轨迹进行行为克隆；2）通过扩散策略策略优化（DPPO）使用强化学习微调策略。利用扩散模型以非自回归方式生成长时轨迹

Result: AID在训练来源的MAIPP规划器基础上持续改进，实现高达4倍执行加速和17%信息增益提升，并能有效扩展到更多智能体

Conclusion: 扩散模型作为表达力强、长时程策略的有效性，可用于构建完全去中心化的MAIPP框架，在计算效率和协调性能方面显著优于现有方法

Abstract: Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as "intent" to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.

</details>


### [5] [Robotic capabilities framework: A boundary object and intermediate-level knowledge artifact for co-designing robotic processes](https://arxiv.org/abs/2512.02549)
*Alessandro Ianniello,Dave Murray-Rust,Sara Muscolo,Olger Siebinga,Nicky Mol,Denis Zatyagov,Eva Verhoef,Deborah Forster,David Abbink*

Main category: cs.RO

TL;DR: 本文提出了一个机器人能力框架，作为跨学科协作的词汇表，帮助设计人类与机器人协作的未来工作方式，重点关注任务分配而非机器人内部技术细节。


<details>
  <summary>Details</summary>
Motivation: 随着机器人变得更加适应性强、响应迅速且能够与人类互动，有效的人机协作设计变得至关重要。然而，当前设计过程通常采用单学科方法，往往忽视了跨学科知识和最终与这些系统共享任务的工人的经验知识。

Method: 开发了机器人能力框架作为跨学科协作的词汇表，通过反思性和迭代过程构建。该框架应用于两个不同场景：让机器人专家使用其词汇描述现有商业机器人，以及与学生进行机器人相关项目的设计活动。

Result: 该框架作为中间层次的知识产物和边界对象出现，能够连接技术和经验领域，指导设计者，赋能工人，并促进更公正和协作的未来工作方式。

Conclusion: 机器人能力框架为跨学科协作提供了有效工具，通过关注高级能力而非机器人内部工作原理，支持围绕任务分配（哪些应由人类主导，哪些可委托给机器人）的对话，有助于塑造更公正和协作的人机协作未来。

Abstract: As robots become more adaptable, responsive, and capable of interacting with humans, the design of effective human-robot collaboration becomes critical. Yet, this design process is typically led by monodisciplinary approaches, often overlooking interdisciplinary knowledge and the experiential knowledge of workers who will ultimately share tasks with these systems. To address this gap, we introduce the robotic capabilities framework, a vocabulary that enables transdisciplinary collaborations to meaningfully shape the future of work when robotic systems are integrated into the workplace. Rather than focusing on the internal workings of robots, the framework centers discussion on high-level capabilities, supporting dialogue around which elements of a task should remain human-led and which can be delegated to robots. We developed the framework through reflexive and iterative processes, and applied it in two distinct settings: by engaging roboticists in describing existing commercial robots using its vocabulary, and through a design activity with students working on robotics-related projects. The framework emerges as an intermediate-level knowledge artifact and a boundary object that bridges technical and experiential domains, guiding designers, empowering workers, and contributing to more just and collaborative futures of work.

</details>


### [6] [SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction](https://arxiv.org/abs/2512.02609)
*Shengkai Wu,Jinrong Yang,Wenqiu Luo,Linfeng Gao,Chaohui Shang,Meiyu Zhi,Mingshan Sun,Fangping Yang,Liangliang Ren,Yong Zhao*

Main category: cs.RO

TL;DR: SAM2Grasp利用SAM2的视觉时序跟踪能力，通过提示条件化解决多目标抓取中的多模态问题，仅训练轻量级动作头实现高性能抓取


<details>
  <summary>Details</summary>
Motivation: 机器人抓取模仿学习面临多模态问题：当场景中存在多个有效目标时，对不同物体的演示会产生冲突的训练信号，标准模仿学习策略会将这些不同动作平均成一个无效动作

Method: 提出SAM2Grasp框架，利用冻结的SAM2模型提供强大的视觉时序跟踪能力，引入轻量级可训练动作头与原生分割头并行工作。仅在小动作头上使用SAM2预计算的时序视觉特征进行训练。推理时通过初始提示（如边界框）指定要抓取的具体物体，动作头为该物体预测唯一明确的抓取轨迹，SAM2的时序跟踪能力自动维持对选定物体的稳定跟踪

Result: SAM2Grasp在杂乱多目标抓取任务中实现了最先进的性能，有效消除了视觉运动策略中的歧义

Conclusion: 通过将多目标抓取重新表述为单模态提示条件化预测问题，SAM2Grasp成功解决了模仿学习中的多模态冲突问题，利用预训练视觉模型的时序跟踪能力实现了高效准确的机器人抓取

Abstract: Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.

</details>


### [7] [RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning](https://arxiv.org/abs/2512.02729)
*Yuhong Zhang,Zihan Gao,Shengpeng Li,Ling-Hao Chen,Kaisheng Liu,Runqing Cheng,Xiao Lin,Junjia Liu,Zhuoheng Li,Jingyi Feng,Ziyan He,Jintian Lin,Zheyan Huang,Zhifang Liu,Haoqian Wang*

Main category: cs.RO

TL;DR: Robowheel是一个数据引擎，可将人类手-物交互视频转换为跨形态机器人学习的训练监督数据，通过重建、物理约束优化和跨具身重定向，生成可执行的机器人动作轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统机器人学习依赖遥操作数据收集，成本高且难以扩展。本文旨在利用丰富的人类手-物交互视频作为监督信号，为跨形态机器人学习提供轻量级、可扩展的数据生成方案。

Method: 1. 从单目RGB或RGB-D输入进行高精度手-物交互重建；2. 使用强化学习优化器在接触和穿透约束下细化相对位姿；3. 将重建的接触丰富轨迹重定向到不同具身（机械臂、灵巧手、人形机器人）；4. 在Isaac Sim中构建模拟增强框架，通过领域随机化丰富数据分布。

Result: 验证表明，该管道生成的轨迹与遥操作数据同样稳定，并能带来持续的性能提升。首次为"手-物交互模态可作为机器人学习有效监督"提供了定量证据。相比遥操作，Robowheel更轻量，仅需单目相机即可提取通用的、具身无关的运动表示。

Conclusion: Robowheel建立了从视频到机器人训练数据的端到端管道，证明了人类手-物交互视频作为机器人学习监督的有效性，为跨形态机器人学习提供了可扩展的数据生成方案，并构建了大规模多模态数据集。

Abstract: We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.

</details>


### [8] [CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy](https://arxiv.org/abs/2512.02777)
*Heye Huang,Yibin Yang,Mingfeng Fan,Haoran Wang,Xiaocong Zhao,Jianqiang Wang*

Main category: cs.RO

TL;DR: CogDrive提出了一种认知驱动的多模态预测与规划框架，通过显式模态推理与安全感知轨迹优化相结合，解决自动驾驶在复杂交通中的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法难以捕捉罕见但安全关键的行为，而基于规则的系统在复杂交互中缺乏适应性，需要一种能统一理解多模态交互并在不确定性下进行动态规划的解决方案。

Method: 采用认知驱动的多模态预测与规划框架：预测模块基于拓扑运动语义和最近邻关系编码建立交互模式的认知表示，使用可微分模态损失和多模态高斯解码；规划模块引入应急响应概念，优化安全稳定轨迹，包含短期一致分支和长期分支。

Result: 在Argoverse2和INTERACTION数据集上，CogDrive在轨迹精度和漏检率方面表现优异；闭环仿真验证了在汇入和交叉口场景中的自适应行为。

Conclusion: 通过结合认知多模态预测与安全导向规划，CogDrive为复杂交通中的安全自主驾驶提供了一个可解释且可靠的范式。

Abstract: Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.

</details>


### [9] [Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols](https://arxiv.org/abs/2512.02787)
*Xianchao Zeng,Xinyu Zhou,Youcheng Li,Jiayou Shi,Tianle Li,Liangming Chen,Lei Ren,Yong-Lu Li*

Main category: cs.RO

TL;DR: ViFailback是一个用于机器人操作失败诊断和纠正的视觉-语言-动作框架，包含大规模真实世界数据集和基准测试，通过视觉符号增强标注效率，并开发了ViFailback-8B模型来提升失败恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在机器人操作中取得了显著进展，但在失败诊断和从失败中学习方面仍然有限。现有的失败数据集大多是在模拟环境中程序化生成的，限制了其在真实世界中的泛化能力。

Method: 提出了ViFailback框架，利用显式视觉符号增强标注效率；发布了包含58,126个视觉问答对和5,202个真实世界操作轨迹的大规模数据集；建立了包含11个细粒度VQA任务的ViFailback-Bench基准；开发了ViFailback-8B视觉-语言模型；将该模型与VLA模型集成进行真实世界机器人实验。

Result: ViFailback-8B在ViFailback-Bench上实现了显著的性能提升，能够生成用于纠正动作指导的视觉符号。通过将ViFailback-8B与VLA模型集成，在真实世界机器人实验中展示了其帮助VLA模型从失败中恢复的能力。

Conclusion: ViFailback框架通过提供大规模真实世界失败数据集、基准测试和专用模型，有效解决了机器人操作中的失败诊断和纠正问题，为视觉-语言-动作模型在真实世界环境中的失败恢复能力提供了系统性的解决方案。

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/

</details>


### [10] [Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach](https://arxiv.org/abs/2512.02834)
*Siyuan Yang,Yang Zhang,Haoran He,Ling Pan,Xiu Li,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出TACO框架，通过测试时缩放和轻量级伪计数估计器解决VLA模型在下游任务微调时的推理不稳定性问题，提高动作执行的成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在预训练阶段整合了多种数据模式，而微调数据集往往包含运动学上不理想或冗余的动作模式，导致推理时出现不稳定性。作者观察到预训练VLA模型在监督微调后，在不同采样噪声下存在关键的推理脆弱性。

Method: 提出TACO框架，采用测试时缩放技术，使用轻量级伪计数估计器作为动作块的高保真验证器。VLA模型集成TACO后，可以从所有采样的动作块中选择具有最大伪计数的动作执行，从而防止分布偏移，同时保持VLA的泛化能力。

Result: 在四个仿真基准（RoboTwin2.0、Robotwin、LIBERO、SimplerEnv）和双臂平台上进行的广泛实验表明，该方法显著提高了下游任务适应中的推理稳定性和成功率。

Conclusion: TACO框架有效解决了VLA模型在下游任务微调时的推理不稳定性问题，通过测试时约束防止分布偏移，同时保持模型的泛化能力，相比强化学习更新具有显著的计算优势。

Abstract: Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.

</details>


### [11] [VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion](https://arxiv.org/abs/2512.02844)
*Xinzheng Wu,Junyi Chen,Naiting Zhong,Yong Shen*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉语言模型和自适应引导扩散模型的安全关键测试场景生成框架，用于高效生成自动驾驶系统测试所需的长尾、高保真、交互式危险场景。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的安全部署需要全面测试，但现实世界中能有效暴露系统漏洞的安全关键场景极其稀少。现有场景生成方法难以高效构建同时保证保真度、关键性和交互性的长尾场景，尤其缺乏对被测车辆的实时动态响应能力。

Method: 提出三层分层架构：战略层（VLM确定场景生成目标）、战术层（制定引导函数）、操作层（执行引导扩散）。首先建立学习真实驾驶场景数据分布的基础扩散模型，然后设计自适应引导扩散方法实现闭环仿真中背景车辆的实时精确控制，最后集成VLM通过深度场景理解和风险推理自主生成场景目标和引导函数。

Result: 实验结果表明，该方法能高效生成真实、多样且高度交互的安全关键测试场景。案例研究验证了方法的适应性和VLM引导生成性能。

Conclusion: 该框架成功整合了VLM的高层语义理解能力和自适应引导扩散模型的细粒度生成能力，解决了现有方法在生成保真、关键、交互式长尾场景方面的挑战，为自动驾驶系统安全测试提供了有效解决方案。

Abstract: The safe deployment of autonomous driving systems (ADSs) relies on comprehensive testing and evaluation. However, safety-critical scenarios that can effectively expose system vulnerabilities are extremely sparse in the real world. Existing scenario generation methods face challenges in efficiently constructing long-tail scenarios that ensure fidelity, criticality, and interactivity, while particularly lacking real-time dynamic response capabilities to the vehicle under test (VUT). To address these challenges, this paper proposes a safety-critical testing scenario generation framework that integrates the high-level semantic understanding capabilities of Vision Language Models (VLMs) with the fine-grained generation capabilities of adaptive guided diffusion models. The framework establishes a three-layer hierarchical architecture comprising a strategic layer for VLM-directed scenario generation objective determination, a tactical layer for guidance function formulation, and an operational layer for guided diffusion execution. We first establish a high-quality fundamental diffusion model that learns the data distribution of real driving scenarios. Next, we design an adaptive guided diffusion method that enables real-time, precise control of background vehicles (BVs) in closed-loop simulation. The VLM is then incorporated to autonomously generate scenario generation objectives and guidance functions through deep scenario understanding and risk reasoning, ultimately guiding the diffusion model to achieve VLM-directed scenario generation. Experimental results demonstrate that the proposed method can efficiently generate realistic, diverse, and highly interactive safety-critical testing scenarios. Furthermore, case studies validate the adaptability and VLM-directed generation performance of the proposed method.

</details>


### [12] [SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots](https://arxiv.org/abs/2512.02851)
*Iana Zhura,Sausar Karaf,Faryal Batool,Nipun Dhananjaya Weerakkodi Mudalige,Valerii Serpiva,Ali Alridha Abdulkarim,Aleksey Fedoseev,Didar Seyidov,Amjad Hajira,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: SwarmDiffusion：一个轻量级端到端扩散模型，从单张RGB图像联合预测可通行性并生成可行轨迹，无需手动提示工程或外部规划器。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的可通行性估计方法依赖手工提示，跨机器人平台泛化能力差，且仅输出可通行性地图，需要外部规划器生成轨迹，速度慢。

Method: 提出无规划器的轨迹构建流程：随机化航点采样、贝塞尔曲线平滑，以及强制连通性、安全性、方向性和路径细度的正则化。利用VLM监督无需提示工程，在扩散过程中加入紧凑的机器人状态条件。

Result: 在室内环境和两种机器人平台（四足和空中）上实现80-100%导航成功率，0.09秒推理时间，仅需500个额外视觉样本即可适应新机器人。在仿真和真实世界试验中可靠泛化到未见环境。

Conclusion: SwarmDiffusion提供了一种可扩展、无需提示的统一可通行性推理和轨迹生成方法，能够跨机器人平台转移，学习稳定运动先验而无需演示。

Abstract: Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100\% navigation success and 0.09 s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.

</details>


### [13] [VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling](https://arxiv.org/abs/2512.02902)
*Weiqi Li,Quande Zhang,Ruifeng Zhai,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: VLA模型在分布内表现良好但面对新视角和视觉扰动时性能急剧下降。研究发现问题主要源于空间建模而非物理建模的对齐问题，提出了一种一次性适应框架，通过轻量级可学习更新重新校准视觉表示。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在分布内表现出色，但在面对新相机视角和视觉扰动时性能急剧下降。研究发现这种脆弱性主要源于空间建模的对齐问题，而非物理建模问题，因此需要解决视觉表示的空间对齐问题。

Method: 提出了一个一次性适应框架，通过轻量级可学习更新重新校准视觉表示。具体包括两种方法：1) 特征令牌调制：对视觉令牌应用全局仿射变换；2) 特征线性适应：在ViT编码器中引入低秩更新。

Result: FTM方法仅用4K参数就将Libero视角准确率从48.5%提升到87.1%。FLA方法用4.7M参数达到90.8%的成功率，与LoRA规模微调相当但成本更低。这些结果表明预训练VLA模型具有大量未开发的鲁棒性潜力。

Conclusion: 研究表明，预训练VLA模型具有显著的未开发鲁棒性，通过有针对性的最小视觉适应就足以恢复视角泛化能力。轻量级视觉适应方法能有效解决空间建模对齐问题，提升模型在新视角下的性能。

Abstract: Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.

</details>


### [14] [Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger](https://arxiv.org/abs/2512.02951)
*Nicholas Baiata,Nilanjan Chakraborty*

Main category: cs.RO

TL;DR: 本文提出了一种三自由度连杆驱动串并联机器人手指，实现了任务空间轨迹的毫米级精度跟踪控制。


<details>
  <summary>Details</summary>
Motivation: 任务空间控制对于灵巧操作至关重要，但现有研究主要集中在大型机械臂上，对于紧凑型多自由度机器人手指的精确任务空间轨迹跟踪研究较少。

Method: 设计了具有解析正向运动学和闭式雅可比矩阵的三自由度连杆驱动串并联机器人手指原型，并实现了基于解析运动速率控制（RMRC）的闭环任务空间轨迹跟踪方案。

Result: 实验评估显示，该手指在直线、圆形和复杂曲线等多种轨迹上都能实现毫米级的指尖跟踪精度。

Conclusion: 这项工作为连杆驱动机器人手指的精确任务空间轨迹跟踪提供了首个系统性实验验证，为未来灵巧手内操作设计建立了基准。

Abstract: Task-space control of robotic fingers is a critical enabler of dexterous manipulation, as manipulation objectives are most naturally specified in terms of fingertip motions and applied forces rather than individual joint angles. While task-space planning and control have been extensively studied for larger, arm-scale manipulators, demonstrations of precise task-space trajectory tracking in compact, multi-DoF robotic fingers remain scarce. In this paper, we present the physical prototyping and experimental characterization of a three-degree-of-freedom, linkage-driven, series-parallel robotic finger with analytic forward kinematics and a closed-form Jacobian. A resolved motion rate control (RMRC) scheme is implemented to achieve closed-loop task-space trajectory tracking. We experimentally evaluate the fingertip tracking performance across a variety of trajectories, including straight lines, circles, and more complex curves, and report millimeter-level accuracy. To the best of our knowledge, this work provides one of the first systematic experimental demonstrations of precise task-space trajectory tracking in a linkage-driven robotic finger, thereby establishing a benchmark for future designs aimed at dexterous in-hand manipulation.

</details>
