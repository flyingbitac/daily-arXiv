<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration](https://arxiv.org/abs/2512.23972)
*Liangtao Feng,Zhenchang Liu,Feng Zhang,Xuefeng Ren*

Main category: cs.RO

TL;DR: SHIELD是一种基于激光雷达的无人机自主探索方法，通过球面投影混合前沿集成技术，解决了激光雷达点云质量不一致、传统前沿方法计算负担重以及开放区域飞行安全等问题。


<details>
  <summary>Details</summary>
Motivation: 激光雷达在无人机探索中面临几个关键挑战：1）激光雷达点云观测质量通常低于深度相机；2）基于已知/未知区域的传统前沿方法计算负担重，特别是处理激光雷达宽视场时；3）无点云区域难以通过光线投射分类为自由空间。这些问题限制了激光雷达在无人机自主探索中的效率和安全性。

Method: SHIELD采用三种核心技术：1）维护观测质量占据地图，在该地图上进行光线投射以解决探索中点云质量不一致问题；2）使用混合前沿方法处理计算负担和点云质量限制；3）提出向外球面投影光线投射策略，在开放区域共同确保飞行安全和探索效率。

Result: 通过仿真和飞行实验证明了SHIELD的有效性。该方法能够有效处理激光雷达点云质量问题，降低计算负担，同时在开放区域确保飞行安全并保持探索效率。

Conclusion: SHIELD成功解决了激光雷达在无人机探索中的关键挑战，提供了一种高效、安全的自主探索方案。作者承诺将开源该工作以促进研究社区发展。

Abstract: This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community.

</details>


### [2] [Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training](https://arxiv.org/abs/2512.24125)
*Yi Liu,Sukai Wang,Dafeng Wei,Xiaowei Cai,Linqing Zhong,Jiange Yang,Guanghui Ren,Jinyu Zhang,Maoqing Yao,Chuankang Li,Xindong He,Liliang Chen,Jianlan Luo*

Main category: cs.RO

TL;DR: 提出ERIQ基准和FACT方法来解决VLA模型中泛化与精确控制的权衡问题，通过解耦推理与执行评估，并开发流匹配动作分词器提升轨迹重建精度。


<details>
  <summary>Details</summary>
Motivation: 通用机器人系统在开放世界环境中需要同时实现广泛泛化和高精度动作执行，但现有VLA模型难以平衡这两者。视觉语言模型虽然提升了语义泛化能力，但缺乏具身推理导致行为脆弱；而强推理能力本身又不足以实现精确控制。

Method: 1. 提出ERIQ基准：包含6K+问答对，覆盖四个推理维度，通过解耦推理与执行来系统评估具身推理能力；2. 提出FACT方法：基于流匹配的动作分词器，将连续控制转换为离散序列，同时保持高保真轨迹重建；3. 开发GenieReasoner：在统一空间中联合优化推理和动作。

Result: ERIQ基准揭示了具身推理能力与端到端VLA泛化之间的强正相关性。FACT方法在轨迹重建精度上优于现有方法。GenieReasoner在真实世界任务中超越了连续动作和先前离散动作基线模型。

Conclusion: ERIQ和FACT提供了一个原则性框架来诊断和克服推理-精度权衡问题，推进了鲁棒、通用机器人操纵的发展。该方法通过解耦评估和联合优化，为平衡泛化与精确控制提供了系统解决方案。

Abstract: General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation.

</details>


### [3] [ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving](https://arxiv.org/abs/2512.24129)
*Manuel Bied,John Arockiasamy,Andy Comeca,Maximilian Schrapel,Victoria Yang,Alexey Rolich,Barbara Bruno,Maike Schwammberger,Dieter Fiems,Alexey Vinel*

Main category: cs.RO

TL;DR: 提出使用社交机器人作为自动驾驶车辆与弱势道路使用者之间的协调者，通过四个关键技术实现：先进感知、车辆通信、社交人机交互和形式化规范。


<details>
  <summary>Details</summary>
Motivation: 在实现完全自动驾驶的过程中，自动驾驶车辆与人类参与者共享道路的混合交通不可避免。即使所有车辆都实现自动化，行人仍需过马路，因此需要协调机制确保交通安全。

Method: 提出社交机器人作为协调者，整合四个关键技术：1) 先进感知技术让机器人感知环境；2) 车辆通信技术让联网车辆共享意图，机器人发送引导指令；3) 社交人机交互技术让机器人有效与弱势道路使用者和驾驶员沟通；4) 形式化规范技术让机器人理解交通规则并进行规划。

Result: 论文概述了这四个关键技术，并报告了前三个技术的初步概念验证集成，设想社交机器人在与协作式自动电动自行车场景中为行人提供建议。

Conclusion: 社交机器人有潜力成为自动驾驶车辆与弱势道路使用者之间的有效协调者，通过整合感知、通信、交互和规范技术，可以提高混合交通环境下的安全性和效率。

Abstract: On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike.

</details>


### [4] [GR-Dexter Technical Report](https://arxiv.org/abs/2512.24210)
*Ruoshi Wen,Guangzeng Chen,Zhongren Cui,Min Du,Yang Gou,Zhigang Han,Liqun Huang,Mingyu Lei,Yunfei Li,Zhuohang Li,Wenlei Liu,Yuxiao Liu,Xiao Ma,Hao Niu,Yutao Ouyang,Zeyu Ren,Haixin Shi,Wei Xu,Haoxiang Zhang,Jiajun Zhang,Xiao Zhang,Liwei Zheng,Weiheng Zhong,Yifei Zhou,Zhengming Zhu,Hang Li*

Main category: cs.RO

TL;DR: GR-Dexter是一个面向双手机器人灵巧手操作的全栈硬件-模型-数据框架，通过紧凑的21自由度灵巧手设计、直观的双手机动系统和大规模视觉-语言-动作模型训练，实现了长时程日常操作任务


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型主要局限于夹爪机器人，扩展到具有高自由度的双手机器人灵巧手面临动作空间扩大、手-物体遮挡严重、真实机器人数据收集成本高等挑战

Method: 1) 设计紧凑的21自由度机器人灵巧手；2) 开发直观的双手机动系统用于真实机器人数据收集；3) 提出训练方法，结合机动轨迹、大规模视觉-语言数据和精心策划的跨具身数据集

Result: 在真实世界评估中，GR-Dexter在长时程日常操作和泛化拾放任务上表现出强大的领域内性能，对未见过的物体和指令具有更好的鲁棒性

Conclusion: GR-Dexter为实现通用灵巧手机器人操作提供了实用步骤，有望推动该领域的发展

Abstract: Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.

</details>


### [5] [RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation](https://arxiv.org/abs/2512.24212)
*Ming-Ming Yu,Yi Chen,Börje F. Karlsson,Wenjun Wu*

Main category: cs.RO

TL;DR: RANGER是一个零样本、开放词汇的语义导航框架，仅使用单目相机，无需深度和姿态信息，具有强大的上下文学习能力，通过观察新环境的短视频即可提升任务效率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标导航方法存在两个关键限制：1）严重依赖模拟器提供的精确深度和姿态信息，限制了在真实场景中的应用；2）缺乏上下文学习能力，难以快速适应新环境。

Method: 提出RANGER框架，基于强大的3D基础模型，包含关键帧3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应路径点选择和低层动作执行等组件。

Result: 在HM3D基准测试和真实环境中，RANGER在导航成功率和探索效率方面达到竞争性性能，展现出优越的上下文学习适应性，且无需环境的先验3D地图。

Conclusion: RANGER成功解决了现有方法的局限性，实现了仅使用单目相机的零样本语义导航，具有强大的环境适应能力，为真实世界机器人应用提供了实用解决方案。

Abstract: Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.

</details>


### [6] [Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning for Accurate and Robust UAV Trajectory Tracking](https://arxiv.org/abs/2512.24249)
*Fuqiang Gu,Jiangshan Ai,Xu Lu,Xianlei Long,Yan Li,Tao Jiang,Chao Chen,Huidong Liu*

Main category: cs.RO

TL;DR: 提出HBO-PID控制算法，结合异方差贝叶斯优化与经典PID控制器，提升四旋翼无人机轨迹跟踪精度和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统控制算法在四旋翼无人机轨迹跟踪中性能有限，因为四旋翼系统具有欠驱动、非线性、强耦合的动态特性，需要更精确和鲁棒的控制方法

Method: 提出HBO-PID算法，将异方差贝叶斯优化框架与经典PID控制器结合，通过显式建模输入依赖的噪声方差来适应动态复杂环境，采用两阶段优化策略加速收敛

Result: 在仿真和真实场景实验中，相比最先进方法，位置精度提升24.7%至42.9%，角度精度提升40.9%至78.4%

Conclusion: HBO-PID算法能显著提高四旋翼无人机轨迹跟踪的精度和鲁棒性，为解决欠驱动非线性系统的控制问题提供了有效方案

Abstract: Unmanned Aerial Vehicles (UAVs) play an important role in various applications, where precise trajectory tracking is crucial. However, conventional control algorithms for trajectory tracking often exhibit limited performance due to the underactuated, nonlinear, and highly coupled dynamics of quadrotor systems. To address these challenges, we propose HBO-PID, a novel control algorithm that integrates the Heteroscedastic Bayesian Optimization (HBO) framework with the classical PID controller to achieve accurate and robust trajectory tracking. By explicitly modeling input-dependent noise variance, the proposed method can better adapt to dynamic and complex environments, and therefore improve the accuracy and robustness of trajectory tracking. To accelerate the convergence of optimization, we adopt a two-stage optimization strategy that allow us to more efficiently find the optimal controller parameters. Through experiments in both simulation and real-world scenarios, we demonstrate that the proposed method significantly outperforms state-of-the-art (SOTA) methods. Compared to SOTA methods, it improves the position accuracy by 24.7% to 42.9%, and the angular accuracy by 40.9% to 78.4%.

</details>


### [7] [Real-world Reinforcement Learning from Suboptimal Interventions](https://arxiv.org/abs/2512.24288)
*Yinuo Zhao,Huiqian Jin,Lechun Jiang,Xinyi Zhang,Kun Wu,Pei Ren,Zhiyuan Xu,Zhengping Che,Lei Sun,Dapeng Wu,Chi Harold Liu,Jian Tang*

Main category: cs.RO

TL;DR: SiLRI是一种状态级拉格朗日强化学习算法，通过将在线机器人操作问题建模为约束RL优化，有效利用次优人类干预加速学习，在真实机器人操作任务中显著提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有真实世界RL方法通常假设人类干预在整个状态空间都是最优的，但现实中即使是专家操作者也无法在所有状态下始终提供最优动作或完全避免错误。盲目混合干预数据与机器人收集数据会继承RL的样本低效性，而纯粹模仿干预数据最终会限制RL可达到的性能。如何利用可能次优且有噪声的人类干预来加速学习而不受其约束仍是一个开放问题。

Method: 提出SiLRI（状态级拉格朗日强化学习算法），将在线操作问题建模为约束RL优化，其中每个状态的约束边界由人类干预的不确定性决定。引入状态级拉格朗日乘子，通过最小-最大优化联合优化策略和拉格朗日乘子以达到鞍点。该方法建立在人类作为副驾驶的遥操作系统之上。

Result: 在多样化操作任务的真实世界实验中，SiLRI能有效利用人类次优干预，相比最先进的RL方法HIL-SERL，达到90%成功率所需时间至少减少50%，并在其他RL方法难以成功的长时程操作任务上实现100%成功率。

Conclusion: SiLRI通过状态级约束优化框架，成功解决了如何有效利用次优人类干预加速真实世界机器人操作学习的问题，在保持RL最终性能的同时显著提高了学习效率。

Abstract: Real-world reinforcement learning (RL) offers a promising approach to training precise and dexterous robotic manipulation policies in an online manner, enabling robots to learn from their own experience while gradually reducing human labor. However, prior real-world RL methods often assume that human interventions are optimal across the entire state space, overlooking the fact that even expert operators cannot consistently provide optimal actions in all states or completely avoid mistakes. Indiscriminately mixing intervention data with robot-collected data inherits the sample inefficiency of RL, while purely imitating intervention data can ultimately degrade the final performance achievable by RL. The question of how to leverage potentially suboptimal and noisy human interventions to accelerate learning without being constrained by them thus remains open. To address this challenge, we propose SiLRI, a state-wise Lagrangian reinforcement learning algorithm for real-world robot manipulation tasks. Specifically, we formulate the online manipulation problem as a constrained RL optimization, where the constraint bound at each state is determined by the uncertainty of human interventions. We then introduce a state-wise Lagrange multiplier and solve the problem via a min-max optimization, jointly optimizing the policy and the Lagrange multiplier to reach a saddle point. Built upon a human-as-copilot teleoperation system, our algorithm is evaluated through real-world experiments on diverse manipulation tasks. Experimental results show that SiLRI effectively exploits human suboptimal interventions, reducing the time required to reach a 90% success rate by at least 50% compared with the state-of-the-art RL method HIL-SERL, and achieving a 100% success rate on long-horizon manipulation tasks where other RL methods struggle to succeed. Project website: https://silri-rl.github.io/.

</details>


### [8] [World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild](https://arxiv.org/abs/2512.24310)
*TARS Robotics,Yuhang Zheng,Jichao Peng,Weize Li,Yupeng Zheng,Xiang Li,Yujie Jin,Julong Wei,Guanhua Zhang,Ruiling Zheng,Ming Cao,Songen Gu,Zhenhong Zou,Kaige Li,Ke Wu,Mingmin Yang,Jiahao Liu,Pengfei Li,Hengjie Si,Feiyu Zhu,Wang Fu,Likun Wang,Ruiwen Yao,Jieru Zhao,Yilun Chen,Wenchao Din*

Main category: cs.RO

TL;DR: WiYH是一个大规模开源生态系统，通过可穿戴数据采集套件、1000+小时多模态操作数据集和丰富标注，解决了灵巧手操作数据规模小、多样性不足的问题，显著提升了策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前灵巧手操作数据存在规模有限、多样性不足、模态不对齐和基准测试不足等问题，限制了策略的泛化能力。需要大规模、多样化的人类中心操作数据集来推动机器人操作学习的发展。

Method: 提出World In Your Hands (WiYH)生态系统，包括：(1) Oracle Suite可穿戴数据采集套件，配备自动标注流水线实现精确运动捕捉；(2) WiYH数据集，包含1000+小时多模态操作数据，涵盖数百种技能和多样化真实场景；(3) 丰富的标注和基准测试，支持从感知到动作的多种任务。

Result: 实验表明，整合WiYH的人类中心数据显著提升了灵巧手策略在桌面操作任务中的泛化能力和鲁棒性。该数据集填补了现有操作数据集的空白。

Conclusion: WiYH为人类中心数据采集和策略学习提供了新的视角和工具，有望推动机器人操作学习领域的发展，特别是在灵巧手操作的泛化能力方面。

Abstract: Large-scale pre-training is fundamental for generalization in language and vision models, but data for dexterous hand manipulation remains limited in scale and diversity, hindering policy generalization. Limited scenario diversity, misaligned modalities, and insufficient benchmarking constrain current human manipulation datasets. To address these gaps, we introduce World In Your Hands (WiYH), a large-scale open-source ecosystem for human-centric manipulation learning. WiYH includes (1) the Oracle Suite, a wearable data collection kit with an auto-labeling pipeline for accurate motion capture; (2) the WiYH Dataset, featuring over 1,000 hours of multi-modal manipulation data across hundreds of skills in diverse real-world scenarios; and (3) extensive annotations and benchmarks supporting tasks from perception to action. Furthermore, experiments based on the WiYH ecosystem show that integrating WiYH's human-centric data significantly enhances the generalization and robustness of dexterous hand policies in tabletop manipulation tasks. We believe that World In Your Hands will bring new insights into human-centric data collection and policy learning to the community.

</details>


### [9] [3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS](https://arxiv.org/abs/2512.24326)
*Camron Alexander Hirst,Chris Reale,Eric Frew*

Main category: cs.RO

TL;DR: 本文提出了两种基于非线性模型预测控制（MPC）的3D路径跟踪制导算法，应用于固定翼小型无人机系统，通过飞行测试验证了其在实际高速飞行中的可行性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 针对固定翼小型无人机系统在3D路径跟踪中的挑战，需要开发能够在高速飞行中处理高曲率路径的先进制导算法，传统方法在路径进展和路径距离这两个竞争目标之间难以实现优化平衡。

Method: 首先对RAAVEN小型无人机进行控制增强建模和系统辨识以支持MPC。然后提出两种MPC算法：第一种在MPC时域内调度静态参考路径速率，激励恒定惯性速度；第二种受模型预测轮廓控制启发，在控制器时域内动态优化参考路径速率，允许在路径进展和路径距离之间进行加权权衡。两种控制器都设计为在一般光滑3D弧长参数化曲线上运行。

Result: 在多个高曲率测试路径上进行了飞行测试，并与基准前瞻制导律进行了比较。结果表明，非线性MPC在3D路径跟踪制导中具有实际可行性，在地面速度高达36米/秒时表现出优越性能。

Conclusion: 非线性模型预测控制为固定翼小型无人机的3D路径跟踪制导提供了有效的解决方案，能够处理高曲率路径并在高速飞行中实现路径进展和路径距离之间的优化平衡，展示了在实际应用中的可行性和性能优势。

Abstract: This paper presents the design, implementation, and flight test results of two novel 3D path-following guidance algorithms based on nonlinear model predictive control (MPC), with specific application to fixed-wing small uncrewed aircraft systems. To enable MPC, control-augmented modelling and system identification of the RAAVEN small uncrewed aircraft is presented. Two formulations of MPC are then showcased. The first schedules a static reference path rate over the MPC horizon, incentivizing a constant inertial speed. The second, with inspiration from model predictive contouring control, dynamically optimizes for the reference path rate over the controller horizon as the system operates. This allows for a weighted tradeoff between path progression and distance from path, two competing objectives in path-following guidance. Both controllers are formulated to operate over general smooth 3D arc-length parameterized curves. The MPC guidance algorithms are flown over several high-curvature test paths, with comparison to a baseline lookahead guidance law. The results showcase the real-world feasibility and superior performance of nonlinear MPC for 3D path-following guidance at ground speeds up to 36 meters per second.

</details>


### [10] [Geometric Multi-Session Map Merging with Learned Local Descriptors](https://arxiv.org/abs/2512.24384)
*Yanlong Ma,Nakul S. Joshi,Christa S. Robison,Philip R. Osteen,Brett T. Lopez*

Main category: cs.RO

TL;DR: GMLD是一个基于学习的局部描述符框架，用于大规模多会话点云地图合并，通过关键点感知编码器和平面几何变换器提取特征，结合因子图优化实现准确的地图对齐。


<details>
  <summary>Details</summary>
Motivation: 大规模环境中进行扩展自主操作需要多会话地图合并，但现有方法在跨会话地图对齐方面存在挑战，需要更鲁棒的特征提取和全局一致性优化。

Method: 提出GMLD框架：1）使用关键点感知编码器提取特征；2）采用平面几何变换器增强几何特征；3）在因子图优化阶段加入跨会话扫描匹配成本因子以提升全局一致性。

Result: 在公开数据集和自收集数据上评估，结果显示准确且鲁棒的地图合并，误差较低，学习到的特征在闭环检测和相对位姿估计方面表现优异。

Conclusion: GMLD框架能够有效解决大规模多会话点云地图合并问题，通过学习到的局部描述符和全局优化策略，实现了准确、鲁棒的地图对齐。

Abstract: Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.

</details>


### [11] [Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack](https://arxiv.org/abs/2512.24402)
*Giovanni Lambertini,Matteo Pini,Eugenio Mascaro,Francesco Moretti,Ayoub Raji,Marko Bertogna*

Main category: cs.RO

TL;DR: 本文描述了一个用于自动驾驶赛车软件栈的自动化仿真与报告流水线，能够以最高三倍于实时速度执行仿真，支持本地和CI/CD环境，包含场景初始化、故障注入和自动化报告功能。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的自动化仿真和报告系统，用于验证自动驾驶赛车软件栈中的关键模块，特别是高速超车和定位等最具挑战性的功能，同时支持故障注入和持续集成/持续交付。

Method: 基于车辆高保真模型构建仿真系统，使用功能模拟单元接口，实现三倍于实时速度的仿真流水线；设计场景初始化系统支持不同初始条件；开发故障注入模块模拟传感器延迟和扰动；建立自动化报告流程。

Result: 成功实现了能够高效验证关键模块的自动化仿真流水线，支持本地和GitHub CI/CD环境，具备场景初始化、故障注入和自动化报告功能，显著提升了自动驾驶赛车软件栈的测试效率。

Conclusion: 该自动化仿真和报告流水线为自动驾驶赛车软件栈提供了高效的验证工具，特别适用于高速超车和定位等挑战性场景的测试，通过故障注入和自动化报告增强了系统的鲁棒性和分析效率。

Abstract: In this paper, we describe the automated simulation and reporting pipeline implemented for our autonomous racing stack, ur.autopilot. The backbone of the simulation is based on a high-fidelity model of the vehicle interfaced as a Functional Mockup Unit (FMU). The pipeline can execute the software stack and the simulation up to three times faster than real-time, locally or on GitHub for Continuous Integration/- Continuous Delivery (CI/CD). As the most important input of the pipeline, there is a set of running scenarios. Each scenario allows the initialization of the ego vehicle in different initial conditions (position and speed), as well as the initialization of any other configuration of the stack. This functionality is essential to validate efficiently critical modules, like the one responsible for high-speed overtaking maneuvers or localization, which are among the most challenging aspects of autonomous racing. Moreover, we describe how we implemented a fault injection module, capable of introducing sensor delays and perturbations as well as modifying outputs of any node of the stack. Finally, we describe the design of our automated reporting process, aimed at maximizing the effectiveness of the simulation analysis.

</details>


### [12] [Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning](https://arxiv.org/abs/2512.24426)
*Zhenghao "Mark" Peng,Wenhao Ding,Yurong You,Yuxiao Chen,Wenjie Luo,Thomas Tian,Yulong Cao,Apoorva Sharma,Danfei Xu,Boris Ivanovic,Boyi Li,Bolei Zhou,Yan Wang,Marco Pavone*

Main category: cs.RO

TL;DR: CF-VLA是一个自反思的视觉-语言-动作框架，通过反事实推理在自动驾驶中实现行动前的自我修正，提升轨迹准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强的VLA模型主要描述感知和意图，但很少质疑计划行动的安全性。需要一种能够自我反思并修正不安全行为的框架。

Method: 提出CF-VLA框架：首先生成时间分段的元动作总结驾驶意图，然后基于元动作和视觉上下文进行反事实推理，模拟潜在结果、识别不安全行为，输出修正后的元动作指导最终轨迹生成。采用rollout-filter-label流程从基础VLA的rollout中挖掘高价值场景并标注反事实推理轨迹用于训练。

Result: 在大规模驾驶数据集上的实验显示：CF-VLA将轨迹准确性提升高达17.6%，安全指标提升20.5%，并表现出适应性思维——仅在挑战性场景中启用反事实推理。

Conclusion: CF-VLA将推理轨迹从一次性描述转变为因果自我修正信号，向能够"三思而后行"的自反思自动驾驶智能体迈出了一步。

Abstract: Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.

</details>


### [13] [Subsecond 3D Mesh Generation for Robot Manipulation](https://arxiv.org/abs/2512.24428)
*Qian Wang,Omar Abdellall,Tony Gao,Xiatao Sun,Daniel Rakita*

Main category: cs.RO

TL;DR: 该研究提出了一种端到端系统，能够从单张RGB-D图像在1秒内生成高质量、上下文接地的3D网格，解决了现有方法速度慢和缺乏场景上下文的问题。


<details>
  <summary>Details</summary>
Motivation: 3D网格在机器人学中至关重要，但现有自动生成方法存在两个关键问题：1) 生成高保真网格速度过慢（通常需要数十秒），无法实时使用；2) 网格本身不足以满足机器人需求，需要正确的场景分割、尺度校准和姿态配准等上下文接地处理。

Method: 开发了一个端到端系统，集成了三个优化组件：1) 开放词汇对象分割，2) 加速的基于扩散的网格生成，3) 鲁棒的点云配准。整个流程针对速度和准确性进行了优化。

Result: 系统能够在1秒内从单张RGB-D图像生成高质量、上下文接地的3D网格。在实际机器人操作任务中验证了有效性，证明网格可以作为机器人感知和规划的实用、按需表示。

Conclusion: 该工作解决了3D网格生成在机器人应用中的关键瓶颈，实现了快速、上下文接地的网格生成，使3D网格成为机器人感知和规划的实用表示形式，有望推动实时机器人感知系统的发展。

Abstract: 3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.

</details>


### [14] [Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models](https://arxiv.org/abs/2512.24470)
*Kim Alexander Christensen,Andreas Gudahl Tufte,Alexey Gusev,Rohan Sinha,Milan Ganai,Ole Andreas Alsos,Marco Pavoned,Martin Steinert*

Main category: cs.RO

TL;DR: 论文提出Semantic Lookout系统，使用视觉语言模型为自主船舶提供语义感知能力，以检测操作设计域外的异常情况并选择安全的备用机动方案，满足IMO MASS Code草案要求。


<details>
  <summary>Details</summary>
Motivation: IMO MASS Code草案要求自主和远程监督的海事船舶能够检测操作设计域的偏离，进入预定义的备用模式并通知操作员。传统海事自主系统在处理依赖语义理解的异常情况（如潜水旗、火灾等）时存在困难，需要语义感知能力来应对这些分布外情况。

Method: 提出Semantic Lookout系统，这是一个仅使用摄像头的候选约束视觉语言模型备用机动选择器。采用快慢异常检测管道，结合短时域、人类可覆盖的备用机动方案。系统从水域有效、世界锚定的轨迹中选择一个谨慎动作（或保持位置），同时保持持续的人类权限。

Result: 在40个港口场景中测试：10秒内模型保留了最先进慢速模型的大部分感知能力；备用机动选择器优于仅基于几何的基线方法；在火灾场景中增加了安全距离；现场运行验证了端到端操作可行性。

Conclusion: 视觉语言模型可作为语义备用机动选择器，与IMO MASS Code草案兼容，在实际延迟预算内可行。未来工作应关注领域适应的混合自主系统，将基础模型语义与多传感器鸟瞰感知和短时域重新规划相结合。

Abstract: The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning.

</details>


### [15] [DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass](https://arxiv.org/abs/2512.24550)
*Tomoya Yamanokuchi,Alberto Bacchin,Emilio Olivastri,Ryotaro Arifuku,Takamitsu Matsubara,Emanuele Menegatti*

Main category: cs.RO

TL;DR: 提出了一种新的表面拟合算法DISF，在保持几何兼容性的同时整合接触稳定性，通过三步优化解决传统方法因忽略接触点分布而导致抓取不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于表面拟合的抓取规划算法主要关注夹爪与物体表面的几何对齐，但忽略了接触点分布的稳定性，导致因接触配置不足而产生不稳定的抓取。

Method: 受人类抓取行为启发，将抓取姿态优化分解为三个顺序步骤：(1)旋转优化以对齐接触法线，(2)平移细化以改善夹爪坐标系原点与物体质心的对齐，(3)夹爪开度调整以优化接触点分布。

Result: 在仿真中对15个物体进行了验证，包括已知形状（干净CAD数据集）和观测形状（YCB物体数据集）设置，并在三个机器人-夹爪平台上进行了跨平台抓取执行。在UR3e机器人上的真实世界抓取实验进一步验证了该方法。DISF在保持几何兼容性的同时减少了质心错位，在仿真和真实世界执行中都实现了比基线更高的抓取成功率。

Conclusion: DISF算法通过整合接触稳定性到表面拟合过程中，有效解决了传统方法因忽略接触点分布而导致的抓取不稳定问题，在保持几何兼容性的同时提高了抓取成功率。

Abstract: In this work, we address the limitation of surface fitting-based grasp planning algorithm, which primarily focuses on geometric alignment between the gripper and object surface while overlooking the stability of contact point distribution, often resulting in unstable grasps due to inadequate contact configurations. To overcome this limitation, we propose a novel surface fitting algorithm that integrates contact stability while preserving geometric compatibility. Inspired by human grasping behavior, our method disentangles the grasp pose optimization into three sequential steps: (1) rotation optimization to align contact normals, (2) translation refinement to improve the alignment between the gripper frame origin and the object Center of Mass (CoM), and (3) gripper aperture adjustment to optimize contact point distribution. We validate our approach in simulation across 15 objects under both Known-shape (with clean CAD-derived dataset) and Observed-shape (with YCB object dataset) settings, including cross-platform grasp execution on three robot--gripper platforms. We further validate the method in real-world grasp experiments on a UR3e robot. Overall, DISF reduces CoM misalignment while maintaining geometric compatibility, translating into higher grasp success in both simulation and real-world execution compared to baselines. Additional videos and supplementary results are available on our project page: https://tomoya-yamanokuchi.github.io/disf-ras-project-page/

</details>


### [16] [Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding](https://arxiv.org/abs/2512.24638)
*Qingda Hu,Ziheng Qiu,Zijun Xu,Kaizhao Zhang,Xizhou Bu,Zuolei Sun,Bo Zhang,Jieru Zhao,Zhongxue Gan,Wenchao Ding*

Main category: cs.RO

TL;DR: PAM是一种具有自适应工作记忆的视觉运动策略，通过两阶段训练支持300帧历史窗口，能在保持20Hz推理速度的同时处理机器人操作中的状态模糊问题。


<details>
  <summary>Details</summary>
Motivation: 机器人操作中普遍存在状态模糊问题，相同观测可能对应多个有效行为轨迹。现有方法要么历史窗口过短无法有效提取信息，要么历史窗口过长导致计算成本高和过拟合问题。

Method: 采用两阶段训练方式，包含分层帧特征提取器（分别提取运动基元和时序消歧特征）、基于范围特定查询的上下文路由器（生成紧凑的跨历史长度上下文特征），以及重建历史信息的辅助目标函数。

Result: 在精心设计的7个任务中验证了PAM能同时处理多种状态模糊场景，支持约10秒的历史窗口，保持稳定训练和20Hz以上的推理速度。

Conclusion: PAM通过自适应工作记忆机制，以最小额外训练成本实现了长历史窗口的高效处理，为解决机器人操作中的状态模糊问题提供了有效方案。

Abstract: State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/

</details>


### [17] [Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation](https://arxiv.org/abs/2512.24651)
*Yury Kolomeytsev,Dmitry Golembiovsky*

Main category: cs.RO

TL;DR: HMP-DRL：一种混合运动规划框架，结合图搜索全局规划器和深度强化学习局部控制器，通过语义感知奖励机制在复杂动态环境中实现安全高效的机器人导航。


<details>
  <summary>Details</summary>
Motivation: 传统图搜索规划器擅长长距离路径规划但缺乏反应性，而深度强化学习方法在避碰方面表现良好但缺乏全局上下文导致难以到达远距离目标。需要一种能结合两者优势的混合方法来解决复杂动态环境中的机器人导航问题。

Method: 提出HMP-DRL混合框架：1）使用图搜索全局规划器生成路径；2）将路径转换为一系列检查点，编码到局部DRL策略的状态空间和奖励函数中；3）采用实体感知奖励结构，根据周围智能体的语义类型动态调整安全边界和惩罚。

Result: 在基于真实地图数据的仿真环境中进行广泛测试，HMP-DRL在机器人导航关键指标（成功率、碰撞率、到达目标时间）上均优于其他方法，包括最先进的方法。

Conclusion: 将长期路径指导与语义感知的局部控制相结合，能显著提升复杂人本环境中自主导航的安全性和可靠性。

Abstract: Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.

</details>


### [18] [RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence](https://arxiv.org/abs/2512.24653)
*Chengkai Hou,Kun Wu,Jiaming Liu,Zhengping Che,Di Wu,Fei Liao,Guangrun Li,Jingyang He,Qiuxuan Feng,Zhao Jin,Chenyang Gu,Zhuoyang Liu,Nuowei Han,Xiangju Mi,Yaoxu Lv,Yankai Fu,Gaole Dai,Langzhe Gu,Tao Li,Yuheng Zhang,Yixue Zhang,Xinhua Wang,Shichao Fan,Meng Li,Zhen Zhao,Ning Liu,Zhiyuan Xu,Pei Ren,Junjie Ji,Haonan Liu,Kuan Cheng,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: RoboMIND 2.0是一个包含31万条双臂操作轨迹的真实世界数据集，涵盖6种机器人平台和739个复杂任务，包含触觉增强和移动操作数据，并配有2万条模拟轨迹的数字孪生。MIND-2系统采用分层双系统框架，通过离线强化学习优化，包含高层语义规划器和低层视觉语言动作执行器。


<details>
  <summary>Details</summary>
Motivation: 当前基于数据驱动的模仿学习方法受限于大规模、多样化真实世界演示数据的稀缺性，导致现有模型在长视野双手机器人任务和非结构化环境中的移动操作方面泛化能力有限。

Method: 提出了RoboMIND 2.0数据集，包含超过31万条双臂操作轨迹，涵盖6种不同机器人平台和739个复杂任务，特别包含1.2万条触觉增强轨迹和2万条移动操作轨迹。同时构建了高保真数字孪生环境，发布了2万条模拟轨迹数据集。在此基础上提出了MIND-2系统，这是一个通过离线强化学习优化的分层双系统框架，包含高层语义规划器（MIND-2-VLM）和低层视觉语言动作执行器（MIND-2-VLA）。

Result: 创建了一个全面的真实世界机器人操作数据集，规模远超现有数据集，特别针对接触丰富和空间扩展任务提供了触觉增强和移动操作数据。同时提出了一个能够将抽象自然语言指令分解为具体子目标，并生成精确本体感知动作的分层系统框架。

Conclusion: RoboMIND 2.0数据集和MIND-2系统为解决机器人操作中数据稀缺和泛化能力有限的问题提供了重要资源和方法框架，有望推动接触丰富任务、移动操作和长视野双手机器人任务的研究进展。

Abstract: While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.

</details>


### [19] [Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids](https://arxiv.org/abs/2512.24657)
*Sungjae Min,Hyungjoo Kim,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 提出一种轻量化仿人手，采用鲍登线驱动和滚动接触关节优化，实现单电机控制、高抓握力和低远端质量


<details>
  <summary>Details</summary>
Motivation: 类人机器人需要具备高抓握力、快速驱动、多自由度、轻量化且尺寸接近人手的手部，但现有设计难以同时满足这些矛盾需求，通常导致较重驱动器和笨重传动系统

Method: 采用鲍登线驱动，结合滚动接触关节优化与拮抗式线缆驱动，实现单电机控制且线缆长度变化极小；将驱动模块移至躯干以减少远端质量

Result: 手部远端质量仅236g，指尖力超过18N，能举起超过自身质量100倍的负载；通过Cutkosky分类抓握和扰动下的轨迹一致性验证了鲁棒性

Conclusion: 该设计成功解决了仿人手在轻量化、高抓握力、快速驱动和多自由度之间的矛盾，为类人机器人实现人类水平灵巧性提供了可行方案

Abstract: Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations.

</details>


### [20] [VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots](https://arxiv.org/abs/2512.24673)
*Yongsheng Zhao,Lei Zhao,Baoping Cheng,Gongxin Yao,Xuanzhang Wen,Han Gao*

Main category: cs.RO

TL;DR: VLA-RAIL框架通过异步推理和轨迹平滑技术，解决了VLA模型在机器人控制中的抖动、停顿问题，实现了连续高速的动作执行。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在机器人动作执行中存在抖动、停滞甚至暂停的问题，这不仅限制了执行速度，还降低了任务成功率。这些问题源于动作块融合策略的不足，需要新的框架来保证平滑、连续、高速的动作执行。

Method: 提出VLA-RAIL框架，核心包括两个组件：1) 轨迹平滑器 - 使用多项式拟合过滤单个动作块轨迹中的噪声和抖动；2) 块融合器 - 无缝对齐当前执行轨迹和新到达的动作块，确保连续动作块之间的位置、速度和加速度连续性。框架采用异步推理和机器人运动控制的方式。

Result: 在动态仿真基准测试和多个真实世界操作任务上的实验结果表明，VLA-RAIL显著减少了运动抖动，提高了执行速度，并改善了任务成功率。

Conclusion: VLA-RAIL解决了VLA模型在机器人控制中的关键问题，将成为VLA模型大规模部署的关键基础设施，能够实现平滑、连续、高速的动作执行。

Abstract: Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.

</details>


### [21] [ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments](https://arxiv.org/abs/2512.24680)
*Kangjie Zhou,Zhaoyang Li,Han Gao,Yao Su,Hangxin Liu,Junzhi Yu,Chang Liu*

Main category: cs.RO

TL;DR: ReSPIRe是一种用于未知杂乱环境中目标搜索与跟踪的信息化轨迹规划方法，通过sigma点近似估计互信息奖励，采用分层粒子结构和可重用信念树搜索提高规划效率。


<details>
  <summary>Details</summary>
Motivation: 目标搜索与跟踪是机器人应用中的基本问题，但在未知杂乱环境中，面临先验目标信息不准确、传感器视野有限、计算复杂度高等挑战，需要高效的轨迹规划方法。

Method: 1) 开发sigma点近似方法快速准确估计非高斯信念分布下的互信息奖励；2) 提出分层粒子结构提取关键粒子进行全局路径引导，并自适应调整粒子数量；3) 基于分层结构开发可重用信念树搜索方法，重用rollout评估提高在线轨迹规划效率。

Result: ReSPIRe在仿真和真实实验中优于基准方法，具有更小的互信息近似误差、更高的搜索效率和更稳定的跟踪性能，同时保持出色的计算效率。

Conclusion: ReSPIRe通过创新的sigma点近似、分层粒子结构和可重用信念树搜索，有效解决了未知杂乱环境中目标搜索与跟踪的挑战，实现了高效、准确的轨迹规划。

Abstract: Target search and tracking (SAT) is a fundamental problem for various robotic applications such as search and rescue and environmental exploration. This paper proposes an informative trajectory planning approach, namely ReSPIRe, for SAT in unknown cluttered environments under considerably inaccurate prior target information and limited sensing field of view. We first develop a novel sigma point-based approximation approach to fast and accurately estimate mutual information reward under non-Gaussian belief distributions, utilizing informative sampling in state and observation spaces to mitigate the computational intractability of integral calculation. To tackle significant uncertainty associated with inadequate prior target information, we propose the hierarchical particle structure in ReSPIRe, which not only extracts critical particles for global route guidance, but also adjusts the particle number adaptively for planning efficiency. Building upon the hierarchical structure, we develop the reusable belief tree search approach to build a policy tree for online trajectory planning under uncertainty, which reuses rollout evaluation to improve planning efficiency. Extensive simulations and real-world experiments demonstrate that ReSPIRe outperforms representative benchmark methods with smaller MI approximation error, higher search efficiency, and more stable tracking performance, while maintaining outstanding computational efficiency.

</details>


### [22] [CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System](https://arxiv.org/abs/2512.24688)
*Zhehan Li,Zheng Wang,Jiadong Lu,Qi Liu,Zhiren Xun,Yue Wang,Fei Gao,Chao Xu,Yanjun Cao*

Main category: cs.RO

TL;DR: CREPES-X是一个用于多机器人相对定位的分层框架，通过紧凑硬件设计和两阶段估计器实现高速、高精度和鲁棒性，无需全局信息。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人相对定位方法依赖共享环境特征或惯性假设，在复杂环境中易受非视距干扰和异常值影响，难以鲁棒高效地融合多种测量数据。

Method: 采用紧凑硬件设计（红外LED、红外相机、超宽带模块、IMU），实现两阶段分层估计器：单帧相对估计器提供即时相对位姿，多帧相对估计器通过IMU预积分和优化提供精确相对状态。

Result: 实验验证显示对高达90%的方位异常值具有鲁棒性，在真实数据集上达到0.073米和1.817°的RMSE精度。

Conclusion: CREPES-X框架在挑战性条件下实现了高速、高精度和鲁棒的多机器人相对定位，无需全局信息，适用于复杂环境。

Abstract: Relative localization is critical for cooperation in autonomous multi-robot systems. Existing approaches either rely on shared environmental features or inertial assumptions or suffer from non-line-of-sight degradation and outliers in complex environments. Robust and efficient fusion of inter-robot measurements such as bearings, distances, and inertials for tens of robots remains challenging. We present CREPES-X (Cooperative RElative Pose Estimation System with multiple eXtended features), a hierarchical relative localization framework that enhances speed, accuracy, and robustness under challenging conditions, without requiring any global information. CREPES-X starts with a compact hardware design: InfraRed (IR) LEDs, an IR camera, an ultra-wideband module, and an IMU housed in a cube no larger than 6cm on each side. Then CREPES-X implements a two-stage hierarchical estimator to meet different requirements, considering speed, accuracy, and robustness. First, we propose a single-frame relative estimator that provides instant relative poses for multi-robot setups through a closed-form solution and robust bearing outlier rejection. Then a multi-frame relative estimator is designed to offer accurate and robust relative states by exploring IMU pre-integration via robocentric relative kinematics with loosely- and tightly-coupled optimization. Extensive simulations and real-world experiments validate the effectiveness of CREPES-X, showing robustness to up to 90% bearing outliers, proving resilience in challenging conditions, and achieving RMSE of 0.073m and 1.817° in real-world datasets.

</details>


### [23] [Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer](https://arxiv.org/abs/2512.24698)
*Dongyun Kang,Min-Gyu Kim,Tae-Gyu Song,Hajun Kim,Sehoon Ha,Hae-Won Park*

Main category: cs.RO

TL;DR: 提出基于延续学习框架，结合简化模型预训练和模型同伦转移，高效生成和优化腿式机器人的动态运动。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人动态运动生成仍具挑战性。强化学习需要大量奖励调优或高质量演示，而简化模型存在模型差异问题，难以迁移到全身动力学环境。

Method: 1. 使用单刚体模型预训练策略，在简化环境中捕获核心运动模式；2. 采用延续策略逐步将策略迁移到全身环境；3. 通过质量惯性在躯干和腿部间的渐进重分布，定义从单刚体到全身模型的同伦路径。

Result: 相比基线方法，该方法收敛更快，迁移过程更稳定。在翻转、墙面辅助机动等动态任务上验证有效，并成功部署到真实四足机器人。

Conclusion: 提出的延续学习框架能高效生成复杂动态行为，通过简化模型预训练和渐进模型同伦转移，有效解决模型差异问题，实现从简化模型到全身环境的稳定策略迁移。

Abstract: Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot.

</details>


### [24] [LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving](https://arxiv.org/abs/2512.24712)
*Qian Cheng,Weitao Zhou,Cheng Jing,Nanshan Deng,Junze Wen,Zhaoyang Liu,Kun Jiang,Diange Yang*

Main category: cs.RO

TL;DR: LSRE框架将稀疏采样的视觉语言模型判断转换为循环世界模型潜在空间中的决策边界，实现实时语义风险评估，无需每帧VLM查询


<details>
  <summary>Details</summary>
Motivation: 现实世界自动驾驶需要遵守超越法定交通规则的复杂人类社交规则，这些语义约束（如让行紧急车辆、遵守交警手势等）对人类直观但对机器难以显式编码，而现有VLM推理成本过高无法实时部署

Method: 提出LSRE（潜在语义规则编码）框架，将稀疏采样的VLM判断转换为循环世界模型潜在空间中的决策边界，通过将语言定义的安全语义编码到轻量级潜在分类器中，实现实时语义风险评估

Result: 在CARLA中的六个语义故障场景实验中，LSRE达到与大型VLM基线相当的语义风险检测准确率，同时提供更早的危险预期，保持低计算延迟，并能泛化到罕见语义相似测试案例

Conclusion: 语言引导的潜在分类为自动驾驶中的语义安全监控提供了有效且可部署的机制，能够在保持实时性能的同时实现复杂的语义理解

Abstract: Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment.This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.

</details>


### [25] [Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow](https://arxiv.org/abs/2512.24766)
*Karthik Dharmarajan,Wenlong Huang,Jiajun Wu,Li Fei-Fei,Ruohan Zhang*

Main category: cs.RO

TL;DR: Dream2Flow提出了一种通过3D物体流作为中间表示，将视频生成模型与机器人控制连接起来的框架，实现零样本的开放世界物体操作。


<details>
  <summary>Details</summary>
Motivation: 生成视频模型能够零样本推理开放世界中的物理交互，但难以将其转化为机器人系统所需的低级动作。需要弥合视频生成与机器人控制之间的"具身鸿沟"。

Method: 使用3D物体流作为中间表示：1）从生成视频中重建3D物体运动；2）将操作任务转化为物体轨迹跟踪问题；3）通过轨迹优化或强化学习将3D物体流转换为可执行的低级指令。

Result: Dream2Flow能够零样本指导预训练视频模型操作多种类型物体（刚性、铰接、可变形、颗粒状），在仿真和真实世界实验中验证了3D物体流作为通用接口的有效性。

Conclusion: 3D物体流是一种通用且可扩展的接口，能够将视频生成模型适配到开放世界机器人操作任务中，无需任务特定演示即可实现零样本指导。

Abstract: Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.

</details>


### [26] [ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation](https://arxiv.org/abs/2512.24845)
*Qiuyi Gu,Yuze Sheng,Jincheng Yu,Jiahao Tang,Xiaolong Shan,Zhaoyang Shen,Tinghao Yi,Xiaodan Liang,Xinlei Chen,Yu Wang*

Main category: cs.RO

TL;DR: ArtiSG框架通过人类演示构建功能性3D场景图，解决现有方法在关节物体功能信息缺失、视觉歧义和细粒度功能元素检测不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图缺乏物理操作所需的功能信息，特别是关节物体的运动机制；现有方法存在视觉歧义、依赖固定摄像头设置、以及无法检测细粒度功能元素（如小把手）的问题

Method: 使用便携式设备收集关节运动数据，准确估计6自由度关节轨迹和轴线；将运动学先验整合到分层开放词汇图中；利用交互数据发现视觉感知遗漏的功能元素

Result: 在真实世界实验中显著优于基线方法，在功能元素召回率和关节估计精度方面表现优异；构建的图可作为可靠的功能记忆，有效指导机器人执行语言指令的操控任务

Conclusion: ArtiSG框架成功构建了功能性3D场景图，解决了关节物体功能信息缺失的问题，为机器人语义理解和物理操作提供了有效解决方案

Abstract: 3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.

</details>


### [27] [Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments](https://arxiv.org/abs/2512.24974)
*Yunxi Tang,Tianqi Yang,Jing Huang,Xiangyu Chu,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 提出了一种用于受限环境中可变形线性物体操作的新框架，结合分层变形规划和神经跟踪，解决高维状态空间和复杂变形动力学带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 可变形线性物体操作面临高维状态空间、复杂变形动力学以及现实工作空间中广泛存在的障碍物等挑战，需要高效的变形规划和鲁棒的变形跟踪。

Method: 框架结合分层变形规划和神经跟踪：1) 变形规划器生成满足同伦约束的空间路径集；2) 路径集引导的优化方法合成最优时间变形序列；3) 基于数据驱动变形模型的神经模型预测控制进行精确跟踪。

Result: 在广泛的受限DLO操作任务中验证了所提框架的有效性。

Conclusion: 提出的结合分层变形规划和神经跟踪的框架能够可靠地在全局变形合成和局部变形跟踪中实现可变形线性物体的有效操作。

Abstract: Deformable linear objects (DLOs) manipulation presents significant challenges due to DLOs' inherent high-dimensional state space and complex deformation dynamics. The wide-populated obstacles in realistic workspaces further complicate DLO manipulation, necessitating efficient deformation planning and robust deformation tracking. In this work, we propose a novel framework for DLO manipulation in constrained environments. This framework combines hierarchical deformation planning with neural tracking, ensuring reliable performance in both global deformation synthesis and local deformation tracking. Specifically, the deformation planner begins by generating a spatial path set that inherently satisfies the homotopic constraints associated with DLO keypoint paths. Next, a path-set-guided optimization method is applied to synthesize an optimal temporal deformation sequence for the DLO. In manipulation execution, a neural model predictive control approach, leveraging a data-driven deformation model, is designed to accurately track the planned DLO deformation sequence. The effectiveness of the proposed framework is validated in extensive constrained DLO manipulation tasks.

</details>


### [28] [Coordinated Humanoid Manipulation with Choice Policies](https://arxiv.org/abs/2512.25072)
*Haozhi Qi,Yen-Jen Wang,Toru Lin,Brent Yi,Yi Ma,Koushil Sreenath,Jitendra Malik*

Main category: cs.RO

TL;DR: 提出结合模块化遥操作界面与可扩展学习框架的系统，通过Choice Policy模仿学习方法实现人形机器人头、手、腿的全身协调控制，在洗碗机装载和白板擦拭任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在以人为中心的环境中具有巨大潜力，但实现头、手、腿的稳健全身协调仍然是一个重大挑战。需要开发能够有效收集高质量演示数据并学习多模态行为的系统。

Method: 1. 模块化遥操作设计：将人形控制分解为手眼协调、抓取原语、手臂末端执行器跟踪和运动等直观子模块，高效收集高质量演示数据。2. Choice Policy模仿学习方法：生成多个候选动作并学习对其评分，实现快速推理和有效建模多模态行为。

Result: 在洗碗机装载和全身运动操作（白板擦拭）两个真实世界任务中验证。Choice Policy显著优于扩散策略和标准行为克隆。实验表明手眼协调对于长时域任务的成功至关重要。

Conclusion: 该工作展示了在非结构化环境中实现可扩展数据收集和人形机器人协调操作的实际路径，为人形机器人在复杂环境中的应用提供了有效解决方案。

Abstract: Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.

</details>
