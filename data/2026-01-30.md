<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs](https://arxiv.org/abs/2601.20968)
*Yulie Arad,Stav Ashur,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文提出"红-绿-灰"范式，改进SPITE方法，通过廉价启发式检查快速更新机器人运动规划路线图，适应非静态环境。


<details>
  <summary>Details</summary>
Motivation: 解决机器人运动规划中路线图在非静态环境中的适应性问题，传统方法难以高效处理环境变化，需要快速更新路线图的有效性状态。

Method: 引入"红-绿-灰"分类范式，使用简单计算几何方法近似机器人扫掠体积，进行惰性碰撞检查，将边标记为无效(红)、有效(绿)或未知(灰)，实现半惰性路线图快速更新。

Result: 与Leven和Hutchinson的成熟技术相比，该方法提高了准确性，能够正确标记无效边，同时保持可比的更新运行时间。

Conclusion: "红-绿-灰"范式为机器人运动规划在非静态环境中提供了一种有效的路线图更新方法，通过廉价启发式检查实现快速状态分类，在准确性和效率之间取得良好平衡。

Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the "Red-Green-Gray" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.

</details>


### [2] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: Meta-ROS是一种新型机器人中间件，旨在解决ROS2等现有框架的复杂性和互操作性问题，通过简化集成、提升性能、确保跨平台兼容性来优化机器人开发。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件框架（如ROS2）存在复杂性和互操作性问题，对新手开发者不友好，阻碍了机器人技术的广泛采用和发展。

Method: Meta-ROS采用现代通信协议（如Zenoh和ZeroMQ）实现高效低延迟通信，支持多种数据类型（音频、图像、视频），并具有跨硬件平台兼容性和开发者友好的设计。

Result: Meta-ROS在性能测试中优于ROS2，吞吐量提升高达30%，显著降低消息延迟，优化资源使用，同时提供强大的硬件支持和易用性。

Conclusion: Meta-ROS通过简化集成、提升性能和确保跨平台兼容性，成为现代实时机器人AI应用的理想解决方案，解决了现有中间件框架的主要痛点。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [3] [Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing](https://arxiv.org/abs/2601.21027)
*Youngim Nam,Jungbin Kim,Kyungtae Kang,Cheolhyeon Kwon*

Main category: cs.RO

TL;DR: 提出一个用于自动驾驶赛车的最小化圈时全局轨迹优化框架，在不确定车辆动力学条件下通过迭代学习和贝叶斯优化直接优化全时域轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在跟踪层面学习动力学，而不更新轨迹本身以适应学习到的动力学，且全时域轨迹优化计算昂贵，在不确定动力学条件下难以保证全局最优性。

Method: 采用赛道中心化方法，通过小波变换在赛道无关参数空间中表示轨迹，使用贝叶斯优化高效探索该空间，并将优化嵌入迭代学习框架，用实际数据更新动力学模型。

Result: 通过仿真和真实世界实验验证了框架有效性，相比基准方法实现了高达20.7%的圈时提升，并持续优于最先进方法。

Conclusion: 提出的框架能够有效处理不确定车辆动力学，通过迭代学习和轨迹优化显著提升自动驾驶赛车的圈时性能。

Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.

</details>


### [4] [WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics](https://arxiv.org/abs/2601.21129)
*Guangping Liu,Tipu Sultan,Vittorio Di Giorgio,Nick Hawkins,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: 本文提出了WheelArm-Sim仿真框架，用于收集轮椅与机械臂集成控制的多模态数据集，为开发数据驱动的机器学习模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 虽然辅助机器人技术已有进步，但针对轮椅与机械臂组合的集成统一控制研究不足，特别是缺乏用于机器学习模型开发的数据集。

Method: 开发了基于Isaac Sim的WheelArm-Sim仿真框架，收集了包含13个任务、232条轨迹、67,783个样本的多模态数据集，并实现了芥末拾取任务的基线动作预测模型。

Result: 收集了大规模多模态数据集，基线模型验证了从WheelArm-Sim收集的数据可用于数据驱动的集成控制机器学习模型。

Conclusion: WheelArm-Sim框架为轮椅与机械臂集成控制的数据驱动方法提供了可行的数据收集平台，推动了辅助机器人领域的进一步发展。

Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.

</details>


### [5] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1是首个用于工业巡检安全评估的多模态基准数据集，包含真实工业环境中巡检机器人采集的多种传感器数据，覆盖5种工业场景，提供像素级分割标注和安全等级标签。


<details>
  <summary>Details</summary>
Motivation: 工业智能化和无人巡检快速发展，但现有数据集多为模拟数据、单模态感知或缺乏细粒度标注，限制了工业基础模型的鲁棒场景理解和多模态安全推理能力。

Method: 从真实工业环境中运行的41台轮式和轨道式巡检机器人收集数据，覆盖隧道、电力设施、烧结设备、石油化工和煤炭输送栈桥5种场景，包含7种同步感知模态（可见光、红外视频、音频、深度点云、雷达点云、气体测量、温湿度）。

Result: 构建了包含2,239个有效巡检站点、5,013个巡检实例的数据集，每个实例提供可见光谱图像的像素级分割标注、语义场景描述和安全等级标签。

Conclusion: InspecSafe-V1填补了工业巡检安全评估多模态基准数据集的空白，支持多模态异常识别、跨模态融合和工业环境综合安全评估，为工业基础模型发展提供重要资源。

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [6] [Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation](https://arxiv.org/abs/2601.21188)
*Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种针对机器人飞艇的扰动感知控制框架，通过移动质量执行器结合移动水平估计器和模型预测控制，显著提升了在风扰动环境下的飞行稳定性。


<details>
  <summary>Details</summary>
Motivation: 机器人飞艇作为轻于空气的航空系统，具有长续航和本质安全的特点，但对风扰动高度敏感。目前缺乏针对LTA平台的扰动感知控制框架，需要能够明确建模和补偿风致效应的控制方法。

Method: 采用移动水平估计器实时推断风扰动，将估计结果提供给模型预测控制器。利用二自由度移动质量机制产生惯性和气动力矩，实现姿态和航向控制，形成集成的MHE-MPC控制框架。

Result: 在顶风和侧风条件下的广泛飞行实验表明，集成的MHE-MPC框架显著优于基准PID控制，证明了其在扰动感知LTA飞行中的有效性。

Conclusion: 该研究成功开发了一种扰动感知控制框架，通过移动质量执行器结合MHE-MPC方法，显著提升了机器人飞艇在风扰动环境下的轨迹和航向调节能力，为LTA平台在扰动环境中的稳定飞行提供了有效解决方案。

Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.

</details>


### [7] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 提出分层主动调优框架HPTune，通过评估未执行动作来改进MPC参数调优，结合快速层和慢速层调优，集成多普勒激光雷达增强运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有MPC运动规划器参数调优方法通常只评估已执行动作，由于失败事件（如障碍物接近或碰撞）的稀疏性，导致参数更新效率低下。

Method: 提出分层主动调优框架HPTune：1）快速层调优采用预测接近速度和预测接近距离的风险指标；2）慢速层调优利用扩展评估损失进行闭环反向传播；3）集成多普勒激光雷达获取障碍物速度信息以增强运动预测。

Result: 在高保真模拟器上的大量实验表明，HPTune实现了高效的MPC调优，在复杂环境中优于各种基线方案，能够制定安全、敏捷的避碰策略。

Conclusion: HPTune通过评估未执行动作扩展了参数调优范围，实现了情境定制的运动规划，提高了MPC在复杂环境中的适应性和性能。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [8] [Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control](https://arxiv.org/abs/2601.21363)
*Weidong Huang,Zhehan Li,Hangxin Liu,Biao Hou,Yao Su,Jingwen Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种结合大规模离线策略预训练和基于模型微调的方法，用于人形机器人控制，实现了零样本部署和高效适应新环境。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人控制中，在线策略方法（如PPO）虽然能实现大规模并行仿真和零样本部署，但样本效率低，难以安全适应新环境。离线策略和基于模型的方法虽然样本效率更高，但在大规模预训练和高效微调之间仍存在差距。

Method: 采用离线策略Soft Actor-Critic (SAC)算法，配合大批量更新和高更新数据比(UTD)，进行大规模预训练。在适应新环境时，使用基于模型的方法微调预训练策略：在新环境中执行确定性策略收集数据，而随机探索则限制在基于物理的世界模型中，从而降低适应过程中的随机探索风险。

Result: SAC预训练策略能够实现人形机器人运动控制的零样本部署到真实机器人。预训练策略可以在新环境和分布外任务中通过基于模型方法进行高效微调。该方法结合了预训练阶段的大规模仿真效率和微调阶段的基于模型学习样本效率。

Conclusion: 该方法成功地将大规模离线策略预训练与基于模型微调相结合，为人形机器人控制提供了既能实现零样本部署又能高效适应新环境的解决方案，平衡了训练效率和适应安全性。

Abstract: Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

</details>


### [9] [Towards Space-Based Environmentally-Adaptive Grasping](https://arxiv.org/abs/2601.21394)
*Leonidas Askianakis,Aleksandr Artemov*

Main category: cs.RO

TL;DR: 该论文提出了一种在学习的潜在流形中直接学习控制策略的方法，用于空间环境中的抓取任务，通过融合多模态信息实现更高效的强化学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作系统在非结构化环境中面临高维动作空间、稀疏奖励和泛化能力差的问题，特别是在空间环境等极端条件下，需要更可靠和自适应的抓取能力。

Method: 使用GPU加速物理仿真创建单次抓取任务，在学习的潜在流形中融合多模态信息形成结构化表示，采用Soft Actor-Critic强化学习算法直接学习控制策略。

Result: 在不到100万环境步数内实现超过95%的任务成功率，在连续变化的抓取条件下从第一步开始就表现出比现有视觉基线更快的收敛速度，对新颖物体、夹爪几何、环境杂乱和传感器配置具有更好的鲁棒性。

Conclusion: 在潜在空间中进行显式推理能够实现更样本高效的学习和更强的鲁棒性，为极端空间条件下的完全自适应和可泛化抓取提供了方向。

Abstract: Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.

</details>


### [10] [Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates](https://arxiv.org/abs/2601.21413)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种将李群积分器与标准多体系统建模框架对接的方法，解决了李群积分与传统绝对坐标方程不兼容的问题，同时保持了刚体运动几何特性。


<details>
  <summary>Details</summary>
Motivation: 传统多体系统建模使用绝对坐标，但空间运动的无奇异性参数化存在问题。李群积分方法能保持运动几何特性，但与现有标准方程不兼容，无法直接应用于现有仿真代码。

Method: 提出两个主要方法：1）建立李群积分器与标准运动方程框架的接口；2）在绝对坐标中一致地融入刚体运动几何特性。使用SO(3)×R3和SE(3)群表示刚体运动，关键是通过局部-全局转换映射更新全局绝对坐标。

Result: 实现了李群积分器与标准多体系统建模框架的兼容，允许使用各种绝对坐标描述系统，同时应用李群积分方案，保持了刚体运动的几何特性。

Conclusion: 提出的框架成功解决了李群积分与标准多体系统建模不兼容的问题，为在现有仿真代码中使用李群积分方法提供了可行方案，同时保持了运动几何的完整性。

Abstract: A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.

</details>


### [11] [Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation](https://arxiv.org/abs/2601.21416)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: 该研究比较了机器人操作策略中三种视觉表示方法：全局特征、密集特征和基于槽的对象中心表示（SBOCR），发现SBOCR在光照、纹理变化和干扰物存在等分布偏移下具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作策略主要依赖预训练编码器提取的全局特征或密集特征，但这些特征混合了任务相关和无关信息，导致在光照、纹理变化或存在干扰物等分布偏移时泛化能力差。需要探索更有效的视觉表示方法。

Method: 提出使用基于槽的对象中心表示（SBOCR）作为中间结构化替代方案，将密集特征分组为有限的对象类实体。在模拟和真实世界的机器人操作任务中，对全局特征、密集特征和SBOCR进行基准测试，评估它们在光照、纹理变化和干扰物存在等视觉条件下的泛化能力。

Result: SBOCR基于策略在泛化设置中优于密集特征和全局特征基于策略，即使没有任务特定的预训练。SBOCR能够自然地减少提供给机器人操作策略的噪声，同时保留足够信息来高效执行任务。

Conclusion: SBOCR是设计在动态、真实世界机器人环境中有效泛化的视觉系统的有前景方向，其对象中心表示有助于提高机器人操作策略的鲁棒性和泛化能力。

Abstract: The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.

</details>


### [12] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Nimbus是一个统一的合成数据生成框架，通过模块化四层架构和异步执行模型，将异构导航和操作管道集成，实现了2-3倍的端到端吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成管道碎片化且任务特定，导致工程效率低下和系统不稳定，无法支持基础模型训练所需的高吞吐量数据生成。

Method: 提出Nimbus框架，采用模块化四层架构，将轨迹规划、渲染和存储解耦为异步阶段，实现动态管道调度、全局负载均衡、分布式容错和后端特定渲染优化。

Result: 相比未优化的基线，Nimbus实现了2-3倍的端到端吞吐量提升，确保在大规模分布式环境中稳健长期运行，作为InternData套件的生产骨干。

Conclusion: Nimbus框架解决了合成数据生成的碎片化问题，通过统一架构和优化技术实现了高效、稳定的跨领域数据合成，支持具身智能的规模化发展。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [13] [4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving](https://arxiv.org/abs/2601.21454)
*Shanliang Yao,Zhuoxiao Li,Runwei Guan,Kebin Cao,Meng Xia,Fuping Hu,Sen Xu,Yong Yue,Xiaohui Zhu,Weiping Ding,Ryan Wen Liu*

Main category: cs.RO

TL;DR: 4D-CAAL是一个统一的4D雷达-相机标定和自动标注框架，通过双用途标定靶设计和鲁棒的对应匹配算法，实现精确的外参标定，并利用标定关系将相机标注转移到雷达点云，显著减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在自动驾驶中日益重要，但现有雷达-相机标定方法通常使用分离的标定靶，难以建立对应关系；同时，手动标注稀疏雷达数据既费时又不可靠。需要一种统一的解决方案来简化标定并减少标注工作量。

Method: 提出双用途标定靶设计：正面采用棋盘格图案用于相机检测，背面中心放置角反射器用于雷达检测。开发鲁棒的对应匹配算法，将棋盘格中心与最强雷达反射点对齐，实现精确外参标定。随后设计自动标注流程，利用标定关系通过几何投影和多特征优化将相机分割标注转移到雷达点云。

Result: 实验表明，该方法实现了高精度的标定效果，同时显著减少了手动标注工作量，加速了自动驾驶鲁棒多模态感知系统的开发。

Conclusion: 4D-CAAL提供了一个有效的统一框架，解决了4D雷达-相机标定和自动标注的关键挑战，为自动驾驶多模态感知系统的开发提供了实用解决方案。

Abstract: 4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.

</details>


### [14] [IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation](https://arxiv.org/abs/2601.21506)
*Joonhee Lee,Hyunseung Shin,Jeonggil Ko*

Main category: cs.RO

TL;DR: IROS框架结合VLM级上下文推理与轻量感知模块，在低成本设备上实现实时室内导航，决策准确率提升，延迟降低66%


<details>
  <summary>Details</summary>
Motivation: 现有室内移动机器人导航方法难以同时满足快速响应和鲁棒语义理解需求。传统几何方法依赖详细地图且无法解释人类导向线索；VLA模型仅基于可见帧反应，无法预见未见的交叉口；VLM计算延迟高，不适合嵌入式平台实时操作

Method: 提出IROS实时导航框架，受双过程理论启发，将快速反射决策（系统一）与慢速审慎推理（系统二）分离，仅在必要时调用VLM。通过为紧凑VLM增强空间和文本线索，实现鲁棒、类人导航

Result: 在五个真实建筑中，IROS相比连续VLM导航提高了决策准确率，并将延迟降低了66%

Conclusion: IROS框架成功结合了VLM级上下文推理与轻量感知模块的效率，在低成本设备上实现了实时、鲁棒的室内导航，解决了现有方法在响应速度和语义理解之间的权衡问题

Abstract: Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.

</details>


### [15] [AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation](https://arxiv.org/abs/2601.21602)
*Jianli Sun,Bin Tian,Qiyao Zhang,Chengxiang Li,Zihan Song,Zhiyong Cui,Yisheng Lv,Yonglin Tian*

Main category: cs.RO

TL;DR: 提出了首个专门针对空中操作系统的视觉-语言-动作基准AIR-VLA，包含物理仿真环境和3000个手动遥操作演示数据集，系统评估了主流VLA和VLM模型在无人机移动、机械臂控制和高层规划方面的能力。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言-动作模型在地面实体智能中取得了显著成功，但在空中操作系统中的应用仍是一个未充分探索的领域。空中操作系统的浮动基座动力学、无人机与机械臂的强耦合性以及多步骤长时程的操作任务特性，对现有为静态或2D移动基座设计的VLA范式构成了严峻挑战。

Method: 构建了基于物理的仿真环境，发布了包含3000个手动遥操作演示的高质量多模态数据集，涵盖基座操作、物体与空间理解、语义推理和长时程规划。利用该平台系统评估了主流VLA模型和最先进的VLM模型。

Result: 实验不仅验证了将VLA范式迁移到空中系统的可行性，还通过针对空中任务的多维度指标揭示了当前模型在无人机移动性、机械臂控制和高层规划方面的能力边界。

Conclusion: AIR-VLA为通用空中机器人研究建立了标准化测试平台和数据基础，填补了视觉-语言-动作模型在空中操作系统应用领域的空白。

Abstract: While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.

</details>


### [16] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 本文提出声音触发的移动操作任务，让智能体主动感知声音源并与之交互，无需明确的行动指令，突破了传统指令驱动范式的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前移动操作研究主要遵循指令驱动范式，智能体依赖预定义的文本指令执行任务。这种设置将智能体限制在被动角色，限制了其自主性和对动态环境事件的反应能力。

Method: 开发了Habitat-Echo数据平台，集成了声学渲染与物理交互；提出了包含高级任务规划器和低级策略模型的基线方法，使智能体能够主动检测和响应听觉事件。

Result: 实验表明，所提基线方法使智能体能够主动检测和响应听觉事件，无需逐案指令。在具有挑战性的双声源场景中，智能体成功从重叠的声学干扰中分离出主要声源执行首次交互，随后继续操作次要对象。

Conclusion: 声音触发的移动操作能够增强智能体的自主性和环境适应能力，所提出的基线方法在复杂声学环境中表现出鲁棒性，为移动操作研究开辟了新的方向。

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [17] [CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation](https://arxiv.org/abs/2601.21712)
*Xuanran Zhai,Binkai Ou,Yemin Wang,Hui Yi Leong,Qiaojun Yu,Ce Hao,Yaohua Liu*

Main category: cs.RO

TL;DR: CoFreeVLA通过添加短时程自碰撞风险估计器来增强端到端VLA模型，减少双臂操作中的自碰撞风险，提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言动作（VLA）模型在双臂操作中存在安全隐患，因为模型未能充分建模双臂之间以及抓取物体之间的自碰撞风险。

Method: 在端到端VLA模型基础上增加短时程自碰撞风险估计器，该估计器从本体感知、视觉嵌入和计划动作中预测碰撞可能性。通过风险门控机制拦截危险指令，通过风险引导调整恢复安全状态，并利用风险信息优化策略训练。采用基于模型的碰撞标签进行预训练，并在真实机器人上通过rollout数据进行后训练校准。

Result: 在PiPER机器人手臂的五个双手任务中，CoFreeVLA相比RDT和APEX方法显著减少了自碰撞，提高了任务成功率。

Conclusion: CoFreeVLA通过集成自碰撞风险估计器有效提升了VLA模型在双臂操作中的安全性，为机器人安全操作提供了实用解决方案。

Abstract: Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.

</details>


### [18] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文质疑了布料操作中常见的端到端学习方法，提出了一种更高效、模块化的强化学习方案，显著减小了模型规模并缩短了训练时间，同时在仿真和真实世界转移中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 布料操作在机器人领域面临高维状态空间、复杂动力学和自遮挡等挑战。现有基于数据的方法通常依赖大型模型和长时间训练，计算成本高昂，阻碍了方法的发展和应用。此外，端到端学习方法虽然便于仿真到真实世界的转移，但使用高度信息损失的环境状态表示也带来了显著计算成本。

Method: 本文探索了一种高效、模块化的布料操作强化学习方法。通过精心设计选择，在仿真学习中显著减小了模型规模和训练时间。同时展示了如何将仿真训练的模型转移到真实世界。

Result: 在SoftGym基准测试中评估了该方法，相比现有基线方法，在任务性能上取得了显著提升，同时使用了明显更小的模型。

Conclusion: 通过质疑布料操作中常见的端到端学习设计选择，本文展示了一种更高效的模块化强化学习方法，能够显著降低计算成本，同时保持优异的性能，为布料操作策略的发展提供了新的方向。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [19] [Flocking behavior for dynamic and complex swarm structures](https://arxiv.org/abs/2601.21772)
*Carmen D. R. Pita-Romero,Pedro Arias-Perez,Miguel Fernandez-Cortizas,Rafael Perez-Segui,Pascual Campoy*

Main category: cs.RO

TL;DR: 提出基于虚拟质心概念的无人机集群编队算法，简化复杂结构形成与轨迹跟踪


<details>
  <summary>Details</summary>
Motivation: 多无人机保持复杂编队并实现复杂轨迹跟踪仍面临重大挑战，需要更简单有效的控制方法

Method: 基于虚拟质心概念设计集群行为算法，扩展经典虚拟行为方法，提供动态控制无人机数量和编队结构的理论框架

Result: 通过仿真测试和实际实验验证，算法即使在复杂编队和复杂轨迹下也表现出简单有效的特性

Conclusion: 提出的虚拟质心方法为无人机集群编队控制提供了简单有效的解决方案，能够处理复杂结构和轨迹

Abstract: Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.

</details>


### [20] [GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2601.21829)
*Bsher Karbouj,Baha Eddin Gaaloul,Jorg Kruger*

Main category: cs.RO

TL;DR: GAZELOAD是一个用于工业人机协作中脑力负荷估计的多模态数据集，包含26名参与者在与协作机器人交互时的眼动追踪数据、环境测量数据和任务上下文信息。


<details>
  <summary>Details</summary>
Motivation: 在工业人机协作场景中，准确估计操作员的脑力负荷对于优化协作效率和安全性至关重要。现有数据集往往缺乏多模态同步数据，特别是眼动信号与环境因素的结合，限制了在真实工业环境中开发脑力负荷估计算法的能力。

Method: 在实验室装配测试平台上，26名参与者佩戴Meta ARIA智能眼镜与两台协作机器人（UR5和Franka Emika Panda）交互。数据集时间同步了多种数据：眼动追踪信号（瞳孔直径、注视点、扫视、眼动轨迹、注视转移熵、注视分散指数）、环境实时连续测量（照度）、任务和机器人上下文（工作台、任务块、诱导故障），并在任务难度和环境条件的受控操作下收集数据。

Result: 为每个参与者和按脑力负荷分级的任务块提供了CSV文件，包含以250毫秒窗口聚合的眼动指标、环境日志以及1-10李克特量表的自我报告脑力负荷评分。数据按参与者特定文件夹组织，并附带文档说明。

Conclusion: GAZELOAD数据集可用于开发和基准测试工业人机协作场景中的脑力负荷估计算法、特征提取和时间建模，特别有助于研究光照等环境因素对基于眼动的脑力负荷标记物的影响。

Abstract: This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.

</details>


### [21] [LLM-Driven Scenario-Aware Planning for Autonomous Driving](https://arxiv.org/abs/2601.21876)
*He Li,Zhaowei Chen,Rui Gao,Guoliang Li,Qi Hao,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出LAP方法，利用大语言模型进行场景理解，通过联合优化模式配置和运动规划，在自动驾驶中实现高速驾驶效率和密集交通中安全操控的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有混合规划器切换框架在密集交通环境中难以实现可靠模式转换和持续高效驾驶，主要受限于启发式场景识别和低频控制更新。

Method: 提出LAP方法：利用大语言模型进行场景理解，将推理结果集成到模式配置和运动规划的联合优化中，采用树搜索模型预测控制和交替最小化求解。

Result: 高保真仿真结果显示，LAP在驾驶时间和成功率方面均优于其他基准方法。

Conclusion: LAP方法通过LLM驱动的自适应规划，成功解决了自动驾驶中高速驾驶效率与密集交通安全操控的平衡问题。

Abstract: Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.

</details>


### [22] [Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation](https://arxiv.org/abs/2601.21884)
*Pratik Ingle,Jørn Lambertsen,Kasper Støy,Andres Faina*

Main category: cs.RO

TL;DR: 多模块MANTA-RAY平台通过分布式模块化设计，在降低致动器密度的同时保持操作性能，实现可扩展的柔性物体操控


<details>
  <summary>Details</summary>
Motivation: 传统密集致动器阵列虽然能产生复杂变形，但带来高自由度、系统复杂和可扩展性受限的问题。现有研究局限于单模块四致动器配置，多模块配置的可行性和优势尚未探索

Method: 提出分布式、模块化、可扩展的MANTA-RAY平台变体，采用模块间物体传递策略和几何变换驱动的PID控制器，直接将倾斜角度控制输出映射到致动器命令，无需大量数据驱动或黑盒训练

Result: 在3x3和4x4模块配置的仿真中评估系统性能，并通过2x2硬件原型验证可行性。系统成功操控具有不同几何形状、质量和纹理的物体，包括鸡蛋、苹果等易碎物品，并支持并行操作

Conclusion: 多模块MANTA-RAY提高了可扩展性，能够在更大区域内协调操作多个物体，展现了实际应用的潜力

Abstract: Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.

</details>


### [23] [Information Filtering via Variational Regularization for Robot Manipulation](https://arxiv.org/abs/2601.21926)
*Jinhao Zhang,Wenlong Xia,Yaojia Wang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 该论文提出了一种名为变分正则化（VR）的轻量级模块，用于解决基于扩散的视觉运动策略中大型去噪解码器带来的中间特征冗余和噪声问题，通过在骨干特征上施加时间步条件高斯分布并应用KL散度正则化器，形成自适应信息瓶颈，显著提升了多个机器人技能学习基准的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视觉运动策略使用过大的去噪解码器，虽然增加模型容量可以改善去噪效果，但经验证据表明这会引入中间特征块的冗余和噪声。研究发现，在推理时随机掩码骨干特征（不改变训练）可以提高性能，这证实了中间特征中存在任务无关噪声。

Method: 提出变分正则化（VR）模块，对骨干特征施加时间步条件高斯分布，并应用KL散度正则化器，形成自适应信息瓶颈。该模块轻量级，通过正则化中间特征来减少冗余和噪声，同时保持模型容量。

Result: 在三个仿真基准（RoboTwin2.0、Adroit和MetaWorld）上的广泛实验表明，与基线DP3相比，该方法在RoboTwin2.0上成功率提高了6.1%，在Adroit和MetaWorld上提高了4.1%，达到了新的最先进结果。真实世界实验进一步证明该方法在实际部署中表现良好。

Conclusion: 变分正则化（VR）是一种有效的轻量级模块，能够减少基于扩散的视觉运动策略中中间特征的冗余和噪声，显著提升机器人技能学习性能，并在仿真和真实环境中都表现出色。

Abstract: Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.

</details>


### [24] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 提出一种用于手术机器人操作的监督混合专家架构，能在少量演示数据下学习复杂的长时程操作任务，并在离分布场景中表现出优越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人操作中已取得显著成功，但在手术机器人领域面临数据稀缺、工作空间受限以及需要极高安全性和可预测性等挑战。现有方法通常需要多摄像头设置或数千次演示，限制了实际应用。

Method: 提出监督混合专家架构，可添加到任何自主策略之上。该方法使用轻量级动作解码器策略（如ACT），仅需不到150次演示和立体内窥镜图像即可学习复杂长时程操作。在肠道抓取和牵拉任务中，机器人助手通过视觉线索理解外科医生意图，执行对可变形组织的精确抓取和持续牵拉。

Result: 通用VLA模型完全无法学习该任务；标准ACT在分布内条件下取得中等成功率；而采用监督MoE架构显著提升了性能，在分布内获得更高成功率，并在离分布场景（新抓取位置、光照减弱、部分遮挡）中表现出优越鲁棒性。该方法能泛化到未见过的测试视角，并零样本迁移到离体猪组织，无需额外训练。

Conclusion: 监督MoE架构为手术机器人模仿学习提供了有前景的路径，能够在少量数据下实现复杂操作，并具备良好的泛化能力和鲁棒性，为体内手术部署奠定了基础。初步体内猪手术策略展示验证了其实际应用潜力。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


### [25] [Macro-Scale Electrostatic Origami Motor](https://arxiv.org/abs/2601.21976)
*Alex S. Miller,Leo McElroy,Jeffrey H. Lang*

Main category: cs.RO

TL;DR: 开发了首个宏观尺度的可折叠折纸旋转电机，使用电晕放电产生扭矩，可折叠平放后展开工作


<details>
  <summary>Details</summary>
Motivation: 现有可折叠机器人的执行器要么嵌入线性执行器，要么附加非折叠旋转电机，且都只能产生线性或折叠运动，无法实现连续旋转运动。宏观尺度上尚未有可折叠的连续旋转执行器。

Method: 开发了基于折纸结构的旋转电机，采用电晕放电产生扭矩。电机可以折叠平放，然后展开工作。

Result: 原型电机实现了2.5:1的展开比，在-29 kV驱动下达到1440 rpm的最高转速，最大输出扭矩超过0.15 mN·m，主动部件扭矩密度为0.04 Nm/kg。

Conclusion: 成功开发了首个宏观尺度的可折叠折纸旋转电机，为可折叠机器人提供了连续旋转运动能力，具有高体积质量比和形状适应性优势。

Abstract: Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.

</details>


### [26] [PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy](https://arxiv.org/abs/2601.22018)
*Jinhao Zhang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Wenlong Xia,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: PocketDP3是一种轻量级3D扩散策略，用基于MLP-Mixer的Diffusion Mixer替换传统U-Net解码器，参数减少99%以上，支持两步推理，在多个仿真基准上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉扩散策略存在架构不匹配问题：高效的3D点云编码器与庞大的解码器配对，导致解码器参数浪费严重。作者认为紧凑的场景表示不需要如此庞大的解码器。

Method: 提出PocketDP3，用轻量级Diffusion Mixer（基于MLP-Mixer块）替换传统条件U-Net解码器，实现时间和通道维度的高效融合，显著减少模型大小。无需一致性蒸馏技术即可支持两步推理。

Result: 在RoboTwin2.0、Adroit和MetaWorld三个仿真基准上达到最先进性能，参数少于先前方法的1%，同时加速推理。真实世界实验验证了方法的实用性和可迁移性。

Conclusion: PocketDP3通过轻量级架构设计解决了3D扩散策略中的参数浪费问题，实现了高效、实用的机器人操作技能学习，为实时部署提供了可行方案。

Abstract: Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.

</details>


### [27] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab是一个轻量级开源机器人学习框架，结合GPU加速仿真、可组合环境和最小化设置复杂度，提供单命令安装和原生MuJoCo数据结构访问。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人学习框架中存在的设置复杂、依赖过多、仿真速度慢等问题，作者开发了一个轻量级、易于使用且高效的框架。

Method: 采用Isaac Lab引入的基于管理器的API，结合MuJoCo Warp进行GPU加速物理仿真，提供模块化构建块用于观测、奖励和事件组合。

Result: mjlab实现了单命令安装、最小化依赖、直接访问原生MuJoCo数据结构，并提供了速度跟踪、运动模仿和操作任务的参考实现。

Conclusion: mjlab是一个高效、易用的机器人学习框架，通过GPU加速仿真和模块化设计降低了机器人学习的研究门槛。

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>


### [28] [ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection](https://arxiv.org/abs/2601.22090)
*Runsheng Wang,Katelyn Lee,Xinyue Zhu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 提出健康到中风的适应管道，利用大规模健康人sEMG预训练模型，仅需少量中风患者特定数据进行微调，显著提升中风意图检测性能


<details>
  <summary>Details</summary>
Motivation: 表面肌电信号(sEMG)是中风后手部康复的潜在控制信号，但从中风瘫痪肌肉检测意图通常需要冗长的受试者特定校准，且对变异性敏感

Method: 健康到中风适应管道：首先在大型健康人sEMG数据集上预训练模型，然后仅使用少量中风患者特定数据进行微调。比较了三种适应策略：仅头部调谐、参数高效的LoRA适配器和完整端到端微调

Result: 健康预训练适应在所有条件下均优于零样本迁移和同等数据预算下的仅中风训练。最佳适应方法将平均过渡准确率从0.42提升至0.61，原始准确率从0.69提升至0.78

Conclusion: 转移可重复使用的健康域EMG表示可以减少校准负担，同时提高实时中风后意图检测的鲁棒性

Abstract: Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.

</details>


### [29] [DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation](https://arxiv.org/abs/2601.22153)
*Haozhe Xie,Beichen Wen,Jiarui Zheng,Zhaoxi Chen,Fangzhou Hong,Haiwen Diao,Ziwei Liu*

Main category: cs.RO

TL;DR: DynamicVLA是一个用于动态物体操作的视觉-语言-动作框架，通过紧凑模型架构、连续推理和潜在感知动作流式处理解决动态场景中的感知延迟问题，并创建了DOM基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在静态操作中表现良好，但在需要快速感知、时间预测和连续控制的动态场景中表现不佳，动态物体操作仍然是一个开放挑战。

Method: 1) 使用卷积视觉编码器的紧凑0.4B VLA模型实现空间高效编码；2) 连续推理实现重叠推理和执行以降低延迟；3) 潜在感知动作流式处理确保时间对齐的动作执行。同时创建了DOM基准数据集，包含合成和真实世界数据。

Result: 广泛评估显示在响应速度、感知和泛化能力方面有显著改进，DynamicVLA成为跨实现方式的通用动态物体操作统一框架。

Conclusion: DynamicVLA通过整合时间推理和闭环适应，解决了动态物体操作中的关键挑战，为动态场景下的机器人操作提供了有效的解决方案。

Abstract: Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.

</details>
