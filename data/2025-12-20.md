<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning](https://arxiv.org/abs/2512.16861)
*Zihan Zhou,Animesh Garg,Ajay Mandlekar,Caelan Garrett*

Main category: cs.RO

TL;DR: ReinforceGen是一个结合任务分解、数据生成、模仿学习和运动规划的机器人操作系统，通过强化学习微调提升长时程操作任务的性能


<details>
  <summary>Details</summary>
Motivation: 长时程操作一直是机器人领域的长期挑战，需要解决复杂任务的分解和执行问题

Method: 首先将任务分解为多个局部技能，通过运动规划连接；在10个人类演示生成的数据集上进行模仿学习训练技能和运动规划目标，然后通过在线适应和强化学习进行微调

Result: 在Robosuite数据集上，ReinforceGen在最高重置范围设置下达到80%的成功率；消融研究表明微调方法贡献了89%的平均性能提升

Conclusion: ReinforceGen通过结合任务分解、模仿学习和强化学习微调，有效解决了长时程机器人操作问题，显著提升了任务成功率

Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/

</details>


### [2] [PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies](https://arxiv.org/abs/2512.16881)
*Arhan Jain,Mingtong Zhang,Kanav Arora,William Chen,Marcel Torne,Muhammad Zubair Irshad,Sergey Zakharov,Yue Wang,Sergey Levine,Chelsea Finn,Wei-Chiu Ma,Dhruv Shah,Abhishek Gupta,Karl Pertsch*

Main category: cs.RO

TL;DR: PolaRiS是一个可扩展的真实到仿真框架，通过神经重建方法将真实世界场景视频转换为交互式仿真环境，用于机器人策略的高保真仿真评估。


<details>
  <summary>Details</summary>
Motivation: 机器人学习研究中准确测量和比较策略性能面临重大挑战，真实世界评估存在随机性、可重复性差和耗时等问题，而现有仿真基准与真实世界存在视觉和物理领域差距，无法可靠反映策略改进。

Method: PolaRiS利用神经重建方法将真实世界场景的短视频扫描转换为交互式仿真环境，并开发了简单的仿真数据协同训练方法，以弥合剩余的真实到仿真差距，实现未见仿真环境的零样本评估。

Result: 通过仿真与真实世界的大量配对评估，证明PolaRiS评估与真实世界通用策略性能的相关性远强于现有仿真基准，且其简单性支持快速创建多样化的仿真环境。

Conclusion: 这项工作为下一代机器人基础模型迈向了分布式和民主化评估的一步，通过真实到仿真框架解决了机器人策略评估的挑战。

Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.

</details>


### [3] [Sceniris: A Fast Procedural Scene Generation Framework](https://arxiv.org/abs/2512.16896)
*Jinghuan Shang,Harsh Patel,Ran Gong,Karl Schmeckpeper*

Main category: cs.RO

TL;DR: Sceniris是一个高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体，相比现有方法有显著速度提升


<details>
  <summary>Details</summary>
Motivation: 现有程序化生成方法输出吞吐量低，成为扩展数据集创建的重要瓶颈，需要更高效的场景生成框架

Method: 引入Sceniris框架，采用批量采样和cuRobo中的快速碰撞检查，优化了Scene Synthesizer的主要性能限制，并扩展了对象间空间关系支持

Result: 相比Scene Synthesizer实现了至少234倍的速度提升，并能生成大规模、无碰撞的场景变体，提供可选机器人可达性检查

Conclusion: Sceniris是一个高效的程序化场景生成框架，显著加速了大规模场景数据集的创建，支持物理AI和生成模型的发展

Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris

</details>
