<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation](https://arxiv.org/abs/2512.17062)
*Muhayy Ud Din,Jan Rosell,Waseem Akram,Irfan Hussain*

Main category: cs.RO

TL;DR: 该研究提出了一个统一管道，将基于LLM的符号规划器与Kautham运动规划框架连接，实现机器人无关的符号到几何操作，支持语言驱动的任务和运动规划。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型使机器人能够从自然语言生成符号计划，但在模拟中执行这些计划通常需要机器人特定的工程或规划器依赖的集成，缺乏通用解决方案。

Method: 开发了一个统一管道，将LLM符号规划器与Kautham运动规划框架连接。系统将语言指令转换为符号动作，然后使用Kautham的任何规划器计算和执行无碰撞轨迹，无需额外编码。

Result: 创建了一个灵活且可扩展的工具，支持语言驱动的任务和运动规划，能够跨机器人、规划模式和操作任务进行通用化处理。

Conclusion: 该系统实现了机器人无关的符号到几何操作，为语言驱动的TAMP提供了一个统一的解决方案，简化了机器人操作系统的开发流程。

Abstract: Simulation is essential for developing robotic manipulation systems, particularly for task and motion planning (TAMP), where symbolic reasoning interfaces with geometric, kinematic, and physics-based execution. Recent advances in Large Language Models (LLMs) enable robots to generate symbolic plans from natural language, yet executing these plans in simulation often requires robot-specific engineering or planner-dependent integration. In this work, we present a unified pipeline that connects an LLM-based symbolic planner with the Kautham motion planning framework to achieve generalizable, robot-agnostic symbolic-to-geometric manipulation. Kautham provides ROS-compatible support for a wide range of industrial manipulators and offers geometric, kinodynamic, physics-driven, and constraint-based motion planning under a single interface. Our system converts language instructions into symbolic actions and computes and executes collision-free trajectories using any of Kautham's planners without additional coding. The result is a flexible and scalable tool for language-driven TAMP that is generalized across robots, planning modalities, and manipulation tasks.

</details>


### [2] [Towards Senior-Robot Interaction: Reactive Robot Dog Gestures](https://arxiv.org/abs/2512.17136)
*Chunyang Meng,Eduardo B. Sandoval,Ricardo Sosa,Francisco Cruz*

Main category: cs.RO

TL;DR: 本文提出了一种面向老年人的四足机器人系统，通过手势和头部运动识别实现直观控制，并采用课程强化学习训练社交表达性动作，验证了框架的可行性和社交表现力。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口老龄化，许多老年人面临孤独问题。陪伴机器人提供了一种潜在解决方案，但现有陪伴机器人功能有限，而任务导向机器人又不适合社交互动，限制了老年人对机器人的接受度。

Method: 1. 输入方面：实现基于MediaPipe的手势和头部运动识别模块，无需遥控器即可控制机器人。2. 输出方面：在Isaac Gym中使用课程强化学习设计和训练机器人狗姿势，从简单站立逐步到三腿平衡和腿部伸展等复杂动作。3. 在Unitree机器人上验证关键社交姿势（抬爪动作）。

Result: 仿真测试平均成功率超过95%，在真实机器人上验证了抬爪社交姿势的可行性。真实世界测试证明了该框架的可行性和社交表现力，同时揭示了仿真到现实转换中的关节柔顺性、负载分布和平衡控制等挑战。

Conclusion: 这项工作推进了四足机器人作为老年人社交伴侣的实用化发展，为仿真到现实的适应提供了路径，并为未来用户研究奠定了基础。

Abstract: As the global population ages, many seniors face the problem of loneliness. Companion robots offer a potential solution. However, current companion robots often lack advanced functionality, while task-oriented robots are not designed for social interaction, limiting their suitability and acceptance by seniors. Our work introduces a senior-oriented system for quadruped robots that allows for more intuitive user input and provides more socially expressive output. For user input, we implemented a MediaPipe-based module for hand gesture and head movement recognition, enabling control without a remote. For output, we designed and trained robotic dog gestures using curriculum-based reinforcement learning in Isaac Gym, progressing from simple standing to three-legged balancing and leg extensions, and more. The final tests achieved over 95\% success on average in simulation, and we validated a key social gesture (the paw-lift) on a Unitree robot. Real-world tests demonstrated the feasibility and social expressiveness of this framework, while also revealing sim-to-real challenges in joint compliance, load distribution, and balance control. These contributions advance the development of practical quadruped robots as social companions for the senior and outline pathways for sim-to-real adaptation and inform future user studies.

</details>


### [3] [Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots](https://arxiv.org/abs/2512.17183)
*Gang Zhang*

Main category: cs.RO

TL;DR: 提出一个端到端框架，用于合成语义相关的伴随语音手势并在人形机器人上实时部署，实现自然、富有表现力的非语言交流。


<details>
  <summary>Details</summary>
Motivation: 解决机器人自然、富有表现力的非语言交流挑战，将先进的手势生成技术与稳健的物理控制相结合，实现语义感知的伴随语音手势实时部署。

Method: 1) 语义感知手势合成模块：利用基于大语言模型的生成式检索机制和自回归Motion-GPT模型，从语音输入生成富有表现力的参考动作；2) 高保真模仿学习控制策略MotionTracker：使机器人能够动态执行复杂动作并保持平衡；3) 通用运动重定向方法：弥合人类运动数据与机器人平台之间的体现差距。

Result: 综合评估表明，该系统能够生成语义恰当、节奏连贯的手势，并能被物理机器人准确跟踪和执行，实现了伴随语音手势的自动生成与实时物理部署的完整流程。

Conclusion: 这项工作代表了向通用现实世界应用迈出的重要一步，提供了在人形机器人上实现自动、语义感知的伴随语音手势生成和同步实时物理部署的完整流程。

Abstract: We present an innovative end-to-end framework for synthesizing semantically meaningful co-speech gestures and deploying them in real-time on a humanoid robot. This system addresses the challenge of creating natural, expressive non-verbal communication for robots by integrating advanced gesture generation techniques with robust physical control. Our core innovation lies in the meticulous integration of a semantics-aware gesture synthesis module, which derives expressive reference motions from speech input by leveraging a generative retrieval mechanism based on large language models (LLMs) and an autoregressive Motion-GPT model. This is coupled with a high-fidelity imitation learning control policy, the MotionTracker, which enables the Unitree G1 humanoid robot to execute these complex motions dynamically and maintain balance. To ensure feasibility, we employ a robust General Motion Retargeting (GMR) method to bridge the embodiment gap between human motion data and the robot platform. Through comprehensive evaluation, we demonstrate that our combined system produces semantically appropriate and rhythmically coherent gestures that are accurately tracked and executed by the physical robot. To our knowledge, this work represents a significant step toward general real-world use by providing a complete pipeline for automatic, semantic-aware, co-speech gesture generation and synchronized real-time physical deployment on a humanoid robot.

</details>


### [4] [Design and Research of a Self-Propelled Pipeline Robot Based on Force Analysis and Dynamic Simulation](https://arxiv.org/abs/2512.17212)
*Yan Gao,Jiliang Wang,Ming Cheng,Tianyun Huang*

Main category: cs.RO

TL;DR: 本文提出了一种基于力分析和动态仿真的自驱动管道机器人设计，重点解决垂直爬升失败和T型分支管道通过性差的核心挑战，为城市中低压燃气管道检测提供技术可行性参考。


<details>
  <summary>Details</summary>
Motivation: 传统有线管道检测机器人受电缆长度和重量限制严重，极大限制了其移动范围和可达性，需要开发不受电缆约束的自驱动解决方案。

Method: 采用轮式配置和模块化设计，首先使用SolidWorks完成机器人3D建模，然后导入ADAMS进行动态仿真，优化驱动模块和运动控制策略，最后搭建亚克力管道实验平台验证动态性能。

Result: 机器人通过调整身体姿态克服障碍和选择方向，能够稳定穿越各种复杂管道场景，特别是在垂直爬升和T型分支管道中表现出良好的通过性。

Conclusion: 该自驱动管道机器人设计为解决传统有线机器人的局限性提供了有效方案，为城市中低压燃气管道检测应用提供了技术可行性参考。

Abstract: In pipeline inspection, traditional tethered inspection robots are severely constrained by cable length and weight, which greatly limit their travel range and accessibility. To address these issues, this paper proposes a self-propelled pipeline robot design based on force analysis and dynamic simulation, with a specific focus on solving core challenges including vertical climbing failure and poor passability in T-branch pipes. Adopting a wheeled configuration and modular design, the robot prioritizes the core demand of body motion control. Specifically, 3D modeling of the robot was first completed using SolidWorks. Subsequently, the model was imported into ADAMS for dynamic simulation, which provided a basis for optimizing the drive module and motion control strategy.To verify the robot's dynamic performance, an experimental platform with acrylic pipes was constructed. Through adjusting its body posture to surmount obstacles and select directions, the robot has demonstrated its ability to stably traverse various complex pipeline scenarios. Notably, this work offers a technical feasibility reference for the application of pipeline robots in the inspection of medium and low-pressure urban gas pipelines.

</details>


### [5] [Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines](https://arxiv.org/abs/2512.17215)
*Yan Gao,Jiliang Wang,Minghan Wang,Xiaohua Chen,Demin Chen,Zhiyong Ren,Tian-Yun Huang*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩展卡尔曼滤波的管道机器人定位方法，通过IMU和轮式里程计融合解决复杂弯曲管道定位问题


<details>
  <summary>Details</summary>
Motivation: 现有管道定位方法依赖定位仪器，在复杂弯曲管道场景中常因电缆缠绕和设备灵活性不足而失效；传统视觉和激光建图方法在管道受限空间内易受光照条件和特征不足影响，导致建图漂移和发散问题

Method: 设计自驱式管道机器人，提出基于扩展卡尔曼滤波的定位方法：首先通过IMU获取初始姿态角，然后用EKF算法提高姿态角估计精度，最后结合轮式里程计实现高精度管道定位

Result: 在矩形环形管道中使用自驱式管道机器人进行实验，验证了所提出的航位推算算法的有效性；需要在机器人运动能力和定位精度之间取得平衡，避免滚轮过紧导致摩擦力过大影响运动控制灵活性

Conclusion: 基于惯性导航和轮式里程计融合的管道定位方法受管道环境因素影响较小，提出的EKF定位方法能有效解决复杂弯曲管道定位问题，为管道机器人自主定位提供了可行方案

Abstract: In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.

</details>


### [6] [A Service Robot's Guide to Interacting with Busy Customers](https://arxiv.org/abs/2512.17241)
*Suraj Nukala,Meera Sushma,Leimin Tian,Akansel Cosgun,Dana Kulic*

Main category: cs.RO

TL;DR: 研究比较服务机器人不同沟通模式（语音、视觉显示、微动作）在繁忙餐厅场景中吸引注意力和传达意图的效果，发现语音最有效吸引注意力，但视觉显示最清晰传达意图。


<details>
  <summary>Details</summary>
Motivation: 随着服务机器人在酒店业的广泛应用，需要了解如何与忙碌的顾客有效沟通。研究旨在探索不同沟通模式在吸引注意力和传达意图方面的效果，以优化服务机器人的沟通策略。

Method: 采用两部分用户研究（N=24），使用Temi机器人模拟餐厅送餐任务。参与者通过打字游戏（MonkeyType）模拟忙碌状态。第一部分比较非语言声音提示与基线条件在单杯送餐任务中的注意力吸引效果；第二部分评估语音、视觉显示、微动作及其多模态组合在两杯送餐任务中传达特定意图（正确选择杯子）的效果。

Result: 语音在吸引注意力方面效果显著，但在清晰传达意图方面效果较差。参与者认为视觉显示是传达意图最有效的模式，其次是语音，微动作排名最低。这些发现揭示了注意力吸引和意图传达在服务机器人沟通中的不同作用。

Conclusion: 研究为优化服务机器人沟通策略提供了重要见解，强调了在动态酒店环境中区分注意力吸引和意图传达的重要性。视觉显示在传达明确意图方面表现最佳，而语音在吸引注意力方面最有效，这有助于提升用户体验。

Abstract: The growing use of service robots in hospitality highlights the need to understand how to effectively communicate with pre-occupied customers. This study investigates the efficacy of commonly used communication modalities by service robots, namely, acoustic/speech, visual display, and micromotion gestures in capturing attention and communicating intention with a user in a simulated restaurant scenario. We conducted a two-part user study (N=24) using a Temi robot to simulate delivery tasks, with participants engaged in a typing game (MonkeyType) to emulate a state of busyness. The participants' engagement in the typing game is measured by words per minute (WPM) and typing accuracy. In Part 1, we compared non-verbal acoustic cue versus baseline conditions to assess attention capture during a single-cup delivery task. In Part 2, we evaluated the effectiveness of speech, visual display, micromotion and their multimodal combination in conveying specific intentions (correct cup selection) during a two-cup delivery task. The results indicate that, while speech is highly effective in capturing attention, it is less successful in clearly communicating intention. Participants rated visual as the most effective modality for intention clarity, followed by speech, with micromotion being the lowest ranked.These findings provide insights into optimizing communication strategies for service robots, highlighting the distinct roles of attention capture and intention communication in enhancing user experience in dynamic hospitality settings.

</details>


### [7] [RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models](https://arxiv.org/abs/2512.17309)
*Asil Kaan Bozcuoglu,Ziyuan Liu*

Main category: cs.RO

TL;DR: RecipeMasterLLM：基于LLM的机器人动作本体自动生成系统，利用微调LLM根据用户提示生成符合RoboEarth标准化知识图的OWL动作本体，通过RAG增强环境知识理解。


<details>
  <summary>Details</summary>
Motivation: 传统RoboEarth知识图谱需要工程师手动创建RDF三元组和OWL本体，知识获取过程繁琐。随着大语言模型的发展，可以显著自动化知识获取过程，提高机器人知识共享效率。

Method: 提出RecipeMasterLLM高层规划器，使用专门针对RoboEarth标准化知识图微调的LLM，根据用户提示生成OWL动作本体。采用检索增强生成技术，在RAG阶段提供环境知识以增强上下文理解和生成准确性。

Result: 系统能够自动生成符合RoboEarth标准化知识图的动作描述，相比传统手工创建方法，显著提高了知识获取的自动化程度和效率。

Conclusion: LLM技术可以有效自动化机器人知识获取过程，RecipeMasterLLM为云机器人知识共享提供了更高效、自动化的解决方案，推动了机器人知识获取从手工创建向AI驱动的转变。

Abstract: RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.

</details>


### [8] [Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks](https://arxiv.org/abs/2512.17321)
*Momina Liaqat Ali,Muhammad Abid*

Main category: cs.RO

TL;DR: 本文提出了一种神经符号控制框架，将语言模型的高层语义推理与神经控制器的低层连续运动执行分离，解决了LLM在连续控制中的不稳定、收敛慢和动作幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在具身系统的语言条件控制中显示出潜力，但其在连续控制应用中存在不稳定、收敛慢和产生幻觉动作等问题，限制了直接应用。

Method: 提出模块化神经符号控制框架：轻量级神经增量控制器在连续空间执行有界增量动作，本地部署的LLM解释符号任务。框架将LLM限制为符号输出，将未解释的执行分配给在人工几何数据上训练的神经控制器。

Result: 在平面操作任务中，神经符号集成相比纯LLM基线显著提升成功率和效率：平均步骤减少超过70%，速度提升最高达8.83倍，且对语言模型质量保持鲁棒性。

Conclusion: 神经符号分解为语言理解与连续控制集成提供了可扩展的原则性方法，无需强化学习或昂贵推演，增强了可解释性、稳定性和泛化能力，促进了可靠有效的语言引导具身系统开发。

Abstract: Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.

</details>


### [9] [Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation](https://arxiv.org/abs/2512.17349)
*Xijie Huang,Jinhan Li,Tianyue Wu,Xin Zhou,Zhichao Han,Fei Gao*

Main category: cs.RO

TL;DR: 提出结合3D高斯泼溅与对抗域适应的框架，使仅使用单目RGB图像的飞行机器人能在杂乱环境中实现零样本真实世界导航


<details>
  <summary>Details</summary>
Motivation: 现有自主导航系统主要依赖激光雷达和深度相机，但能否仅用单目RGB图像实现飞行机器人在杂乱环境中的导航仍是一个基本问题。由于真实世界数据收集成本高昂，在仿真中学习策略是可行路径，但存在显著的仿真到真实感知差距

Method: 提出结合3D高斯泼溅（3DGS）环境的光真实感与对抗域适应的框架。在高保真仿真中训练策略，同时通过对抗域适应显式最小化特征差异，确保策略依赖领域不变的特征线索

Result: 实验结果表明，该方法能够实现到物理世界的鲁棒零样本迁移，使飞行机器人能够在具有不同光照条件的非结构化环境中实现安全和敏捷的飞行

Conclusion: 该框架成功解决了单目RGB图像导航中的仿真到真实迁移问题，证明了仅使用RGB图像实现飞行机器人在杂乱环境中导航的可行性，为低成本自主导航系统提供了新途径

Abstract: Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.

</details>


### [10] [TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data](https://arxiv.org/abs/2512.17370)
*Deqing Liu,Yinfeng Gao,Deheng Qian,Qichao Zhang,Xiaoqing Ye,Junyu Han,Yupeng Zheng,Xueyi Liu,Zhongpu Xia,Dawei Ding,Yifeng Pan,Dongbin Zhao*

Main category: cs.RO

TL;DR: TakeAD是一个基于偏好的后优化框架，通过专家接管数据微调预训练的模仿学习策略，以提升闭环驾驶性能并减少系统脱离。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法依赖模仿学习，但面临开环训练与闭环部署之间的不对齐问题，导致驾驶员接管和系统脱离。如何利用脱离场景中的专家接管数据扩展IL策略能力是一个有价值但未探索的挑战。

Method: 提出TakeAD框架：1) 设计受真实自动驾驶系统人类接管机制启发的专家接管数据收集流程；2) 集成迭代DAgger进行模仿学习和DPO进行偏好对齐的后优化框架。DAgger阶段通过直接模仿专家干预使策略具备处理脱离状态的基本能力，DPO阶段则优化策略行为以更好地对齐专家在脱离场景中的偏好。

Result: 在闭环Bench2Drive基准测试中，该方法相比纯IL方法表现出有效性，综合消融实验确认了每个组件的贡献。

Conclusion: TakeAD通过迭代学习脱离状态的恢复策略，有效缓解了开环差距，提升了闭环驾驶性能。

Abstract: Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.

</details>


### [11] [Personalized Gait Patterns During Exoskeleton-Aided Training May Have Minimal Effect on User Experience. Insights from a Pilot Study](https://arxiv.org/abs/2512.17425)
*Beatrice Luciani,Katherine Lin Poggensee,Heike Vallery,Alex van den Berg,Severin David Woernle,Mostafa Mogharabi,Stefano Dalla Gasperina,Laura Marchal-Crespo*

Main category: cs.RO

TL;DR: 开发了一个数据驱动的步态个性化框架，用于支持多平面运动的康复外骨骼，但实验发现个性化轨迹与标准轨迹在舒适度和自然度上没有显著差异，用户适应过程更为重要。


<details>
  <summary>Details</summary>
Motivation: 现有康复外骨骼大多依赖预录制、非个性化的步态轨迹，且局限于矢状面运动，限制了运动的自然性和用户舒适度。需要开发支持多平面运动并能提供个性化步态轨迹的解决方案。

Method: 提出了一个数据驱动的步态个性化框架，使用回归模型基于人体测量学、人口统计学和步行速度数据从规范数据库中生成个性化轨迹。在10名健康参与者中进行受试者内实验，比较个性化轨迹与两种标准模式（平均轨迹和随机选择轨迹）。

Result: 尽管所有轨迹都能通过刚性位置导数控制器高精度执行，但不同模式条件之间没有发现显著差异。然而，后期试验中的模式条件比第一次试验被评为更舒适和自然，表明参与者可能适应了在外骨骼中行走，与强制执行的步态模式无关。

Conclusion: 研究强调了在设计个性化步态控制器时整合主观反馈的重要性，以及在实验过程中考虑用户适应性的必要性。用户对外骨骼的适应过程可能比轨迹个性化本身对舒适度和自然度感知有更大影响。

Abstract: Robot-aided gait rehabilitation facilitates high-intensity and repeatable therapy. However, most exoskeletons rely on pre-recorded, non-personalized gait trajectories constrained to the sagittal plane, potentially limiting movement naturalness and user comfort. We present a data-driven gait personalization framework for an exoskeleton that supports multi-planar motion, including hip abduction/adduction and pelvic translation and rotation. Personalized trajectories to individual participants were generated using regression models trained on anthropometric, demographic, and walking speed data from a normative database. In a within-subject experiment involving ten unimpaired participants, these personalized trajectories were evaluated in regard to comfort, naturalness, and overall experience and compared against two standard patterns from the same database: one averaging all the trajectories, and one randomly selected. We did not find relevant differences across pattern conditions, despite all trajectories being executed with high accuracy thanks to a stiff position-derivative controller. We found, however, that pattern conditions in later trials were rated as more comfortable and natural than those in the first trial, suggesting that participants might have adapted to walking within the exoskeleton, regardless of the enforced gait pattern. Our findings highlight the importance of integrating subjective feedback when designing personalized gait controllers and accounting for user adaptation during experimentation.

</details>


### [12] [ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination](https://arxiv.org/abs/2512.17435)
*Teng Wang,Xinxin Zhao,Wenzhe Cai,Changyin Sun*

Main category: cs.RO

TL;DR: ImagineNav++：基于视觉语言模型的无地图视觉导航框架，通过想象未来观测图像和选择性注视记忆机制，将导航规划转化为简单的"最佳视角图像选择"问题，在开放词汇对象导航任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的导航方法受限于文本表示，无法充分捕捉空间占用和场景几何信息，而这些信息对导航决策至关重要。因此需要探索视觉语言模型（VLM）能否仅使用机载RGB/RGB-D流实现无地图视觉导航。

Method: 提出ImagineNav++框架：1）未来视角想象模块，通过蒸馏人类导航偏好生成具有高探索潜力的语义化视点；2）将导航规划转化为VLM的最佳视角图像选择问题；3）选择性注视记忆机制，通过稀疏到密集的层次化集成构建紧凑而全面的长期空间记忆；4）将目标导向导航转化为一系列可处理的点目标导航任务。

Result: 在开放词汇对象和实例导航基准测试中，ImagineNav++在无地图设置下达到了最先进的性能，甚至超越了大多数基于地图的方法，突显了场景想象和记忆在VLM空间推理中的重要性。

Conclusion: 研究表明视觉语言模型能够通过想象未来观测和选择性记忆机制实现高效的无地图视觉导航，为自主家庭辅助机器人提供了新的导航范式，证明了场景想象和空间记忆在VLM空间推理中的关键作用。

Abstract: Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.

</details>


### [13] [Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry](https://arxiv.org/abs/2512.17505)
*Ufuk Asil,Efendi Nasibov*

Main category: cs.RO

TL;DR: 提出了一种用于无人机的混合视觉惯性里程计方法，结合误差状态EKF和UKF，通过动态传感器可靠性评估提升复杂环境下的姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂环境中面临环境挑战和传感器可靠性变化的问题，需要一种既能保持计算效率又能提供准确姿态估计的解决方案。

Method: 采用松耦合传感器融合架构，提出混合四元数误差状态EKF/UKF方法：先用误差状态EKF传播整个状态，再用缩放UKF专门优化姿态估计；通过图像熵、强度变化、运动模糊等指标动态评估视觉测量可靠性。

Result: 在EuRoC MAV数据集上测试显示：在挑战性场景中位置精度平均提升49%，旋转精度比ESKF方法平均提升57%，计算成本比完整SUKF实现降低约48%同时保持可比精度。

Conclusion: 该方法在计算效率和估计精度之间取得了有效平衡，显著提升了无人机在传感器可靠性变化的复杂环境中的姿态估计性能。

Abstract: This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.

</details>


### [14] [Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests](https://arxiv.org/abs/2512.17553)
*Guglielmo Del Col,Väinö Karjalainen,Teemu Hakala,Yibo Zhang,Eija Honkavaara*

Main category: cs.RO

TL;DR: 提出改进的深度学习导航框架，集成语义增强深度编码与神经运动基元评估，用于密集森林环境中的无人机自主导航，通过多个模块优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集自然环境中无人机自主导航的挑战，包括能见度有限、细薄不规则障碍物、GNSS信号缺失以及频繁的感知退化问题。

Method: 在原始sevae-ORACLE算法基础上集成多个模块：语义增强深度编码、神经运动基元评估、横向控制、时间一致性机制、立体视觉惯性里程计、安全监督层、深度细化阶段和GPU优化。

Result: 相比现有学习方法，在相同环境条件和硬件约束下，展示了更高的成功率、更稳定的轨迹和更好的碰撞避免能力；在三种北方森林环境中，中等和密集植被条件下所有飞行均成功完成，高度密集灌木丛中15次飞行成功12次。

Conclusion: 该导航框架在复杂自然环境中相比现有方法具有更高的可靠性和安全性，实现了密集森林环境中的稳健自主飞行。

Abstract: Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.
  The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.

</details>


### [15] [Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration](https://arxiv.org/abs/2512.17560)
*M. Faroni,A. Spano,A. M. Zanchettin,P. Rocco*

Main category: cs.RO

TL;DR: 提出一种安全感知方法，通过深度学习模型学习系统状态与安全减速之间的关系，优化任务选择以减少协作机器人循环时间


<details>
  <summary>Details</summary>
Motivation: 传统安全措施在频繁人机交互时会增加机器人循环时间，影响效率，需要在不假设安全逻辑先验知识的情况下减少效率损失

Method: 使用深度学习模型基于执行数据学习系统状态与安全减速之间的关系，不显式预测人体运动，直接建模交互对机器人速度的影响，运行时优化任务选择

Result: 在拾取包装场景实验中，循环时间显著减少

Conclusion: 该方法简化了实现，增强了对不同安全逻辑的泛化能力，能在遵守安全要求的同时最小化循环时间

Abstract: Ensuring human safety in collaborative robotics can compromise efficiency because traditional safety measures increase robot cycle time when human interaction is frequent. This paper proposes a safety-aware approach to mitigate efficiency losses without assuming prior knowledge of safety logic. Using a deep-learning model, the robot learns the relationship between system state and safety-induced speed reductions based on execution data. Our framework does not explicitly predict human motions but directly models the interaction effects on robot speed, simplifying implementation and enhancing generalizability to different safety logics. At runtime, the learned model optimizes task selection to minimize cycle time while adhering to safety requirements. Experiments on a pick-and-packaging scenario demonstrated significant reductions in cycle times.

</details>


### [16] [Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation](https://arxiv.org/abs/2512.17568)
*Kangchen Lv,Mingrui Yu,Yongyi Jia,Chenyu Zhang,Xiang Li*

Main category: cs.RO

TL;DR: 提出一种基于3D空间一致表示的运动学感知模仿学习框架，用于机械臂全身控制，通过点云表示和扩散策略提高样本效率和空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的关节空间策略学习存在关节空间与任务空间不对齐的问题，增加了策略学习的复杂性，需要内在理解非线性手臂运动学，难以从有限演示中学习。

Method: 提出运动学感知模仿学习框架，使用3D空间中的点集表示机器人状态和动作，与3D点云观测自然对齐；基于扩散策略，在扩散过程中融入运动学先验保证输出动作的运动学可行性；最后通过优化式全身逆运动学求解器计算关节角度命令。

Result: 仿真和真实世界实验结果表明，相比现有方法，该方法在身体感知操作策略学习中具有更高的成功率和更强的空间泛化能力。

Conclusion: 通过3D空间一致表示和运动学感知的扩散策略，有效解决了全身控制中关节空间与任务空间不对齐的问题，提高了策略学习的样本效率和空间泛化性能。

Abstract: Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation policy learning.

</details>


### [17] [Optimized Scheduling and Positioning of Mobile Manipulators in Collaborative Applications](https://arxiv.org/abs/2512.17584)
*Christian Cella,Sole Ester Sonnino,Marco Faroni,Andrea Zanchettin,Paolo Rocco*

Main category: cs.RO

TL;DR: 提出基于数字模型的移动机械臂优化框架，使用粒子群算法协调路径规划与任务调度，提升人机协作装箱场景的效率


<details>
  <summary>Details</summary>
Motivation: 移动机器人在共享工作空间中的集成日益增多，需要高效的路径规划和协调机制，同时兼顾安全性和生产效率

Method: 采用基于数字模型的优化框架，将完整问题视为黑盒，使用粒子群优化算法平衡冲突的关键性能指标，确定机器人基座姿态序列和任务调度

Result: 在协作装箱场景中展示了在周期时间、任务排序和适应人类存在方面的改进

Conclusion: 提出的数字模型优化框架能有效协调移动机械臂在人机协作环境中的路径规划和任务调度，提升整体系统性能

Abstract: The growing integration of mobile robots in shared workspaces requires efficient path planning and coordination between the agents, accounting for safety and productivity. In this work, we propose a digital model-based optimization framework for mobile manipulators in human-robot collaborative environments, in order to determine the sequence of robot base poses and the task scheduling for the robot. The complete problem is treated as black-box, and Particle Swarm Optimization (PSO) is employed to balance conflicting Key-Performance Indicators (KPIs). We demonstrate improvements in cycle time, task sequencing, and adaptation to human presence in a collaborative box-packing scenario.

</details>


### [18] [Vidarc: Embodied Video Diffusion Model for Closed-loop Control](https://arxiv.org/abs/2512.17661)
*Yao Feng,Chendong Xiang,Xinyi Mao,Hengkai Tan,Zuyue Zhang,Shuhe Huang,Kaiwen Zheng,Haitian Liu,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: Vidarc是一种基于视频扩散的机器人控制方法，通过掩码逆动力学模型和自回归生成实现快速闭环控制，在数据稀缺环境下显著提升机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺环境下，机器人手臂操作面临复杂动力学和多样化场景的挑战。现有基于视频的方法虽然能捕捉时空交互，但通常未针对特定机器人的闭环控制进行优化，存在延迟高和基础不牢的问题。

Method: 提出Vidarc方法：1）基于视频扩散的自回归体现控制框架；2）使用掩码逆动力学模型，通过动作相关掩码来基础化视频预测；3）通过缓存自回归生成实现实时反馈；4）在100万跨体现片段上进行预训练。

Result: Vidarc在真实世界部署中比最先进基线至少提高15%的成功率，延迟降低91%。同时展现出对未见机器人平台的强大泛化能力和错误纠正能力。

Conclusion: Vidarc通过结合视频扩散、掩码逆动力学和自回归生成，实现了快速、准确的闭环机器人控制，在数据稀缺环境下表现出卓越的性能和泛化能力。

Abstract: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

</details>


### [19] [A Dual Quaternion based RRT* Path Planning Approach for Satellite Rendezvous and Docking](https://arxiv.org/abs/2512.17680)
*Ana Stankovic,Mohamed Khalil Ben-Larbi,Wolfgang H. Müller*

Main category: cs.RO

TL;DR: 提出基于对偶四元数的RRT*运动规划器，用于生成卫星交会对接的平滑六自由度位姿轨迹，在避障约束下实现自然螺旋运动插值。


<details>
  <summary>Details</summary>
Motivation: 卫星交会对接需要生成平滑的六自由度位姿轨迹，同时满足避障约束。传统方法将平移和旋转分开处理，难以保证位姿连续性，需要更自然的SE(3)空间运动表示方法。

Method: 将对偶四元数代数直接集成到RRT*框架中，利用对偶四元数在SE(3)空间中实现自然螺旋运动插值，生成平滑的六自由度位姿轨迹。

Result: 与使用分离平移和四元数转向的标准RRT*相比，该方法在Python实现中表现出更好的位姿连续性和避障能力，成功应用于多障碍物场景。

Conclusion: 该方法为纯运动学方法，不考虑相对轨道动力学，生成的路径可作为后续基于优化的轨迹规划器的初步估计，为实际卫星交会对接任务提供基础。

Abstract: This paper proposes a sampling-based motion planner that employs a dual quaternion representation to generate smooth, collision-free six-degree-of-freedom pose trajectories for satellite rendezvous and docking under keep-out zone constraints. The proposed planner integrates the dual quaternion algebra directly into an RRT* framework, thereby enabling natural screw motion interpolation in SE(3). The dual quaternion-based RRT* has been implemented in Python and demonstrated on a representative multi-obstacle scenario. A comparison with a standard RRT* using separate translation and quaternion steering highlights the enhanced pose continuity and obstacle avoidance of the proposed method. The present approach is purely kinematic in nature and does not take into account relative orbital dynamics. Consequently, the resulting path provides a preliminary estimate for a subsequent optimisation-based trajectory planner, which will refine the motion with dynamic constraints for the purpose of practical satellite rendezvous and docking missions.

</details>


### [20] [UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation](https://arxiv.org/abs/2512.17764)
*Kangchen Lv,Mingrui Yu,Shihefeng Wang,Xiangyang Ji,Xiang Li*

Main category: cs.RO

TL;DR: UniStateDLO：首个基于深度学习的完整可变形线性物体感知管道，使用扩散模型处理严重遮挡下的状态估计与跟踪，仅需合成数据训练即可实现零样本仿真到真实迁移。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法对遮挡高度敏感，在受限操作环境中面临自遮挡、多物体遮挡等挑战，且高维状态空间、缺乏视觉特征和传感器噪声进一步增加了可靠感知的难度。

Method: 将单帧状态估计和跨帧状态跟踪都构建为条件生成问题，利用扩散模型强大的映射能力，从部分点云到高维DLO状态。仅使用大规模合成数据集训练，无需真实世界数据。

Result: 在仿真和真实实验中均优于所有最先进基线，即使在严重遮挡下也能实时生成全局平滑且局部精确的DLO状态预测，支持闭环DLO操作系统的稳定反馈控制。

Conclusion: UniStateDLO是首个完整的深度学习DLO感知管道，能有效处理多种遮挡模式，具有强大的数据效率和零样本仿真到真实迁移能力，为复杂受限环境中的DLO操作提供了可靠的前端模块。

Abstract: Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.

</details>


### [21] [Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes](https://arxiv.org/abs/2512.17846)
*Carlos Vélez García,Miguel Cazorla,Jorge Pomares*

Main category: cs.RO

TL;DR: PaD是一个离线目标条件强化学习框架，通过验证驱动轨迹合成，学习目标条件能量函数，使用梯度下降进行规划，在训练和推理时使用相同计算减少不匹配。


<details>
  <summary>Details</summary>
Motivation: 解决离线目标条件强化学习中训练-测试不匹配的问题，传统解耦建模管道中训练和推理计算不同，导致性能下降。希望通过验证驱动的规划提供更稳健的替代方案。

Method: 学习目标条件能量函数，为可行且目标一致的未来分配低能量。规划通过在该能量景观中进行梯度优化实现，训练和推理使用相同计算。通过自监督后视目标重标记训练，在规划动态周围塑造能量景观。推理时在不同时间假设下优化多个轨迹候选，选择平衡可行性和效率的低能量计划。

Result: 在OGBench立方体操作任务上评估。在狭窄专家演示上训练时达到95%成功率，显著优于先前68%的方法。在噪声次优数据上训练进一步提高了成功率和计划效率。

Conclusion: 学习评估和优化轨迹为离线、无奖励规划提供了比直接策略学习更稳健的替代方案，验证驱动的规划具有显著优势。

Abstract: We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.
  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.
  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\% success, strongly outperforming prior methods that peak at 68\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.

</details>
