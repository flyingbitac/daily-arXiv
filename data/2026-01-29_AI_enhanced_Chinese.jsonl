{"id": "2601.19969", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19969", "abs": "https://arxiv.org/abs/2601.19969", "authors": ["Haoyuan Deng", "Yuanjiang Xue", "Haoyang Du", "Boyang Zhou", "Zhenyu Wu", "Ziwei Wang"], "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning", "comment": "Project page: https://e2hil.github.io/", "summary": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\method\u7684\u6837\u672c\u9ad8\u6548\u4eba\u673a\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u6837\u672c\u6765\u51cf\u5c11\u4eba\u7c7b\u5e72\u9884\u9700\u6c42\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u4f4e\u7684\u4eba\u5de5\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6837\u672c\u6548\u7387\u4f4e\uff0c\u9700\u8981\u5927\u91cf\u4eba\u7c7b\u5e72\u9884\u624d\u80fd\u6536\u655b\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u52b3\u52a8\u529b\u6210\u672c\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u6765\u51cf\u5c11\u4eba\u7c7b\u5e72\u9884\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7b56\u7565\u71b5\u7684\u7a33\u5b9a\u51cf\u5c11\u6765\u6539\u5584\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\u3002\u9996\u5148\u6784\u5efa\u4e0d\u540c\u6837\u672c\u5bf9\u7b56\u7565\u71b5\u7684\u5f71\u54cd\u51fd\u6570\uff0c\u901a\u8fc7\u52a8\u4f5c\u6982\u7387\u548c\u7b56\u7565\u8f6f\u4f18\u52bf\u7684\u534f\u65b9\u5dee\u9ad8\u6548\u4f30\u8ba1\u3002\u7136\u540e\u9009\u62e9\u5f71\u54cd\u51fd\u6570\u503c\u9002\u4e2d\u7684\u6837\u672c\uff0c\u5254\u9664\u5bfc\u81f4\u71b5\u6025\u5267\u4e0b\u964d\u7684\u6377\u5f84\u6837\u672c\u548c\u5f71\u54cd\u53ef\u5ffd\u7565\u7684\u566a\u58f0\u6837\u672c\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\\method\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u4eba\u673a\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e8642.1%\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u9700\u898110.1%\u66f4\u5c11\u7684\u4eba\u7c7b\u5e72\u9884\u3002", "conclusion": "\\method\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u6837\u672c\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u4eba\u673a\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u4eba\u7c7b\u5e72\u9884\u9700\u6c42\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.19972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19972", "abs": "https://arxiv.org/abs/2601.19972", "authors": ["Kuanqi Cai", "Liding Zhang", "Xinwen Su", "Kejia Chen", "Chaoqun Wang", "Sami Haddadin", "Alois Knoll", "Arash Ajoudani", "Luis Figueredo"], "title": "Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning", "comment": null, "summary": "In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes \"Just-in-Time Edge,\" which dynamically refines edge connectivity, and \"Just-in-Time Sample,\" which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\\mathbb{R}^4$ to $\\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.", "AI": {"tldr": "JIT*\u7b97\u6cd5\u901a\u8fc7\u5373\u65f6\u6a21\u5757\u548c\u8fd0\u52a8\u6027\u80fd\u6a21\u5757\u6539\u8fdb\u9ad8\u7ef4\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u6bd4\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u8868\u73b0\u66f4\u597d", "motivation": "\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5728\u9ad8\u7ef4\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u96be\u4ee5\u5728\u590d\u6742\u591a\u969c\u788d\u73af\u5883\u4e2d\u9ad8\u6548\u627e\u5230\u53ef\u884c\u4e14\u6700\u4f18\u7684\u8def\u5f84\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u8fd0\u52a8\u5947\u70b9\u548c\u81ea\u78b0\u649e\u98ce\u9669\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86\u8fd0\u52a8\u6548\u7387\u548c\u5b89\u5168\u6027\u6311\u6218", "method": "\u63d0\u51faJIT*\u7b97\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u5373\u65f6\u6a21\u5757\uff08\u5305\u62ec\u52a8\u6001\u4f18\u5316\u8fb9\u8fde\u63a5\u7684\"\u5373\u65f6\u8fb9\"\u548c\u8c03\u6574\u74f6\u9888\u533a\u57df\u91c7\u6837\u5bc6\u5ea6\u7684\"\u5373\u65f6\u91c7\u6837\"\uff09\u548c\u8fd0\u52a8\u6027\u80fd\u6a21\u5757\uff08\u901a\u8fc7\u52a8\u6001\u5207\u6362\u5e73\u8861\u53ef\u64cd\u4f5c\u6027\u548c\u8f68\u8ff9\u6210\u672c\uff09", "result": "JIT*\u5728R^4\u5230R^16\u7ef4\u5ea6\u4e0a\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5728\u5355\u81c2\u548c\u53cc\u81c2\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027", "conclusion": "JIT*\u7b97\u6cd5\u901a\u8fc7\u5373\u65f6\u4f18\u5316\u548c\u8fd0\u52a8\u6027\u80fd\u5e73\u8861\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u95ee\u9898"}}
{"id": "2601.20149", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20149", "abs": "https://arxiv.org/abs/2601.20149", "authors": ["Muzaffar Qureshi", "Tochukwu Elijah Ogri", "Kyle Volle", "Rushikesh Kamalapurkar"], "title": "A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes", "comment": null, "summary": "Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u96c5\u53ef\u6bd4\u548c\u6d77\u68ee\u77e9\u9635\uff0c\u5229\u7528\u6d4b\u91cf\u4f4d\u7f6e\u8bef\u5dee\u6570\u636e\u8fdb\u884c\u4e8c\u9636\u6821\u6b63\uff0c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u8fdb\u884c\u6807\u91cf\u573a\u6620\u5c04\u65f6\uff0c\u4e0d\u5b8c\u7f8e\u7684\u5b9a\u4f4d\u4f1a\u5f15\u5165\u72b6\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u6d4b\u91cf\u4f4d\u7f6e\u4f30\u8ba1\u8bef\u5dee\uff0c\u4ece\u800c\u964d\u4f4e\u9ad8\u65af\u8fc7\u7a0b\u7684\u5747\u503c\u548c\u534f\u65b9\u5dee\u4f30\u8ba1\u7cbe\u5ea6", "method": "\u5229\u7528\u6838\u51fd\u6570\u7684\u53ef\u5fae\u6027\uff0c\u5f00\u53d1\u4e8c\u9636\u6821\u6b63\u7b97\u6cd5\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684GP\u5747\u503c\u548c\u534f\u65b9\u5dee\u51fd\u6570\u7684\u96c5\u53ef\u6bd4\u548c\u6d77\u68ee\u77e9\u9635\uff0c\u57fa\u4e8e\u6d4b\u91cf\u4f4d\u7f6e\u8bef\u5dee\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u7cbe\u70bc", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5b8c\u6574\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u79fb\u52a8\u673a\u5668\u4eba\u6807\u91cf\u573a\u6620\u5c04\u4e2d\u7684\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u66f4\u65b0\u65b9\u6848"}}
{"id": "2601.20208", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20208", "abs": "https://arxiv.org/abs/2601.20208", "authors": ["Wanjun Jia", "Kang Li", "Fan Yang", "Mengfei Duan", "Wenrui Chen", "Yiming Jiang", "Hui Zhang", "Kailun Yang", "Zhiyong Li", "Yaonan Wang"], "title": "TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement", "comment": "The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER", "summary": "The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.", "AI": {"tldr": "TRACER\u6846\u67b6\u901a\u8fc7\u7eb9\u7406\u9c81\u68d2\u7684\u89c6\u89c9\u63a8\u7406\u94fe\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7ec6\u5316\uff0c\u89e3\u51b3\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u4e2d\u8bed\u4e49\u6307\u4ee4\u4e0e\u7269\u7406\u4ea4\u4e92\u70b9\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u529f\u80fd\u533a\u57df\u8bc6\u522b\u7cbe\u5ea6\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u590d\u6742\u5916\u89c2\u548c\u7eb9\u7406\u53d8\u5316\u4e0b\uff0c\u9ad8\u5c42\u8bed\u4e49\u6307\u4ee4\u4e0e\u7269\u7406\u4ea4\u4e92\u70b9\u7684\u5bf9\u9f50\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u529f\u80fd\u9884\u6d4b\u65b9\u6cd5\u7531\u4e8e\u81ea\u7531\u5ea6\u8fd1\u65e0\u9650\u3001\u590d\u6742\u52a8\u529b\u5b66\u548c\u5f02\u8d28\u6a21\u5f0f\uff0c\u5e38\u51fa\u73b0\u8fb9\u754c\u6ea2\u51fa\u548c\u529f\u80fd\u533a\u57df\u788e\u7247\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faTRACER\u6846\u67b6\uff1a1) \u6811\u72b6\u529f\u80fd\u63a8\u7406\u94fe(TA-CoT)\u5c06\u9ad8\u5c42\u4efb\u52a1\u610f\u56fe\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u5b50\u4efb\u52a1\u8bed\u4e49\uff1b2) \u7a7a\u95f4\u7ea6\u675f\u8fb9\u754c\u7ec6\u5316(SCBR)\u673a\u5236\u6291\u5236\u9884\u6d4b\u6ea2\u51fa\uff1b3) \u4ea4\u4e92\u6536\u655b\u7ec6\u5316\u6d41(ICRF)\u805a\u5408\u53d7\u5916\u89c2\u566a\u58f0\u6c61\u67d3\u7684\u79bb\u6563\u50cf\u7d20\u3002", "result": "\u5728Fine-AGDDO15\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cTRACER\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4e0d\u540c\u7eb9\u7406\u548c\u6a21\u5f0f\u7684\u529f\u80fd\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u66f4\u91cd\u8981\u7684\u662f\u63d0\u9ad8\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "conclusion": "TRACER\u6709\u6548\u5f25\u5408\u4e86\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u4e0e\u4f4e\u5c42\u7269\u7406\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7eb9\u7406\u9c81\u68d2\u7684\u529f\u80fd\u8bc6\u522b\u6846\u67b6\u3002"}}
{"id": "2601.20239", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20239", "abs": "https://arxiv.org/abs/2601.20239", "authors": ["Zhemeng Zhang", "Jiahua Ma", "Xincheng Yang", "Xin Wen", "Yuzhi Zhang", "Boyan Li", "Yiran Qin", "Jin Liu", "Can Zhao", "Li Kang", "Haoqin Hong", "Zhenfei Yin", "Philip Torr", "Hao Su", "Ruimao Zhang", "Daolin Ma"], "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance", "comment": null, "summary": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.", "AI": {"tldr": "TouchGuide\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u7b56\u7565\u89c6\u89c9-\u89e6\u89c9\u878d\u5408\u8303\u5f0f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406\u5f15\u5bfc\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5229\u7528\u89e6\u89c9\u53cd\u9988\u4f18\u5316\u63a5\u89e6\u4e30\u5bcc\u7684\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u673a\u5668\u4eba\u7cbe\u7ec6\u64cd\u4f5c\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u89e6\u89c9\u53cd\u9988\u5229\u7528\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u89e6\u89c9\u4fe1\u606f\u6765\u6307\u5bfc\u52a8\u4f5c\u751f\u6210\uff0c\u5bfc\u81f4\u7269\u7406\u63a5\u89e6\u7ea6\u675f\u65e0\u6cd5\u5f97\u5230\u6709\u6548\u6ee1\u8db3\u3002", "method": "\u63d0\u51faTouchGuide\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a1\uff09\u65e9\u671f\u91c7\u6837\u9636\u6bb5\u4f7f\u7528\u89c6\u89c9\u8f93\u5165\u751f\u6210\u7c97\u7565\u7684\u89c6\u89c9\u5408\u7406\u52a8\u4f5c\uff1b2\uff09\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u63a5\u89e6\u7269\u7406\u6a21\u578b\uff08CPM\uff09\u63d0\u4f9b\u89e6\u89c9\u6307\u5bfc\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3CPM\u751f\u6210\u89e6\u89c9\u611f\u77e5\u7684\u53ef\u884c\u6027\u5206\u6570\uff0c\u5f15\u5bfc\u91c7\u6837\u8fc7\u7a0b\u6ee1\u8db3\u7269\u7406\u63a5\u89e6\u7ea6\u675f\u3002\u540c\u65f6\u5f00\u53d1TacUMI\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u4f7f\u7528\u521a\u6027\u6307\u5c16\u83b7\u53d6\u76f4\u63a5\u89e6\u89c9\u53cd\u9988\uff0c\u5e73\u8861\u7cbe\u5ea6\u4e0e\u6210\u672c\u3002", "result": "\u5728\u4e94\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\uff08\u5982\u7cfb\u978b\u5e26\u3001\u82af\u7247\u4ea4\u63a5\u7b49\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTouchGuide\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4e14\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u89e6\u89c9\u7b56\u7565\u3002", "conclusion": "TouchGuide\u901a\u8fc7\u5c06\u89e6\u89c9\u53cd\u9988\u878d\u5165\u52a8\u4f5c\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7cbe\u7ec6\u64cd\u4f5c\u4e2d\u7684\u7269\u7406\u63a5\u89e6\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6TacUMI\u7cfb\u7edf\u4e3a\u9ad8\u8d28\u91cf\u89e6\u89c9\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2601.20262", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20262", "abs": "https://arxiv.org/abs/2601.20262", "authors": ["Boseong Jeon", "Yunho Choi", "Taehan Kim"], "title": "Shallow-\u03c0: Knowledge Distillation for Flow-based VLAs", "comment": null, "summary": "The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.", "AI": {"tldr": "Shallow-pi\u662f\u4e00\u4e2a\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06VLA\u6a21\u578b\u7684Transformer\u5c42\u6570\u4ece18\u5c42\u538b\u7f29\u52306\u5c42\uff0c\u5b9e\u73b02\u500d\u4ee5\u4e0a\u63a8\u7406\u52a0\u901f\uff0c\u6210\u529f\u7387\u4e0b\u964d\u5c0f\u4e8e1%\uff0c\u5e76\u5728\u5de5\u4e1a\u7ea7\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1", "motivation": "\u5b9e\u65f6\u673a\u5668\u4eba\u90e8\u7f72\u9700\u8981\u5feb\u901f\u3001\u8bbe\u5907\u7aef\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8token\u7ea7\u6548\u7387\uff08\u5982\u89c6\u89c9token\u526a\u679d\uff09\uff0c\u800c\u7cfb\u7edf\u6027\u7684Transformer\u5c42\u7f29\u51cf\u5728\u57fa\u4e8e\u6d41\u7684VLA\u6a21\u578b\u4e2d\u5c1a\u672a\u5728\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u4e0b\u63a2\u7d22", "method": "\u63d0\u51faShallow-pi\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u6fc0\u8fdb\u5730\u51cf\u5c11VLM\u4e3b\u5e72\u548c\u57fa\u4e8e\u6d41\u7684\u52a8\u4f5c\u5934\u7684Transformer\u6df1\u5ea6\uff0c\u5c06\u6a21\u578b\u4ece18\u5c42\u538b\u7f29\u52306\u5c42", "result": "\u5b9e\u73b0\u8d85\u8fc72\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5728\u6807\u51c6\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u7edd\u5bf9\u4e0b\u964d\u5c0f\u4e8e1%\uff0c\u5728\u7f29\u51cf\u7684VLA\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728Jetson Orin\u548cJetson Thor\u7b49\u5de5\u4e1a\u7ea7\u5e73\u53f0\u4e0a\uff0c\u5728\u590d\u6742\u52a8\u6001\u64cd\u4f5c\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5", "conclusion": "Shallow-pi\u901a\u8fc7\u7cfb\u7edf\u6027\u7684Transformer\u5c42\u7f29\u51cf\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684VLA\u6a21\u578b\u538b\u7f29\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6"}}
{"id": "2601.20321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20321", "abs": "https://arxiv.org/abs/2601.20321", "authors": ["Yuzhe Huang", "Pei Lin", "Wanlin Li", "Daohan Li", "Jiajun Li", "Jiaming Jiang", "Chenxi Xiao", "Ziyuan Jiao"], "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation", "comment": "17pages,9fig", "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.", "AI": {"tldr": "TaF-VLA\u6846\u67b6\u901a\u8fc7\u89e6\u89c9-\u529b\u5bf9\u9f50\u800c\u975e\u4f20\u7edf\u7684\u89e6\u89c9-\u89c6\u89c9\u5bf9\u9f50\uff0c\u5c06\u9ad8\u7ef4\u89e6\u89c9\u89c2\u6d4b\u4e0e\u7269\u7406\u4ea4\u4e92\u529b\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u6a21\u6001\uff0c\u7f3a\u4e4f\u5bf9\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u6240\u9700\u7684\u7269\u7406\u76f4\u89c9\u548c\u7cbe\u786e\u529b\u8c03\u8282\u80fd\u529b\u3002\u73b0\u6709\u5c06\u89e6\u89c9\u4f20\u611f\u878d\u5165VLA\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u89e6\u89c9\u8f93\u5165\u89c6\u4e3a\u8f85\u52a9\u89c6\u89c9\u7eb9\u7406\uff0c\u5ffd\u7565\u4e86\u8868\u9762\u53d8\u5f62\u4e0e\u4ea4\u4e92\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u5185\u5728\u5173\u8054\u3002", "method": "\u63d0\u51fa\u89e6\u89c9-\u529b\u5bf9\u9f50\u8303\u5f0f\uff0c\u5f00\u53d1\u81ea\u52a8\u89e6\u89c9-\u529b\u6570\u636e\u91c7\u96c6\u8bbe\u5907\u5e76\u521b\u5efaTaF-Dataset\u3002\u6838\u5fc3\u662fTactile-Force Adapter\uff08TaF-Adapter\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u89e6\u89c9\u4f20\u611f\u5668\u7f16\u7801\u5668\uff0c\u63d0\u53d6\u79bb\u6563\u5316\u6f5c\u5728\u4fe1\u606f\u6765\u7f16\u7801\u89e6\u89c9\u89c2\u6d4b\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u8868\u793a\u6355\u6349\u5386\u53f2\u4f9d\u8d56\u3001\u566a\u58f0\u4e0d\u654f\u611f\u7684\u7269\u7406\u52a8\u6001\u800c\u975e\u9759\u6001\u89c6\u89c9\u7eb9\u7406\u3002", "result": "TaF-VLA\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89e6\u89c9-\u89c6\u89c9\u5bf9\u9f50\u548c\u7eaf\u89c6\u89c9\u57fa\u7ebf\uff0c\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u8fc7\u8de8\u6a21\u6001\u7269\u7406\u63a8\u7406\u5b9e\u73b0\u7a33\u5065\u3001\u529b\u611f\u77e5\u64cd\u4f5c\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89e6\u89c9\u89c2\u6d4b\u4e0e\u7269\u7406\u4ea4\u4e92\u529b\u660e\u786e\u5bf9\u9f50\uff0cTaF-VLA\u6846\u67b6\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u4e86\u7269\u7406\u76f4\u89c9\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u9700\u8981\u7cbe\u786e\u529b\u8c03\u8282\u548c\u7269\u7406\u63a8\u7406\u7684\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u4ee3\u8868\u4e86\u4ece\u89e6\u89c9-\u89c6\u89c9\u5bf9\u9f50\u5230\u89e6\u89c9-\u529b\u5bf9\u9f50\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2601.20334", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20334", "abs": "https://arxiv.org/abs/2601.20334", "authors": ["Brian Y. Tsui", "Alan Y. Fang", "Tiffany J. Hwu"], "title": "Demonstration-Free Robotic Control via LLM Agents", "comment": null, "summary": "Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim", "AI": {"tldr": "FAEA\u5c06\u524d\u6cbfLLM\u667a\u80fd\u4f53\u6846\u67b6\u76f4\u63a5\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u65e0\u9700\u4efb\u52a1\u6f14\u793a\u6216\u5fae\u8c03\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u63a5\u8fd1VLA\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u6f14\u793a\u548c\u5fae\u8c03\uff0c\u4e14\u9886\u57df\u8fc1\u79fb\u80fd\u529b\u5dee\uff0c\u7814\u7a76\u662f\u5426\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u66ff\u4ee3\u63a7\u5236\u8303\u5f0f", "method": "FAEA\u5c06\u672a\u4fee\u6539\u7684Claude Agent SDK\u76f4\u63a5\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u5229\u7528\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u8c03\u8bd5\u4ee3\u7801\u7684\u8fed\u4ee3\u63a8\u7406\u80fd\u529b\uff0c\u8ba9\u673a\u5668\u4eba\u667a\u80fd\u4f53\u80fd\u591f\u63a8\u7406\u64cd\u4f5c\u7b56\u7565", "result": "\u5728LIBERO\u3001ManiSkill3\u548cMetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAEA\u5206\u522b\u8fbe\u523084.9%\u300185.7%\u548c96%\u7684\u6210\u529f\u7387\uff0c\u63a5\u8fd1\u9700\u8981\u5c11\u4e8e100\u6b21\u6f14\u793a\u8bad\u7ec3\u7684VLA\u6a21\u578b\u6027\u80fd\uff1b\u52a0\u5165\u4e00\u8f6e\u4eba\u7c7b\u53cd\u9988\u540e\uff0cLIBERO\u4e0a\u6027\u80fd\u63d0\u5347\u81f388.2%", "conclusion": "\u901a\u7528\u667a\u80fd\u4f53\u8db3\u4ee5\u5904\u7406\u4e00\u7c7b\u4ee5\u5ba1\u614e\u4efb\u52a1\u7ea7\u89c4\u5212\u4e3a\u4e3b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5229\u7528\u524d\u6cbf\u6a21\u578b\u57fa\u7840\u8bbe\u65bd\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u5177\u6709\u5373\u65f6\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2601.20377", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20377", "abs": "https://arxiv.org/abs/2601.20377", "authors": ["Xinyan Chen", "Qinchun Li", "Ruiqin Ma", "Jiaqi Bai", "Li Yi", "Jianfei Yang"], "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification", "comment": "Accepted by ICLR 2026", "summary": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.", "AI": {"tldr": "RF-MatID\uff1a\u9996\u4e2a\u5f00\u6e90\u3001\u5927\u89c4\u6a21\u3001\u5bbd\u9891\u5e26\u3001\u51e0\u4f55\u591a\u6837\u7684\u5c04\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u6750\u6599\u8bc6\u522b\uff0c\u5305\u542b16\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\u3001142k\u6837\u672c\uff0c\u8986\u76d64-43.5GHz\u9891\u7387\u8303\u56f4\uff0c\u5e76\u5efa\u7acb\u4e86\u591a\u8bbe\u7f6e\u3001\u591a\u534f\u8bae\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u7684\u6750\u6599\u8bc6\u522b\u53d7\u5149\u5b66\u4f20\u611f\u5668\u56fa\u6709\u9650\u5236\uff0c\u800c\u57fa\u4e8e\u5c04\u9891\u7684\u65b9\u6cd5\u80fd\u63ed\u793a\u6750\u6599\u5185\u5728\u7279\u6027\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b66\u4e60\u65b9\u6cd5\u7684\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u521b\u5efaRF-MatID\u6570\u636e\u96c6\uff0c\u5305\u542b16\u4e2a\u7ec6\u7c92\u5ea6\u6750\u6599\u7c7b\u522b\uff08\u5206\u4e3a5\u4e2a\u8d85\u7c7b\uff09\uff0c\u8986\u76d64-43.5GHz\u5bbd\u9891\u5e26\uff0c\u5305\u542b142k\u4e2a\u6837\u672c\u7684\u9891\u57df\u548c\u65f6\u57df\u8868\u793a\uff0c\u7cfb\u7edf\u5f15\u5165\u5165\u5c04\u89d2\u548c\u8ddd\u79bb\u7b49\u51e0\u4f55\u6270\u52a8\u63a7\u5236\u53d8\u91cf\u3002", "result": "\u5efa\u7acb\u4e86\u591a\u8bbe\u7f6e\u3001\u591a\u534f\u8bae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u5185\u6027\u80fd\u548c\u5206\u5e03\u5916\u9c81\u68d2\u6027\uff08\u8de8\u89d2\u5ea6\u548c\u8de8\u8ddd\u79bb\u504f\u79fb\uff09\uff0c\u901a\u8fc75\u4e2a\u9891\u7387\u5206\u914d\u534f\u8bae\u652f\u6301\u9891\u7387\u548c\u533a\u57df\u7ea7\u5206\u6790\u3002", "conclusion": "RF-MatID\u65e8\u5728\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3001\u52a0\u901f\u7b97\u6cd5\u8fdb\u6b65\u3001\u589e\u5f3a\u8de8\u9886\u57df\u9c81\u68d2\u6027\uff0c\u5e76\u652f\u6301\u5c04\u9891\u6750\u6599\u8bc6\u522b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u96c6\u7a7a\u767d\u3002"}}
{"id": "2601.20529", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.20529", "abs": "https://arxiv.org/abs/2601.20529", "authors": ["Julia Richter", "David Oberacker", "Gabriela Ligeza", "Valentin T. Bickel", "Philip Arm", "William Talbot", "Marvin Grosse Besselmann", "Florian Kehl", "Tristan Schnell", "Hendrik Kolvenbach", "R\u00fcdiger Dillmann", "Arne Roennau", "Marco Hutter"], "title": "A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests", "comment": null, "summary": "Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u591a\u673a\u5668\u4eba\u6708\u7403\u52d8\u63a2\u4efb\u52a1\u8bc4\u4f30\u7684\u7ed3\u6784\u5316KPI\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u91ce\u5916\u8bd5\u9a8c\u56e0\u5e73\u53f0\u548c\u5b9e\u9a8c\u8bbe\u7f6e\u5dee\u5f02\u800c\u96be\u4ee5\u6bd4\u8f83\u7684\u95ee\u9898\uff0c\u5f3a\u8c03\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u7684\u573a\u666f\u4f9d\u8d56\u6027\u4f18\u5148\u7ea7\u3002", "motivation": "\u6708\u7403\u5173\u952e\u8d44\u6e90\uff08\u5982\u949b\u94c1\u77ff\u3001\u7a00\u571f\u5143\u7d20\u548c\u6c34\u51b0\uff09\u7684\u673a\u5668\u4eba\u52d8\u63a2\u9700\u8981\u7a33\u5065\u7684\u63a2\u7d22\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7684\u91ce\u5916\u6a21\u62df\u8bd5\u9a8c\u7531\u4e8e\u673a\u5668\u4eba\u5e73\u53f0\u548c\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5dee\u5f02\uff0c\u7ed3\u679c\u96be\u4ee5\u6bd4\u8f83\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u4f7f\u7528\u7279\u5b9a\u573a\u666f\u7684\u5de5\u7a0b\u6307\u6807\u8bc4\u4f30\u6027\u80fd\uff0c\u672a\u80fd\u5efa\u7acb\u91ce\u5916\u6027\u80fd\u4e0e\u79d1\u5b66\u76ee\u6807\u4e4b\u95f4\u7684\u660e\u786e\u8054\u7cfb\u3002", "method": "\u4ece\u4e09\u4e2a\u73b0\u5b9e\u7684\u591a\u673a\u5668\u4eba\u6708\u7403\u573a\u666f\u4e2d\u63a8\u5bfc\u51fa\u7ed3\u6784\u5316KPI\u6846\u67b6\uff0c\u8fd9\u4e9b\u573a\u666f\u53cd\u6620\u4e86\u79d1\u5b66\u76ee\u6807\u548c\u64cd\u4f5c\u7ea6\u675f\u3002\u8be5\u6846\u67b6\u5f3a\u8c03\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u7684\u573a\u666f\u4f9d\u8d56\u6027\u4f18\u5148\u7ea7\uff0c\u5e76\u4e13\u95e8\u4e3a\u91ce\u5916\u90e8\u7f72\u7684\u5b9e\u9645\u5e94\u7528\u800c\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u673a\u5668\u4eba\u91ce\u5916\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u53d1\u73b0\u6548\u7387\u548c\u9c81\u68d2\u6027\u76f8\u5173\u7684KPI\u5b9e\u7528\u4e14\u6613\u4e8e\u5e94\u7528\uff0c\u800c\u7cbe\u5ea6\u5bfc\u5411\u7684KPI\u9700\u8981\u53ef\u9760\u7684\u57fa\u51c6\u6570\u636e\uff0c\u8fd9\u5728\u6237\u5916\u6a21\u62df\u73af\u5883\u4e2d\u5e76\u4e0d\u603b\u662f\u53ef\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u901a\u7528\u8bc4\u4f30\u6807\u51c6\uff0c\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u91ce\u5916\u8bd5\u9a8c\u7684\u4e00\u81f4\u3001\u76ee\u6807\u5bfc\u5411\u6bd4\u8f83\uff0c\u5e76\u652f\u6301\u672a\u6765\u884c\u661f\u63a2\u7d22\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7cfb\u7edf\u5316\u5f00\u53d1\u3002"}}
{"id": "2601.20555", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20555", "abs": "https://arxiv.org/abs/2601.20555", "authors": ["Wadhah Zai El Amri", "Nicol\u00e1s Navarro-Guerrero"], "title": "Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands", "comment": "Under Review: Springer Autonomous Robots Journal", "summary": "Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.", "AI": {"tldr": "\u901a\u8fc7\u4f4e\u6210\u672c\u538b\u7535\u9ea6\u514b\u98ce\u548c\u97f3\u9891\u8c31\u56feTransformer\u5b9e\u73b0\u673a\u5668\u4eba\u5168\u8eab\u89e6\u89c9\u5b9a\u4f4d\uff0c\u9759\u6001\u6761\u4ef6\u4e0b\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e5\u6beb\u7c73\uff0c\u6750\u6599\u7279\u6027\u5bf9\u5b9a\u4f4d\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd", "motivation": "\u4f20\u7edf\u89e6\u89c9\u76ae\u80a4\u6210\u672c\u9ad8\u4e14\u96c6\u6210\u590d\u6742\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u5b9e\u73b0\u673a\u5668\u4eba\u5168\u8eab\u63a5\u89e6\u611f\u77e5", "method": "\u4f7f\u7528\u4e03\u4e2a\u4f4e\u6210\u672c\u538b\u7535\u9ea6\u514b\u98ce\u88c5\u5907\u673a\u68b0\u624b\uff0c\u5229\u7528\u97f3\u9891\u8c31\u56feTransformer\u89e3\u7801\u7269\u7406\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u632f\u52a8\u4fe1\u53f7", "result": "\u9759\u6001\u6761\u4ef6\u4e0b\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e5\u6beb\u7c73\uff0c\u4e0d\u540c\u6750\u6599\u7279\u6027\u5bf9\u5b9a\u4f4d\u6027\u80fd\u6709\u4e0d\u540c\u5f71\u54cd\uff1a\u521a\u6027\u6750\u6599\u5728\u8109\u51b2\u54cd\u5e94\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7eb9\u7406\u6750\u6599\u5728\u8f68\u8ff9\u8ddf\u8e2a\u4e2d\u63d0\u4f9b\u66f4\u597d\u7684\u6469\u64e6\u7279\u5f81", "conclusion": "\u590d\u6742\u7269\u7406\u63a5\u89e6\u52a8\u529b\u5b66\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u632f\u52a8\u4fe1\u53f7\u6709\u6548\u89e3\u7801\uff0c\u4e3a\u5b9e\u73b0\u5e7f\u6cdb\u3001\u7ecf\u6d4e\u5b9e\u60e0\u7684\u673a\u5668\u4eba\u63a5\u89e6\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\uff0c\u6240\u6709\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u5b9e\u9a8c\u8bbe\u7f6e\u5df2\u5f00\u6e90"}}
{"id": "2601.20577", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20577", "abs": "https://arxiv.org/abs/2601.20577", "authors": ["Baiqing Wang", "Helei Cui", "Bo Zhang", "Xiaolong Zheng", "Bin Guo", "Zhiwen Yu"], "title": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization", "comment": null, "summary": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.", "AI": {"tldr": "MeCo\u662f\u4e00\u4e2a\u57fa\u4e8e\u76f8\u4f3c\u6027\u611f\u77e5\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\"\u7f13\u5b58\u4e0e\u91cd\u7528\"\u673a\u5236\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\uff0c\u63d0\u9ad8LLM\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u6548\u7387", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u65b9\u6cd5\u5728\u9047\u5230\u76f8\u540c\u6216\u76f8\u4f3c\u4efb\u52a1\u65f6\u9700\u8981\u91cd\u65b0\u89c4\u5212\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u7ea7\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51faMeCo\u6846\u67b6\uff0c\u5f15\u5165\u76f8\u4f3c\u6027\u6d4b\u8bd5\u65b9\u6cd5\u68c0\u7d22\u5148\u524d\u89e3\u51b3\u7684\u9ad8\u76f8\u5173\u6027\u4efb\u52a1\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8c03\u7528LLM\u7684\u6709\u6548\u89c4\u5212\u91cd\u7528", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMeCo\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u89c4\u5212\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6210\u529f\u7387", "conclusion": "MeCo\u901a\u8fc7\u76f8\u4f3c\u6027\u611f\u77e5\u548c\u7f13\u5b58\u91cd\u7528\u673a\u5236\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u76f8\u4f3c\u4efb\u52a1\u534f\u4f5c\u573a\u666f\u57fa\u51c6MeCoBench"}}
{"id": "2601.20668", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20668", "abs": "https://arxiv.org/abs/2601.20668", "authors": ["Shuhao Liao", "Peizhuo Li", "Xinrong Yang", "Linnan Chang", "Zhaoxin Fan", "Qing Wang", "Lei Shi", "Yuhong Cao", "Wenjun Wu", "Guillaume Sartoretti"], "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control", "comment": null, "summary": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.", "AI": {"tldr": "GPO\u662f\u4e00\u79cd\u7528\u4e8e\u8db3\u5f0f\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u53d8\u52a8\u4f5c\u53d8\u6362\u9650\u5236\u65e9\u671f\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4fc3\u8fdb\u6709\u6548\u6570\u636e\u6536\u96c6\uff0c\u7136\u540e\u9010\u6b65\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u589e\u5f3a\u63a2\u7d22\uff0c\u6700\u7ec8\u83b7\u5f97\u66f4\u9ad8\u56de\u62a5\u3002", "motivation": "\u8db3\u5f0f\u673a\u5668\u4eba\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9762\u4e34\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u3001\u786c\u4ef6\u9650\u5236\u548c\u6709\u9650\u63a2\u7d22\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4f4d\u7f6e\u63a7\u5236\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u529b\u77e9\u63a7\u5236\u4e2d\u6548\u679c\u8f83\u5dee\uff0c\u56e0\u4e3a\u529b\u77e9\u63a7\u5236\u9700\u8981\u66f4\u5145\u5206\u7684\u52a8\u4f5c\u7a7a\u95f4\u63a2\u7d22\u548c\u66f4\u6709\u4fe1\u606f\u91cf\u7684\u68af\u5ea6\u4fe1\u53f7\u3002", "method": "\u63d0\u51faGrowing Policy Optimization (GPO)\u6846\u67b6\uff0c\u5e94\u7528\u65f6\u53d8\u52a8\u4f5c\u53d8\u6362\uff1a\u5728\u8bad\u7ec3\u65e9\u671f\u9650\u5236\u6709\u6548\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u4fc3\u8fdb\u6709\u6548\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u5b66\u4e60\uff0c\u7136\u540e\u9010\u6b65\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u589e\u5f3a\u63a2\u7d22\u3002\u8bc1\u660e\u8be5\u53d8\u6362\u4fdd\u6301\u4e86PPO\u66f4\u65b0\u89c4\u5219\uff0c\u4ec5\u5f15\u5165\u6709\u754c\u3001\u6e10\u8fd1\u6d88\u5931\u7684\u68af\u5ea6\u5931\u771f\u3002", "result": "\u5728\u56db\u8db3\u548c\u516d\u8db3\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30GPO\uff0c\u5305\u62ec\u5c06\u4eff\u771f\u8bad\u7ec3\u7684\u7b56\u7565\u96f6\u6837\u672c\u90e8\u7f72\u5230\u786c\u4ef6\u4e0a\u3002\u4f7f\u7528GPO\u8bad\u7ec3\u7684\u7b56\u7565\u59cb\u7ec8\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "GPO\u4e3a\u5b66\u4e60\u8db3\u5f0f\u8fd0\u52a8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u3001\u4e0e\u73af\u5883\u65e0\u5173\u7684\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u529b\u77e9\u63a7\u5236\u4e2d\u7684\u63a2\u7d22\u96be\u9898\u3002"}}
{"id": "2601.20682", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20682", "abs": "https://arxiv.org/abs/2601.20682", "authors": ["P\u00e9ter Polcz", "Katalin Sch\u00e4ffer", "Mikl\u00f3s Koller"], "title": "Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model", "comment": null, "summary": "Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u808c\u8171\u4f4d\u79fb\u548c\u5f20\u529b\u4f30\u8ba1\u5173\u8282\u4f4d\u7f6e\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u5173\u8282\u7f16\u7801\u5668\u7684\u4eff\u4eba\u673a\u5668\u4eba\u624b\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u6c42\u89e3\u5173\u8282\u89d2\u5ea6\uff0c\u5e76\u7528\u4e8e\u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u95ed\u73af\u63a7\u5236\u5b9e\u73b0\u624b\u52bf\u8ddf\u8e2a\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u7684\u4eff\u4eba\u673a\u5668\u4eba\u624b\u901a\u5e38\u7f3a\u4e4f\u76f4\u63a5\u5173\u8282\u89d2\u5ea6\u4f20\u611f\uff0c\u56e0\u4e3a\u96c6\u6210\u5173\u8282\u7f16\u7801\u5668\u4f1a\u5f71\u54cd\u673a\u68b0\u7d27\u51d1\u6027\u548c\u7075\u5de7\u6027\u3002\u9700\u8981\u4e00\u79cd\u4ece\u808c\u8171\u72b6\u6001\u95f4\u63a5\u4f30\u8ba1\u5173\u8282\u4f4d\u7f6e\u7684\u65b9\u6cd5\u3002", "method": "1. \u57fa\u4e8eDenavit-Hartenberg\u7ea6\u5b9a\u5efa\u7acb\u4eff\u4eba\u624b\u7684\u8fd0\u52a8\u5b66\u5efa\u6a21\u6846\u67b6\uff1b2. \u4f7f\u7528\u7b80\u5316\u808c\u8171\u6a21\u578b\u63a8\u5bfc\u808c\u8171\u72b6\u6001\u4e0e\u5173\u8282\u4f4d\u7f6e\u7684\u975e\u7ebf\u6027\u65b9\u7a0b\u7ec4\uff1b3. \u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\u6c42\u89e3\u5173\u8282\u89d2\u5ea6\uff1b4. \u91c7\u7528\u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u6bd4\u4f8b\u79ef\u5206\u63a7\u5236\u5668\u52a0\u524d\u9988\u9879\u8fdb\u884c\u95ed\u73af\u63a7\u5236\u3002", "result": "\u5728MuJoCo\u4eff\u771f\u73af\u5883\u4e2d\u4f7f\u7528\u89e3\u5256\u5b66\u6b63\u786e\u7684\u751f\u7269\u673a\u7535\u624b\uff08\u6bcf\u4e2a\u957f\u63075\u81ea\u7531\u5ea6\uff0c\u62c7\u63076\u81ea\u7531\u5ea6\uff09\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u4f30\u8ba1\u548c\u63a7\u5236\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4f7f\u7528\u76f4\u63a5\u5173\u8282\u4f20\u611f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u624b\u52bf\u8ddf\u8e2a\uff0c\u4e3a\u808c\u8171\u9a71\u52a8\u4eff\u4eba\u673a\u5668\u4eba\u624b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u5173\u8282\u4f4d\u7f6e\u4f30\u8ba1\u548c\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20701", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20701", "abs": "https://arxiv.org/abs/2601.20701", "authors": ["Guowei Zou", "Haitao Wang", "Hejun Wu", "Yukun Qian", "Yuhang Wang", "Weibing Li"], "title": "One Step Is Enough: Dispersive MeanFlow Policy Optimization", "comment": "Code and project page: https://guowei-zou.github.io/dmpo-page/", "summary": "Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step\n  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that\n  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,\n  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments\n  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With\n  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with\n  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world\n  applicability.", "AI": {"tldr": "DMPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u6b65\u751f\u6210\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7MeanFlow\u5b9e\u73b0\u65e0\u9700\u77e5\u8bc6\u84b8\u998f\u7684\u5355\u6b65\u63a8\u7406\uff0c\u7ed3\u5408\u5206\u6563\u6b63\u5219\u5316\u548cRL\u5fae\u8c03\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff08>120Hz\uff09\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u751f\u6210\u7b56\u7565\u9700\u8981\u591a\u6b65\u91c7\u6837\uff0c\u9650\u5236\u4e86\u5728\u65f6\u95f4\u5173\u952e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u3002\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u9700\u8981\u5feb\u901f\u7684\u52a8\u4f5c\u751f\u6210\uff0c\u4f46\u591a\u6b65\u91c7\u6837\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "DMPO\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) MeanFlow - \u6570\u5b66\u63a8\u5bfc\u7684\u5355\u6b65\u63a8\u7406\u65b9\u6cd5\uff0c\u65e0\u9700\u77e5\u8bc6\u84b8\u998f\uff1b2) \u5206\u6563\u6b63\u5219\u5316 - \u9632\u6b62\u8868\u793a\u5d29\u6e83\uff1b3) \u5f3a\u5316\u5b66\u4e60\u5fae\u8c03 - \u8d85\u8d8a\u4e13\u5bb6\u6f14\u793a\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728RoboMimic\u64cd\u4f5c\u548cOpenAI Gym\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDMPO\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u591a\u6b65\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002\u63a8\u7406\u901f\u5ea6\u63d0\u53475-20\u500d\uff0c\u8fbe\u5230>120Hz\u7684\u5b9e\u65f6\u63a7\u5236\u8981\u6c42\uff0c\u5728\u9ad8\u6027\u80fdGPU\u4e0a\u53ef\u8fbe\u6570\u767e\u8d6b\u5179\u3002\u5728Franka-Emika-Panda\u673a\u5668\u4eba\u4e0a\u7684\u7269\u7406\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "DMPO\u6846\u67b6\u901a\u8fc7\u5355\u6b65\u751f\u6210\u7b56\u7565\u89e3\u51b3\u4e86\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u7684\u74f6\u9888\u95ee\u9898\uff0c\u7ed3\u5408\u6570\u5b66\u63a8\u5bfc\u7684\u5355\u6b65\u63a8\u7406\u3001\u6b63\u5219\u5316\u548cRL\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u4e3a\u65f6\u95f4\u5173\u952e\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20797", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20797", "abs": "https://arxiv.org/abs/2601.20797", "authors": ["Guillermo GP-Lenza", "Carmen DR. Pita-Romero", "Miguel Fernandez-Cortizas", "Pascual Campoy"], "title": "A Methodology for Designing Knowledge-Driven Missions for Robots", "comment": null, "summary": "This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728ROS 2\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u77e5\u8bc6\u56fe\u8c31\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6548\u7387\u548c\u667a\u80fd\u6027\uff0c\u5e76\u5728Aerostack2\u6846\u67b6\u4e2d\u901a\u8fc7\u6a21\u62df\u641c\u6551\u4efb\u52a1\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6548\u7387\u548c\u667a\u80fd\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u5230ROS 2\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u6539\u5584\u51b3\u7b56\u5236\u5b9a\u548c\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e94\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u5b9a\u4e49\u521d\u59cb\u548c\u76ee\u6807\u6761\u4ef6\u3001\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u5b50\u4efb\u52a1\u3001\u89c4\u5212\u4efb\u52a1\u5e8f\u5217\u3001\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8868\u793a\u4efb\u52a1\u76f8\u5173\u6570\u636e\u3001\u4f7f\u7528\u9ad8\u7ea7\u8bed\u8a00\u8bbe\u8ba1\u4efb\u52a1\u3002\u6bcf\u4e2a\u6b65\u9aa4\u90fd\u5efa\u7acb\u5728\u524d\u4e00\u6b65\u7684\u57fa\u7840\u4e0a\uff0c\u786e\u4fdd\u4ece\u521d\u59cb\u8bbe\u7f6e\u5230\u6700\u7ec8\u6267\u884c\u7684\u8fde\u8d2f\u8fc7\u7a0b\u3002", "result": "\u5728Aerostack2\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u8be5\u65b9\u6cd5\uff0c\u901a\u8fc7Gazebo\u73af\u5883\u4e2d\u7684\u6a21\u62df\u641c\u6551\u4efb\u52a1\uff08\u65e0\u4eba\u673a\u81ea\u4e3b\u5b9a\u4f4d\u76ee\u6807\uff09\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002\u5b9e\u65bd\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u6539\u5584\u51b3\u7b56\u5236\u5b9a\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u77e5\u8bc6\u56fe\u8c31\u5b9e\u65bd\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347ROS 2\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6548\u7387\u548c\u667a\u80fd\u6027\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\u548c\u89c4\u5212\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2601.20846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20846", "abs": "https://arxiv.org/abs/2601.20846", "authors": ["Jamie Hathaway", "Alireza Rastegarpanah", "Rustam Stolkin"], "title": "End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting", "comment": "14 pages, 9 figures. Submitted to Nature Scientific Reports", "summary": "Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7279\u5f81\u8868\u793a\u5e76\u751f\u6210\u5f31\u914d\u5bf9\u8f68\u8ff9\uff0c\u5728\u673a\u5668\u4eba\u5207\u5272\u672a\u77e5\u6750\u6599\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u9762\u4e34\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u4e14\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6709\u9650\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u6709\u9650\u771f\u5b9e\u6570\u636e\u5b9e\u73b0\u6709\u6548\u7b56\u7565\u8fc1\u79fb\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u91cd\u65b0\u89e3\u91ca\u5e76\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8fc1\u79fb\uff0c\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8054\u5408\u5b66\u4e60\u81ea\u76d1\u7763\u7279\u5f81\u8868\u793a\uff0c\u751f\u6210\u5f31\u914d\u5bf9\u7684\u6e90-\u76ee\u6807\u8f68\u8ff9\uff0c\u63d0\u9ad8\u5408\u6210\u8f68\u8ff9\u7684\u7269\u7406\u771f\u5b9e\u6027\u3002", "result": "\u5728\u673a\u5668\u4eba\u5207\u5272\u672a\u77e5\u6750\u6599\u6848\u4f8b\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u5148\u524d\u5de5\u4f5c\u3001CycleGAN\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u65f6\u95f4\u5e8f\u5217\u7ffb\u8bd1\uff09\uff0c\u672c\u65b9\u6cd5\u5728\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u884c\u4e3a\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u6570\u636e\uff0c\u5bf9\u51e0\u4f55\u548c\u6750\u6599\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u5956\u52b1\u4fe1\u606f\u4e0d\u53ef\u7528\u7684\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0c\u7b56\u7565\u9002\u5e94\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u6846\u67b6\u3002"}}
