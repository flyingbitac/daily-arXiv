{"id": "2511.21886", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21886", "abs": "https://arxiv.org/abs/2511.21886", "authors": ["Jingtian Yan", "Shuai Zhou", "Stephen F. Smith", "Jiaoyang Li"], "title": "Bridging Planning and Execution: Multi-Agent Path Finding Under Real-World Deadlines", "comment": null, "summary": "The Multi-Agent Path Finding (MAPF) problem aims to find collision-free paths for multiple agents while optimizing objectives such as the sum of costs or makespan. MAPF has wide applications in domains like automated warehouses, manufacturing systems, and airport logistics. However, most MAPF formulations assume a simplified robot model for planning, which overlooks execution-time factors such as kinodynamic constraints, communication latency, and controller variability. This gap between planning and execution is problematic for time-sensitive applications. To bridge this gap, we propose REMAP, an execution-informed MAPF planning framework that can be combined with leading search-based MAPF planners with minor changes. Our framework integrates the proposed ExecTimeNet to accurately estimate execution time based on planned paths. We demonstrate our method for solving MAPF with Real-world Deadlines (MAPF-RD) problem, where agents must reach their goals before a predefined wall-clock time. We integrate our framework with two popular MAPF methods, MAPF-LNS and CBS. Experiments show that REMAP achieves up to 20% improvement in solution quality over baseline methods (e.g., constant execution speed estimators) on benchmark maps with up to 300 agents.", "AI": {"tldr": "REMAP\u662f\u4e00\u4e2a\u6267\u884c\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7ExecTimeNet\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u89e3\u51b3\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728MAPF-RD\u95ee\u9898\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534720%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u5047\u8bbe\u7b80\u5316\u7684\u673a\u5668\u4eba\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u6267\u884c\u65f6\u7684\u52a8\u529b\u5b66\u7ea6\u675f\u3001\u901a\u4fe1\u5ef6\u8fdf\u548c\u63a7\u5236\u5668\u53d8\u5f02\u6027\u7b49\u56e0\u7d20\uff0c\u5bfc\u81f4\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u65f6\u95f4\u654f\u611f\u5e94\u7528\u5c24\u5176\u6210\u95ee\u9898\u3002", "method": "\u63d0\u51faREMAP\u6267\u884c\u611f\u77e5MAPF\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408ExecTimeNet\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u4e3b\u6d41\u641c\u7d22\u5f0fMAPF\u89c4\u5212\u5668\u4e2d\u3002\u6846\u67b6\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u5b9e\u65f6\u622a\u6b62\u65f6\u95f4\u7684MAPF-RD\u95ee\u9898\u3002", "result": "\u5c06REMAP\u4e0eMAPF-LNS\u548cCBS\u4e24\u79cd\u6d41\u884cMAPF\u65b9\u6cd5\u96c6\u6210\uff0c\u5728\u6700\u591a300\u4e2a\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u5730\u56fe\u4e0a\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u6052\u5b9a\u6267\u884c\u901f\u5ea6\u4f30\u8ba1\u5668\uff09\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe20%\u3002", "conclusion": "REMAP\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86MAPF\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u51c6\u786e\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u63d0\u9ad8\u4e86\u65f6\u95f4\u654f\u611f\u5e94\u7528\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89c4\u5212\u65b9\u6848\u3002"}}
{"id": "2511.21957", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.21957", "abs": "https://arxiv.org/abs/2511.21957", "authors": ["Cahit Ikbal Er", "Amin Kashiri", "Yasin Yazicioglu"], "title": "RSPECT: Robust and Scalable Planner for Energy-Aware Coordination of UAV-UGV Teams in Aerial Monitoring", "comment": null, "summary": "We consider the robust planning of energy-constrained unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), which act as mobile charging stations, to perform long-horizon aerial monitoring missions. More specifically, given a set of points to be visited by the UAVs and desired final positions of the UAV-UGV teams, the objective is to find a robust plan (the vehicle trajectories) that can be realized without a major revision in the face of uncertainty (e.g., unknown obstacles/terrain, wind) to complete this mission in minimum time. We provide a formal description of this problem as a mixed-integer program (MIP), which is NP-hard. Since exact solution methods are computationally intractable for such problems, we propose RSPECT, a scalable and efficient heuristic. We provide theoretical results on the complexity of our algorithm and the feasibility and robustness of resulting plans. We also demonstrate the performance of our method via simulations and experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRSPECT\u7b97\u6cd5\uff0c\u7528\u4e8e\u89c4\u5212\u65e0\u4eba\u673a\u548c\u5730\u9762\u5145\u7535\u8f66\u7684\u534f\u540c\u8def\u5f84\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u7a7a\u4e2d\u76d1\u6d4b\u4efb\u52a1", "motivation": "\u65e0\u4eba\u673a\u6267\u884c\u957f\u65f6\u7a0b\u76d1\u6d4b\u4efb\u52a1\u65f6\u9762\u4e34\u80fd\u91cf\u9650\u5236\uff0c\u9700\u8981\u5730\u9762\u5145\u7535\u8f66\u534f\u540c\u652f\u6301\u3002\u540c\u65f6\uff0c\u5b9e\u9645\u73af\u5883\u4e2d\u5b58\u5728\u672a\u77e5\u969c\u788d\u3001\u5730\u5f62\u3001\u98ce\u529b\u7b49\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\uff0c\u9700\u8981\u9c81\u68d2\u7684\u89c4\u5212\u65b9\u6848", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\uff0c\u63d0\u51faRSPECT\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406NP-hard\u7684\u590d\u6742\u89c4\u5212\u95ee\u9898", "result": "\u63d0\u4f9b\u4e86\u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u8ba1\u5212\u7684\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "RSPECT\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a-\u5730\u9762\u8f66\u534f\u540c\u7684\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u5b9e\u73b0\u6700\u5c0f\u65f6\u95f4\u5b8c\u6210\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2511.22042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22042", "abs": "https://arxiv.org/abs/2511.22042", "authors": ["Lei Li", "Jiale Gong", "Ziyang Li", "Hong Wang"], "title": "Constant-Volume Deformation Manufacturing for Material-Efficient Shaping", "comment": "46 pages, 27 figures", "summary": "Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f53\u79ef\u4fdd\u6301\u7684\u6570\u5b57\u6a21\u5177\u8303\u5f0f\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u4fe1\u606f\u53d8\u5f62\u9884\u6d4b\u548c\u8bef\u5dee\u8865\u507f\u7b56\u7565\uff0c\u5b9e\u73b0\u5851\u6599\u6750\u6599\u7684\u9ad8\u5ea6\u53ef\u9884\u6d4b\u6210\u5f62\uff0c\u8fbe\u523098%\u4ee5\u4e0a\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "motivation": "\u589e\u6750\u548c\u51cf\u6750\u5236\u9020\u867d\u7136\u80fd\u5b9e\u73b0\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff0c\u4f46\u4f9d\u8d56\u4e8e\u79bb\u6563\u5806\u53e0\u6216\u5c40\u90e8\u53bb\u9664\uff0c\u9650\u5236\u4e86\u8fde\u7eed\u53ef\u63a7\u53d8\u5f62\uff0c\u5bfc\u81f4\u4f53\u79ef\u635f\u5931\u548c\u5f62\u72b6\u504f\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u4f53\u79ef\u4e00\u81f4\u6027\u7684\u53ef\u6301\u7eed\u3001\u96f6\u6d6a\u8d39\u7684\u6210\u5f62\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f53\u79ef\u4fdd\u6301\u7684\u6570\u5b57\u6a21\u5177\u8303\u5f0f\uff0c\u96c6\u6210\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u4fe1\u606f\u53d8\u5f62\u9884\u6d4b\u548c\u8bef\u5dee\u8865\u507f\u7b56\u7565\u3002\u901a\u8fc7\u5206\u6790\u6210\u5f62\u540e\u70b9\u4e91\u7684\u53d8\u5f62\u6a21\u5f0f\u548c\u8bef\u5dee\u8d8b\u52bf\uff0c\u6821\u6b63\u5f39\u6027\u56de\u5f39\u548c\u7d2f\u79ef\u8bef\u5dee\uff0c\u4fdd\u6301\u4f53\u79ef\u4e00\u81f4\u6027\u548c\u8868\u9762\u8fde\u7eed\u6027\u3002", "result": "\u5728\u4e94\u79cd\u4ee3\u8868\u6027\u51e0\u4f55\u5f62\u72b6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u518d\u73b0\u76ee\u6807\u5f62\u72b6\uff0c\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc798%\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6301\u7eed\u3001\u96f6\u6d6a\u8d39\u7684\u7528\u6237\u81ea\u5b9a\u4e49\u8bbe\u8ba1\u6210\u5f62\u5efa\u7acb\u4e86\u6570\u5b57\u5316\u9a71\u52a8\u3001\u53ef\u91cd\u590d\u7684\u9014\u5f84\uff0c\u8fde\u63a5\u4e86\u6570\u5b57\u5efa\u6a21\u3001\u5b9e\u65f6\u4f20\u611f\u548c\u81ea\u9002\u5e94\u6210\u5f62\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u53ef\u6301\u7eed\u548c\u53ef\u5b9a\u5236\u5236\u9020\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.22043", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22043", "abs": "https://arxiv.org/abs/2511.22043", "authors": ["Xuchen Liu", "Ruocheng Li", "Bin Xin", "Weijia Yao", "Qigeng Duan", "Jinqiang Cui", "Ben M. Chen", "Jie Chen"], "title": "SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields", "comment": "For an experimental demo, see https://www.youtube.com/watch?v=tKYCg266c4o. For the lemma proof, see https://github.com/SmartGroupSystems/GVF_close_loop_planning/blob/main/proofs.md", "summary": "Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f15\u5bfc\u5411\u91cf\u573a\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u6784\u5efaGVF\u5b9e\u73b0\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u95ed\u73af\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56db\u65cb\u7ffc\u5bfc\u822a\u6846\u67b6\u5927\u591a\u91c7\u7528\u5f00\u73af\u65b9\u5f0f\u8fd0\u884c\uff0c\u96be\u4ee5\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u98ce\u6270\u3001\u5916\u90e8\u6270\u52a8\uff09\uff0c\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u673a\u8f7d\u611f\u77e5\u6784\u5efaESDF\u73af\u5883\u8868\u793a\uff0c\u4f7f\u7528\u5168\u5c40\u89c4\u5212\u5668\u751f\u6210\u79bb\u6563\u65e0\u78b0\u649e\u8def\u5f84\u70b9\uff0c\u901a\u8fc7\u5747\u5300B\u6837\u6761\u53c2\u6570\u5316\u751f\u6210\u5e73\u6ed1\u53c2\u8003\u8f68\u8ff9\uff0c\u7136\u540e\u4eceESDF\u548c\u4f18\u5316B\u6837\u6761\u8f68\u8ff9\u5408\u6210\u81ea\u9002\u5e94\u5f15\u5bfc\u5411\u91cf\u573a\uff0c\u5b9e\u73b0\u95ed\u73af\u5bfc\u822a\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfGVF\u65b9\u6cd5\u80fd\u76f4\u63a5\u9002\u5e94\u79bb\u6563\u5316\u8def\u5f84\uff0c\u4e0e\u6807\u51c6\u89c4\u5212\u7b97\u6cd5\u517c\u5bb9\uff0c\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u7edf\u4e00\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u6784\u5efa\u5f15\u5bfc\u5411\u91cf\u573a\uff0c\u5b9e\u73b0\u4e86\u56db\u65cb\u7ffc\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u95ed\u73af\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u3002"}}
{"id": "2511.22100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22100", "abs": "https://arxiv.org/abs/2511.22100", "authors": ["Zelong Zhou", "Wenrui Chen", "Zeyun Hu", "Qiang Diao", "Qixin Gao", "Yaonan Wang"], "title": "Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation", "comment": "7 pages, 8 figures", "summary": "Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u75312\u4e2a\u9a71\u52a8\u5668\u9a71\u52a84\u81ea\u7531\u5ea6\u7684\u4eff\u4eba\u624b\u6307\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u81ea\u9002\u5e94\u6a21\u5757\u5316\u7075\u5de7\u624b\uff0c\u5e73\u8861\u9a71\u52a8\u590d\u6742\u6027\u4e0e\u7075\u5de7\u6027", "motivation": "\u751f\u7269\u534f\u540c\u4f5c\u7528\u5df2\u6210\u4e3a\u7075\u5de7\u624b\u8bbe\u8ba1\u7684\u5e7f\u6cdb\u91c7\u7528\u8303\u5f0f\uff0c\u4f46\u8fc7\u5ea6\u8026\u5408\u4f1a\u964d\u4f4e\u624b\u7684\u7075\u5de7\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9a71\u52a8\u590d\u6742\u6027\u4e0e\u7075\u5de7\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa4\u81ea\u7531\u5ea62\u9a71\u52a8\u7684\u4eff\u4eba\u624b\u6307\u62d3\u6251\u7ed3\u6784\uff1b\u63a2\u7d22\u624b\u90e8\u534f\u540c\u4f5c\u7528\u7684\u751f\u7269\u5b66\u57fa\u7840\u548c\u4eba\u7c7b\u624b\u52bf\u5206\u6790\uff1b\u5c06\u5173\u8282\u7ea7\u534f\u8c03\u548c\u7ed3\u6784\u5c5e\u6027\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u624b\u6307\u67b6\u6784\uff1b\u57fa\u4e8e\u8fd9\u4e9b\u4eff\u751f\u6620\u5c04\u8bbe\u8ba1\u4e94\u6307\u6a21\u5757\u5316\u624b\u5e76\u5efa\u7acb\u5176\u8fd0\u52a8\u5b66\u6a21\u578b", "result": "\u6784\u5efa\u4e86\u7269\u7406\u539f\u578b\u5e76\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u8bbe\u8ba1\u548c\u5206\u6790\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u4eff\u4eba\u624b\u6307\u62d3\u6251\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6210\u529f\u5e73\u8861\u4e86\u9a71\u52a8\u590d\u6742\u6027\u4e0e\u7075\u5de7\u6027\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u6293\u53d6\u548c\u624b\u5185\u64cd\u4f5c"}}
{"id": "2511.22225", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22225", "abs": "https://arxiv.org/abs/2511.22225", "authors": ["Gabriel Aguirre", "Simay Atasoy Bing\u00f6l", "Heiko Hamann", "Jonas Kuckling"], "title": "Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates", "comment": "7 pages, 3 figures, submitted to IEEE MRS 2025", "summary": "Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4f7f\u7b80\u5355\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u5728\u4e24\u4e2a\u5177\u6709\u672a\u77e5\u5371\u9669\u4e8b\u4ef6\u7387\u7684\u533a\u57df\u4e2d\u8bc6\u522b\u66f4\u5b89\u5168\u7684\u533a\u57df\uff0c\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u5efa\u6a21\u5371\u9669\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u5b89\u5168\u51b3\u7b56\u3002", "motivation": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0c\u7fa4\u4f53\u673a\u5668\u4eba\u9700\u8981\u5e73\u8861\u63a2\u7d22\u3001\u901a\u4fe1\u548c\u4e2a\u4f53\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u76f4\u63a5\u6d4b\u91cf\u53d7\u9650\u6216\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc6\u522b\u5b89\u5168\u533a\u57df\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u673a\u5668\u4eba\u4f7f\u7528\u5171\u8f6d\u5148\u9a8c\u9010\u6b65\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u4e4b\u95f4\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5e76\u63a8\u5bfc\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6765\u8c03\u6574\u884c\u4e3a\uff0c\u57fa\u4e8e\u6cca\u677e\u8fc7\u7a0b\u5bf9\u5371\u9669\u4e8b\u4ef6\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u4e00\u81f4\u9009\u62e9\u6b63\u786e\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u6027\u51cf\u5c11\u66b4\u9732\u4e8e\u5371\u9669\u4e8b\u4ef6\u7684\u98ce\u9669\u3002\u76f8\u6bd4\u57fa\u7ebf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u573a\u666f\u6709\u6f5c\u529b\u6269\u5c55\u96c6\u4f53\u51b3\u7b56\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5371\u9669\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u9669\u611f\u77e5\u91c7\u6837\u548c\u63a2\u7d22\u65b9\u9762\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.22238", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22238", "abs": "https://arxiv.org/abs/2511.22238", "authors": ["Ryosuke Ofuchi", "Yuichiro Toda", "Naoki Masuyama", "Takayuki Matsuno"], "title": "MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory", "comment": null, "summary": "This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u81ea\u9002\u5e94\u5171\u632f\u7406\u8bba\u62d3\u6251\u805a\u7c7b\uff08MLATC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece3D\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6784\u5efa\u5168\u5c40\u62d3\u6251\u5730\u56fe\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u663e\u8457\u63d0\u9ad8\u4e86\u539f\u59cbATC-DT\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5730\u56fe\u6784\u5efa\u3002", "motivation": "\u539f\u59cbATC-DT\u7b97\u6cd5\u867d\u7136\u80fd\u591f\u6784\u5efa\u5168\u5c40\u62d3\u6251\u5730\u56fe\u5e76\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f46\u5176\u57fa\u4e8e\u7a77\u4e3e\u6700\u8fd1\u90bb\u641c\u7d22\u7684\u83b7\u80dc\u8282\u70b9\u9009\u62e9\u673a\u5236\u5bfc\u81f4\u968f\u7740\u5730\u56fe\u89c4\u6a21\u589e\u5927\uff0c\u8ba1\u7b97\u6548\u7387\u6025\u5267\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u5c42\u81ea\u9002\u5e94\u5171\u632f\u7406\u8bba\u62d3\u6251\u805a\u7c7b\uff08MLATC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8282\u70b9\u7ec4\u7ec7\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f\u6700\u8fd1\u90bb\u641c\u7d22\u80fd\u591f\u4ece\u7c97\u5230\u7ec6\u5206\u8fa8\u7387\u8fdb\u884c\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u6bcf\u6b21\u67e5\u8be2\u7684\u8ddd\u79bb\u8ba1\u7b97\u6b21\u6570\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u81ea\u9002\u5e94\u5c42\u589e\u52a0\u673a\u5236\uff0c\u5f53\u8f83\u4f4e\u5c42\u9971\u548c\u65f6\u81ea\u52a8\u52a0\u6df1\u5c42\u6b21\u7ed3\u6784\uff0c\u65e0\u9700\u9884\u5148\u56fa\u5b9a\u5c42\u6570\u3002", "result": "\u5728\u5408\u6210\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cMLATC\u76f8\u6bd4\u539f\u59cbATC-DT\u663e\u8457\u52a0\u901f\u4e86\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u641c\u7d22\u65f6\u95f4\u4e0e\u8282\u70b9\u6570\u91cf\u5448\u4e9a\u7ebf\u6027\uff08\u8fd1\u4f3c\u5bf9\u6570\uff09\u5173\u7cfb\u3002\u5728\u6821\u56ed\u89c4\u6a21\u7684\u771f\u5b9e\u4e16\u754cLiDAR\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cMLATC\u4fdd\u6301\u4e86\u6beb\u79d2\u7ea7\u7684\u6bcf\u5e27\u8fd0\u884c\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5168\u5c40\u62d3\u6251\u5730\u56fe\u6784\u5efa\u3002", "conclusion": "MLATC\u901a\u8fc7\u5206\u5c42\u7ec4\u7ec7\u7ed3\u6784\u6709\u6548\u89e3\u51b3\u4e86ATC-DT\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5728\u5927\u89c4\u6a21\u52a8\u6001\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u5168\u5c40\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5927\u89c4\u6a21\u73af\u5883\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22318", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22318", "abs": "https://arxiv.org/abs/2511.22318", "authors": ["Yuki Origane", "Koya Cho", "Hideyuki Tsukagoshi"], "title": "Soft Fluidic Sheet Transistor for Soft Robotic System Enabling Fluid Logic Operations", "comment": "7 pages, 16 figures", "summary": "Aiming to achieve both high functionality and flexibility in soft robot system, this paper presents a soft urethane sheet-like valve with an amplifier that can perform logical operations using only pneumatic signals. When the control chamber in the valve is pressurized, the main path is compressed along its central axis, buckling and being pressed,resulting in blockage. This allows control by a pressure signal smaller than that within the main channel. Furthermore, similar to transistors in electrical circuits, when combined, the proposed valve can perform a variety of logical operations. The basic type operates as a NOT logic element, which is named the fluidic sheet transistor (FST). By integrating multiple FSTs, logical operations such as positive logic, NAND, and NOR can be performed on a single sheet. This paper describes the operating principle, fabrication method, and characteristics of the FST,followed by a method for configuring logical operations.Moreover, we demonstrate the construction of a latch circuit(self-holding logic circuit) using FST, introducing a prototype of a fluid robot system that combines a tactile tube as a fluidic detector and fluid actuators. This demonstrates that it is possible to generate behavior that actively changes posture when hitting an obstacle using only air pressure from a single pipe, which verifies the effectiveness of the proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u67d4\u6027\u805a\u6c28\u916f\u7247\u72b6\u9600\u95e8\uff0c\u901a\u8fc7\u6c14\u52a8\u4fe1\u53f7\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97\uff0c\u7c7b\u4f3c\u7535\u5b50\u6676\u4f53\u7ba1\uff0c\u53ef\u6784\u5efa\u6d41\u4f53\u673a\u5668\u4eba\u7cfb\u7edf", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u529f\u80fd\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ec5\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u6267\u884c\u903b\u8f91\u8fd0\u7b97\u7684\u67d4\u6027\u9600\u95e8\u7cfb\u7edf", "method": "\u8bbe\u8ba1\u8f6f\u805a\u6c28\u916f\u7247\u72b6\u9600\u95e8\uff0c\u5f53\u63a7\u5236\u8154\u52a0\u538b\u65f6\uff0c\u4e3b\u901a\u9053\u6cbf\u4e2d\u5fc3\u8f74\u538b\u7f29\u3001\u5c48\u66f2\u5e76\u88ab\u538b\u7d27\uff0c\u5b9e\u73b0\u963b\u65ad\u3002\u8fd9\u79cd\u9600\u95e8\u7c7b\u4f3c\u6676\u4f53\u7ba1\uff0c\u53ef\u7ec4\u5408\u5b9e\u73b0\u5404\u79cd\u903b\u8f91\u8fd0\u7b97\u3002\u57fa\u672c\u7c7b\u578b\u4f5c\u4e3aNOT\u903b\u8f91\u5143\u4ef6\uff0c\u79f0\u4e3a\u6d41\u4f53\u7247\u6676\u4f53\u7ba1(FST)\u3002\u901a\u8fc7\u96c6\u6210\u591a\u4e2aFST\uff0c\u53ef\u5728\u5355\u5f20\u7247\u4e0a\u5b9e\u73b0\u6b63\u903b\u8f91\u3001NAND\u548cNOR\u7b49\u903b\u8f91\u8fd0\u7b97", "result": "\u6210\u529f\u5f00\u53d1\u4e86FST\uff0c\u63cf\u8ff0\u4e86\u5176\u5de5\u4f5c\u539f\u7406\u3001\u5236\u9020\u65b9\u6cd5\u548c\u7279\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u903b\u8f91\u8fd0\u7b97\u914d\u7f6e\u65b9\u6cd5\u3002\u4f7f\u7528FST\u6784\u5efa\u4e86\u9501\u5b58\u7535\u8def(\u81ea\u4fdd\u6301\u903b\u8f91\u7535\u8def)\uff0c\u7ed3\u5408\u89e6\u89c9\u7ba1\u4f5c\u4e3a\u6d41\u4f53\u68c0\u6d4b\u5668\u548c\u6d41\u4f53\u6267\u884c\u5668\uff0c\u6784\u5efa\u4e86\u6d41\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u539f\u578b", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u5355\u7ba1\u6c14\u538b\u5c31\u80fd\u5b9e\u73b0\u5f53\u78b0\u5230\u969c\u788d\u7269\u65f6\u4e3b\u52a8\u6539\u53d8\u59ff\u6001\u7684\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6c14\u52a8\u903b\u8f91\u63a7\u5236\u65b9\u6848"}}
{"id": "2511.22338", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22338", "abs": "https://arxiv.org/abs/2511.22338", "authors": ["Denghan Xiong", "Yanzhe Zhao", "Yutong Chen", "Zichun Wang"], "title": "Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning", "comment": "14 pages, 5 figures, 1 table, submitted to arXiv", "summary": "Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u963f\u514b\u66fc\u8f6c\u5411\u8f66\u8f86\u5728\u72ed\u7a84\u6b7b\u80e1\u540c\u4e2d\u7684\u975e\u5b8c\u6574\u7ea6\u675f\u5bfc\u822a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u5728\u89e3\u51b3\u6210\u529f\u7387\u3001\u673a\u52a8\u6b21\u6570\u7b49\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u975e\u5b8c\u6574\u7ea6\u675f\u9650\u5236\u4e86\u963f\u514b\u66fc\u8f6c\u5411\u8f66\u8f86\u7684\u53ef\u884c\u901f\u5ea6\uff0c\u4f7f\u5176\u65e0\u6cd5\u6267\u884c\u539f\u5730\u65cb\u8f6c\uff0c\u5728\u72ed\u7a84\u6b7b\u80e1\u540c\u4e2d\u9700\u8981\u590d\u6742\u7684\u8fdb\u9000\u673a\u52a8\u5e8f\u5217\u3002\u4f20\u7edf\u89c4\u5212\u5668\u5c06\u5168\u5c40\u641c\u7d22\u4e0e\u5c40\u90e8\u8f6c\u5411\u89e3\u8026\uff0c\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u91c7\u6837\u6548\u7387\u4f4e\u4e14\u5bf9\u95f4\u9699\u654f\u611f\uff0c\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u6784\u5efa\u751f\u6210\u5668\uff0c\u91c7\u6837\u4e0e\u963f\u514b\u66fc\u8fd0\u52a8\u5b66\u517c\u5bb9\u7684\u591a\u9636\u6bb5\u8fdb\u9000\u8f68\u8ff9\uff0c\u5e76\u81a8\u80c0\u5176\u5305\u7edc\u4ee5\u5408\u6210\u4fdd\u8bc1\u81f3\u5c11\u5b58\u5728\u4e00\u4e2a\u53ef\u884c\u9003\u9038\u8def\u5f84\u7684\u72ed\u7a84\u6b7b\u80e1\u540c\u5bb6\u65cf\uff1b2)\u6784\u5efa\u5f3a\u5236\u6267\u884c\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u4f7f\u7528\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u8bad\u7ec3\u7b56\u7565\uff1b3)\u4e0e\u7ed3\u5408\u5168\u5c40\u641c\u7d22\u548c\u975e\u5b8c\u6574\u8f6c\u5411\u7684\u4ee3\u8868\u6027\u4f20\u7edf\u89c4\u5212\u5668\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5728\u53c2\u6570\u5316\u7684\u6b7b\u80e1\u540c\u5bb6\u65cf\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u89e3\u51b3\u4e86\u66f4\u5927\u6bd4\u4f8b\u7684\u5b9e\u4f8b\uff0c\u51cf\u5c11\u4e86\u673a\u52a8\u6b21\u6570\uff0c\u5728\u76f8\u540c\u611f\u77e5\u548c\u63a7\u5236\u9650\u5236\u4e0b\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8def\u5f84\u957f\u5ea6\u548c\u89c4\u5212\u65f6\u95f4\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u963f\u514b\u66fc\u8f66\u8f86\u7684\u975e\u5b8c\u6574\u72ed\u7a84\u6b7b\u80e1\u540c\u9003\u9038\u95ee\u9898\u4e0a\u4f18\u4e8e\u4f20\u7edf\u89c4\u5212\u5668\uff0c\u4e3a\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u975e\u5b8c\u6574\u7ea6\u675f\u5bfc\u822a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.22354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22354", "abs": "https://arxiv.org/abs/2511.22354", "authors": ["Suraj Borate", "Bhavish Rai B", "Vipul Pardeshi", "Madhu Vadali"], "title": "LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning", "comment": "submitted to ICRA 2026", "summary": "This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.", "AI": {"tldr": "CoMuRoS\u662f\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u5206\u5c42\u67b6\u6784\uff0c\u7528\u4e8e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\uff0c\u7ed3\u5408\u4e86\u96c6\u4e2d\u5f0f\u89c4\u5212\u548c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u652f\u6301\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u65b0\u89c4\u5212\uff0c\u901a\u8fc7LLM\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89e3\u91ca\u3001\u4efb\u52a1\u5206\u914d\u548c\u4ee3\u7801\u751f\u6210\uff0c\u5728\u786c\u4ef6\u548c\u4eff\u771f\u4e2d\u5c55\u793a\u4e86\u81ea\u4e3b\u6062\u590d\u3001\u534f\u8c03\u8fd0\u8f93\u548c\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684LLM\u673a\u5668\u4eba\u7cfb\u7edf\u901a\u5e38\u7f3a\u4e4f\u8fd0\u884c\u65f6\u7684\u4e8b\u4ef6\u9a71\u52a8\u91cd\u65b0\u89c4\u5212\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u73b0\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u52a8\u6001\u73af\u5883\u53d8\u5316\u3001\u4efb\u52a1\u5931\u8d25\u548c\u4eba\u7c7b\u610f\u56fe\u53d8\u5316\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u9876\u5c42\u4efb\u52a1\u7ba1\u7406\u5668LLM\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u3001\u5206\u7c7b\u4efb\u52a1\u5e76\u4f7f\u7528\u9759\u6001\u89c4\u5219\u52a0\u52a8\u6001\u4e0a\u4e0b\u6587\u5206\u914d\u5b50\u4efb\u52a1\uff1b\u6bcf\u4e2a\u673a\u5668\u4eba\u8fd0\u884c\u672c\u5730LLM\uff0c\u4ece\u539f\u59cb\u6280\u80fd\uff08ROS2\u8282\u70b9\u3001\u7b56\u7565\uff09\u7ec4\u5408\u53ef\u6267\u884c\u7684Python\u4ee3\u7801\uff1b\u673a\u8f7d\u611f\u77e5\uff08VLM/\u56fe\u50cf\u5904\u7406\uff09\u6301\u7eed\u76d1\u63a7\u4e8b\u4ef6\u5e76\u5206\u7c7b\u4e3a\u76f8\u5173\u6216\u65e0\u5173\uff1b\u4efb\u52a1\u5931\u8d25\u6216\u7528\u6237\u610f\u56fe\u53d8\u5316\u89e6\u53d1\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u786c\u4ef6\u7814\u7a76\u5c55\u793a\u4e86\u81ea\u4e3b\u6062\u590d\u5e72\u6270\u4e8b\u4ef6\u3001\u8fc7\u6ee4\u65e0\u5173\u5e72\u6270\u3001\u7d27\u5bc6\u534f\u8c03\u8fd0\u8f93\u548c\u6d8c\u73b0\u7684\u4eba\u673a\u534f\u4f5c\uff08\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7269\u4f53\u6062\u590d\u6210\u529f\u7387\uff1a9/10\uff0c\u534f\u8c03\u8fd0\u8f93\uff1a8/8\uff0c\u4eba\u7c7b\u8f85\u52a9\u6062\u590d\uff1a5/5\uff09\u3002\u4eff\u771f\u7814\u7a76\u663e\u793a\u610f\u56fe\u611f\u77e5\u7684\u91cd\u65b0\u89c4\u5212\u3002\u572822\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u5206\u914d\u3001\u5206\u7c7b\u3001IoU\u3001\u53ef\u6267\u884c\u6027\u548c\u6b63\u786e\u6027\u5f97\u5206\u9ad8\uff08\u6b63\u786e\u6027\u6700\u9ad8\u8fbe0.91\uff09\uff0c\u5355\u72ec\u7684\u91cd\u65b0\u89c4\u5212\u96c6\uff085\u4e2a\u573a\u666f\uff09\u8fbe\u52301.0\u6b63\u786e\u6027\u3002", "conclusion": "CoMuRoS\u76f8\u6bd4\u4e4b\u524d\u7684LLM\u7cfb\u7edf\uff0c\u72ec\u7279\u5730\u5c55\u793a\u4e86\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u8fd0\u884c\u65f6\u4e8b\u4ef6\u9a71\u52a8\u91cd\u65b0\u89c4\u5212\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u7075\u6d3b\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u52a8\u6001\u73af\u5883\u53d8\u5316\u548c\u4efb\u52a1\u5931\u8d25\uff0c\u652f\u6301\u81ea\u4e3b\u6062\u590d\u548c\u4eba\u7c7b\u534f\u52a9\u3002"}}
{"id": "2511.22364", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22364", "abs": "https://arxiv.org/abs/2511.22364", "authors": ["Seongwon Cho", "Daechul Ahn", "Donghyun Shin", "Hyeonbeom Choi", "San Kim", "Jonghyun Choi"], "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands", "comment": "12 pages, 8 figures", "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.", "AI": {"tldr": "BINDER\u662f\u4e00\u4e2a\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6218\u7565\u89c4\u5212\u548c\u8fde\u7eed\u73af\u5883\u76d1\u63a7\u6765\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u52a8\u6001\u73af\u5883\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u65b9\u6cd5\u53ea\u5728\u79bb\u6563\u66f4\u65b0\u70b9\uff08\u5982\u5bfc\u822a\u76ee\u6807\u3001\u8def\u5f84\u70b9\u6216\u52a8\u4f5c\u7ed3\u675f\u65f6\uff09\u66f4\u65b0\u4e16\u754c\u8868\u793a\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u66f4\u65b0\u4e4b\u95f4\u5904\u4e8e\"\u76f2\u533a\"\uff0c\u9020\u6210\u7ea7\u8054\u5931\u8d25\uff1a\u9057\u6f0f\u5bf9\u8c61\u3001\u9519\u8bef\u68c0\u6d4b\u5ef6\u8fdf\u548c\u91cd\u89c4\u5212\u6ede\u540e\u3002", "method": "\u63d0\u51faBINDER\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1aDeliberative Response Module\uff08\u591a\u6a21\u6001LLM\u7528\u4e8e\u4efb\u52a1\u89c4\u5212\uff09\u548cInstant Response Module\uff08VideoLLM\u7528\u4e8e\u8fde\u7eed\u76d1\u63a7\uff09\u3002DRM\u8fdb\u884c\u6218\u7565\u89c4\u5212\u5e76\u6307\u5bfcIRM\u5173\u6ce8\u70b9\uff0cIRM\u5206\u6790\u89c6\u9891\u6d41\u66f4\u65b0\u8bb0\u5fc6\u3001\u7ea0\u6b63\u6b63\u5728\u8fdb\u884c\u7684\u52a8\u4f5c\u5e76\u5728\u5fc5\u8981\u65f6\u89e6\u53d1\u91cd\u89c4\u5212\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u7269\u4f53\u653e\u7f6e\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cBINDER\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "conclusion": "BINDER\u901a\u8fc7\u53cc\u5411\u534f\u8c03\u89e3\u51b3\u4e86\u4fdd\u6301\u73af\u5883\u611f\u77e5\u4e0e\u907f\u514d\u6602\u8d35\u66f4\u65b0\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u80fd\u591f\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u9002\u5e94\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.22445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22445", "abs": "https://arxiv.org/abs/2511.22445", "authors": ["Yikai Tang", "Haoran Geng", "Sheng Zang", "Pieter Abbeel", "Jitendra Malik"], "title": "Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion", "comment": null, "summary": "Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.", "AI": {"tldr": "VGDP\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u611f\u77e5\u878d\u5408\u6a21\u5757\u5e73\u8861\u4f7f\u7528RGB\u548c\u70b9\u4e91\u4fe1\u606f\uff0c\u5728\u89c6\u89c9\u548c\u7a7a\u95f4\u968f\u673a\u5316\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u89c6\u89c9\u968f\u673a\u5316\u6761\u4ef6\u4e0b\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u6cdb\u5316\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u89c2\u5bdf\u7f16\u7801\u5668\u6765\u63d0\u5347\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u51e0\u4f55\u6269\u6563\u7b56\u7565(VGDP)\uff0c\u91c7\u7528\u4e92\u8865\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u6a21\u6001\u7ea7dropout\u5f3a\u5236\u5e73\u8861\u4f7f\u7528RGB\u548c\u70b9\u4e91\u7ebf\u7d22\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u4ec5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u5c42\u3002", "result": "\u572818\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cVGDP\u5e73\u5747\u6027\u80fd\u63d0\u534739.1%\uff0c\u5728\u89c6\u89c9\u6270\u52a8\u4e0b\u5e73\u5747\u63d0\u534741.5%\uff0c\u5728\u7a7a\u95f4\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u534715.2%\uff0c\u663e\u8457\u4f18\u4e8e7\u4e2a\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "VGDP\u901a\u8fc7\u5f3a\u5236\u6a21\u6001\u4e92\u8865\u6027\u800c\u975e\u4f9d\u8d56\u4ea4\u53c9\u6ce8\u610f\u529b\u4f5c\u4e3a\u4e3b\u8981\u9c81\u68d2\u6027\u6765\u6e90\uff0c\u5728\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.22505", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22505", "abs": "https://arxiv.org/abs/2511.22505", "authors": ["Xiujian Liang", "Jiacheng Liu", "Mingyang Sun", "Qichen He", "Cewu Lu", "Jianhua Sun"], "title": "RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion", "comment": null, "summary": "Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.", "AI": {"tldr": "\u63d0\u51faRealD\u00b2iff\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5e72\u51c0\u5230\u566a\u58f0\u7684\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5408\u6210\u771f\u5b9e\u566a\u58f0\u6df1\u5ea6\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9sim2real\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u53d7\u9650\u4e8e\u89c6\u89c9sim2real\u5dee\u8ddd\uff0c\u6a21\u62df\u4e2d\u7684\u6df1\u5ea6\u89c2\u6d4b\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4f20\u611f\u5668\u7684\u590d\u6742\u566a\u58f0\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u79cd\u7eaf\u6a21\u62df\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRealD\u00b2iff\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u566a\u58f0\u5206\u89e3\u4e3a\u5168\u5c40\u7ed3\u6784\u5931\u771f\u548c\u5c40\u90e8\u6270\u52a8\uff1b\u91c7\u7528\u9891\u7387\u5f15\u5bfc\u76d1\u7763(FGS)\u8fdb\u884c\u5168\u5c40\u5efa\u6a21\u548c\u5dee\u5f02\u5f15\u5bfc\u4f18\u5316(DGO)\u8fdb\u884c\u5c40\u90e8\u7ec6\u5316\uff1b\u6784\u5efa\u516d\u9636\u6bb5\u6a21\u4eff\u5b66\u4e60\u6d41\u7a0b\u3002", "result": "RealD\u00b2iff\u80fd\u591f\u751f\u6210\u771f\u5b9e\u4e16\u754c\u822c\u7684\u6df1\u5ea6\u6570\u636e\u6784\u5efa\u5e72\u51c0-\u566a\u58f0\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u65e0\u9700\u624b\u52a8\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\uff1b\u5b9e\u73b0\u96f6\u6837\u672csim2real\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u800c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "conclusion": "\u901a\u8fc7\u5e72\u51c0\u5230\u566a\u58f0\u7684\u8303\u5f0f\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\u53cd\u5411\u5b66\u4e60\u5408\u6210\u566a\u58f0\u6df1\u5ea6\uff0c\u6210\u529f\u6865\u63a5\u89c6\u89c9sim2real\u5dee\u8ddd\uff0c\u4e3a\u7eaf\u6a21\u62df\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22541", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22541", "abs": "https://arxiv.org/abs/2511.22541", "authors": ["Jinyang Li", "Marcello Farina", "Luca Mozzarelli", "Luca Cattaneo", "Panita Rattamasanaprapai", "Eleonora A. Tagarelli", "Matteo Corno", "Paolo Perego", "Giuseppe Andreoni", "Emanuele Lettieri"], "title": "BUDD-e: an autonomous robotic guide for visually impaired users", "comment": "14 pages", "summary": "This paper describes the design and the realization of a prototype of the novel guide robot BUDD-e for visually impaired users. The robot has been tested in a real scenario with the help of visually disabled volunteers at ASST Grande Ospedale Metropolitano Niguarda, in Milan. The results of the experimental campaign are throughly described in the paper, displaying its remarkable performance and user-acceptance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u4e3a\u89c6\u969c\u7528\u6237\u8bbe\u8ba1\u7684\u65b0\u578b\u5bfc\u76f2\u673a\u5668\u4ebaBUDD-e\u7684\u539f\u578b\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\uff0c\u5e76\u5728\u7c73\u5170Niguarda\u533b\u9662\u901a\u8fc7\u89c6\u969c\u5fd7\u613f\u8005\u8fdb\u884c\u4e86\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6027\u80fd\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "motivation": "\u4e3a\u89c6\u969c\u7528\u6237\u5f00\u53d1\u5b9e\u7528\u7684\u5bfc\u76f2\u673a\u5668\u4eba\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5728\u590d\u6742\u73af\u5883\u4e2d\u72ec\u7acb\u5bfc\u822a\uff0c\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86BUDD-e\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\uff0c\u5728\u7c73\u5170Niguarda\u533b\u9662\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u4e0e\u89c6\u969c\u5fd7\u613f\u8005\u5408\u4f5c\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u8868\u73b0\u51fa\u8272\uff0c\u83b7\u5f97\u4e86\u826f\u597d\u7684\u7528\u6237\u63a5\u53d7\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "BUDD-e\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\u5728\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u89c6\u969c\u7528\u6237\u7684\u8f85\u52a9\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22685", "abs": "https://arxiv.org/abs/2511.22685", "authors": ["Haoyi Wang", "Licheng Luo", "Yiannis Kantaros", "Bruno Sinopoli", "Mingyu Cai"], "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation", "comment": null, "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u5bc6\u96c6\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9762\u4e34\u57fa\u672c\u6311\u6218\uff1a\u9700\u8981\u5728\u53cd\u5e94\u5f0f\u907f\u78b0\u4e0e\u957f\u671f\u76ee\u6807\u8fbe\u6210\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u5728\u72ed\u7a84\u901a\u9053\u6216\u53d7\u9650\u7a7a\u95f4\u4e2d\uff0c\u6b7b\u9501\u95ee\u9898\u9891\u7e41\u51fa\u73b0\uff0c\u7279\u522b\u662f\u5f53\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u9047\u5230\u8d85\u51fa\u5b66\u4e60\u5206\u5e03\u7684\u65b0\u914d\u7f6e\u65f6\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u65e0\u7f1d\u96c6\u6210\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u3002\u5305\u542b\u5b89\u5168\u5c42\u76d1\u63a7\u667a\u80fd\u4f53\u8fdb\u5ea6\u4ee5\u68c0\u6d4b\u6b7b\u9501\uff0c\u89e6\u53d1\u65f6\u542f\u52a8\u53d7\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u534f\u8c03\u63a7\u5236\u5668\u3002\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6784\u5efa\u5168\u5c40\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u8c03\u8282\u822a\u70b9\u8fdb\u5ea6\u4ee5\u51cf\u5c11\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u667a\u80fd\u4f53\u95f4\u51b2\u7a81\u3002", "result": "\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u4ece\u8fb9\u7f18\u63d0\u5347\u5230\u63a5\u8fd1\u666e\u904d\u6210\u529f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6b7b\u9501\u548c\u78b0\u649e\u3002\u4e0e\u5206\u5c42\u4efb\u52a1\u89c4\u5212\u96c6\u6210\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u5f02\u6784\u673a\u5668\u4eba\u7684\u534f\u8c03\u5bfc\u822a\u3002", "conclusion": "\u5c06\u53cd\u5e94\u5f0f\u5f3a\u5316\u5b66\u4e60\u5bfc\u822a\u4e0e\u9009\u62e9\u6027\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u5e72\u9884\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.22697", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22697", "abs": "https://arxiv.org/abs/2511.22697", "authors": ["Chancharik Mitra", "Yusen Luo", "Raj Saravanan", "Dantong Niu", "Anirudh Pai", "Jesse Thomason", "Trevor Darrell", "Abrar Anwar", "Deva Ramanan", "Roei Herzig"], "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations", "comment": null, "summary": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.", "AI": {"tldr": "Robotic Steering\uff1a\u4e00\u79cd\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684VLA\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u6f14\u793a\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u5fae\u8c03\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u7269\u7406\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u9700\u6c42\u5bf9\u9f50\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u76f8\u6bd4LoRA\u5728\u4efb\u52a1\u53d8\u5316\u4e0b\u8868\u73b0\u66f4\u4f18\u3001\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3001\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u7f3a\u4e4f\u7279\u5f02\u6027\uff0c\u65e0\u8bba\u4efb\u52a1\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u7269\u7406\u7279\u6027\u5982\u4f55\u90fd\u8c03\u6574\u76f8\u540c\u7684\u53c2\u6570\u96c6\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u529f\u80fd\u7279\u5f02\u6027\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u8ba4\u4e3a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u7a00\u758f\u6a21\u578b\u8868\u793a\u66f4\u6709\u6548\u3002", "method": "\u63d0\u51faRobotic Steering\u65b9\u6cd5\uff0c\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u5229\u7528\u5c11\u6837\u672c\u6f14\u793a\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u5fae\u8c03\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u7269\u7406\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u9700\u6c42\u5bf9\u9f50\u7684\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002", "result": "\u5728Franka Emika\u673a\u5668\u4eba\u81c2\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0cRobotic Steering\u5728\u4efb\u52a1\u53d8\u5316\u4e0b\u6bd4LoRA\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3001\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Robotic Steering\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u4e3a\u9002\u5e94\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684VLA\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2511.22705", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22705", "abs": "https://arxiv.org/abs/2511.22705", "authors": ["Ian Lalonde", "Jeff Denis", "Mathieu Lamy", "Camille Martin", "Karina Lebel", "Alexandre Girard"], "title": "A Two Degrees-of-Freedom Floor-Based Robot for Transfer and Rehabilitation Applications", "comment": "13 pages, 16 figures", "summary": "The ability to accomplish a sit-to-stand (STS) motion is key to increase functional mobility and reduce rehospitalization risks. While raising aid (transfer) devices and partial bodyweight support (rehabilitation) devices exist, both are unable to adjust the STS training to different mobility levels. Therefore, We have developed an STS training device that allows various configurations of impedance and vertical/forward forces to adapt to many training needs while maintaining commercial raising aid transfer capabilities. Experiments with healthy adults (both men and women) of various heights and weights show that the device 1) has a low impact on the natural STS kinematics, 2) can provide precise weight unloading at the patient's center of mass and 3) can add a forward virtual spring to assist the transfer of the bodyweight to the feet for seat-off, at the start of the STS motion.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\uff0c\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u963b\u6297\u548c\u5782\u76f4/\u524d\u5411\u529b\u6765\u9002\u5e94\u4e0d\u540c\u6d3b\u52a8\u80fd\u529b\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u5546\u4e1a\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u7684\u8f6c\u79fb\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u548c\u90e8\u5206\u4f53\u91cd\u652f\u6491\u8bbe\u5907\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u7684\u6d3b\u52a8\u80fd\u529b\u6c34\u5e73\u8c03\u6574\u5750\u7acb\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u591a\u79cd\u8bad\u7ec3\u9700\u6c42\u7684\u8bbe\u5907\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\uff0c\u5141\u8bb8\u914d\u7f6e\u4e0d\u540c\u7684\u963b\u6297\u548c\u5782\u76f4/\u524d\u5411\u529b\uff0c\u4ee5\u9002\u5e94\u7528\u6237\u7684\u8bad\u7ec3\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u5546\u4e1a\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u7684\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u5bf9\u5065\u5eb7\u6210\u5e74\u4eba\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u8bbe\u5907\u5bf9\u81ea\u7136\u5750\u7acb\u8fd0\u52a8\u5b66\u5f71\u54cd\u5c0f\uff1b2\uff09\u80fd\u5728\u60a3\u8005\u8d28\u5fc3\u63d0\u4f9b\u7cbe\u786e\u7684\u51cf\u91cd\u652f\u6301\uff1b3\uff09\u80fd\u5728\u5750\u7acb\u52a8\u4f5c\u5f00\u59cb\u65f6\u589e\u52a0\u865a\u62df\u524d\u5411\u5f39\u7c27\u8f85\u52a9\u4f53\u91cd\u8f6c\u79fb\u5230\u811a\u90e8\u3002", "conclusion": "\u8be5\u8bbe\u5907\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u8bad\u7ec3\u9700\u6c42\uff0c\u6709\u6548\u8f85\u52a9\u5750\u7acb\u52a8\u4f5c\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.22744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22744", "abs": "https://arxiv.org/abs/2511.22744", "authors": ["R\u00e9my Rahem", "Wael Suleiman"], "title": "Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion", "comment": "12 pages, 6 figures, code available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "summary": "Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "AI": {"tldr": "\u591a\u89c6\u89d2\u6df1\u5ea6\u611f\u77e5\u6846\u67b6\u7ed3\u5408\u673a\u5668\u4eba\u672c\u4f53\u89c6\u89d2\u548c\u5916\u90e8\u89c6\u89d2\uff0c\u901a\u8fc7\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\u5b66\u4e60\u878d\u5408\u53cc\u6df1\u5ea6\u6d41\uff0c\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027", "motivation": "\u73b0\u6709\u817f\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u672c\u4f53\u89c6\u89d2\u611f\u77e5\uff0c\u5f53\u673a\u5668\u4eba\u89c6\u89d2\u88ab\u906e\u6321\u65f6\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u589e\u5f3a\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u4e92\u8865\u4fe1\u606f\u63d0\u5347\u8fd0\u52a8\u6027\u80fd", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u6df1\u5ea6\u611f\u77e5\u8fd0\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u672c\u4f53\u89c6\u89d2\u548c\u5916\u90e8\u89c6\u89d2\u89c2\u6d4b\u3002\u91c7\u7528\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\uff0c\u5b66\u751f\u7b56\u7565\u5b66\u4e60\u878d\u5408\u672c\u4f53\u611f\u77e5\u548c\u53cc\u6df1\u5ea6\u6d41\uff0c\u5e76\u5f15\u5165\u5e7f\u6cdb\u57df\u968f\u673a\u5316\uff08\u5305\u62ec\u968f\u673a\u8fdc\u7a0b\u76f8\u673a\u4e22\u5931\u548c3D\u4f4d\u7f6e\u6270\u52a8\uff09\u6765\u6a21\u62df\u7a7a\u5730\u534f\u540c\u611f\u77e5", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u591a\u89c6\u89d2\u7b56\u7565\u5728\u8de8\u8d8a\u95f4\u9699\u3001\u53f0\u9636\u4e0b\u964d\u7b49\u52a8\u6001\u52a8\u4f5c\u4e0a\u4f18\u4e8e\u5355\u89c6\u89d2\u57fa\u7ebf\uff0c\u5f53\u5916\u90e8\u76f8\u673a\u90e8\u5206\u6216\u5b8c\u5168\u4e0d\u53ef\u7528\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u3002\u4e2d\u7b49\u7a0b\u5ea6\u7684\u89c6\u89d2\u9519\u4f4d\u5728\u8bad\u7ec3\u4e2d\u88ab\u826f\u597d\u5bb9\u5fcd", "conclusion": "\u5f02\u6784\u89c6\u89c9\u53cd\u9988\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u591a\u89c6\u89d2\u611f\u77e5\u5728\u52a8\u6001\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u76f8\u5173\u5b9e\u73b0\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u53ef\u590d\u73b0\u6027"}}
{"id": "2511.22773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22773", "abs": "https://arxiv.org/abs/2511.22773", "authors": ["Rui Heng Yang", "Xuan Zhao", "Leo Maxime Brunswic", "Montgomery Alban", "Mateo Clemente", "Tongtong Cao", "Jun Jin", "Amir Rasouli"], "title": "CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance", "comment": "4 tables, 9 figures", "summary": "In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.", "AI": {"tldr": "CAPE\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u8fd1\u7aef\u6a21\u5f0f\u6269\u5c55\u6765\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u907f\u969c\u7b49\u6311\u6218\u6027\u4efb\u52a1\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u80fd\u6355\u6349\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u4f46\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8fd9\u5728\u907f\u969c\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u6210\u672c\u9ad8\u6602\u3002\u6d4b\u8bd5\u65f6\u9700\u8981\u6cdb\u5316\u5230\u591a\u79cd\u969c\u788d\u7269\u7c7b\u578b\u548c\u7a7a\u95f4\u914d\u7f6e\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51faCAPE\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\u548c\u5f15\u5bfc\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\u3002\u91c7\u7528\u5148\u9a8c\u79cd\u5b50\u8fed\u4ee3\u5f15\u5bfc\u7cbe\u70bc\u8fc7\u7a0b\uff1a\u751f\u6210\u521d\u59cb\u8f68\u8ff9\u8ba1\u5212\uff0c\u6267\u884c\u77ed\u524d\u7f00\u8f68\u8ff9\uff0c\u5c06\u5269\u4f59\u8f68\u8ff9\u6270\u52a8\u5230\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u5f62\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\uff0c\u7136\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5f15\u5bfc\u53bb\u566a\u8fed\u4ee3\u6269\u5c55\u6a21\u5f0f\u652f\u6301\u3002", "result": "\u5728\u6742\u4e71\u672a\u89c1\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u8bc4\u4f30CAPE\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u9ad8\u8fbe26%\u548c80%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u91c7\u6837\u65e0\u78b0\u649e\u8f68\u8ff9\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u4e00\u81f4\u6027\u3002", "conclusion": "CAPE\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\u548c\u5f15\u5bfc\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u907f\u969c\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22777", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22777", "abs": "https://arxiv.org/abs/2511.22777", "authors": ["Sajjad Pakdamansavoji", "Mozhgan Pourkeshavarz", "Adam Sigal", "Zhiyuan Li", "Rui Heng Yang", "Amir Rasouli"], "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery", "comment": "11 figures, 3 tables", "summary": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.", "AI": {"tldr": "NICE\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u56fe\u50cf\u4fee\u590d\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\uff0c\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u5dee\u8ddd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u8bad\u7ec3", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c6\u89c9\u5e72\u6270\u7269\u4f1a\u663e\u8457\u964d\u4f4e\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u89c6\u89c9\u9c81\u68d2\u6027", "method": "\u5229\u7528\u56fe\u50cf\u751f\u6210\u6846\u67b6\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u73b0\u6709\u6f14\u793a\u6570\u636e\u8fdb\u884c\u4e09\u79cd\u7f16\u8f91\u64cd\u4f5c\uff1a\u5bf9\u8c61\u66ff\u6362\u3001\u91cd\u65b0\u98ce\u683c\u5316\u548c\u79fb\u9664\u5e72\u6270\u7269\uff0c\u4fdd\u6301\u7a7a\u95f4\u5173\u7cfb\u548c\u52a8\u4f5c\u6807\u7b7e\u4e00\u81f4\u6027", "result": "\u5728\u9ad8\u5ea6\u6742\u4e71\u573a\u666f\u4e2d\uff0c\u7a7a\u95f4\u53ef\u53ca\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc720%\uff1b\u5728\u542b\u5e72\u6270\u7269\u7684\u73af\u5883\u4e2d\uff0c\u64cd\u4f5c\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad811%\uff1b\u76ee\u6807\u6df7\u6dc6\u7387\u964d\u4f4e6%\uff0c\u78b0\u649e\u7387\u51cf\u5c117%", "conclusion": "NICE\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u6709\u6548\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u7684\u5206\u5e03\u5916\u5dee\u8ddd\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u8bad\u7ec3"}}
{"id": "2511.22780", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22780", "abs": "https://arxiv.org/abs/2511.22780", "authors": ["Amir Rasouli", "Montgomery Alban", "Sajjad Pakdamansavoji", "Zhiyuan Li", "Zhanguang Zhang", "Aaron Wu", "Xuan Zhao"], "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation", "comment": "12 figures, 2 tables", "summary": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u6027\u80fd\u7684\u534f\u8bae\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d1\u73b0\u573a\u666f\u6742\u4e71\u4f1a\u663e\u8457\u964d\u4f4eVLA\u6a21\u578b\u6027\u80fd\u8fbe34%\uff0c\u4e0d\u540c\u7b56\u7565\u6709\u72ec\u7279\u5f31\u70b9\uff0c\u4e14\u5fae\u8c03\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u6742\u4e71\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u6027\u80fd\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6807\u51c6\u6765\u7efc\u5408\u8003\u8651\u73af\u5883\u56e0\u7d20\u3001\u5e72\u6270\u7269\u6570\u91cf\u3001\u7279\u5f81\u548c\u6392\u5217\u65b9\u5f0f\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5fc3\u7406\u7269\u7406\u5b66\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6807\u51c6\uff0c\u5728\u8d85\u73b0\u5b9e\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7cfb\u7edf\u6784\u5efa\u8bc4\u4f30\u573a\u666f\uff0c\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u6742\u4e71\u5ea6\u91cf\u4e0e\u6027\u80fd\u4e0b\u964d\u7684\u5173\u7cfb\u4ee5\u53ca\u5e72\u6270\u7269\u7684\u6570\u91cf\u548c\u906e\u6321\u5f71\u54cd\u3002", "result": "\u573a\u666f\u6742\u4e71\u663e\u8457\u964d\u4f4e\u7b56\u7565\u6027\u80fd\u8fbe34%\uff1b\u4e0d\u540cVLA\u7b56\u7565\u867d\u7136\u5e73\u5747\u6027\u80fd\u76f8\u4f3c\uff0c\u4f46\u5404\u6709\u72ec\u7279\u5f31\u70b9\u4e14\u6210\u529f\u573a\u666f\u4e00\u81f4\u6027\u8f83\u4f4e\uff1b\u6742\u4e71\u5ea6\u91cf\u80fd\u6709\u6548\u6307\u793a\u6027\u80fd\u4e0b\u964d\uff1b\u5e72\u6270\u7269\u6570\u91cf\u548c\u906e\u6321\u5f71\u54cd\u663e\u8457\uff1b\u57fa\u4e8e\u589e\u5f3a\u6570\u636e\u7684\u5fae\u8c03\u867d\u7136\u6709\u6548\uff0c\u4f46\u4e0d\u80fd\u5e73\u7b49\u89e3\u51b3\u6240\u6709\u6742\u4e71\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6807\u51c6\u80fd\u6709\u6548\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u4e0d\u540c\u7b56\u7565\u6709\u72ec\u7279\u8106\u5f31\u6027\uff0c\u4ec5\u9760\u6570\u636e\u589e\u5f3a\u5fae\u8c03\u4e0d\u8db3\u4ee5\u5b8c\u5168\u89e3\u51b3\u6742\u4e71\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u66f4\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2511.22829", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22829", "abs": "https://arxiv.org/abs/2511.22829", "authors": ["Zhen Tian", "Zhihao Lin"], "title": "Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation", "comment": null, "summary": "This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u9053\u53d8\u6362\u7b49\u590d\u6742\u573a\u666f\u7684\u65b0\u578b\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u98ce\u9669\u611f\u77e5\u89c4\u5212\u4e0e\u4fdd\u8bc1\u78b0\u649e\u907f\u514d\u96c6\u6210\u5230\u7edf\u4e00\u4f18\u5316\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u9ad8\u6548\u3001\u8212\u9002\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\uff0c\u7279\u522b\u662f\u5728\u8f66\u9053\u53d8\u6362\u7b49\u4ea4\u4e92\u573a\u666f\u4e2d\u9700\u8981\u5e73\u8861\u98ce\u9669\u89c4\u907f\u4e0e\u8f68\u8ff9\u5e73\u6ed1\u6027\u3002", "method": "\u9996\u5148\u6784\u5efa\u52a8\u6001\u98ce\u9669\u573a(DRF)\u6355\u6349\u9759\u6001\u548c\u52a8\u6001\u78b0\u649e\u98ce\u9669\uff0c\u7136\u540e\u751f\u6210\u65f6\u53d8\u51f8\u53ef\u884c\u7a7a\u95f4\u786e\u4fdd\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5b89\u5168\u6027\uff0c\u6700\u540e\u5c06\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u5efa\u6a21\u4e3a\u6709\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u7ea6\u675f\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668(iLQR)\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u5e73\u6ed1\u6027\u3001\u63a7\u5236\u52aa\u529b\u548c\u98ce\u9669\u66b4\u9732\u3002", "result": "\u5728\u5bc6\u96c6\u73af\u5c9b\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4APF\u3001MPC\u548cRRT\u57fa\u51c6\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u9002\u5e94\u6027\uff0c\u4ea7\u751f\u66f4\u5927\u7684\u5b89\u5168\u88d5\u5ea6\u3001\u66f4\u4f4e\u7684\u6025\u52a8\u5ea6\u548c\u66f4\u4f18\u7684\u66f2\u7387\u5e73\u6ed1\u6027\uff0c\u8f66\u9053\u53d8\u6362\u8ddd\u79bb(28.59 m)\u548c\u65f6\u95f4(2.84 s)\u66f4\u77ed\u3002", "conclusion": "\u96c6\u6210\u7684DRF\u4e0e\u51f8\u53ef\u884c\u7a7a\u95f4\u4ee5\u53ca\u7ea6\u675fiLQR\u6c42\u89e3\u5668\u4e3a\u52a8\u6001\u4ea4\u4e92\u4ea4\u901a\u573a\u666f\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u9ad8\u6548\u3001\u8212\u9002\u7684\u5e73\u8861\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2511.22847", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22847", "abs": "https://arxiv.org/abs/2511.22847", "authors": ["Yuying Zhang", "Na Fan", "Haowen Zheng", "Junning Liang", "Zongliang Pan", "Qifeng Chen", "Ximin Lyu"], "title": "Threat-Aware UAV Dodging of Human-Thrown Projectiles with an RGB-D Camera", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) performing tasks such as transportation and aerial photography are vulnerable to intentional projectile attacks from humans. Dodging such a sudden and fast projectile poses a significant challenge for UAVs, requiring ultra-low latency responses and agile maneuvers. Drawing inspiration from baseball, in which pitchers' body movements are analyzed to predict the ball's trajectory, we propose a novel real-time dodging system that leverages an RGB-D camera. Our approach integrates human pose estimation with depth information to predict the attacker's motion trajectory and the subsequent projectile trajectory. Additionally, we introduce an uncertainty-aware dodging strategy to enable the UAV to dodge incoming projectiles efficiently. Our perception system achieves high prediction accuracy and outperforms the baseline in effective distance and latency. The dodging strategy addresses temporal and spatial uncertainties to ensure UAV safety. Extensive real-world experiments demonstrate the framework's reliable dodging capabilities against sudden attacks and its outstanding robustness across diverse scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB-D\u76f8\u673a\u548c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65e0\u4eba\u673a\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\uff0c\u7528\u4e8e\u9632\u5fa1\u4eba\u7c7b\u6295\u63b7\u7684\u7a81\u7136\u6027\u9ad8\u901f\u629b\u5c04\u7269\u653b\u51fb\u3002", "motivation": "\u6267\u884c\u8fd0\u8f93\u3001\u822a\u62cd\u7b49\u4efb\u52a1\u7684\u65e0\u4eba\u673a\u5bb9\u6613\u53d7\u5230\u4eba\u7c7b\u6545\u610f\u6295\u63b7\u629b\u5c04\u7269\u7684\u653b\u51fb\u3002\u8eb2\u907f\u8fd9\u79cd\u7a81\u7136\u4e14\u9ad8\u901f\u7684\u629b\u5c04\u7269\u5bf9\u65e0\u4eba\u673a\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\uff0c\u9700\u8981\u8d85\u4f4e\u5ef6\u8fdf\u54cd\u5e94\u548c\u654f\u6377\u673a\u52a8\u80fd\u529b\u3002", "method": "\u53d7\u68d2\u7403\u8fd0\u52a8\u4e2d\u901a\u8fc7\u5206\u6790\u6295\u624b\u8eab\u4f53\u52a8\u4f5c\u9884\u6d4b\u7403\u8f68\u8ff9\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u7cfb\u7edf\u3002\u901a\u8fc7RGB-D\u76f8\u673a\u9884\u6d4b\u653b\u51fb\u8005\u7684\u8fd0\u52a8\u8f68\u8ff9\u548c\u629b\u5c04\u7269\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8eb2\u907f\u7b56\u7565\u3002", "result": "\u611f\u77e5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u6709\u6548\u8ddd\u79bb\u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u8eb2\u907f\u7b56\u7565\u80fd\u591f\u5904\u7406\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u4ee5\u786e\u4fdd\u65e0\u4eba\u673a\u5b89\u5168\u3002\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u7a81\u7136\u653b\u51fb\u4e0b\u7684\u53ef\u9760\u8eb2\u907f\u80fd\u529b\u548c\u8de8\u573a\u666f\u7684\u51fa\u8272\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u8eb2\u907f\u4eba\u7c7b\u629b\u5c04\u7269\u653b\u51fb\u7684\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u4f53\u59ff\u6001\u5206\u6790\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b56\u7565\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22860", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22860", "abs": "https://arxiv.org/abs/2511.22860", "authors": ["Sacchin Sundar", "Atman Kikani", "Aaliya Alam", "Sumukh Shrote", "A. Nayeemulla Khan", "A. Shahina"], "title": "MARVO: Marine-Adaptive Radiance-aware Visual Odometry", "comment": "10 pages, 5 figures, 3 tables, Submitted to CVPR2026", "summary": "Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).", "AI": {"tldr": "MARVO\u662f\u4e00\u4e2a\u878d\u5408\u6c34\u4e0b\u7269\u7406\u6a21\u578b\u3001\u53ef\u5fae\u5206\u5339\u914d\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u89c6\u89c9-\u60ef\u6027-\u6c14\u538b\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "motivation": "\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u9762\u4e34\u6ce2\u957f\u76f8\u5173\u8870\u51cf\u3001\u7eb9\u7406\u5dee\u548c\u975e\u9ad8\u65af\u4f20\u611f\u5668\u566a\u58f0\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u590d\u6742\u7684\u6c34\u4e0b\u73af\u5883\u6761\u4ef6\u3002", "method": "1. \u524d\u7aef\uff1a\u57fa\u4e8eTransformer\u7684\u7279\u5f81\u5339\u914d\u5668\uff0c\u52a0\u5165\u7269\u7406\u611f\u77e5\u8f90\u5c04\u9002\u914d\u5668\u8865\u507f\u989c\u8272\u901a\u9053\u8870\u51cf\u548c\u5bf9\u6bd4\u5ea6\u635f\u5931\uff1b2. \u540e\u7aef\uff1a\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u89c6\u89c9-\u60ef\u6027-\u6c14\u538b\u4f30\u8ba1\u5668\uff0c\u4f7f\u7528GTSAM\u5e93\uff1b3. \u5168\u5c40\u4f18\u5316\uff1a\u5f3a\u5316\u5b66\u4e60\u59ff\u6001\u56fe\u4f18\u5316\u5668\uff0c\u5b66\u4e60SE(2)\u4e0a\u7684\u6700\u4f18\u56de\u7f29\u52a8\u4f5c\u3002", "result": "\u6846\u67b6\u80fd\u591f\u5728\u6c34\u4e0b\u6d51\u6d4a\u6761\u4ef6\u4e0b\u4ea7\u751f\u51e0\u4f55\u4e00\u81f4\u7684\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5168\u72b6\u6001\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\uff0c\u5e76\u80fd\u8d85\u8d8a\u7ecf\u5178\u6700\u5c0f\u4e8c\u4e58\u6c42\u89e3\u5668\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u4f18\u5316\u5168\u5c40\u8f68\u8ff9\u3002", "conclusion": "MARVO\u901a\u8fc7\u878d\u5408\u7269\u7406\u5efa\u6a21\u3001\u53ef\u5fae\u5206\u5339\u914d\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4e3a\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u7684\u7279\u6b8a\u6311\u6218\u3002"}}
{"id": "2511.22865", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22865", "abs": "https://arxiv.org/abs/2511.22865", "authors": ["Wonjeong Ryu", "Seungjun Yu", "Seokha Moon", "Hojun Choi", "Junsung Park", "Jinkyu Kim", "Hyunjung Shim"], "title": "SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving", "comment": null, "summary": "End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u76f8\u673a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u5728BEV\u7a7a\u95f4\u76f4\u63a5\u4f30\u8ba1\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5e76\u878d\u5165\u89c4\u5212\uff0c\u751f\u6210\u5bc6\u96c6\u7684\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53ef\u884c\u9a76\u5730\u56fe\uff0c\u7ed3\u5408\u8f66\u9053\u8ddf\u968f\u6b63\u5219\u5316\u5b9e\u73b0\u9c81\u68d2\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff1a\u5047\u8bbe\u611f\u77e5\u8f93\u51fa\u5b8c\u5168\u53ef\u9760\uff0c\u5373\u4f7f\u5728\u6a21\u7cca\u6216\u89c2\u6d4b\u4e0d\u826f\u7684\u573a\u666f\u4e2d\u4e5f\u4e0d\u8003\u8651\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u89c4\u5212\u5668\u7f3a\u4e4f\u660e\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u3002", "method": "1. \u5728BEV\u7a7a\u95f4\u76f4\u63a5\u4f30\u8ba1\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff1b2. \u751f\u6210\u5bc6\u96c6\u7684\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53ef\u884c\u9a76\u5730\u56fe\uff0c\u540c\u65f6\u6355\u83b7\u8bed\u4e49\u7ed3\u6784\u548c\u51e0\u4f55\u5e03\u5c40\uff1b3. \u5f15\u5165\u8f66\u9053\u8ddf\u968f\u6b63\u5219\u5316\uff0c\u7f16\u7801\u8f66\u9053\u7ed3\u6784\u548c\u4ea4\u901a\u89c4\u8303\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684NAVHARD\u548cNAVSAFE\u5b50\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e0e\u9a7e\u9a76\u5148\u9a8c\u7ed3\u5408\u80fd\u663e\u8457\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "conclusion": "\u901a\u8fc7\u539f\u5219\u6027\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7ed3\u5408\u9a7e\u9a76\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eaf\u76f8\u673a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u6311\u6218\u6027\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2511.22928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22928", "abs": "https://arxiv.org/abs/2511.22928", "authors": ["Jiaxin Liu", "Xiangyu Yan", "Liang Peng", "Lei Yang", "Lingjun Zhang", "Yuechen Luo", "Yueming Tao", "Ashton Yu Xuan Tan", "Mu Li", "Lei Zhang", "Ziqi Zhan", "Sai Guo", "Hong Wang", "Jun Li"], "title": "Seeing before Observable: Potential Risk Reasoning in Autonomous Driving via Vision Language Models", "comment": null, "summary": "Ensuring safety remains a key challenge for autonomous vehicles (AVs), especially in rare and complex scenarios. One critical but understudied aspect is the \\textbf{potential risk} situations, where the risk is \\textbf{not yet observable} but can be inferred from subtle precursors, such as anomalous behaviors or commonsense violations. Recognizing these precursors requires strong semantic understanding and reasoning capabilities, which are often absent in current AV systems due to the scarcity of such cases in existing driving or risk-centric datasets. Moreover, current autonomous driving accident datasets often lack annotations of the causal reasoning chains behind incidents, which are essential for identifying potential risks before they become observable. To address these gaps, we introduce PotentialRiskQA, a novel vision-language dataset designed for reasoning about potential risks prior to observation. Each sample is annotated with structured scene descriptions, semantic precursors, and inferred risk outcomes. Based on this dataset, we further propose PR-Reasoner, a vision-language-model-based framework tailored for onboard potential risk reasoning. Experimental results show that fine-tuning on PotentialRiskQA enables PR-Reasoner to significantly enhance its performance on the potential risk reasoning task compared to baseline VLMs. Together, our dataset and model provide a foundation for developing autonomous systems with improved foresight and proactive safety capabilities, moving toward more intelligent and resilient AVs.", "AI": {"tldr": "\u63d0\u51fa\u4e86PotentialRiskQA\u6570\u636e\u96c6\u548cPR-Reasoner\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u98ce\u9669\u53ef\u89c2\u6d4b\u524d\u8fdb\u884c\u6f5c\u5728\u98ce\u9669\u63a8\u7406", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u98ce\u9669\uff08\u98ce\u9669\u5c1a\u672a\u53ef\u89c2\u6d4b\u4f46\u53ef\u4ece\u7ec6\u5fae\u524d\u5146\u63a8\u65ad\uff09\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u6b64\u7c7b\u7f55\u89c1\u590d\u6742\u573a\u666f\u548c\u56e0\u679c\u63a8\u7406\u94fe\u6807\u6ce8", "method": "\u5f15\u5165PotentialRiskQA\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u3001\u8bed\u4e49\u524d\u5146\u548c\u63a8\u65ad\u98ce\u9669\u7ed3\u679c\u6807\u6ce8\uff1b\u63d0\u51faPR-Reasoner\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\u8fdb\u884c\u8f66\u8f7d\u6f5c\u5728\u98ce\u9669\u63a8\u7406", "result": "\u5728PotentialRiskQA\u4e0a\u5fae\u8c03\u7684PR-Reasoner\u5728\u6f5c\u5728\u98ce\u9669\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u597d\u9884\u89c1\u6027\u548c\u4e3b\u52a8\u5b89\u5168\u80fd\u529b\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u66f4\u667a\u80fd\u3001\u66f4\u5177\u97e7\u6027\u7684\u81ea\u52a8\u9a7e\u9a76\u53d1\u5c55"}}
{"id": "2511.22963", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22963", "abs": "https://arxiv.org/abs/2511.22963", "authors": ["Zhirui Liu", "Kaiyang Ji", "Ke Yang", "Jingyi Yu", "Ye Shi", "Jingya Wang"], "title": "Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary", "comment": "Project page: https://humanoidlla.github.io/", "summary": "Enabling humanoid robots to follow free-form language commands is critical for seamless human-robot interaction, collaborative task execution, and general-purpose embodied intelligence. While recent advances have improved low-level humanoid locomotion and robot manipulation, language-conditioned whole-body control remains a significant challenge. Existing methods are often limited to simple instructions and sacrifice either motion diversity or physical plausibility. To address this, we introduce Humanoid-LLA, a Large Language Action Model that maps expressive language commands to physically executable whole-body actions for humanoid robots. Our approach integrates three core components: a unified motion vocabulary that aligns human and humanoid motion primitives into a shared discrete space; a vocabulary-directed controller distilled from a privileged policy to ensure physical feasibility; and a physics-informed fine-tuning stage using reinforcement learning with dynamics-aware rewards to enhance robustness and stability. Extensive evaluations in simulation and on a real-world Unitree G1 humanoid show that Humanoid-LLA delivers strong language generalization while maintaining high physical fidelity, outperforming existing language-conditioned controllers in motion naturalness, stability, and execution success rate.", "AI": {"tldr": "Humanoid-LLA\uff1a\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u53ef\u5c06\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u4e3a\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u5168\u8eab\u52a8\u4f5c\uff0c\u5b9e\u73b0\u8bed\u8a00\u6cdb\u5316\u4e0e\u7269\u7406\u53ef\u884c\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u7c7b\u4eba\u673a\u5668\u4eba\u867d\u7136\u4f4e\u7ea7\u8fd0\u52a8\u63a7\u5236\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u8bed\u8a00\u6761\u4ef6\u5168\u8eab\u63a7\u5236\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6307\u4ee4\u7b80\u5355\uff0c\u8981\u4e48\u727a\u7272\u8fd0\u52a8\u591a\u6837\u6027\u6216\u7269\u7406\u5408\u7406\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8bed\u8a00\u6307\u4ee4\u4e0e\u7269\u7406\u53ef\u884c\u52a8\u4f5c\u4e4b\u95f4\u7684\u6620\u5c04\u95ee\u9898\u3002", "method": "\u63d0\u51faHumanoid-LLA\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u7edf\u4e00\u8fd0\u52a8\u8bcd\u6c47\u8868\uff0c\u5c06\u4eba\u7c7b\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u8fd0\u52a8\u57fa\u5143\u5bf9\u9f50\u5230\u5171\u4eab\u79bb\u6563\u7a7a\u95f4\uff1b2\uff09\u4ece\u7279\u6743\u7b56\u7565\u84b8\u998f\u7684\u8bcd\u6c47\u5bfc\u5411\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\uff1b3\uff09\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u529b\u5b66\u611f\u77e5\u5956\u52b1\u7684\u7269\u7406\u611f\u77e5\u5fae\u8c03\u9636\u6bb5\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9eUnitree G1\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cHumanoid-LLA\u5728\u4fdd\u6301\u9ad8\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u8fd0\u52a8\u81ea\u7136\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6267\u884c\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u8bed\u8a00\u6761\u4ef6\u63a7\u5236\u5668\u3002", "conclusion": "Humanoid-LLA\u6210\u529f\u89e3\u51b3\u4e86\u7c7b\u4eba\u673a\u5668\u4eba\u8bed\u8a00\u6761\u4ef6\u5168\u8eab\u63a7\u5236\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7edf\u4e00\u8fd0\u52a8\u8868\u793a\u3001\u7269\u7406\u53ef\u884c\u63a7\u5236\u5668\u548c\u7269\u7406\u611f\u77e5\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u4ee4\u5230\u7269\u7406\u53ef\u6267\u884c\u52a8\u4f5c\u7684\u6709\u6548\u6620\u5c04\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u548c\u901a\u7528\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22996", "abs": "https://arxiv.org/abs/2511.22996", "authors": ["Ke Chen"], "title": "Analytical Inverse Kinematic Solution for \"Moz1\" NonSRS 7-DOF Robot arm with novel arm angle", "comment": null, "summary": "This paper presents an analytical solution to the inverse kinematic problem(IKP) for the seven degree-of-freedom (7-DOF) Moz1 Robot Arm with offsets on wrist. We provide closed-form solutions with the novel arm angle . it allow fully self-motion and solve the problem of algorithmic singularities within the workspace. It also provides information on how the redundancy is resolved in a new arm angle representation where traditional SEW angle faied to be defined and how singularities are handled. The solution is simple, fast and exact, providing full solution space (i.e. all 16 solutions) per pose.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5e26\u6709\u8155\u90e8\u504f\u79fb\u76847\u81ea\u7531\u5ea6Moz1\u673a\u68b0\u81c2\u63d0\u51fa\u4e86\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u7684\u89e3\u6790\u89e3\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u81c2\u89d2\u6982\u5ff5\u63d0\u4f9b\u95ed\u5f0f\u89e3\uff0c\u89e3\u51b3\u4e86\u5de5\u4f5c\u7a7a\u95f4\u5185\u7684\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898", "motivation": "\u89e3\u51b37\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u8155\u90e8\u504f\u79fb\u7684\u60c5\u51b5\uff0c\u4f20\u7edfSEW\u89d2\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u65e0\u6cd5\u5b9a\u4e49\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5197\u4f59\u5ea6\u548c\u5947\u5f02\u6027", "method": "\u63d0\u51fa\u57fa\u4e8e\u65b0\u9896\u81c2\u89d2\u6982\u5ff5\u7684\u95ed\u5f0f\u89e3\u6790\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u81c2\u89d2\u8868\u793a\u89e3\u51b3\u5197\u4f59\u5ea6\u95ee\u9898\uff0c\u5904\u7406\u4f20\u7edfSEW\u89d2\u65e0\u6cd5\u5b9a\u4e49\u7684\u60c5\u51b5\uff0c\u5e76\u6709\u6548\u5904\u7406\u5947\u5f02\u6027", "result": "\u5b9e\u73b0\u4e86\u7b80\u5355\u3001\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u9006\u8fd0\u52a8\u5b66\u89e3\uff0c\u80fd\u591f\u4e3a\u6bcf\u4e2a\u4f4d\u59ff\u63d0\u4f9b\u5b8c\u6574\u7684\u89e3\u7a7a\u95f4\uff08\u6240\u670916\u4e2a\u89e3\uff09\uff0c\u89e3\u51b3\u4e86\u5de5\u4f5c\u7a7a\u95f4\u5185\u7684\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a7\u81ea\u7531\u5ea6\u5e26\u504f\u79fb\u8155\u90e8\u673a\u68b0\u81c2\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9006\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5b9a\u4e49\u95ee\u9898\u548c\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u7684\u81ea\u8fd0\u52a8\u63a7\u5236"}}
{"id": "2511.23017", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23017", "abs": "https://arxiv.org/abs/2511.23017", "authors": ["Elham Ahmadi", "Alireza Olama", "Petri V\u00e4lisuo", "Heidi Kuusniemi"], "title": "Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin", "comment": null, "summary": "Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u9c81\u68d2\u81ea\u9002\u5e94GNSS/IMU\u878d\u5408\u6846\u67b6\uff0c\u91c7\u7528Barron\u635f\u5931\u51fd\u6570\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u548c\u5f02\u5e38\u503c\uff0c\u5728GNSS\u4fe1\u53f7\u53d7\u9650\u7684\u57ce\u5e02\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "GNSS\u4fe1\u53f7\u53d7\u9650\u73af\u5883\u4e0b\u7684\u53ef\u9760\u5b9a\u4f4d\u662f\u5bfc\u822a\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u7684\u7d27\u8026\u5408GNSS/IMU\u878d\u5408\u867d\u7136\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u975e\u9ad8\u65af\u566a\u58f0\u548c\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u878d\u5408\u6846\u67b6\uff0c\u76f4\u63a5\u96c6\u6210GNSS\u4f2a\u8ddd\u6d4b\u91cf\u548cIMU\u9884\u79ef\u5206\u56e0\u5b50\uff0c\u5e76\u5f15\u5165Barron\u635f\u5931\u51fd\u6570\uff08\u4e00\u79cd\u901a\u8fc7\u5355\u4e2a\u53ef\u8c03\u53c2\u6570\u7edf\u4e00\u591a\u4e2aM\u4f30\u8ba1\u5668\u7684\u901a\u7528\u9c81\u68d2\u635f\u5931\u51fd\u6570\uff09\uff0c\u81ea\u9002\u5e94\u964d\u4f4e\u4e0d\u53ef\u9760GNSS\u6d4b\u91cf\u7684\u6743\u91cd\u3002", "result": "\u5728UrbanNav\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6807\u51c6\u56e0\u5b50\u56fe\u4f18\u5316\uff08FGO\uff09\u5b9a\u4f4d\u8bef\u5dee\u51cf\u5c11\u9ad8\u8fbe41%\uff0c\u5728\u90fd\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u76f8\u6bd4\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u57fa\u7ebf\u6709\u66f4\u5927\u6539\u8fdb\u3002", "conclusion": "Barron\u635f\u5931\u51fd\u6570\u80fd\u663e\u8457\u589e\u5f3aGNSS/IMU\u5bfc\u822a\u5728\u57ce\u5e02\u573a\u666f\u548c\u4fe1\u53f7\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4e3aGNSS\u6311\u6218\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23030", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23030", "abs": "https://arxiv.org/abs/2511.23030", "authors": ["Casimir Feldmann", "Maximum Wilder-Smith", "Vaishakh Patil", "Michael Oechsle", "Michael Niemeyer", "Keisuke Tateno", "Marco Hutter"], "title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.", "AI": {"tldr": "DiskChunGS\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u76843D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5916\u5b58\u65b9\u6cd5\u5c06\u573a\u666f\u5206\u5272\u6210\u7a7a\u95f4\u5757\uff0c\u4ec5\u5c06\u6d3b\u52a8\u533a\u57df\u4fdd\u7559\u5728GPU\u5185\u5b58\u4e2d\uff0c\u89e3\u51b3\u4e863DGS\u4e0eSLAM\u96c6\u6210\u65f6\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u5b9e\u65f6\u6e32\u67d3\u80fd\u529b\uff0c\u4f46\u4e0eSLAM\u7cfb\u7edf\u96c6\u6210\u65f6\u9762\u4e34\u6839\u672c\u6027\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff1a\u53d7GPU\u5185\u5b58\u5bb9\u91cf\u7ea6\u675f\uff0c\u53ea\u80fd\u91cd\u5efa\u5c0f\u89c4\u6a21\u73af\u5883\u3002", "method": "\u91c7\u7528\u5916\u5b58\u65b9\u6cd5\uff0c\u5c06\u573a\u666f\u5206\u5272\u6210\u7a7a\u95f4\u5757\uff0c\u4ec5\u5c06\u6d3b\u52a8\u533a\u57df\u4fdd\u7559\u5728GPU\u5185\u5b58\u4e2d\uff0c\u975e\u6d3b\u52a8\u533a\u57df\u5b58\u50a8\u5728\u78c1\u76d8\u4e0a\u3002\u8be5\u67b6\u6784\u4e0e\u73b0\u6709\u7684SLAM\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\uff0c\u7528\u4e8e\u59ff\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u68c0\u6d4b\u3002", "result": "\u5728\u5ba4\u5185\u573a\u666f\uff08Replica\u3001TUM-RGBD\uff09\u3001\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\uff08KITTI\uff09\u548c\u8d44\u6e90\u53d7\u9650\u7684Nvidia Jetson\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u72ec\u7279\u5730\u5b8c\u6210\u4e86\u6240\u670911\u4e2aKITTI\u5e8f\u5217\u800c\u6ca1\u6709\u5185\u5b58\u6545\u969c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "DiskChunGS\u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u514b\u670d\u4e86\u5148\u524d3DGS SLAM\u65b9\u6cd5\u7684\u5185\u5b58\u9650\u5236\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u7684\u91cd\u5efa\u3002"}}
{"id": "2511.23034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23034", "abs": "https://arxiv.org/abs/2511.23034", "authors": ["Zuolei Li", "Xingyu Gao", "Xiaofan Wang", "Jianlong Fu"], "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models", "comment": "Project Page: https://mm-robot.github.io/distill_latent_action/", "summary": "Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e27\u8f93\u5165\u548c\u4efb\u52a1\u6307\u4ee4\uff0c\u7ed3\u5408\u672a\u6765\u5e27\u91cd\u5efa\u548c\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\uff0c\u5b66\u4e60\u53ef\u8f6c\u79fb\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u91cd\u5efa\u76ee\u6807\u800c\u5ffd\u7565\u7269\u7406\u5148\u9a8c\uff0c\u5bfc\u81f4\u5b66\u4e60\u901a\u7528\u8868\u793a\u7684\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u4ece\u5927\u89c4\u6a21\u7269\u4f53\u64cd\u4f5c\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8f6c\u79fb\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u4ee5\u589e\u5f3a\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u8f93\u5165\u4efb\u52a1\u6307\u4ee4\u548c\u591a\u5e27\u56fe\u50cf\uff0c\u540c\u65f6\u4f18\u5316\u672a\u6765\u5e27\u91cd\u5efa\u548c\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\u3002\u5c06\u6f5c\u5728\u52a8\u4f5c\u5206\u89e3\u4e3a\u53ef\u5b66\u4e60\u7684\u8fd0\u52a8\u548c\u573a\u666ftoken\uff0c\u533a\u5206\u673a\u5668\u4eba\u4e3b\u52a8\u8fd0\u52a8\u548c\u73af\u5883\u53d8\u5316\u3002\u5c06\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u52a8\u4f5c\u84b8\u998f\u5230\u6700\u65b0\u7684VLA\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\uff08SIMPLER\u548cLIBERO\uff09\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002\u5728Franka\u673a\u5668\u4eba\u4e0a\uff0c\u6bcf\u4e2a\u4efb\u52a1\u4ec5\u970010\u6761\u771f\u5b9e\u8f68\u8ff9\uff0c\u5c31\u80fd\u6210\u529f\u5b8c\u6210\u6240\u6709\u4e94\u4e2a\u6311\u6218\u6027\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u91cd\u5efa\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u80fd\u591f\u5b66\u4e60\u5305\u542b\u4e30\u5bcc\u7269\u7406\u5148\u9a8c\u7684\u901a\u7528\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5b9e\u73b0\u4ece\u5927\u89c4\u6a21\u89c6\u9891\u5230\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u65e0\u7f1d\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.23143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23143", "abs": "https://arxiv.org/abs/2511.23143", "authors": ["Enrico Saccon", "Davide De Martini", "Matteo Saveriano", "Edoardo Lamon", "Luigi Palopoli", "Marco Roveri"], "title": "Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications", "comment": "9 pages, 11 figures, 2 tables, 2 algorithms, accepted for publication in IEEE Robotics and Automation Letters", "summary": "We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u52a8\u89c4\u5212\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u521b\u5efa\u548c\u4f7f\u7528\uff0c\u5728\u673a\u5668\u4eba\u4ea4\u4e92\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfMDP\u521b\u5efa\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u81ea\u52a8\u5316MDP\u6784\u5efa\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "1) \u4f7f\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u6784\u5efaProlog\u77e5\u8bc6\u5e93\uff1b2) \u901a\u8fc7\u53ef\u8fbe\u6027\u5206\u6790\u81ea\u52a8\u6784\u5efaMDP\uff1b3) \u4f7f\u7528Storm\u6a21\u578b\u68c0\u67e5\u5668\u5408\u6210\u6700\u4f18\u7b56\u7565\uff1b4) \u5c06\u7b56\u7565\u5bfc\u51fa\u4e3a\u72b6\u6001-\u52a8\u4f5c\u8868\u4f9b\u6267\u884c\u3002", "result": "\u5728\u4e09\u4e2a\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u4eba\u5de5\u52aa\u529b\u751f\u6210\u53ef\u6267\u884c\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7ed3\u5408\u5728\u673a\u5668\u4eba\u6982\u7387\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u6613\u8bbf\u95ee\u548c\u53ef\u6269\u5c55\u7684\u89c4\u5212\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.23186", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23186", "abs": "https://arxiv.org/abs/2511.23186", "authors": ["Runyu Jiao", "Matteo Bortolon", "Francesco Giuliari", "Alice Fasoli", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Obstruction reasoning for robotic grasping", "comment": null, "summary": "Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.", "AI": {"tldr": "UNOGrasp\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u7684\u969c\u788d\u7269\u63a8\u7406\u548c\u53ef\u53ca\u6027\u89c4\u5212\uff0c\u901a\u8fc7\u591a\u6b65\u63a8\u7406\u548c\u89c6\u89c9\u7ebf\u7d22\u63d0\u5347\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u5177\u8eab\u63a8\u7406\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5728\u969c\u788d\u7269\u63a8\u7406\u548c\u53ef\u53ca\u6027\u89c4\u5212\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5728\u6742\u4e71\u73af\u5883\u4e2d\u673a\u5668\u4eba\u6293\u53d6\u65f6\u7684\u969c\u788d\u7269\u6e05\u9664\u95ee\u9898\u3002", "method": "\u63d0\u51faUNOGrasp\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u76ee\u6807\u7269\u4f53\u969c\u788d\u8def\u5f84\u7684\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u7ed3\u5408\u969c\u788d\u611f\u77e5\u89c6\u89c9\u7ebf\u7d22\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9a8c\u8bc1\u63a8\u7406\u5956\u52b1\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b10\u4e07+\u969c\u788d\u8def\u5f84\u6807\u6ce8\u7684UNOBench\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUNOGrasp\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u969c\u788d\u7269\u63a8\u7406\u80fd\u529b\u548c\u6293\u53d6\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u548c\u4e13\u6709\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "UNOGrasp\u901a\u8fc7\u4e13\u95e8\u7684\u969c\u788d\u7269\u63a8\u7406\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6742\u4e71\u73af\u5883\u4e2d\u673a\u5668\u4eba\u6293\u53d6\u7684\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23215", "categories": ["cs.RO", "cond-mat.other", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2511.23215", "abs": "https://arxiv.org/abs/2511.23215", "authors": ["Eduardo Sergio Oliveros-Mata", "Oleksandr V. Pylypovskyi", "Eleonora Raimondo", "Rico Illing", "Yevhen Zabila", "Lin Guo", "Guannan Mu", "M\u00f3nica Navarro L\u00f3pez", "Xu Wang", "Georgios Tzortzinis", "Angelos Filippatos", "Gilbert Santiago Ca\u00f1\u00f3n Berm\u00fadez", "Francesca Garesc\u00ec", "Giovanni Finocchio", "Denys Makarov"], "title": "Field-programmable dynamics in a soft magnetic actuator enabling true random number generation and reservoir computing", "comment": null, "summary": "Complex and even chaotic dynamics, though prevalent in many natural and engineered systems, has been largely avoided in the design of electromechanical systems due to concerns about wear and controlability. Here, we demonstrate that complex dynamics might be particularly advantageous in soft robotics, offering new functionalities beyond motion not easily achievable with traditional actuation methods. We designed and realized resilient magnetic soft actuators capable of operating in a tunable dynamic regime for tens of thousands cycles without fatigue. We experimentally demonstrated the application of these actuators for true random number generation and stochastic computing. {W}e validate soft robots as physical reservoirs capable of performing Mackey--Glass time series prediction. These findings show that exploring the complex dynamics in soft robotics would extend the application scenarios in soft computing, human-robot interaction and collaborative robots as we demonstrate with biomimetic blinking and randomized voice modulation.", "AI": {"tldr": "\u8f6f\u673a\u5668\u4eba\u4e2d\u5229\u7528\u590d\u6742\u52a8\u529b\u5b66\u5b9e\u73b0\u65b0\u529f\u80fd\uff1a\u8bbe\u8ba1\u51fa\u53ef\u8c03\u52a8\u6001\u78c1\u8f6f\u6267\u884c\u5668\uff0c\u7528\u4e8e\u771f\u968f\u673a\u6570\u751f\u6210\u3001\u968f\u673a\u8ba1\u7b97\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b", "motivation": "\u4f20\u7edf\u673a\u7535\u7cfb\u7edf\u901a\u5e38\u907f\u514d\u590d\u6742\u548c\u6df7\u6c8c\u52a8\u529b\u5b66\uff0c\u62c5\u5fc3\u78e8\u635f\u548c\u53ef\u63a7\u6027\u95ee\u9898\u3002\u4f46\u7814\u7a76\u8868\u660e\u590d\u6742\u52a8\u529b\u5b66\u5728\u8f6f\u673a\u5668\u4eba\u4e2d\u53ef\u80fd\u7279\u522b\u6709\u5229\uff0c\u80fd\u591f\u63d0\u4f9b\u4f20\u7edf\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7684\u65b0\u529f\u80fd", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u5177\u6709\u53ef\u8c03\u52a8\u6001\u673a\u5236\u7684\u5f39\u6027\u78c1\u8f6f\u6267\u884c\u5668\uff0c\u80fd\u591f\u5728\u6570\u4e07\u6b21\u5faa\u73af\u4e2d\u8fd0\u884c\u800c\u4e0d\u75b2\u52b3\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6267\u884c\u5668\u5728\u771f\u968f\u673a\u6570\u751f\u6210\u3001\u968f\u673a\u8ba1\u7b97\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5e94\u7528", "result": "\u6210\u529f\u5c55\u793a\u4e86\u8f6f\u673a\u5668\u4eba\u4f5c\u4e3a\u7269\u7406\u50a8\u5c42\u80fd\u591f\u6267\u884cMackey-Glass\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002\u6267\u884c\u5668\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u590d\u6742\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u53ef\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f", "conclusion": "\u63a2\u7d22\u8f6f\u673a\u5668\u4eba\u4e2d\u7684\u590d\u6742\u52a8\u529b\u5b66\u80fd\u591f\u6269\u5c55\u5176\u5728\u8f6f\u8ba1\u7b97\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u534f\u4f5c\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u4eff\u751f\u7728\u773c\u548c\u968f\u673a\u8bed\u97f3\u8c03\u5236\u7b49\u6f14\u793a\u6240\u793a"}}
{"id": "2511.23300", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23300", "abs": "https://arxiv.org/abs/2511.23300", "authors": ["Yara Mahmoud", "Jeffrin Sam", "Nguyen Khang", "Marcelino Fernando", "Issatay Tokmurziyev", "Miguel Altamirano Cabrera", "Muhammad Haris Khan", "Artem Lykov", "Dzmitry Tsetserukou"], "title": "SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot", "comment": null, "summary": "Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.", "AI": {"tldr": "SafeHumanoid\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u4eff\u4eba\u673a\u5668\u4eba\u963b\u6297\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u8bed\u4e49\u7406\u89e3\u81ea\u9002\u5e94\u8c03\u6574\u521a\u5ea6\u548c\u901f\u5ea6\u53c2\u6570\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u5b89\u5168\u6027", "motivation": "\u5b89\u5168\u53ef\u4fe1\u7684\u4eba\u673a\u4ea4\u4e92\u9700\u8981\u673a\u5668\u4eba\u4e0d\u4ec5\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u8981\u6839\u636e\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u63a5\u8fd1\u7a0b\u5ea6\u8c03\u8282\u963b\u6297\u548c\u901f\u5ea6\u53c2\u6570\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u573a\u666f\u8bed\u4e49\u7684\u7406\u89e3\u548c\u81ea\u9002\u5e94\u8c03\u8282\u80fd\u529b\u3002", "method": "\u63d0\u51faSafeHumanoid\u7cfb\u7edf\uff1a1\uff09\u4f7f\u7528\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u7ba1\u9053\u5904\u7406\u573a\u666f\uff1b2\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u5206\u6790\u573a\u666f\uff1b3\uff09\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5339\u914d\u5df2\u9a8c\u8bc1\u573a\u666f\u6570\u636e\u5e93\uff1b4\uff09\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u5c06\u8bed\u4e49\u4fe1\u606f\u6620\u5c04\u5230\u5173\u8282\u7ea7\u963b\u6297\u547d\u4ee4\u3002", "result": "\u5728\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\uff08\u64e6\u62ed\u3001\u7269\u4f53\u4ea4\u63a5\u3001\u6db2\u4f53\u503e\u5012\uff09\u4e2d\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u8c03\u6574\u521a\u5ea6\u3001\u963b\u5c3c\u548c\u901f\u5ea6\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5b89\u5168\u6027\u3002\u5f53\u524d\u63a8\u7406\u5ef6\u8fdf\u8fbe1.4\u79d2\uff0c\u9650\u5236\u5728\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u4e2d\u7684\u54cd\u5e94\u6027\u3002", "conclusion": "\u8bed\u4e49\u57fa\u7840\u7684\u963b\u6297\u63a7\u5236\u662f\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u7b26\u5408\u6807\u51c6\u7684\u4eff\u4eba\u673a\u5668\u4eba\u534f\u4f5c\u7684\u53ef\u884c\u8def\u5f84\u3002\u867d\u7136\u5f53\u524d\u5ef6\u8fdf\u9650\u5236\u4e86\u52a8\u6001\u73af\u5883\u5e94\u7528\uff0c\u4f46\u8bc1\u660e\u4e86\u8bed\u4e49\u7406\u89e3\u5728\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2511.23407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23407", "abs": "https://arxiv.org/abs/2511.23407", "authors": ["Jan Baumg\u00e4rtner", "Malte Hansjosten", "David Hald", "Adrian Hauptmannl", "Alexander Puchta", "J\u00fcrgen Fleischer"], "title": "From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products", "comment": null, "summary": "To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u62c6\u5378\u4efb\u52a1\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u5f00\u53d1\u57fa\u4e8eCAD\u6570\u636e\u3001\u673a\u5668\u4eba\u80fd\u529b\u548c\u68c0\u6d4b\u7ed3\u679c\u7684\u6982\u7387\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5904\u7406EOL\u4ea7\u54c1\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\u9700\u8981\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u62c6\u5378\u62a5\u5e9f\u4ea7\u54c1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u786e\u5b9a\u6027\u4e14\u5b8c\u5168\u53ef\u89c2\u6d4b\u7684\u4ea7\u54c1\u6a21\u578b\uff0c\u800c\u5b9e\u9645EOL\u4ea7\u54c1\u5e38\u56e0\u78e8\u635f\u3001\u8150\u8680\u6216\u672a\u8bb0\u5f55\u7684\u7ef4\u4fee\u800c\u504f\u79bb\u539f\u59cb\u8bbe\u8ba1\uff0c\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002", "method": "1. \u5c06\u62c6\u5378\u4efb\u52a1\u6570\u5b66\u5efa\u6a21\u4e3aPOMDP\uff0c\u9690\u85cf\u53d8\u91cf\u8868\u793a\u4e0d\u786e\u5b9a\u7684\u7ed3\u6784\u6216\u7269\u7406\u5c5e\u6027\uff1b2. \u63d0\u51fa\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u4eceCAD\u6570\u636e\u3001\u673a\u5668\u4eba\u80fd\u529b\u548c\u68c0\u6d4b\u7ed3\u679c\u81ea\u52a8\u63a8\u5bfcPOMDP\u6a21\u578b\uff1b3. \u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fd1\u4f3c\u6c42\u89e3\uff0c\u7ed3\u5408\u68c0\u6d4b\u5148\u9a8c\u7684\u968f\u673a\u52a8\u4f5c\u7ed3\u679c\uff1b4. \u4f7f\u7528\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u6301\u7eed\u7ef4\u62a4\u5bf9\u6f5c\u5728EOL\u72b6\u6001\u7684\u4fe1\u5ff5\u3002", "result": "\u5728\u4e24\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u7684\u4e09\u4e2a\u4ea7\u54c1\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u6982\u7387\u89c4\u5212\u6846\u67b6\u5728\u5e73\u5747\u62c6\u5378\u65f6\u95f4\u548c\u65b9\u5dee\u65b9\u9762\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u8bbe\u7f6e\u6cdb\u5316\uff0c\u5e76\u6210\u529f\u9002\u5e94CAD\u6a21\u578b\u7684\u504f\u5dee\uff08\u5982\u7f3a\u5931\u6216\u5361\u4f4f\u96f6\u4ef6\uff09\u3002", "conclusion": "\u5c06\u62c6\u5378\u4efb\u52a1\u5efa\u6a21\u4e3aPOMDP\u5e76\u91c7\u7528\u6982\u7387\u89c4\u5212\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406EOL\u4ea7\u54c1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u62c6\u5378\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5faa\u73af\u7ecf\u6d4e\u4e2d\u7684\u81ea\u52a8\u5316\u62c6\u5378\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
