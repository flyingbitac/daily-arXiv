{"id": "2602.22243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22243", "abs": "https://arxiv.org/abs/2602.22243", "authors": ["Jan Nausner", "Kilian Wohlleben", "Michael Hubner"], "title": "SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online", "comment": "8 pages, 5 figures; Submitted to the 2026 International Conference on Information Fusion (FUSION 2026). Under review", "summary": "The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.", "AI": {"tldr": "SODA-CitrON\uff1a\u4e00\u79cd\u7528\u4e8e\u9759\u6001\u7269\u4f53\u6570\u636e\u5173\u8054\u7684\u5728\u7ebf\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u68c0\u6d4b\u5b9e\u73b0\u672a\u77e5\u6570\u91cf\u9759\u6001\u7269\u4f53\u7684\u4f4d\u7f6e\u4f30\u8ba1\u548c\u6301\u7eed\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5173\u8054\u65b9\u6cd5\uff08\u5982JPDA\uff09\u4e3b\u8981\u9002\u7528\u4e8e\u52a8\u6001\u76ee\u6807\uff0c\u4f46\u5bf9\u4e8e\u95f4\u6b47\u6027\u89c2\u6d4b\u3001\u4e0d\u786e\u5b9a\u6027\u5f02\u8d28\u7684\u9759\u6001\u7269\u4f53\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd0\u52a8\u6a21\u578b\u5bf9\u6742\u6ce2\u533a\u5206\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faSODA-CitrON\u65b9\u6cd5\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5728\u7ebf\u805a\u7c7b\u591a\u6a21\u6001\u4f20\u611f\u5668\u68c0\u6d4b\uff0c\u5904\u7406\u65f6\u95f4\u4e0d\u76f8\u5173\u548c\u591a\u4f20\u611f\u5668\u6d4b\u91cf\uff0c\u5177\u6709\u6700\u574f\u60c5\u51b5\u4e0b\u5bf9\u6570\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "result": "\u5728\u8499\u7279\u5361\u6d1b\u4eff\u771f\u573a\u666f\u4e2d\uff0cSODA-CitrON\u5728F1\u5206\u6570\u3001\u4f4d\u7f6eRMSE\u3001MOTP\u548cMOTA\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u8d1d\u53f6\u65af\u6ee4\u6ce2\u3001DBSTREAM\u805a\u7c7b\u548cJPDA\u7b49\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SODA-CitrON\u4e3a\u9759\u6001\u7269\u4f53\u6570\u636e\u5173\u8054\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u7cfb\u7edf\u548c\u73af\u5883\u5efa\u56fe\u4e2d\u9759\u6001\u7269\u4f53\u7684\u878d\u5408\u4e0e\u8ddf\u8e2a\u95ee\u9898\u3002"}}
{"id": "2602.22459", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22459", "abs": "https://arxiv.org/abs/2602.22459", "authors": ["Yicheng Chen", "Jinjie Li", "Haokun Liu", "Zicheng Luo", "Kotaro Kaneko", "Moju Zhao"], "title": "Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments", "comment": "Accepted to IEEE T-ASE; DOI pending", "summary": "Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u5f15\u5bfc\u548c\u914d\u7f6e\u611f\u77e5\u7684\u5c40\u90e8\u4f18\u5316\uff0c\u76f4\u63a5\u4ece\u70b9\u4e91\u6570\u636e\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "motivation": "\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u5728\u98de\u884c\u4e2d\u53ef\u4ee5\u6539\u53d8\u5f62\u6001\uff0c\u9002\u5408\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5e94\u7528\uff0c\u4f46\u8f68\u8ff9\u89c4\u5212\u9762\u4e34\u9ad8\u7ef4\u3001\u7ea6\u675f\u4e30\u5bcc\u7684\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u907f\u969c\u3001\u8fd0\u52a8\u5b66\u9650\u5236\u548c\u52a8\u6001\u53ef\u884c\u6027\u3002", "method": "1) \u5229\u7528\u673a\u5668\u4eba\u7684\u53cc\u91cd\u7279\u6027\uff08\u6839\u8fde\u6746\u4f5c\u4e3a\u521a\u4f53\u5f15\u5bfc\uff0c\u5173\u8282\u63d0\u4f9b\u7075\u6d3b\u6027\uff09\u751f\u6210\u5168\u5c40\u951a\u70b9\u72b6\u6001\uff0c\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u5904\u7406\u6bb5\uff1b2) \u8bbe\u8ba1\u5c40\u90e8\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u5e76\u884c\u4f18\u5316\u5404\u6bb5\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u76ee\u6807\u548c\u7ea6\u675f\uff0c\u7cfb\u7edf\u786e\u4fdd\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u907f\u514d\u63a7\u5236\u5947\u70b9\u4fdd\u6301\u52a8\u6001\u53ef\u884c\u6027\uff1b3) \u5b9e\u73b0\u76f4\u63a5\u5904\u7406\u70b9\u4e91\u6570\u636e\u7684\u5b8c\u6574\u7cfb\u7edf\uff0c\u65e0\u9700\u624b\u5de5\u969c\u788d\u7269\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u4f7f\u5173\u8282\u5f0f\u7a7a\u4e2d\u673a\u5668\u4eba\u80fd\u591f\u5229\u7528\u5176\u5f62\u6001\u5b9e\u73b0\u521a\u4f53\u673a\u5668\u4eba\u65e0\u6cd5\u5b8c\u6210\u7684\u673a\u52a8\u3002\u8fd9\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u6f14\u793a\u7684\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u59cb\u70b9\u4e91\u8f93\u5165\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6d6e\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u5728\u9ad8\u7ef4\u7ea6\u675f\u7a7a\u95f4\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u5f15\u5bfc\u548c\u5c40\u90e8\u4f18\u5316\uff0c\u76f4\u63a5\u4ece\u70b9\u4e91\u6570\u636e\u751f\u6210\u53ef\u884c\u8f68\u8ff9\uff0c\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22461", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22461", "abs": "https://arxiv.org/abs/2602.22461", "authors": ["Daesol Cho", "Youngseok Jang", "Danfei Xu", "Sehoon Ha"], "title": "EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow", "comment": null, "summary": "Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.", "AI": {"tldr": "EgoAVFlow\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\uff0c\u901a\u8fc73D\u6d41\u8868\u793a\u5b9e\u73b0\u51e0\u4f55\u53ef\u89c1\u6027\u63a8\u7406\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u5373\u53ef\u8fc1\u79fb", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u4eba\u7c7b\u89c6\u9891\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u64cd\u4f5c\u6f14\u793a\u6765\u6e90\uff0c\u4f46\u4eba\u7c7b\u89c6\u89d2\u6a21\u4eff\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u4efb\u52a1\u5173\u952e\u53ef\u89c1\u6027\uff0c\u56e0\u4e3a\u4eba\u7c7b\u5177\u6709\u7279\u5b9a\u5148\u9a8c\uff0c\u9700\u8981\u4e3b\u52a8\u89c6\u89d2\u63a7\u5236\u6765\u7ef4\u6301\u53ef\u89c1\u6027", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u52a8\u4f5c\u3001\u672a\u67653D\u6d41\u548c\u76f8\u673a\u8f68\u8ff9\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u6d4b\u8fd0\u52a8\u548c\u573a\u666f\u51e0\u4f55\u7684\u53ef\u89c1\u6027\u611f\u77e5\u5956\u52b1\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5956\u52b1\u6700\u5927\u5316\u53bb\u566a\u6765\u4f18\u5316\u89c6\u89d2", "result": "\u5728\u4e3b\u52a8\u53d8\u5316\u89c6\u89d2\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cEgoAVFlow\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6709\u6548\u7684\u53ef\u89c1\u6027\u7ef4\u62a4\u548c\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u7684\u9c81\u68d2\u64cd\u4f5c", "conclusion": "EgoAVFlow\u901a\u8fc7\u5171\u4eab\u76843D\u6d41\u8868\u793a\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5b66\u4e60\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\uff0c\u652f\u6301\u51e0\u4f55\u53ef\u89c1\u6027\u63a8\u7406\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2602.22474", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22474", "abs": "https://arxiv.org/abs/2602.22474", "authors": ["Jessie Yuan", "Yilin Wu", "Andrea Bajcsy"], "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering", "comment": null, "summary": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/", "AI": {"tldr": "UPS\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u5668\u6765\u533a\u5206\u81ea\u4fe1\u3001\u6a21\u7cca\u548c\u65e0\u6cd5\u5904\u7406\u7684\u60c5\u666f\uff0c\u5e76\u9009\u62e9\u76f8\u5e94\u7b56\u7565\uff08\u6267\u884c\u3001\u6f84\u6e05\u6216\u5e72\u9884\uff09\uff0c\u540c\u65f6\u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6301\u7eed\u6539\u8fdb\u57fa\u7840\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\u5047\u8bbe\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6821\u51c6\u826f\u597d\uff0c\u4f46\u5b9e\u9645\u4e0aVLM\u7684\u8fc7\u5ea6\u81ea\u4fe1\u5224\u65ad\u4f1a\u5728\u4efb\u52a1\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u548c\u57fa\u7840\u7b56\u7565\u52a8\u4f5c\u4e0d\u786e\u5b9a\u6027/\u80fd\u529b\u4e0d\u8db3\u65f6\u964d\u4f4e\u5f15\u5bfc\u6027\u80fd\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b56\u7565\u5f15\u5bfc\uff08UPS\uff09\u6846\u67b6\uff1a1\uff09\u8054\u5408\u63a8\u7406\u8bed\u4e49\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u548c\u4f4e\u5c42\u52a8\u4f5c\u53ef\u884c\u6027\uff1b2\uff09\u4f7f\u7528\u7b26\u5408\u9884\u6d4b\u6821\u51c6VLM\u548c\u57fa\u7840\u7b56\u7565\u7684\u7ec4\u5408\uff1b3\uff09\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u7b56\u7565\uff08\u6267\u884c\u9ad8\u7f6e\u4fe1\u52a8\u4f5c\u3001\u81ea\u7136\u8bed\u8a00\u6f84\u6e05\u4efb\u52a1\u3001\u8bf7\u6c42\u52a8\u4f5c\u5e72\u9884\uff09\uff1b4\uff09\u90e8\u7f72\u4e2d\u6536\u96c6\u5e72\u9884\u540e\u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6539\u8fdb\u57fa\u7840\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUPS\u80fd\u591f\u6709\u6548\u533a\u5206\u81ea\u4fe1\u3001\u6a21\u7cca\u548c\u65e0\u6cd5\u5904\u7406\u7684\u60c5\u666f\uff0c\u76f8\u6bd4\u672a\u6821\u51c6\u57fa\u7ebf\u548c\u5148\u524d\u7684\u4eba\u673a\u95e8\u63a7\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6602\u8d35\u7684\u7528\u6237\u5e72\u9884\u9700\u6c42\u3002", "conclusion": "UPS\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7b56\u7565\u5f15\u5bfc\uff0c\u5728\u4fdd\u8bc1\u7edf\u8ba1\u53ef\u9760\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u4eba\u7c7b\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u884c\u4e3a\u7684\u81ea\u9002\u5e94\u90e8\u7f72\u548c\u6301\u7eed\u5b66\u4e60\u3002"}}
{"id": "2602.22514", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22514", "abs": "https://arxiv.org/abs/2602.22514", "authors": ["Xinyu Tan", "Ningwei Bai", "Harry Gardener", "Zhengyang Zhong", "Luoyu Zhang", "Liuhaichen Yang", "Zhekai Duan", "Monkgogi Galeitsiwe", "Zezhi Tang"], "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation", "comment": "7 pages, 2 figures", "summary": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.\n  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.\n  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u624b\u8bed\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u65e0\u9700\u4e2d\u95f4\u6ce8\u91ca\uff0c\u76f4\u63a5\u6620\u5c04\u624b\u8bed\u624b\u52bf\u5230\u673a\u5668\u4eba\u6307\u4ee4\uff0c\u5b9e\u73b0\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92", "motivation": "\u4f20\u7edf\u624b\u8bed\u4eba\u673a\u4ea4\u4e92\u4f9d\u8d56\u6ce8\u91ca\u4f5c\u4e3a\u4e2d\u95f4\u76d1\u7763\uff0c\u6210\u672c\u9ad8\u4e14\u4fe1\u606f\u635f\u5931\u3002\u9700\u8981\u66f4\u81ea\u7136\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u4e3a\u5b89\u5168\u5173\u952e\u73af\u5883\u63d0\u4f9b\u53ef\u9760\u3001\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\u901a\u9053", "method": "\u91c7\u7528\u65e0\u6ce8\u91ca\u8303\u5f0f\uff0c\u76f4\u63a5\u89c6\u89c9\u6620\u5c04\u624b\u8bed\u5230\u8bed\u4e49\u6307\u4ee4\u3002\u4e13\u6ce8\u4e8e\u5b57\u6bcd\u7ea7\u6307\u62fc\u754c\u9762\uff0c\u901a\u8fc7\u51e0\u4f55\u5f52\u4e00\u5316\u3001\u65f6\u95f4\u5e73\u6ed1\u548c\u8bcd\u6c47\u7cbe\u70bc\u5c06\u8fde\u7eed\u624b\u52bf\u6d41\u8f6c\u6362\u4e3a\u8fde\u8d2f\u8bed\u8a00\u547d\u4ee4", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u5c06\u624b\u8bed\u6307\u4ee4\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5728\u4e0d\u540c\u4ea4\u4e92\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\u3002\u6846\u67b6\u652f\u6301\u672a\u6765\u96c6\u6210\u57fa\u4e8eTransformer\u7684\u65e0\u6ce8\u91ca\u624b\u8bed\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u63a8\u8fdb\u65e0\u969c\u788d\u3001\u53ef\u6269\u5c55\u548c\u591a\u6a21\u6001\u5177\u8eab\u667a\u80fd\u7684\u6f5c\u529b\uff0c\u4e3a\u5305\u5bb9\u6027\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.22579", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22579", "abs": "https://arxiv.org/abs/2602.22579", "authors": ["Pablo Valle", "Sergio Segura", "Shaukat Ali", "Aitor Arrieta"], "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots", "comment": null, "summary": "Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u8715\u53d8\u6d4b\u8bd5\u6765\u7f13\u89e3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u79cd\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u548c\u4e94\u79cd\u8715\u53d8\u5173\u7cfb\u6765\u8bc4\u4f30\u8f93\u5165\u53d8\u5316\u5bf9\u673a\u5668\u4eba\u8f68\u8ff9\u7684\u5f71\u54cd\u3002", "motivation": "VLA\u6a21\u578b\u9762\u4e34\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff1a\u4e00\u65b9\u9762\u9700\u8981\u4e3a\u6bcf\u4e2a\u6307\u4ee4\u63d0\u793a\u5b9a\u4e49\u6d4b\u8bd5\u9884\u8a00\uff0c\u8fd9\u590d\u6742\u4e14\u4e0d\u53ef\u6cdb\u5316\uff1b\u53e6\u4e00\u65b9\u9762\u73b0\u6709\u9884\u8a00\u901a\u5e38\u53ea\u80fd\u8bc4\u4f30\u4efb\u52a1\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4efb\u52a1\u6267\u884c\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u3001\u80fd\u8bc4\u4f30\u6267\u884c\u8d28\u91cf\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u548c\u4e94\u79cd\u8715\u53d8\u5173\u7cfb\uff0c\u901a\u8fc7\u6539\u53d8\u6d4b\u8bd5\u8f93\u5165\u6765\u8bc4\u4f30VLA\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\u3002\u4f7f\u7528\u4e94\u79cdVLA\u6a21\u578b\u3001\u4e24\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u548c\u56db\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u8715\u53d8\u6d4b\u8bd5\u80fd\u6709\u6548\u7f13\u89e3\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u81ea\u52a8\u68c0\u6d4b\u591a\u79cd\u7c7b\u578b\u7684\u6545\u969c\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u672a\u5b8c\u6210\u4efb\u52a1\uff09\u3002\u63d0\u51fa\u7684\u8715\u53d8\u5173\u7cfb\u5177\u6709\u6cdb\u5316\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540cVLA\u6a21\u578b\u3001\u673a\u5668\u4eba\u548c\u4efb\u52a1\u3002", "conclusion": "\u8715\u53d8\u6d4b\u8bd5\u662f\u89e3\u51b3VLA\u6a21\u578b\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u5177\u6709\u901a\u7528\u6027\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u6d4b\u8bd5\u9884\u8a00\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5e94\u7528\u3002"}}
{"id": "2602.22628", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22628", "abs": "https://arxiv.org/abs/2602.22628", "authors": ["Michael F. Xu", "Enhui Zhao", "Yawen Zhang", "Joseph E. Michaelis", "Sarah Sebo", "Bilge Mutlu"], "title": "Designing Robots for Families: In-Situ Prototyping for Contextual Reminders on Family Routines", "comment": "Proceedings of the 21st ACM/IEEE International Conference on Human Robot Interaction (HRI 2026)", "summary": "Robots are increasingly entering the daily lives of families, yet their successful integration into domestic life remains a challenge. We explore family routines as a critical entry point for understanding how robots might find a sustainable role in everyday family settings. Together with each of the ten families, we co-designed robot interactions and behaviors, and a plan for the robot to support their chosen routines, accounting for contextual factors such as timing, participants, locations, and the activities in the environment. We then designed, prototyped, and deployed a mobile social robot as a four-day, in-home user study. Families welcomed the robot's reminders, with parents especially appreciating the offloading of some reminding tasks. At the same time, interviews revealed tensions around timing, authority, and family dynamics, highlighting the complexity of integrating robots into households beyond the immediate task of reminders. Based on these insights, we offer design implications for robot-facilitated contextual reminders and discuss broader considerations for designing robots for family settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u652f\u6301\u5bb6\u5ead\u65e5\u5e38\u60ef\u4f8b\u6765\u4fc3\u8fdb\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u53ef\u6301\u7eed\u878d\u5165\uff0c\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u3001\u539f\u578b\u5f00\u53d1\u548c4\u5929\u5bb6\u5ead\u90e8\u7f72\u7814\u7a76\uff0c\u53d1\u73b0\u673a\u5668\u4eba\u63d0\u9192\u529f\u80fd\u53d7\u5230\u6b22\u8fce\u4f46\u5f15\u53d1\u5bb6\u5ead\u52a8\u6001\u590d\u6742\u6027", "motivation": "\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u8fdb\u5165\u5bb6\u5ead\u751f\u6d3b\uff0c\u4f46\u6210\u529f\u878d\u5165\u5bb6\u5ead\u65e5\u5e38\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5bb6\u5ead\u65e5\u5e38\u60ef\u4f8b\u4f5c\u4e3a\u7406\u89e3\u673a\u5668\u4eba\u5982\u4f55\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u627e\u5230\u53ef\u6301\u7eed\u89d2\u8272\u7684\u5173\u952e\u5207\u5165\u70b9", "method": "\u4e0e10\u4e2a\u5bb6\u5ead\u5171\u540c\u8bbe\u8ba1\u673a\u5668\u4eba\u4ea4\u4e92\u548c\u884c\u4e3a\uff0c\u8003\u8651\u65f6\u95f4\u3001\u53c2\u4e0e\u8005\u3001\u5730\u70b9\u548c\u73af\u5883\u6d3b\u52a8\u7b49\u60c5\u5883\u56e0\u7d20\uff1b\u8bbe\u8ba1\u3001\u539f\u578b\u5f00\u53d1\u5e76\u90e8\u7f72\u79fb\u52a8\u793e\u4ea4\u673a\u5668\u4eba\u8fdb\u884c\u4e3a\u671f4\u5929\u7684\u5bb6\u5ead\u7528\u6237\u7814\u7a76", "result": "\u5bb6\u5ead\u6b22\u8fce\u673a\u5668\u4eba\u7684\u63d0\u9192\u529f\u80fd\uff0c\u7236\u6bcd\u5c24\u5176\u6b23\u8d4f\u5c06\u90e8\u5206\u63d0\u9192\u4efb\u52a1\u5916\u5305\uff1b\u540c\u65f6\u8bbf\u8c08\u63ed\u793a\u4e86\u56f4\u7ed5\u65f6\u95f4\u5b89\u6392\u3001\u6743\u5a01\u6027\u548c\u5bb6\u5ead\u52a8\u6001\u7684\u7d27\u5f20\u5173\u7cfb\uff0c\u7a81\u663e\u4e86\u5c06\u673a\u5668\u4eba\u878d\u5165\u5bb6\u5ead\u8d85\u8d8a\u7b80\u5355\u63d0\u9192\u4efb\u52a1\u7684\u590d\u6742\u6027", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u4fc3\u8fdb\u60c5\u5883\u63d0\u9192\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e3a\u5bb6\u5ead\u73af\u5883\u8bbe\u8ba1\u673a\u5668\u4eba\u65f6\u66f4\u5e7f\u6cdb\u7684\u8003\u8651\u56e0\u7d20\uff0c\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u529f\u80fd\u4efb\u52a1\uff0c\u5173\u6ce8\u5bb6\u5ead\u52a8\u6001\u548c\u793e\u4f1a\u5173\u7cfb"}}
{"id": "2602.22663", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22663", "abs": "https://arxiv.org/abs/2602.22663", "authors": ["Wenxuan Song", "Jiayi Chen", "Xiaoquan Sun", "Huashuo Lei", "Yikai Qin", "Wei Zhao", "Pengxiang Ding", "Han Zhao", "Tongxin Wang", "Pengxu Hou", "Zhide Zhong", "Haodong Yan", "Donglin Wang", "Jun Ma", "Haoang Li"], "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline", "comment": "Accepted by ICRA 2026", "summary": "Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.", "AI": {"tldr": "\u63d0\u51faCEBench\u57fa\u51c6\u6d4b\u8bd5\u548cLLaVA-VLA\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u53c2\u6570\u8fc7\u5927\u3001\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u9002\u7528\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u6d88\u8d39\u7ea7GPU\u90e8\u7f72\u548c\u79fb\u52a8\u64cd\u4f5c\u7edf\u4e00", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5b58\u5728\u53c2\u6570\u89c4\u6a21\u8fc7\u5927\u3001\u9884\u8bad\u7ec3\u8981\u6c42\u8fc7\u9ad8\u3001\u5bf9\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u9002\u7528\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528", "method": "1) \u63d0\u51faCEBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6837\u5316\u673a\u5668\u4eba\u5f62\u6001\u6570\u636e\uff1b2) \u8bbe\u8ba1LLaVA-VLA\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u96c6\u6210\u7d27\u51d1VLM\u9aa8\u5e72\u3001\u591a\u89c6\u89d2\u611f\u77e5\u3001\u672c\u4f53\u611f\u77e5\u6807\u8bb0\u5316\u548c\u52a8\u4f5c\u5206\u5757\uff1b3) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff08\u540e\u8bad\u7ec3+\u5fae\u8c03\uff09\uff0c\u907f\u514d\u6602\u8d35\u9884\u8bad\u7ec3\uff1b4) \u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u7edf\u4e00\u5bfc\u822a\u548c\u64cd\u4f5c", "result": "\u5b9e\u9a8c\u8bc1\u660eLLaVA-VLA\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u591a\u529f\u80fd\u6027\uff0c\u6210\u4e3a\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u79fb\u52a8\u64cd\u4f5cVLA\u6a21\u578b\uff0c\u53ef\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u90e8\u7f72", "conclusion": "\u901a\u8fc7CEBench\u57fa\u51c6\u548cLLaVA-VLA\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c06\u5f00\u6e90\u6240\u6709\u8d44\u6e90\u4fc3\u8fdb\u7814\u7a76"}}
{"id": "2602.22671", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.22671", "abs": "https://arxiv.org/abs/2602.22671", "authors": ["Georgios Papaioannou", "Barys Shyrokau"], "title": "Does the testing environment matter? Carsickness across on-road, test-track, and driving simulator conditions", "comment": null, "summary": "Carsickness has gained significant attention with the rise of automated vehicles, prompting extensive research across on-road, test-track, and driving simulator environments to understand its occurrence and develop mitigation strategies. However, the lack of carsickness standardization complicates comparisons across studies and environments. Previous works demonstrate measurement validity between two setups at most (e.g., on-road vs. driving simulator), leaving gaps in multi-environment comparisons. This study investigates the recreation of an on-road motion sickness exposure - previously replicated on a test track - using a motion-based driving simulator. Twenty-eight participants performed an eyes-off-road non-driving task while reporting motion sickness using the Misery Scale during the experiment and the Motion Sickness Assessment Questionnaire afterward. Psychological factors known to influence motion sickness were also assessed. The results present subjective and objective measurements for motion sickness across the considered environments. In this paper, acceleration measurements, objective metrics and subjective motion sickness ratings across environments are compared, highlighting key differences in sickness occurrence for simulator-based research validity. Significantly lower motion sickness scores are reported in the simulator compared to on-road and test-track conditions, due to its limited working envelope to reproduce low-frequency (<0.5 Hz) motions, which are the most provocative for motion sickness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u9053\u8def\u3001\u6d4b\u8bd5\u8dd1\u9053\u548c\u9a7e\u9a76\u6a21\u62df\u5668\u4e09\u79cd\u73af\u5883\u4e2d\u6655\u8f66\u75c7\u72b6\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u6a21\u62df\u5668\u7531\u4e8e\u65e0\u6cd5\u91cd\u73b0\u4f4e\u9891\u8fd0\u52a8\uff08<0.5 Hz\uff09\uff0c\u5bfc\u81f4\u6655\u8f66\u8bc4\u5206\u663e\u8457\u4f4e\u4e8e\u5b9e\u9645\u9053\u8def\u548c\u6d4b\u8bd5\u8dd1\u9053\u6761\u4ef6\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u53d1\u5c55\uff0c\u6655\u8f66\u95ee\u9898\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u4f7f\u5f97\u4e0d\u540c\u7814\u7a76\u73af\u5883\uff08\u9053\u8def\u3001\u6d4b\u8bd5\u8dd1\u9053\u3001\u9a7e\u9a76\u6a21\u62df\u5668\uff09\u4e4b\u95f4\u7684\u6bd4\u8f83\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u7814\u7a76\u6700\u591a\u53ea\u9a8c\u8bc1\u4e86\u4e24\u79cd\u73af\u5883\u4e4b\u95f4\u7684\u6d4b\u91cf\u6709\u6548\u6027\uff0c\u7f3a\u4e4f\u591a\u73af\u5883\u7efc\u5408\u6bd4\u8f83\u3002", "method": "28\u540d\u53c2\u4e0e\u8005\u5728\u8fd0\u52a8\u578b\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u6267\u884c\u89c6\u7ebf\u79bb\u5f00\u9053\u8def\u7684\u975e\u9a7e\u9a76\u4efb\u52a1\uff0c\u4f7f\u7528Misery\u91cf\u8868\u5b9e\u65f6\u62a5\u544a\u6655\u8f66\u75c7\u72b6\uff0c\u5b9e\u9a8c\u540e\u4f7f\u7528Motion Sickness Assessment Questionnaire\u8bc4\u4f30\u3002\u540c\u65f6\u6d4b\u91cf\u52a0\u901f\u5ea6\u7b49\u5ba2\u89c2\u6307\u6807\uff0c\u5e76\u8bc4\u4f30\u5f71\u54cd\u6655\u8f66\u7684\u5fc3\u7406\u56e0\u7d20\u3002", "result": "\u6a21\u62df\u5668\u4e2d\u7684\u6655\u8f66\u8bc4\u5206\u663e\u8457\u4f4e\u4e8e\u9053\u8def\u548c\u6d4b\u8bd5\u8dd1\u9053\u6761\u4ef6\u3002\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u62df\u5668\u7684\u5de5\u4f5c\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u91cd\u73b0\u4f4e\u9891\u8fd0\u52a8\uff08<0.5 Hz\uff09\uff0c\u800c\u8fd9\u4e9b\u4f4e\u9891\u8fd0\u52a8\u662f\u8bf1\u53d1\u6655\u8f66\u7684\u6700\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "\u9a7e\u9a76\u6a21\u62df\u5668\u5728\u91cd\u73b0\u5b9e\u9645\u6655\u8f66\u4f53\u9a8c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u9891\u8fd0\u52a8\u518d\u73b0\u65b9\u9762\u3002\u8fd9\u4e3a\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u6655\u8f66\u7814\u7a76\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u62df\u5668\u8bbe\u8ba1\u4e2d\u8003\u8651\u4f4e\u9891\u8fd0\u52a8\u91cd\u73b0\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.22707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22707", "abs": "https://arxiv.org/abs/2602.22707", "authors": ["Kai Li", "Shengtao Zheng", "Linkun Xiu", "Yuze Sheng", "Xiao-Ping Zhang", "Dongyue Huang", "Xinlei Chen"], "title": "SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration", "comment": "This paper has been accepted for publication in the IEEE ROBOTICS AND AUTOMATION LETTERS (RA-L). Please cite the paper using appropriate formats", "summary": "Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system's robustness and low latency in practical scenarios.", "AI": {"tldr": "SCOPE\u6846\u67b6\u901a\u8fc7\u589e\u91cf\u6784\u5efa\u5b9e\u65f6\u9aa8\u67b6\u56fe\u548c\u9690\u5f0f\u672a\u77e5\u533a\u57df\u5206\u6790\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u4e3b\u63a2\u7d22\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e86.9%", "motivation": "\u5f53\u524d\u81ea\u4e3b\u63a2\u7d22\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u7684\u5168\u5c40\u4f18\u5316\uff0c\u5b58\u5728\u9ad8\u8ba1\u7b97\u5ef6\u8fdf\u548c\u8f68\u8ff9\u632f\u8361\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8868\u73b0\u4e0d\u4f73", "method": "\u63d0\u51faSCOPE\u6846\u67b6\uff1a1) \u589e\u91cf\u6784\u5efa\u5b9e\u65f6\u9aa8\u67b6\u56fe\u5e76\u5f15\u5165\u9690\u5f0f\u672a\u77e5\u533a\u57df\u5206\u6790\u8fdb\u884c\u9ad8\u6548\u7a7a\u95f4\u63a8\u7406\uff1b2) \u91c7\u7528\u5206\u5c42\u6309\u9700\u89c4\u5212\u7b56\u7565\uff1a\u8fd1\u7aef\u89c4\u5212\u5668\u751f\u6210\u9ad8\u9891\u5c40\u90e8\u8f68\u8ff9\uff0c\u533a\u57df\u5e8f\u5217\u89c4\u5212\u5668\u4ec5\u5728\u5fc5\u8981\u65f6\u4f18\u5316\u5168\u5c40\u8bbf\u95ee\u987a\u5e8f", "result": "\u4eff\u771f\u5bf9\u6bd4\u663e\u793aSCOPE\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u5168\u5c40\u89c4\u5212\u5668\u76f8\u5f53\u7684\u63a2\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u5e73\u5747\u51cf\u5c1186.9%\u7684\u8ba1\u7b97\u6210\u672c\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u4f4e\u5ef6\u8fdf", "conclusion": "SCOPE\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u4e3b\u63a2\u7d22\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9aa8\u67b6\u56fe\u6784\u5efa\u548c\u5206\u5c42\u89c4\u5212\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500"}}
{"id": "2602.22714", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22714", "abs": "https://arxiv.org/abs/2602.22714", "authors": ["Philipp Schitz", "Paolo Mercorelli", "Johann C. Dauer"], "title": "Robust Helicopter Ship Deck Landing With Guaranteed Timing Using Shrinking-Horizon Model Predictive Control", "comment": "This version was submitted to the American Control Conference 2026 and has been accepted", "summary": "We present a runtime efficient algorithm for autonomous helicopter landings on moving ship decks based on Shrinking-Horizon Model Predictive Control (SHMPC). First, a suitable planning model capturing the relevant aspects of the full nonlinear helicopter dynamics is derived. Next, we use the SHMPC together with a touchdown controller stage to ensure a pre-specified maneuver time and an associated landing time window despite the presence of disturbances. A high disturbance rejection performance is achieved by designing an ancillary controller with disturbance feedback. Thus, given a target position and time, a safe landing with suitable terminal conditions is be guaranteed if the initial optimization problem is feasible. The efficacy of our approach is shown in simulation where all maneuvers achieve a high landing precision in strong winds while satisfying timing and operational constraints with maximum computation times in the millisecond range.", "AI": {"tldr": "\u57fa\u4e8e\u6536\u7f29\u65f6\u57df\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u76f4\u5347\u673a\u81ea\u4e3b\u7740\u8230\u7b97\u6cd5\uff0c\u80fd\u5728\u5f3a\u98ce\u6270\u52a8\u4e0b\u5b9e\u73b0\u6beb\u79d2\u7ea7\u8ba1\u7b97\u7684\u9ad8\u7cbe\u5ea6\u7740\u8230", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u79fb\u52a8\u8230\u8239\u7532\u677f\u4e0a\u5b9e\u73b0\u81ea\u4e3b\u76f4\u5347\u673a\u7740\u9646\u7684\u8fd0\u884c\u65f6\u9ad8\u6548\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\u786e\u4fdd\u9884\u5b9a\u7684\u673a\u52a8\u65f6\u95f4\u548c\u7740\u9646\u65f6\u95f4\u7a97\u53e3", "method": "\u91c7\u7528\u6536\u7f29\u65f6\u57df\u6a21\u578b\u9884\u6d4b\u63a7\u5236(SHMPC)\u7ed3\u5408\u89e6\u5730\u63a7\u5236\u5668\u9636\u6bb5\uff0c\u8bbe\u8ba1\u5305\u542b\u6270\u52a8\u53cd\u9988\u7684\u8f85\u52a9\u63a7\u5236\u5668\uff0c\u4ece\u5b8c\u6574\u975e\u7ebf\u6027\u76f4\u5347\u673a\u52a8\u529b\u5b66\u4e2d\u63a8\u5bfc\u51fa\u5408\u9002\u7684\u89c4\u5212\u6a21\u578b", "result": "\u4eff\u771f\u663e\u793a\u6240\u6709\u673a\u52a8\u90fd\u80fd\u5728\u5f3a\u98ce\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u7740\u9646\u7cbe\u5ea6\uff0c\u6ee1\u8db3\u65f6\u95f4\u548c\u64cd\u4f5c\u7ea6\u675f\uff0c\u6700\u5927\u8ba1\u7b97\u65f6\u95f4\u5728\u6beb\u79d2\u8303\u56f4\u5185", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u8bc1\u5728\u7ed9\u5b9a\u76ee\u6807\u4f4d\u7f6e\u548c\u65f6\u95f4\u4e0b\uff0c\u5982\u679c\u521d\u59cb\u4f18\u5316\u95ee\u9898\u53ef\u884c\uff0c\u5c31\u80fd\u5b9e\u73b0\u5177\u6709\u5408\u9002\u7ec8\u7aef\u6761\u4ef6\u7684\u5b89\u5168\u7740\u9646\uff0c\u5177\u6709\u9ad8\u6270\u52a8\u6291\u5236\u6027\u80fd"}}
{"id": "2602.22731", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22731", "abs": "https://arxiv.org/abs/2602.22731", "authors": ["Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n", "Nived Chebrolu", "Sruthi M. Krishna Moorthy", "Yifu Tao", "Fernando Torres", "Roberto Salguero-G\u00f3mez", "Maurice Fallon"], "title": "Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring", "comment": null, "summary": "Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408NeRF\u3001LiDAR SLAM\u548cGNSS\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5b9e\u73b0\u53ef\u91cd\u590d\u3001\u5730\u7406\u5b9a\u4f4d\u7684\u6811\u82d7\u751f\u6001\u76d1\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u4f20\u611f\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6811\u82d7\u7cbe\u7ec6\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u6811\u82d7\u662f\u68ee\u6797\u518d\u751f\u548c\u5065\u5eb7\u7684\u5173\u952e\u6307\u6807\uff0c\u4f46\u73b0\u6709\u76843D\u4f20\u611f\u65b9\u6cd5\uff08\u5982TLS\u3001MLS\u3001\u4f20\u7edf\u6444\u5f71\u6d4b\u91cf\uff09\u96be\u4ee5\u91cd\u5efa\u7ec6\u679d\u3001\u5bc6\u96c6\u53f6\u7247\uff0c\u4e14\u7f3a\u4e4f\u957f\u671f\u76d1\u6d4b\u6240\u9700\u7684\u5c3a\u5ea6\u4e00\u81f4\u6027\u3002\u9690\u5f0f3D\u91cd\u5efa\u65b9\u6cd5\u5982NeRF\u548c3DGS\u867d\u524d\u666f\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u6062\u590d\u573a\u666f\u771f\u5b9e\u5c3a\u5ea6\u4e14\u7f3a\u4e4f\u7cbe\u786e\u5730\u7406\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u7ea7\u8868\u793a\u7ba1\u9053\uff1a1) \u4f7f\u7528GNSS\u8fdb\u884c\u7c97\u7565\u5730\u7403\u5750\u6807\u7cfb\u5b9a\u4f4d\uff1b2) \u57fa\u4e8eLiDAR\u7684SLAM\u5b9e\u73b0\u5398\u7c73\u7ea7\u7cbe\u786e\u5b9a\u4f4d\u548c\u91cd\u5efa\uff1b3) NeRF\u884d\u751f\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u5bc6\u96c6\u91cd\u5efa\u5355\u4e2a\u6811\u82d7\u3002\u878d\u5408NeRF\u3001LiDAR SLAM\u548cGNSS\u6280\u672f\u3002", "result": "\u5728\u82f1\u56fdWytham Woods\u548c\u82ac\u5170Evo\u7684\u68ee\u6797\u6837\u5730\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4TLS\uff0c\u8be5\u7cfb\u7edf\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u830e\u9ad8\u3001\u5206\u679d\u6a21\u5f0f\u548c\u53f6\u6728\u6bd4\u3002\u53ef\u6d4b\u91cf0.5-2\u7c73\u9ad8\u6811\u82d7\u7684\u7cbe\u786e\u830e\u9aa8\u67b6\u548c\u53f6\u7247\u5206\u5e03\uff0c\u4e3a\u751f\u6001\u5b66\u5bb6\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u548c\u5b9a\u91cf\u6570\u636e\u3002", "conclusion": "\u8be5\u7ba1\u9053\u5b9e\u73b0\u4e86\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u6811\u82d7\u6027\u72b6\u7684\u957f\u671f\u76d1\u6d4b\uff0c\u4e3a\u751f\u6001\u5b66\u5bb6\u5206\u6790\u68ee\u6797\u52a8\u6001\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u548c\u5b9a\u91cf\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u4f20\u611f\u65b9\u6cd5\u5728\u6811\u82d7\u76d1\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.22801", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22801", "abs": "https://arxiv.org/abs/2602.22801", "authors": ["Yinan Zheng", "Tianyi Tan", "Bin Huang", "Enguang Liu", "Ruiming Liang", "Jianlin Zhang", "Jianwei Cui", "Guang Chen", "Kun Ma", "Hangjun Ye", "Long Chen", "Ya-Qin Zhang", "Xianyuan Zhan", "Jingjing Liu"], "title": "Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving", "comment": null, "summary": "Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyper Diffusion Planner (HDP)\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5b9e\u73b0\u4e8610\u500d\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u51b3\u7b56\u4efb\u52a1\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u4ecd\u5c40\u9650\u4e8e\u4eff\u771f\u6216\u5b9e\u9a8c\u5ba4\u73af\u5883\u3002\u6269\u6563\u6a21\u578b\u5728\u5927\u89c4\u6a21\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff08\u5982\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u5927\u91cf\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u7814\u7a76\u6269\u6563\u6a21\u578b\u4f5c\u4e3aE2E AD\u89c4\u5212\u5668\u7684\u6f5c\u529b\u3002\u8bc6\u522b\u6269\u6563\u635f\u5931\u7a7a\u95f4\u3001\u8f68\u8ff9\u8868\u793a\u548c\u6570\u636e\u7f29\u653e\u7b49\u5173\u952e\u56e0\u7d20\u5bf9\u89c4\u5212\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u63d0\u51fa\u7684Hyper Diffusion Planner (HDP)\u5728\u771f\u5b9e\u8f66\u8f86\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u57286\u4e2a\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u548c200\u516c\u91cc\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8610\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f53\u7ecf\u8fc7\u9002\u5f53\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65f6\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u590d\u6742\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u3002"}}
{"id": "2602.22818", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22818", "abs": "https://arxiv.org/abs/2602.22818", "authors": ["Remi Cadene", "Simon Aliberts", "Francesco Capuano", "Michel Aractingi", "Adil Zouitine", "Pepijn Kooijmans", "Jade Choghari", "Martino Russi", "Caroline Pascal", "Steven Palma", "Mustafa Shukor", "Jess Moss", "Alexander Soare", "Dana Aubakirova", "Quentin Lhoest", "Quentin Gallou\u00e9dec", "Thomas Wolf"], "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning", "comment": "https://github.com/huggingface/lerobot", "summary": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.", "AI": {"tldr": "lerobot\u662f\u4e00\u4e2a\u5f00\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u5e93\uff0c\u6574\u5408\u4e86\u4ece\u5e95\u5c42\u7535\u673a\u63a7\u5236\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5904\u7406\u7684\u5b8c\u6574\u673a\u5668\u4eba\u5b66\u4e60\u6808\uff0c\u65e8\u5728\u964d\u4f4e\u673a\u5668\u4eba\u5b66\u4e60\u95e8\u69db\u5e76\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\u53d7\u5230\u788e\u7247\u5316\u3001\u95ed\u6e90\u5de5\u5177\u7684\u963b\u788d\uff0c\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u53ea\u89e3\u51b3\u673a\u5668\u4eba\u6808\u4e2d\u7684\u7279\u5b9a\u5b50\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u5e93lerobot\uff0c\u6574\u5408\u4e86\u4ece\u5e95\u5c42\u4e2d\u95f4\u4ef6\u901a\u4fe1\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u3001\u5b58\u50a8\u548c\u6d41\u5f0f\u5904\u7406\u7684\u5b8c\u6574\u673a\u5668\u4eba\u5b66\u4e60\u6808\uff0c\u652f\u6301\u53ef\u8bbf\u95ee\u7684\u786c\u4ef6\u5e73\u53f0\u548c\u591a\u79cd\u6700\u5148\u8fdb\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "lerobot\u5e93\u5b9e\u73b0\u4e86\u5bf9\u5b8c\u6574\u673a\u5668\u4eba\u5b66\u4e60\u6808\u7684\u96c6\u6210\uff0c\u652f\u6301\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u548c\u7b97\u6cd5\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u5f02\u6b65\u63a8\u7406\u6808\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "lerobot\u901a\u8fc7\u5176\u53ef\u8bbf\u95ee\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5f00\u653e\u6027\uff0c\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4e3a\u53ef\u590d\u73b0\u7684\u6700\u5148\u8fdb\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002"}}
{"id": "2602.22854", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22854", "abs": "https://arxiv.org/abs/2602.22854", "authors": ["Annika Delucchi", "Vincenzo Di Paola", "Andreas M\u00fcller", "and Matteo Zoppi"], "title": "Performance and Experimental Analysis of Strain-based Models for Continuum Robots", "comment": null, "summary": "Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u4e2d\u5e94\u53d8\u63d2\u503c\u6a21\u578b\u7684\u5f62\u72b6\u91cd\u5efa\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u5728\u6355\u6349\u53d8\u5f62\u6548\u5e94\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u51e0\u4f55\u53d8\u91cf\u5e94\u53d8\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5c3d\u7ba1\u5e94\u53d8\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u6027\u80fd\u8bc4\u4f30\u6807\u51c6\u3002\u968f\u7740\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u539f\u578b\u5f00\u53d1\u7684\u589e\u52a0\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u9002\u7528\u6027\u5e76\u8fdb\u884c\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u8fdb\u884c\u5f62\u72b6\u91cd\u5efa\u80fd\u529b\u5206\u6790\uff0c\u4e0e\u51e0\u4f55\u53d8\u91cf\u5e94\u53d8\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u673a\u68b0\u81c2\u79fb\u52a8\u7ec6\u6746\u672b\u7aef\u5e76\u5229\u7528\u76f8\u673a\u8bb0\u5f55\u914d\u7f6e\uff0c\u901a\u8fc7\u53cd\u5c04\u6807\u8bb0\u63d0\u53d6\u5f62\u72b6\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u9884\u6d4b\u4e0e\u89c2\u6d4b\u5f62\u72b6\u543b\u5408\u826f\u597d\uff0c\u5e73\u5747\u8bef\u5dee\u4e3a\u6746\u957f\u76840.58%\uff0c\u6bcf\u4e2a\u914d\u7f6e\u7684\u5e73\u5747\u8ba1\u7b97\u65f6\u95f4\u4e3a0.32\u79d2\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5e94\u53d8\u6a21\u578b\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2602.22862", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22862", "abs": "https://arxiv.org/abs/2602.22862", "authors": ["Enda Xiang", "Haoxiang Ma", "Xinzhu Ma", "Zicheng Liu", "Di Huang"], "title": "GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion", "comment": "Accepted to CVPR 2026", "summary": "This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u6269\u6563\u7b56\u7565\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u7b56\u7565\u5f15\u5bfc\u52a8\u4f5c\u89e3\u7801\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u91cd\u5efa\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u6293\u53d6\u7b56\u7565\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u5df2\u6210\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u6293\u53d6\u6280\u672f\u5b58\u5728\u6293\u53d6\u6267\u884c\u4e0d\u7cbe\u786e\u3001\u7a7a\u95f4\u6cdb\u5316\u6709\u9650\u548c\u7269\u4f53\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u6293\u53d6\u4f5c\u4e3a\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u5b50\u4efb\u52a1\uff0c\u5176\u7cbe\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u503c\u5f97\u7279\u522b\u5173\u6ce8\u3002", "method": "1. \u5c06\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u6269\u6563\u7b56\u7565\u6846\u67b6\uff1b2. \u4f7f\u7528\u6f5c\u5728\u6269\u6563\u7b56\u7565\u901a\u8fc7\u6293\u53d6\u59ff\u6001\u5148\u9a8c\u5f15\u5bfc\u52a8\u4f5c\u5757\u89e3\u7801\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8fd0\u52a8\u8f68\u8ff9\u7d27\u5bc6\u9075\u5faa\u53ef\u884c\u7684\u6293\u53d6\u914d\u7f6e\uff1b3. \u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u81ea\u76d1\u7763\u91cd\u5efa\u76ee\u6807\uff0c\u4ece\u4e2d\u95f4\u8868\u793a\u4e2d\u91cd\u6784\u624b\u8155\u76f8\u673a\u56fe\u50cf\u4ee5\u5d4c\u5165\u6293\u53d6\u5148\u9a8c\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u52a8\u6001\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u6293\u53d6\u7b56\u7565\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6293\u53d6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22896", "abs": "https://arxiv.org/abs/2602.22896", "authors": ["Zebin Yang", "Yijiahao Qi", "Tong Xie", "Bo Yu", "Shaoshan Liu", "Meng Li"], "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation", "comment": "DAC 2026", "summary": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.", "AI": {"tldr": "DySL-VLA\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7VLA\u6a21\u578b\u5c42\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u57fa\u4e8e\u52a8\u4f5c\u91cd\u8981\u6027\u8fdb\u884c\u667a\u80fd\u5c42\u8df3\u8fc7\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u89c2\u5bdf\u5230\u4efb\u52a1\u4e2d\u4e0d\u540c\u52a8\u4f5c\u7684\u91cd\u8981\u6027\u4e0d\u540c\uff0c\u5173\u952e\u6b65\u9aa4\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u6b21\u8981\u6b65\u9aa4\u53ef\u5bb9\u5fcd\u66f4\u5927\u65b9\u5dee\u3002", "method": "\u63d0\u51faDySL-VLA\u6846\u67b6\uff0c\u5c06\u5c42\u5206\u4e3a\u4fe1\u606f\u5c42\uff08\u59cb\u7ec8\u6267\u884c\uff09\u548c\u589e\u91cf\u5c42\uff08\u53ef\u9009\u62e9\u6027\u8df3\u8fc7\uff09\u3002\u8bbe\u8ba1\u4e86\u5148\u9a8c-\u540e\u9a8c\u8df3\u8fc7\u6307\u5bfc\u673a\u5236\u6765\u51b3\u5b9a\u4f55\u65f6\u542f\u52a8\u5c42\u8df3\u8fc7\uff0c\u5e76\u63d0\u51fa\u8df3\u8fc7\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b97\u6cd5\u6765\u9ad8\u6548\u8bad\u7ec3\u6807\u51c6VLA\u4e3aDySL-VLA\u3002", "result": "\u5728Calvin\u6570\u636e\u96c6\u4e0a\uff0cDySL-VLA\u76f8\u6bd4Deer-VLA\u5728\u6210\u529f\u957f\u5ea6\u4e0a\u63d0\u53472.1%\uff0c\u540c\u65f6\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1185.7\u500d\uff0c\u76f8\u5bf9\u4e8eRoboFlamingo\u57fa\u7ebf\u5728\u76f8\u540c\u51c6\u786e\u5ea6\u4e0b\u63d0\u4f9b3.75\u500d\u52a0\u901f\u3002", "conclusion": "DySL-VLA\u901a\u8fc7\u52a8\u6001\u5c42\u8df3\u8fc7\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22922", "abs": "https://arxiv.org/abs/2602.22922", "authors": ["Sophia Taddei", "Wouter Koppen", "Eligia Alfio", "Stefano Nuzzo", "Louis Flynn", "Maria Alejandra Diaz", "Sebastian Rojas Gonzalez", "Tom Dhaene", "Kevin De Pauw", "Ivo Couckuyt", "Tom Verstraten"], "title": "Bayesian Preference Elicitation: Human-In-The-Loop Optimization of An Active Prosthesis", "comment": "8 pages, 5 figures", "summary": "Tuning active prostheses for people with amputation is time-consuming and relies on metrics that may not fully reflect user needs. We introduce a human-in-the-loop optimization (HILO) approach that leverages direct user preferences to personalize a standard four-parameter prosthesis controller efficiently. Our method employs preference-based Multiobjective Bayesian Optimization that uses a state-or-the-art acquisition function especially designed for preference learning, and includes two algorithmic variants: a discrete version (\\textit{EUBO-LineCoSpar}), and a continuous version (\\textit{BPE4Prost}). Simulation results on benchmark functions and real-application trials demonstrate efficient convergence, robust preference elicitation, and measurable biomechanical improvements, illustrating the potential of preference-driven tuning for user-centered prosthesis control.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u8c03\u6574\u5047\u80a2\u63a7\u5236\u5668\u53c2\u6570\uff0c\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u4f18\u5316\u63d0\u9ad8\u8c03\u8c10\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5047\u80a2\u8c03\u8c10\u8fc7\u7a0b\u8017\u65f6\u4e14\u4f9d\u8d56\u7684\u6307\u6807\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u53cd\u6620\u7528\u6237\u9700\u6c42\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7528\u6237\u4e2d\u5fc3\u7684\u4e2a\u6027\u5316\u8c03\u8c10\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4eba\u673a\u4ea4\u4e92\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u504f\u597d\u7684\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u5305\u542b\u4e24\u79cd\u7b97\u6cd5\u53d8\u4f53\uff1a\u79bb\u6563\u7248\u672cEUBO-LineCoSpar\u548c\u8fde\u7eed\u7248\u672cBPE4Prost\u3002", "result": "\u5728\u57fa\u51c6\u51fd\u6570\u548c\u5b9e\u9645\u5e94\u7528\u8bd5\u9a8c\u4e2d\u663e\u793a\u51fa\u9ad8\u6548\u6536\u655b\u3001\u7a33\u5065\u7684\u504f\u597d\u83b7\u53d6\u4ee5\u53ca\u53ef\u6d4b\u91cf\u7684\u751f\u7269\u529b\u5b66\u6539\u8fdb\u3002", "conclusion": "\u57fa\u4e8e\u504f\u597d\u7684\u8c03\u8c10\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\u5b9e\u73b0\u7528\u6237\u4e2d\u5fc3\u7684\u5047\u80a2\u63a7\u5236\uff0c\u63d0\u9ad8\u4e2a\u6027\u5316\u8c03\u8c10\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2602.22940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22940", "abs": "https://arxiv.org/abs/2602.22940", "authors": ["Leon Tolksdorf", "Arturo Tejada", "Christian Birkner", "Nathan van de Wouw"], "title": "Considering Perspectives for Automated Driving Ethics: Collective Risk in Vehicular Motion Planning", "comment": "17 pages, 6 figures, 2 tables", "summary": "Recent automated vehicle (AV) motion planning strategies evolve around minimizing risk in road traffic. However, they exclusively consider risk from the AV's perspective and, as such, do not address the ethicality of its decisions for other road users. We argue that this does not reduce the risk of each road user, as risk may be different from the perspective of each road user. Indeed, minimizing the risk from the AV's perspective may not imply that the risk from the perspective of other road users is also being minimized; in fact, it may even increase. To test this hypothesis, we propose an AV motion planning strategy that supports switching risk minimization strategies between all road user perspectives. We find that the risk from the perspective of other road users can generally be considered different to the risk from the AV's perspective. Taking a collective risk perspective, i.e., balancing the risks of all road users, we observe an AV that minimizes overall traffic risk the best, while putting itself at slightly higher risk for the benefit of others, which is consistent with human driving behavior. In addition, adopting a collective risk minimization strategy can also be beneficial to the AV's travel efficiency by acting assertively when other road users maintain a low risk estimate of the AV. Yet, the AV drives conservatively when its planned actions are less predictable to other road users, i.e., associated with high risk. We argue that such behavior is a form of self-reflection and a natural prerequisite for socially acceptable AV behavior. We conclude that to facilitate ethicality in road traffic that includes AVs, the risk-perspective of each road user must be considered in the decision-making of AVs.", "AI": {"tldr": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u9700\u8981\u8003\u8651\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u89c6\u89d2\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u81ea\u8eab\u89c6\u89d2\uff0c\u91c7\u7528\u96c6\u4f53\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u53ef\u4ee5\u964d\u4f4e\u6574\u4f53\u4ea4\u901a\u98ce\u9669\u5e76\u63d0\u9ad8\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\u4e3b\u8981\u4eceAV\u81ea\u8eab\u89c6\u89d2\u6700\u5c0f\u5316\u98ce\u9669\uff0c\u4f46\u8fd9\u5ffd\u7565\u4e86\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u89c6\u89d2\uff0c\u53ef\u80fd\u5bfc\u81f4\u51b3\u7b56\u4e0d\u9053\u5fb7\u4e14\u65e0\u6cd5\u771f\u6b63\u964d\u4f4e\u6bcf\u4e2a\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u652f\u6301\u5728\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u89c6\u89d2\u4e4b\u95f4\u5207\u6362\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u7684AV\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u96c6\u4f53\u98ce\u9669\u89c6\u89d2\uff08\u5e73\u8861\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\uff09\u6765\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ece\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u89c6\u89d2\u8bc4\u4f30\u7684\u98ce\u9669\u901a\u5e38\u4e0eAV\u81ea\u8eab\u89c6\u89d2\u4e0d\u540c\uff1b\u91c7\u7528\u96c6\u4f53\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u7684AV\u80fd\u591f\u6700\u597d\u5730\u964d\u4f4e\u6574\u4f53\u4ea4\u901a\u98ce\u9669\uff0c\u540c\u65f6\u81ea\u8eab\u627f\u62c5\u7565\u9ad8\u98ce\u9669\u4ee5\u60e0\u53ca\u4ed6\u4eba\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u8be5\u7b56\u7565\u8fd8\u80fd\u901a\u8fc7\u5728\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u5bf9AV\u98ce\u9669\u4f30\u8ba1\u8f83\u4f4e\u65f6\u91c7\u53d6\u81ea\u4fe1\u884c\u52a8\u6765\u63d0\u9ad8AV\u7684\u51fa\u884c\u6548\u7387\u3002", "conclusion": "\u4e3a\u5b9e\u73b0\u5305\u542b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u9053\u5fb7\u9053\u8def\u4ea4\u901a\uff0cAV\u7684\u51b3\u7b56\u5fc5\u987b\u8003\u8651\u6bcf\u4e2a\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u89c6\u89d2\uff0c\u8fd9\u79cd\u81ea\u6211\u53cd\u601d\u884c\u4e3a\u662f\u793e\u4f1a\u53ef\u63a5\u53d7\u7684AV\u884c\u4e3a\u7684\u81ea\u7136\u524d\u63d0\u3002"}}
{"id": "2602.22952", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22952", "abs": "https://arxiv.org/abs/2602.22952", "authors": ["Yuan Tang", "Bruno V. Adorno", "Brendan A. McGrath", "Andrew Weightman"], "title": "Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy", "comment": null, "summary": "Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient's body and tissues near the target. Simulations were performed to validate the controller's implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u901f\u5ea6\u63a7\u5236\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u81ea\u52a8\u8fdb\u884c\u7ecf\u76ae\u6269\u5f20\u6c14\u7ba1\u5207\u5f00\u672f\u7a7f\u523a\uff0c\u901a\u8fc7\u7535\u78c1\u4f20\u611f\u5668\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u7ea6\u675f\u63a7\u5236\u5668\u63d0\u9ad8\u7a7f\u523a\u7cbe\u5ea6\uff0c\u51cf\u5c11\u5e76\u53d1\u75c7\u98ce\u9669\u3002", "motivation": "\u7ecf\u76ae\u6269\u5f20\u6c14\u7ba1\u5207\u5f00\u672f\u7a7f\u523a\u6b65\u9aa4\u5b58\u5728\u8f83\u5927\u4f4d\u7f6e\u548c\u89d2\u5ea6\u8bef\u5dee\uff085\u6beb\u7c73\u548c30\u5ea6\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u5e76\u53d1\u75c7\u5982\u5927\u51fa\u8840\u548c\u6c14\u7ba1\u540e\u58c1\u7a7f\u5b54\u3002\u76ee\u524d\u6240\u6709\u7a7f\u523a\u64cd\u4f5c\u5747\u4e3a\u624b\u52a8\u4e14\u65e0\u5bfc\u822a\u8f85\u52a9\uff0c\u9700\u8981\u63d0\u9ad8\u7cbe\u5ea6\u548c\u964d\u4f4e\u624b\u672f\u96be\u5ea6\u3002", "method": "\u4f7f\u7528\u901f\u5ea6\u63a7\u5236\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u81ea\u52a8\u8fdb\u884c\u9488\u5934\u63d2\u5165\uff0c\u901a\u8fc7\u4e24\u4e2a\u7535\u78c1\u4f20\u611f\u5668\uff08\u4e00\u4e2a\u5728\u9488\u5c16\uff0c\u4e00\u4e2a\u5728\u6c14\u7ba1\u5185\uff09\u63d0\u4f9b\u59ff\u6001\u6570\u636e\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7ea6\u675f\u63a7\u5236\u5668\u5728\u7ebf\u9002\u5e94\u4e0d\u786e\u5b9a\u7684\u8fd0\u52a8\u5b66\u53c2\u6570\uff0c\u907f\u514d\u4e0e\u60a3\u8005\u8eab\u4f53\u548c\u9776\u70b9\u9644\u8fd1\u7ec4\u7ec7\u78b0\u649e\u3002", "result": "\u5728\u4eba\u4f53\u6a21\u578b\u4e0a\u8fdb\u884c400\u6b21\u7a7f\u523a\u5b9e\u9a8c\uff0c\u7edd\u5bf9\u4e2d\u4f4d\u7a7f\u523a\u4f4d\u7f6e\u8bef\u5dee\u4e3a1.7\u6beb\u7c73\uff08IQR\uff1a1.9\u6beb\u7c73\uff09\uff0c\u4e2d\u7ebf\u504f\u5dee\u4e3a4.13\u5ea6\uff08IQR\uff1a4.55\u5ea6\uff09\u3002\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u4e0e\u540d\u4e49\u7a7f\u523a\u7684\u504f\u5dee\u5f88\u5c0f\uff0c\u4e14\u80fd\u4fdd\u8bc1\u65e0\u78b0\u649e\u63d2\u5165\u3002", "conclusion": "\u673a\u5668\u4eba\u7ecf\u76ae\u6269\u5f20\u6c14\u7ba1\u5207\u5f00\u672f\u7a7f\u523a\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u7a7f\u523a\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u4f4d\u7f6e\u548c\u89d2\u5ea6\u8bef\u5dee\uff0c\u8bc1\u660e\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u7a7f\u523a\u7684\u53ef\u884c\u6027\uff0c\u6709\u671b\u964d\u4f4e\u624b\u672f\u5e76\u53d1\u75c7\u98ce\u9669\u3002"}}
{"id": "2602.23017", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23017", "abs": "https://arxiv.org/abs/2602.23017", "authors": ["Dean Zadok", "Tom Naamani", "Yuval Bar-Ratson", "Elisha Barash", "Oren Salzman", "Alon Wolf", "Alex M. Bronstein", "Nili Krausz"], "title": "DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks", "comment": null, "summary": "Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.", "AI": {"tldr": "\u63d0\u51fa\u4f4e\u6210\u672c\u3001\u8f7b\u91cf\u5316\u76843D\u6253\u5370\u673a\u5668\u4eba\u5047\u80a2\u624b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u589e\u5f3a\u5bf9\u7535\u5b50\u8bbe\u5907\uff08\u5982\u952e\u76d8\u3001\u94a2\u7434\uff09\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u7269\u4f53\u6293\u53d6\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u5047\u80a2\u624b\u65e0\u6cd5\u590d\u73b0\u4eba\u624b\u7075\u5de7\u6027\u548c\u76f4\u89c2\u63a7\u5236\uff0c\u5546\u4e1a\u5047\u80a2\u624b\u7f3a\u4e4f\u7cbe\u7ec6\u64cd\u4f5c\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u952e\u76d8\u6253\u5b57\u3001\u94a2\u7434\u6f14\u594f\u7b49\u9700\u8981\u7cbe\u786e\u624b\u6307\u6309\u538b\u7684\u5e94\u7528\u9700\u6c42\uff0c\u9700\u8981\u53ef\u8bbf\u95ee\u3001\u53ef\u590d\u5236\u7684\u8bbe\u8ba1\u6765\u589e\u5f3a\u5047\u80a2\u529f\u80fd\u3002", "method": "\u8bbe\u8ba1\u4f4e\u6210\u672c\u3001\u8f7b\u91cf\u5316\u76843D\u6253\u5370\u673a\u5668\u4eba\u5047\u80a2\u624b\uff0c\u5305\u542b\u624b\u6307\u5916\u5c55/\u5185\u6536\u95f4\u8ddd\u8c03\u8282\u673a\u5236\u3001\u4f18\u5316\u6253\u5b57\u529f\u80fd\u76842-D\u8155\u5173\u8282\uff08\u5c3a\u9aa8/\u6861\u9aa8\u504f\u8f6c\u63a7\u5236\uff09\u4ee5\u53ca\u72ec\u7acb\u624b\u6307\u6309\u538b\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u7814\u7a76\u5c55\u793a\u53c2\u4e0e\u8005\u80fd\u591f\u4f7f\u7528\u8be5\u673a\u5668\u4eba\u5047\u80a2\u624b\u5b9e\u65f6\u6267\u884c\u952e\u76d8\u6253\u5b57\u548c\u94a2\u7434\u6f14\u594f\uff0c\u652f\u6301\u4e0d\u540c\u7ea7\u522b\u7684\u624b\u6307\u548c\u624b\u8155\u8fd0\u52a8\uff0c\u8bc1\u660e\u8bbe\u8ba1\u80fd\u591f\u66f4\u6709\u6548\u5730\u6267\u884c\u6309\u952e\u6253\u5b57\u52a8\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bbe\u8ba1\u80fd\u591f\u589e\u5f3a\u5047\u80a2\u624b\u7684\u529f\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u7535\u5b50\u8bbe\u5907\u64cd\u4f5c\u65b9\u9762\uff0c\u4e3a\u9700\u8981\u7cbe\u7ec6\u624b\u6307\u8fd0\u52a8\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23051", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23051", "abs": "https://arxiv.org/abs/2602.23051", "authors": ["Aihong Wang", "Tenghui Xie", "Fuxi Wen", "Jun Li"], "title": "An Empirical Analysis of Cooperative Perception for Occlusion Risk Mitigation", "comment": "Accepted for publication in IEEE Internet of Things Journal (Regular Article), 2026. DOI: 10.1109/JIOT.2026.3668184", "summary": "Occlusions present a significant challenge for connected and automated vehicles, as they can obscure critical road users from perception systems. Traditional risk metrics often fail to capture the cumulative nature of these threats over time adequately. In this paper, we propose a novel and universal risk assessment metric, the Risk of Tracking Loss (RTL), which aggregates instantaneous risk intensity throughout occluded periods. This provides a holistic risk profile that encompasses both high-intensity, short-term threats and prolonged exposure. Utilizing diverse and high-fidelity real-world datasets, a large-scale statistical analysis is conducted to characterize occlusion risk and validate the effectiveness of the proposed metric. The metric is applied to evaluate different vehicle-to-everything (V2X) deployment strategies. Our study shows that full V2X penetration theoretically eliminates this risk, the reduction is highly nonlinear; a substantial statistical benefit requires a high penetration threshold of 75-90%. To overcome this limitation, we propose a novel asymmetric communication framework that allows even non-connected vehicles to receive warnings. Experimental results demonstrate that this paradigm achieves better risk mitigation performance. We found that our approach at 25% penetration outperforms the traditional symmetric model at 75%, and benefits saturate at only 50% penetration. This work provides a crucial risk assessment metric and a cost-effective, strategic roadmap for accelerating the safety benefits of V2X deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u8bc4\u4f30\u6307\u6807RTL\uff0c\u7528\u4e8e\u91cf\u5316\u906e\u6321\u98ce\u9669\uff0c\u5e76\u901a\u8fc7V2X\u90e8\u7f72\u7b56\u7565\u5206\u6790\u53d1\u73b0\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\u80fd\u66f4\u6709\u6548\u5730\u964d\u4f4e\u98ce\u9669\u3002", "motivation": "\u906e\u6321\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u4f20\u7edf\u98ce\u9669\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u6355\u6349\u968f\u65f6\u95f4\u7d2f\u79ef\u7684\u98ce\u9669\u7279\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u98ce\u9669\u8ffd\u8e2a\u635f\u5931(RTL)\u6307\u6807\uff0c\u805a\u5408\u906e\u6321\u671f\u95f4\u7684\u77ac\u65f6\u98ce\u9669\u5f3a\u5ea6\uff1b\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u7edf\u8ba1\u5206\u6790\uff1b\u63d0\u51fa\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\uff0c\u5141\u8bb8\u975e\u8fde\u63a5\u8f66\u8f86\u63a5\u6536\u8b66\u544a\u3002", "result": "RTL\u80fd\u6709\u6548\u8868\u5f81\u906e\u6321\u98ce\u9669\uff1b\u5b8c\u5168V2X\u6e17\u900f\u7406\u8bba\u4e0a\u53ef\u6d88\u9664\u98ce\u9669\uff0c\u4f46\u9700\u898175-90%\u7684\u9ad8\u6e17\u900f\u9608\u503c\uff1b\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\u572825%\u6e17\u900f\u7387\u4e0b\u4f18\u4e8e\u4f20\u7edf\u5bf9\u79f0\u6a21\u578b75%\u6e17\u900f\u7387\uff0c50%\u6e17\u900f\u7387\u5373\u53ef\u8fbe\u5230\u9971\u548c\u6548\u76ca\u3002", "conclusion": "RTL\u4e3a\u906e\u6321\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u6807\uff0c\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\u4e3a\u5b9e\u73b0V2X\u90e8\u7f72\u7684\u5b89\u5168\u6548\u76ca\u63d0\u4f9b\u4e86\u6210\u672c\u6709\u6548\u7684\u6218\u7565\u8def\u7ebf\u56fe\u3002"}}
{"id": "2602.23109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23109", "abs": "https://arxiv.org/abs/2602.23109", "authors": ["Kai Chen", "Yuyao Huang", "Guang Chen"], "title": "Towards Intelligible Human-Robot Interaction: An Active Inference Approach to Occluded Pedestrian Scenarios", "comment": "14 pages, 6 figures, Proceedings of the 2026 ACM/IEEE International Conference on Human-Robot Interaction (HRI'26)", "summary": "The sudden appearance of occluded pedestrians presents a critical safety challenge in autonomous driving. Conventional rule-based or purely data-driven approaches struggle with the inherent high uncertainty of these long-tail scenarios. To tackle this challenge, we propose a novel framework grounded in Active Inference, which endows the agent with a human-like, belief-driven mechanism. Our framework leverages a Rao-Blackwellized Particle Filter (RBPF) to efficiently estimate the pedestrian's hybrid state. To emulate human-like cognitive processes under uncertainty, we introduce a Conditional Belief Reset mechanism and a Hypothesis Injection technique to explicitly model beliefs about the pedestrian's multiple latent intentions. Planning is achieved via a Cross-Entropy Method (CEM) enhanced Model Predictive Path Integral (MPPI) controller, which synergizes the efficient, iterative search of CEM with the inherent robustness of MPPI. Simulation experiments demonstrate that our approach significantly reduces the collision rate compared to reactive, rule-based, and reinforcement learning (RL) baselines, while also exhibiting explainable and human-like driving behavior that reflects the agent's internal belief state.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u6846\u67b6\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u4e2d\u906e\u6321\u884c\u4eba\u7a81\u7136\u51fa\u73b0\u7684\u6311\u6218\uff0c\u7ed3\u5408RBPF\u72b6\u6001\u4f30\u8ba1\u3001\u6761\u4ef6\u4fe1\u5ff5\u91cd\u7f6e\u3001\u5047\u8bbe\u6ce8\u5165\u548cCEM\u589e\u5f3a\u7684MPPI\u63a7\u5236\u5668\uff0c\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u5e76\u5c55\u73b0\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\u3002", "motivation": "\u906e\u6321\u884c\u4eba\u7a81\u7136\u51fa\u73b0\u662f\u81ea\u52a8\u9a7e\u9a76\u7684\u5173\u952e\u5b89\u5168\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u6216\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u957f\u5c3e\u573a\u666f\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u4f7f\u7528Rao-Blackwellized\u7c92\u5b50\u6ee4\u6ce2\u5668\u4f30\u8ba1\u884c\u4eba\u6df7\u5408\u72b6\u6001\uff0c\u5f15\u5165\u6761\u4ef6\u4fe1\u5ff5\u91cd\u7f6e\u673a\u5236\u548c\u5047\u8bbe\u6ce8\u5165\u6280\u672f\u5efa\u6a21\u884c\u4eba\u7684\u591a\u4e2a\u6f5c\u5728\u610f\u56fe\uff0c\u91c7\u7528\u4ea4\u53c9\u71b5\u65b9\u6cd5\u589e\u5f3a\u7684\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u5668\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u3001\u57fa\u4e8e\u89c4\u5219\u548c\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u78b0\u649e\u7387\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u53ef\u89e3\u91ca\u7684\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\uff0c\u53cd\u6620\u4e86\u667a\u80fd\u4f53\u7684\u5185\u90e8\u4fe1\u5ff5\u72b6\u6001\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u906e\u6321\u884c\u4eba\u573a\u666f\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u901a\u8fc7\u4fe1\u5ff5\u9a71\u52a8\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u7c7b\u4eba\u7684\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\u3002"}}
{"id": "2602.23206", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23206", "abs": "https://arxiv.org/abs/2602.23206", "authors": ["Chung Hee Kim", "Shivani Kamtikar", "Tye Brady", "Taskin Padir", "Joshua Migdal"], "title": "Grasp, Slide, Roll: Comparative Analysis of Contact Modes for Tactile-Based Shape Reconstruction", "comment": "8 pages, 11 figures, Accepted by ICRA 2026", "summary": "Tactile sensing allows robots to gather detailed geometric information about objects through physical interaction, complementing vision-based approaches. However, efficiently acquiring useful tactile data remains challenging due to the time-consuming nature of physical contact and the need to strategically choose contact locations that maximize information gain while minimizing physical interactions. This paper studies how different contact modes affect object shape reconstruction using a tactile-enabled dexterous gripper. We compare three contact interaction modes: grasp-releasing, sliding induced by finger-grazing, and palm-rolling. These contact modes are combined with an information-theoretic exploration framework that guides subsequent sampling locations using a shape completion model. Our results show that the improved tactile sensing efficiency of finger-grazing and palm-rolling translates into faster convergence in shape reconstruction, requiring 34% fewer physical interactions while improving reconstruction accuracy by 55%. We validate our approach using a UR5e robot arm equipped with an Inspire-Robots Dexterous Hand, showing robust performance across primitive object geometries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u89e6\u89c9\u63a5\u89e6\u6a21\u5f0f\u5bf9\u7269\u4f53\u5f62\u72b6\u91cd\u5efa\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u6293\u53d6\u91ca\u653e\u3001\u624b\u6307\u6ed1\u52a8\u548c\u624b\u638c\u6eda\u52a8\u4e09\u79cd\u6a21\u5f0f\uff0c\u7ed3\u5408\u4fe1\u606f\u8bba\u63a2\u7d22\u6846\u67b6\uff0c\u53d1\u73b0\u6ed1\u52a8\u548c\u6eda\u52a8\u6a21\u5f0f\u80fd\u51cf\u5c1134%\u7684\u7269\u7406\u4ea4\u4e92\u6b21\u6570\uff0c\u540c\u65f6\u63d0\u9ad855%\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u89e6\u89c9\u4f20\u611f\u80fd\u8ba9\u673a\u5668\u4eba\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u83b7\u53d6\u7269\u4f53\u7684\u8be6\u7ec6\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u9ad8\u6548\u83b7\u53d6\u6709\u7528\u89e6\u89c9\u6570\u636e\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7269\u7406\u63a5\u89e6\u8017\u65f6\u4e14\u9700\u8981\u7b56\u7565\u6027\u5730\u9009\u62e9\u63a5\u89e6\u4f4d\u7f6e\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u540c\u65f6\u6700\u5c0f\u5316\u7269\u7406\u4ea4\u4e92\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u89e6\u89c9\u63a5\u89e6\u6a21\u5f0f\uff1a\u6293\u53d6\u91ca\u653e\u3001\u624b\u6307\u6ed1\u52a8\uff08\u624b\u6307\u6ed1\u52a8\u5f15\u8d77\uff09\u548c\u624b\u638c\u6eda\u52a8\u3002\u8fd9\u4e9b\u63a5\u89e6\u6a21\u5f0f\u4e0e\u4fe1\u606f\u8bba\u63a2\u7d22\u6846\u67b6\u7ed3\u5408\uff0c\u4f7f\u7528\u5f62\u72b6\u8865\u5168\u6a21\u578b\u6307\u5bfc\u540e\u7eed\u91c7\u6837\u4f4d\u7f6e\u3002\u5b9e\u9a8c\u4f7f\u7528\u914d\u5907Inspire-Robots\u7075\u5de7\u624b\u7684UR5e\u673a\u5668\u4eba\u81c2\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u624b\u6307\u6ed1\u52a8\u548c\u624b\u638c\u6eda\u52a8\u6a21\u5f0f\u63d0\u9ad8\u4e86\u89e6\u89c9\u4f20\u611f\u6548\u7387\uff0c\u4f7f\u5f62\u72b6\u91cd\u5efa\u6536\u655b\u66f4\u5feb\uff0c\u51cf\u5c11\u4e8634%\u7684\u7269\u7406\u4ea4\u4e92\u6b21\u6570\uff0c\u540c\u65f6\u63d0\u9ad8\u4e8655%\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u5728\u57fa\u672c\u51e0\u4f55\u4f53\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u89e6\u89c9\u63a5\u89e6\u6a21\u5f0f\u548c\u4fe1\u606f\u8bba\u63a2\u7d22\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u89e6\u89c9\u4f20\u611f\u7684\u6548\u7387\uff0c\u51cf\u5c11\u7269\u7406\u4ea4\u4e92\u6b21\u6570\u540c\u65f6\u63d0\u9ad8\u5f62\u72b6\u91cd\u5efa\u7cbe\u5ea6\uff0c\u4e3a\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.23253", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23253", "abs": "https://arxiv.org/abs/2602.23253", "authors": ["Yijie Guo", "Iretiayo Akinola", "Lars Johannsmeier", "Hugo Hadfield", "Abhishek Gupta", "Yashraj Narang"], "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly", "comment": null, "summary": "Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.", "AI": {"tldr": "SPARR\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5c06\u4eff\u771f\u8bad\u7ec3\u7684\u57fa\u7840\u7b56\u7565\u4e0e\u771f\u5b9e\u4e16\u754c\u6b8b\u5dee\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u4ee5\u9ad8\u6548\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u53d8\u5316\uff0c\u5728\u65e0\u9700\u4eba\u7c7b\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u88c5\u914d\u9762\u4e34\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff08sim-to-real gap\uff09\uff0c\u800c\u7eaf\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53c8\u4e25\u91cd\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u4e14\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u4eff\u771f\u8bad\u7ec3\u4f18\u52bf\uff0c\u53c8\u80fd\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u53d8\u5316\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPARR\u65b9\u6cd5\uff1a1\uff09\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u57fa\u7840\u7b56\u7565\uff0c\u4f7f\u7528\u4f4e\u5c42\u72b6\u6001\u89c2\u6d4b\u548c\u5bc6\u96c6\u5956\u52b1\uff1b2\uff09\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b66\u4e60\u6b8b\u5dee\u7b56\u7565\uff0c\u4f7f\u7528\u89c6\u89c9\u89c2\u6d4b\u548c\u7a00\u758f\u5956\u52b1\u6765\u8865\u507f\u52a8\u529b\u5b66\u5dee\u5f02\u548c\u4f20\u611f\u5668\u566a\u58f0\uff1b3\uff09\u5c06\u4e24\u8005\u7ed3\u5408\u5f62\u6210\u6df7\u5408\u7b56\u7565\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u4e24\u90e8\u4ef6\u88c5\u914d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6210\u529f\u7387\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672csim-to-real\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u534738.4%\uff0c\u5468\u671f\u65f6\u95f4\u51cf\u5c1129.7%\u3002\u76f8\u6bd4\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u7684\u771f\u5b9e\u4e16\u754cRL\u65b9\u6cd5\uff0cSPARR\u65e0\u9700\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "SPARR\u901a\u8fc7\u7ed3\u5408\u4eff\u771f\u8bad\u7ec3\u7684\u57fa\u7840\u7b56\u7565\u548c\u771f\u5b9e\u4e16\u754c\u5b66\u4e60\u7684\u6b8b\u5dee\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86sim-to-real gap\u95ee\u9898\uff0c\u5728\u65e0\u9700\u4eba\u7c7b\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u673a\u5668\u4eba\u88c5\u914d\uff0c\u4e3a\u63a5\u89e6\u5f0f\u7cbe\u7ec6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23283", "abs": "https://arxiv.org/abs/2602.23283", "authors": ["Mike Y. Michelis", "Nana Obayashi", "Josie Hughes", "Robert K. Katzschmann"], "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots", "comment": null, "summary": "Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment using a simplified, stateless hydrodynamics formulation implemented in the widespread robotics framework MuJoCo. With just two real-world swimming trajectories, we identify five fluid parameters that allow a matching to experimental behavior and generalize across a range of actuation frequencies. We show that this stateless fluid model can generalize to unseen actuation and outperform classical analytical models such as the elongated body theory. This simulation environment runs faster than real-time and can easily enable downstream learning algorithms such as reinforcement learning for target tracking, reaching a 93% success rate. Due to the simplicity and ease of use of the model and our open-source simulation environment, our results show that even simple, stateless models -- when carefully matched to physical data -- can serve as effective digital twins for soft underwater robots, opening up new directions for scalable learning and control in aquatic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b80\u5316\u65e0\u72b6\u6001\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u7684\u808c\u8171\u9a71\u52a8\u9c7c\u7c7b\u673a\u5668\u4eba\u4eff\u771f\u73af\u5883\uff0c\u4ec5\u9700\u4e24\u6761\u771f\u5b9e\u6e38\u6cf3\u8f68\u8ff9\u5373\u53ef\u5339\u914d\u5b9e\u9a8c\u884c\u4e3a\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9a71\u52a8\u9891\u7387\uff0c\u4e3a\u6c34\u4e0b\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u7684\u6570\u5b57\u5b6a\u751f\u5e73\u53f0\u3002", "motivation": "\u6a21\u4eff\u6e38\u6cf3\u52a8\u7269\u7684\u4f18\u96c5\u8fd0\u52a8\u662f\u8f6f\u4f53\u673a\u5668\u4eba\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u5efa\u6a21\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u7528\u4e8e\u590d\u6742\u63a7\u5236\u6216\u5f3a\u5316\u5b66\u4e60\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u4eff\u771f\u73af\u5883\u6765\u652f\u6301\u6c34\u4e0b\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5b66\u4e60\u548c\u63a7\u5236\u3002", "method": "\u5728MuJoCo\u673a\u5668\u4eba\u6846\u67b6\u4e2d\u5b9e\u73b0\u7b80\u5316\u7684\u65e0\u72b6\u6001\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u6784\u5efa\u808c\u8171\u9a71\u52a8\u9c7c\u7c7b\u673a\u5668\u4eba\u4eff\u771f\u73af\u5883\u3002\u4ec5\u4f7f\u7528\u4e24\u6761\u771f\u5b9e\u6e38\u6cf3\u8f68\u8ff9\u8bc6\u522b\u4e94\u4e2a\u6d41\u4f53\u53c2\u6570\uff0c\u4f7f\u4eff\u771f\u4e0e\u5b9e\u9a8c\u884c\u4e3a\u5339\u914d\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u9a71\u52a8\u9891\u7387\u3002", "result": "\u8be5\u65e0\u72b6\u6001\u6d41\u4f53\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9a71\u52a8\uff0c\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u7684\u7ec6\u957f\u4f53\u7406\u8bba\u7b49\u5206\u6790\u6a21\u578b\u3002\u4eff\u771f\u73af\u5883\u8fd0\u884c\u901f\u5ea6\u5feb\u4e8e\u5b9e\u65f6\uff0c\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u76ee\u6807\u8ddf\u8e2a\uff0c\u6210\u529f\u7387\u53ef\u8fbe93%\u3002", "conclusion": "\u5373\u4f7f\u7b80\u5355\u7684\u65e0\u72b6\u6001\u6a21\u578b\uff0c\u53ea\u8981\u4e0e\u7269\u7406\u6570\u636e\u4ed4\u7ec6\u5339\u914d\uff0c\u5c31\u80fd\u4f5c\u4e3a\u8f6f\u4f53\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u6709\u6548\u6570\u5b57\u5b6a\u751f\uff0c\u4e3a\u6c34\u751f\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u548c\u63a7\u5236\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.23287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23287", "abs": "https://arxiv.org/abs/2602.23287", "authors": ["Demiana R. Barsoum", "Mahdieh Nejati Javaremi", "Larisa Y. C. Loke", "Brenna D. Argall"], "title": "Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning", "comment": "13 pages, 8 figures, to appear in the proceedings of the 2026 Human-Robot Interaction (HRI) Conference", "summary": "Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06\u4f4e\u7ef4\u63a5\u53e3\u63a7\u5236\u7684\u673a\u5668\u4eba\u6f14\u793a\u63d0\u5347\u5230\u5b8c\u6574\u63a7\u5236\u7a7a\u95f4\uff0c\u89e3\u51b3\u63a5\u53e3\u9650\u5236\u5bfc\u81f4\u7684\u6b21\u4f18\u8fd0\u52a8\u95ee\u9898\u3002", "motivation": "\u8f85\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u4f4e\u7ef4\u63a5\u53e3\uff08\u59821\u7ef4\u5438\u5439\u63a5\u53e3\uff09\u63a7\u5236\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u65f6\uff0c\u63a5\u53e3\u7ef4\u5ea6\u4e0d\u5339\u914d\u5bfc\u81f4\u53ea\u80fd\u8bbf\u95ee\u63a7\u5236\u7ef4\u5ea6\u7684\u5b50\u96c6\uff0c\u9020\u6210\u673a\u5668\u4eba\u8fd0\u52a8\u53d7\u5230\u4eba\u4e3a\u9650\u5236\u3002\u63a5\u53e3\u53d7\u9650\u7684\u6f14\u793a\u5305\u542b\u53cd\u6620\u63a5\u53e3\u9650\u5236\u800c\u975e\u7528\u6237\u610f\u56fe\u7684\u6b21\u4f18\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\uff0c\u8003\u8651\u4efb\u52a1\u3001\u73af\u5883\u548c\u63a5\u53e3\u7ea6\u675f\uff0c\u5c06\u6f14\u793a\u63d0\u5347\u5230\u673a\u5668\u4eba\u7684\u5b8c\u6574\u63a7\u5236\u7a7a\u95f4\u3002\u4f7f\u75282\u7ef4\u64cd\u7eb5\u6746\u548c1\u7ef4\u5438\u5439\u63a7\u5236\u63a5\u53e3\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e0d\u540c\u76847\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8fdb\u884c\u771f\u5b9e\u4e16\u754cADL\u4efb\u52a1\u6f14\u793a\u8bc4\u4f30\u3002", "result": "\u91cd\u5efa\u7684\u6f14\u793a\u548c\u6d3e\u751f\u7684\u63a7\u5236\u7b56\u7565\u5206\u6790\u8868\u660e\uff0c\u63d0\u5347\u540e\u7684\u8f68\u8ff9\u6bd4\u63a5\u53e3\u53d7\u9650\u7684\u5bf9\u5e94\u8f68\u8ff9\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\uff0c\u540c\u65f6\u5c0a\u91cd\u7528\u6237\u504f\u597d\u3002", "conclusion": "\u8be5\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8f85\u52a9\u673a\u5668\u4eba\u4e2d\u63a5\u53e3\u7ef4\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u751f\u6210\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u7528\u6237\u610f\u56fe\u3002"}}
