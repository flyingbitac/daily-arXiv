{"id": "2601.10721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10721", "abs": "https://arxiv.org/abs/2601.10721", "authors": ["Xinyu Li", "Qian Tang", "Guoxin Yin", "Gang Zheng", "Jessica Burgner-Kahrs", "Cesare Stefanini", "Ke Wu"], "title": "Collaborative Continuum Robots: A Survey", "comment": null, "summary": "Continuum robots (CRs), owing to their compact structure, inherent compliance, and flexible deformation, have been widely applied in various fields. By coordinating multiple CRs to form collaborative continuum robots (CCRs), task adaptability, workspace, flexibility, load capacity, and operational stability can be further improved, thus offering significant advantages. In recent years, interest in this emerging field has grown steadily within the continuum-robotics community, accompanied by a consistent rise in related publications. By presenting a comprehensive overview of recent progress from different system-architecture levels, this survey provides a clear framework for research on CCRs. First, CCRs are classified into the three collaboration modes of separated collaboration, assistance collaboration, and parallel collaboration, with definitions provided. Next, advances in structural design, modeling, motion planning, and control for each mode are systematically summarized. Finally, current challenges and future opportunities for CCRs are discussed.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u534f\u4f5c\u8fde\u7eed\u4f53\u673a\u5668\u4eba(CCRs)\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5c06\u5176\u5206\u4e3a\u4e09\u79cd\u534f\u4f5c\u6a21\u5f0f\uff0c\u603b\u7ed3\u4e86\u7ed3\u6784\u8bbe\u8ba1\u3001\u5efa\u6a21\u3001\u8fd0\u52a8\u89c4\u5212\u4e0e\u63a7\u5236\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u673a\u9047\u3002", "motivation": "\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u56e0\u5176\u7d27\u51d1\u7ed3\u6784\u3001\u56fa\u6709\u67d4\u987a\u6027\u548c\u7075\u6d3b\u53d8\u5f62\u80fd\u529b\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5f62\u6210\u534f\u4f5c\u7cfb\u7edf\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4efb\u52a1\u9002\u5e94\u6027\u3001\u5de5\u4f5c\u7a7a\u95f4\u3001\u7075\u6d3b\u6027\u3001\u8d1f\u8f7d\u80fd\u529b\u548c\u64cd\u4f5c\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u8fd1\u5e74\u6765\u8be5\u65b0\u5174\u9886\u57df\u7684\u7814\u7a76\u5174\u8da3\u6301\u7eed\u589e\u957f\uff0c\u76f8\u5173\u51fa\u7248\u7269\u4e0d\u65ad\u589e\u52a0\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u9996\u5148\u5c06\u534f\u4f5c\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5206\u4e3a\u4e09\u79cd\u534f\u4f5c\u6a21\u5f0f\uff1a\u5206\u79bb\u534f\u4f5c\u3001\u8f85\u52a9\u534f\u4f5c\u548c\u5e76\u884c\u534f\u4f5c\uff0c\u5e76\u4e3a\u6bcf\u79cd\u6a21\u5f0f\u63d0\u4f9b\u5b9a\u4e49\u3002\u7136\u540e\u4ece\u7cfb\u7edf\u67b6\u6784\u5c42\u9762\u7cfb\u7edf\u603b\u7ed3\u4e86\u6bcf\u79cd\u6a21\u5f0f\u5728\u7ed3\u6784\u8bbe\u8ba1\u3001\u5efa\u6a21\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u65b9\u9762\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "result": "\u8bba\u6587\u4e3a\u534f\u4f5c\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u6846\u67b6\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u901a\u8fc7\u5206\u7c7b\u548c\u603b\u7ed3\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u7406\u89e3\u4e0d\u540c\u534f\u4f5c\u6a21\u5f0f\u7684\u7279\u70b9\u3001\u6280\u672f\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u534f\u4f5c\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u662f\u4e00\u4e2a\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\u7684\u65b0\u5174\u9886\u57df\u3002\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7814\u7a76\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u7684\u53d1\u5c55\u673a\u9047\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.10722", "categories": ["cs.RO", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10722", "abs": "https://arxiv.org/abs/2601.10722", "authors": ["Daniel Casini", "Jian-Jia Chen", "Jing Li", "Federico Reghenzani", "Harun Teper"], "title": "A Survey of Real-Time Support, Analysis, and Advancements in ROS 2", "comment": null, "summary": "The Robot Operating System 2 (ROS~2) has emerged as a relevant middleware framework for robotic applications, offering modularity, distributed execution, and communication. In the last six years, ROS~2 has drawn increasing attention from the real-time systems community and industry. This survey presents a comprehensive overview of research efforts that analyze, enhance, and extend ROS~2 to support real-time execution. We first provide a detailed description of the internal scheduling mechanisms of ROS~2 and its layered architecture, including the interaction with DDS-based communication and other communication middleware. We then review key contributions from the literature, covering timing analysis for both single- and multi-threaded executors, metrics such as response time, reaction time, and data age, and different communication modes. The survey also discusses community-driven enhancements to the ROS~2 runtime, including new executor algorithm designs, real-time GPU management, and microcontroller support via micro-ROS. Furthermore, we summarize techniques for bounding DDS communication delays, message filters, and profiling tools that have been developed to support analysis and experimentation. To help systematize this growing body of work, we introduce taxonomies that classify the surveyed contributions based on different criteria. This survey aims to guide both researchers and practitioners in understanding and improving the real-time capabilities of ROS~2.", "AI": {"tldr": "\u672c\u6587\u5bf9ROS~2\u5b9e\u65f6\u6027\u7814\u7a76\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u8c03\u5ea6\u673a\u5236\u3001\u65f6\u5e8f\u5206\u6790\u3001\u901a\u4fe1\u4f18\u5316\u548c\u793e\u533a\u589e\u5f3a\u7b49\u65b9\u9762\uff0c\u65e8\u5728\u6307\u5bfc\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u7406\u89e3\u5e76\u6539\u8fdbROS~2\u7684\u5b9e\u65f6\u80fd\u529b\u3002", "motivation": "ROS~2\u4f5c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u7684\u91cd\u8981\u4e2d\u95f4\u4ef6\u6846\u67b6\uff0c\u5728\u8fc7\u53bb\u516d\u5e74\u4e2d\u53d7\u5230\u5b9e\u65f6\u7cfb\u7edf\u793e\u533a\u548c\u5de5\u4e1a\u754c\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u548c\u5206\u6790\u73b0\u6709\u7684\u5b9e\u65f6\u6027\u7814\u7a76\u6210\u679c\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u3002", "method": "\u9996\u5148\u8be6\u7ec6\u63cf\u8ff0ROS~2\u7684\u5185\u90e8\u8c03\u5ea6\u673a\u5236\u548c\u5206\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u4e0eDDS\u901a\u4fe1\u548c\u5176\u4ed6\u901a\u4fe1\u4e2d\u95f4\u4ef6\u7684\u4ea4\u4e92\u3002\u7136\u540e\u7efc\u8ff0\u6587\u732e\u4e2d\u7684\u5173\u952e\u8d21\u732e\uff0c\u6db5\u76d6\u5355\u7ebf\u7a0b\u548c\u591a\u7ebf\u7a0b\u6267\u884c\u5668\u7684\u65f6\u5e8f\u5206\u6790\u3001\u54cd\u5e94\u65f6\u95f4\u3001\u53cd\u5e94\u65f6\u95f4\u3001\u6570\u636e\u5e74\u9f84\u7b49\u6307\u6807\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684\u901a\u4fe1\u6a21\u5f0f\u3002\u8fd8\u8ba8\u8bba\u4e86\u793e\u533a\u5bf9ROS~2\u8fd0\u884c\u65f6\u7684\u589e\u5f3a\uff0c\u5305\u62ec\u65b0\u7684\u6267\u884c\u5668\u7b97\u6cd5\u8bbe\u8ba1\u3001\u5b9e\u65f6GPU\u7ba1\u7406\u548c\u901a\u8fc7micro-ROS\u7684\u5fae\u63a7\u5236\u5668\u652f\u6301\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u4e0d\u540c\u6807\u51c6\u7684\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u5316\u8fd9\u4e9b\u7814\u7a76\u6210\u679c\uff0c\u603b\u7ed3\u4e86\u9650\u5236DDS\u901a\u4fe1\u5ef6\u8fdf\u3001\u6d88\u606f\u8fc7\u6ee4\u5668\u548c\u6027\u80fd\u5206\u6790\u5de5\u5177\u7684\u6280\u672f\uff0c\u4e3aROS~2\u7684\u5b9e\u65f6\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u7406\u89e3ROS~2\u5b9e\u65f6\u80fd\u529b\u7684\u7cfb\u7edf\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\uff0c\u5305\u62ec\u6267\u884c\u5668\u7b97\u6cd5\u4f18\u5316\u3001\u901a\u4fe1\u5ef6\u8fdf\u63a7\u5236\u548c\u5b9e\u65f6\u5206\u6790\u5de5\u5177\u5f00\u53d1\u7b49\u3002"}}
{"id": "2601.10724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10724", "abs": "https://arxiv.org/abs/2601.10724", "authors": ["Rishabh Dev Yadav"], "title": "Adaptive Sliding Mode Control for Vehicle Platoons with State-Dependent Friction Uncertainty", "comment": "Extended version based on the author MSc thesis. Related to an earlier IEEE ICAR 2021 publication", "summary": "Multi-robot formation control has various applications in domains such as vehicle troops, platoons, payload transportation, and surveillance. Maintaining formation in a vehicle platoon requires designing a suitable control scheme that can tackle external disturbances and uncertain system parameters while maintaining a predefined safe distance between the robots. A crucial challenge in this context is dealing with the unknown/uncertain friction forces between wheels and the ground, which vary with changes in road surface, wear in tires, and speed of the vehicle. Although state-of-the-art adaptive controllers can handle a priori bounded uncertainties, they struggle with accurately modeling and identifying frictional forces, which are often state-dependent and cannot be a priori bounded.\n  This thesis proposes a new adaptive sliding mode controller for wheeled mobile robot-based vehicle platoons that can handle the unknown and complex behavior of frictional forces without prior knowledge of their parameters and structures. The controller uses the adaptive sliding mode control techniques to regulate the platoon's speed and maintain a predefined inter-robot distance, even in the presence of external disturbances and uncertain system parameters. This approach involves a two-stage process: first, the kinematic controller calculates the desired velocities based on the desired trajectory; and second, the dynamics model generates the commands to achieve the desired motion. By separating the kinematics and dynamics of the robot, this approach can simplify the control problem and allow for more efficient and robust control of the wheeled mobile robot.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u8f86\u7f16\u961f\u7684\u65b0\u578b\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5904\u7406\u672a\u77e5\u590d\u6742\u7684\u6469\u64e6\u529b\u548c\u5916\u90e8\u5e72\u6270\uff0c\u65e0\u9700\u6469\u64e6\u529b\u7684\u5148\u9a8c\u53c2\u6570\u548c\u7ed3\u6784\u77e5\u8bc6\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7f16\u961f\u63a7\u5236\u5728\u8f66\u8f86\u7f16\u961f\u3001\u6709\u6548\u8f7d\u8377\u8fd0\u8f93\u548c\u76d1\u89c6\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5904\u7406\u8f6e\u5730\u4e4b\u95f4\u7684\u672a\u77e5/\u4e0d\u786e\u5b9a\u6469\u64e6\u529b\uff0c\u8fd9\u4e9b\u6469\u64e6\u529b\u968f\u8def\u9762\u6761\u4ef6\u3001\u8f6e\u80ce\u78e8\u635f\u548c\u8f66\u901f\u53d8\u5316\u800c\u53d8\u5316\u3002\u73b0\u6709\u81ea\u9002\u5e94\u63a7\u5236\u5668\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u548c\u8bc6\u522b\u8fd9\u4e9b\u72b6\u6001\u4f9d\u8d56\u4e14\u65e0\u6cd5\u5148\u9a8c\u6709\u754c\u7684\u6469\u64e6\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a1) \u8fd0\u52a8\u5b66\u63a7\u5236\u5668\u6839\u636e\u671f\u671b\u8f68\u8ff9\u8ba1\u7b97\u671f\u671b\u901f\u5ea6\uff1b2) \u52a8\u529b\u5b66\u6a21\u578b\u751f\u6210\u5b9e\u73b0\u671f\u671b\u8fd0\u52a8\u7684\u547d\u4ee4\u3002\u901a\u8fc7\u5206\u79bb\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\uff0c\u7b80\u5316\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u9c81\u68d2\u7684\u63a7\u5236\u3002", "result": "\u8be5\u63a7\u5236\u5668\u80fd\u591f\u5904\u7406\u672a\u77e5\u590d\u6742\u7684\u6469\u64e6\u884c\u4e3a\uff0c\u65e0\u9700\u6469\u64e6\u529b\u7684\u5148\u9a8c\u53c2\u6570\u548c\u7ed3\u6784\u77e5\u8bc6\uff0c\u80fd\u591f\u8c03\u8282\u7f16\u961f\u901f\u5ea6\u5e76\u7ef4\u6301\u9884\u5b9a\u7684\u673a\u5668\u4eba\u95f4\u8ddd\u79bb\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u5916\u90e8\u5e72\u6270\u548c\u4e0d\u786e\u5b9a\u7cfb\u7edf\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u5668\u4e3a\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u8f86\u7f16\u961f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u672a\u77e5\u6469\u64e6\u529b\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7f16\u961f\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2601.10725", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.10725", "abs": "https://arxiv.org/abs/2601.10725", "authors": ["Hieu Do Quang", "Chien Truong-Quoc", "Quoc Van Tran"], "title": "Multi-Agent Formation Navigation Using Diffusion-Based Trajectory Generation", "comment": "8 pages, 3 figures, full version of a paper submitted to a conference", "summary": "This paper introduces a diffusion-based planner for leader--follower formation control in cluttered environments. The diffusion policy is used to generate the trajectory of the midpoint of two leaders as a rigid bar in the plane, thereby defining their desired motion paths in a planar formation. While the followers track the leaders and form desired foramtion geometry using a distance-constrained formation controller based only on the relative positions in followers' local coordinates. The proposed approach produces smooth motions and low tracking errors, with most failures occurring in narrow obstacle-free space, or obstacle configurations that are not in the training data set. Simulation results demonstrate the potential of diffusion models for reliable multi-agent formation planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7f16\u961f\u63a7\u5236\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7f16\u961f\u8fd0\u52a8", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7f16\u961f\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u751f\u6210\u5e73\u6ed1\u7684\u8fd0\u52a8\u8f68\u8ff9\u5e76\u4fdd\u6301\u7f16\u961f\u51e0\u4f55\u5f62\u72b6\uff0c\u540c\u65f6\u907f\u514d\u969c\u788d\u7269", "method": "\u4f7f\u7528\u6269\u6563\u7b56\u7565\u751f\u6210\u4e24\u4e2a\u9886\u5bfc\u8005\u4e2d\u70b9\u4f5c\u4e3a\u521a\u6027\u6746\u7684\u8f68\u8ff9\uff0c\u5b9a\u4e49\u5176\u5728\u5e73\u9762\u7f16\u961f\u4e2d\u7684\u671f\u671b\u8fd0\u52a8\u8def\u5f84\uff1b\u8ddf\u968f\u8005\u4f7f\u7528\u57fa\u4e8e\u76f8\u5bf9\u4f4d\u7f6e\u7684\u5c40\u90e8\u5750\u6807\u8ddd\u79bb\u7ea6\u675f\u7f16\u961f\u63a7\u5236\u5668\u6765\u8ddf\u8e2a\u9886\u5bfc\u8005\u5e76\u5f62\u6210\u671f\u671b\u7684\u7f16\u961f\u51e0\u4f55\u5f62\u72b6", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u5e73\u6ed1\u7684\u8fd0\u52a8\u548c\u8f83\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5927\u591a\u6570\u5931\u8d25\u53d1\u751f\u5728\u72ed\u7a84\u7684\u65e0\u969c\u788d\u7a7a\u95f4\u6216\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u672a\u5305\u542b\u7684\u969c\u788d\u7269\u914d\u7f6e\u4e2d", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6269\u6563\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u7f16\u961f\u89c4\u5212\u4e2d\u5177\u6709\u53ef\u9760\u6027\u7684\u6f5c\u529b\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.10796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10796", "abs": "https://arxiv.org/abs/2601.10796", "authors": ["Junxiang Wang", "Cindy Wang", "Rana Soltani Zarrin", "Zackory Erickson"], "title": "Bidirectional Human-Robot Communication for Physical Human-Robot Interaction", "comment": "12 pages, 8 figures. To be published in 2026 ACM/IEEE International Conference on Human-Robot Interaction", "summary": "Effective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot's planned trajectory -- position, velocity, and force -- in real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot's verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website: https://bidir-comm.github.io/", "AI": {"tldr": "BRIDGE\u7cfb\u7edf\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u4eba\u673a\u53cc\u5411\u4ea4\u4e92\uff0c\u5141\u8bb8\u7528\u6237\u5b9e\u65f6\u4fee\u6539\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528LLM\u89e3\u91ca\u7528\u6237\u6307\u4ee4\uff0c\u63d0\u4f9b\u8bed\u97f3\u53cd\u9988\u63d0\u5347\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u9700\u8981\u7cfb\u7edf\u65e2\u80fd\u9002\u5e94\u7528\u6237\u504f\u597d\uff0c\u53c8\u80fd\u900f\u660e\u5c55\u793a\u81ea\u8eab\u884c\u4e3a\u3002\u73b0\u6709\u7cfb\u7edf\u5728\u5b9e\u65f6\u53cc\u5411\u6c9f\u901a\u548c\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u89e3\u6790\u7528\u6237\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5728\u89c4\u5212\u8fd0\u52a8\u8f68\u8ff9\u548c\u5bf9\u8bdd\u5386\u53f2\u80cc\u666f\u4e0b\u7406\u89e3\u8f68\u8ff9\u4fee\u6539\u610f\u56fe\uff0c\u63d0\u4f9b\u8bed\u97f3\u53cd\u9988\u786e\u8ba4\u4fee\u6539\u6216\u6f84\u6e05\u7591\u95ee\u3002", "result": "18\u540d\u8001\u5e74\u7528\u6237\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u6210\u529f\u5b9e\u65f6\u4fee\u6539\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u53cc\u5411\u53cd\u9988\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u4e92\u6027\u548c\u900f\u660e\u5ea6\u8bc4\u5206\uff0c\u8bc1\u660e\u8bed\u97f3\u53cd\u9988\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "BRIDGE\u7cfb\u7edf\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u8bed\u97f3\u53cd\u9988\u5b9e\u73b0\u4e86\u66f4\u76f4\u89c2\u7684\u4eba\u673a\u7269\u7406\u534f\u4f5c\uff0c\u53cc\u5411\u6c9f\u901a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u4f53\u9a8c\u7684\u900f\u660e\u5ea6\u548c\u81ea\u7136\u6027\u3002"}}
{"id": "2601.10827", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.10827", "abs": "https://arxiv.org/abs/2601.10827", "authors": ["Simin Liu", "Tong Zhao", "Bernhard Paus Graesdal", "Peter Werner", "Jiuguang Wang", "John Dolan", "Changliu Liu", "Tao Pang"], "title": "Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets", "comment": "17 pages, 14 figures; under submission to IEEE Transactions on Robotics", "summary": "If we consider human manipulation, it is clear that contact-rich manipulation (CRM)-the ability to use any surface of the manipulator to make contact with objects-can be far more efficient and natural than relying solely on end-effectors (i.e., fingertips). However, state-of-the-art model-based planners for CRM are still focused on feasibility rather than optimality, limiting their ability to fully exploit CRM's advantages. We introduce a new paradigm that computes approximately optimal manipulator plans. This approach has two phases. Offline, we construct a graph of mutual reachable sets, where each set contains all object orientations reachable from a starting object orientation and grasp. Online, we plan over this graph, effectively computing and sequencing local plans for globally optimized motion. On a challenging, representative contact-rich task, our approach outperforms a leading planner, reducing task cost by 61%. It also achieves a 91% success rate across 250 queries and maintains sub-minute query times, ultimately demonstrating that globally optimized contact-rich manipulation is now practical for real-world tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u6784\u5efa\u53ef\u8fbe\u96c6\u56fe\u548c\u5728\u7ebf\u89c4\u5212\uff0c\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u4efb\u52a1\u6210\u672c\u964d\u4f4e61%\uff0c\u6210\u529f\u738791%", "motivation": "\u4eba\u7c7b\u64cd\u4f5c\u4e2d\uff0c\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\uff08\u4f7f\u7528\u64cd\u7eb5\u5668\u4efb\u4f55\u8868\u9762\u63a5\u89e6\u7269\u4f53\uff09\u6bd4\u4ec5\u4f9d\u8d56\u672b\u7aef\u6267\u884c\u5668\u66f4\u9ad8\u6548\u81ea\u7136\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u5668\u4ecd\u4e3b\u8981\u5173\u6ce8\u53ef\u884c\u6027\u800c\u975e\u6700\u4f18\u6027\uff0c\u9650\u5236\u4e86\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4f18\u52bf\u7684\u5145\u5206\u53d1\u6325", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u6784\u5efa\u76f8\u4e92\u53ef\u8fbe\u96c6\u56fe\uff0c\u6bcf\u4e2a\u96c6\u5408\u5305\u542b\u4ece\u8d77\u59cb\u7269\u4f53\u65b9\u5411\u548c\u6293\u63e1\u53ef\u8fbe\u7684\u6240\u6709\u7269\u4f53\u65b9\u5411\uff1b\u5728\u7ebf\u9636\u6bb5\u5728\u8be5\u56fe\u4e0a\u89c4\u5212\uff0c\u6709\u6548\u8ba1\u7b97\u548c\u6392\u5e8f\u5c40\u90e8\u8ba1\u5212\u4ee5\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u8fd0\u52a8", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u9886\u5148\u89c4\u5212\u5668\uff0c\u4efb\u52a1\u6210\u672c\u964d\u4f4e61%\uff0c\u5728250\u4e2a\u67e5\u8be2\u4e2d\u8fbe\u523091%\u6210\u529f\u7387\uff0c\u5e76\u4fdd\u6301\u4e9a\u5206\u949f\u7ea7\u67e5\u8be2\u65f6\u95f4", "conclusion": "\u5168\u5c40\u4f18\u5316\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u73b0\u5728\u5df2\u5b9e\u9645\u53ef\u884c\uff0c\u53ef\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u89c4\u5212\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u4f18\u8d8a\u6027"}}
{"id": "2601.10877", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10877", "abs": "https://arxiv.org/abs/2601.10877", "authors": ["Ludovic Righetti", "Vincent Boulanin"], "title": "Is open robotics innovation a threat to international peace and security?", "comment": null, "summary": "Open access to publication, software and hardware is central to robotics: it lowers barriers to entry, supports reproducible science and accelerates reliable system development. However, openness also exacerbates the inherent dual-use risks associated with research and innovation in robotics. It lowers barriers for states and non-state actors to develop and deploy robotics systems for military use and harmful purposes. Compared to other fields of engineering where dual-use risks are present - e.g., those that underlie the development of weapons of mass destruction (chemical, biological, radiological, and nuclear weapons) and even the field of AI, robotics offers no specific regulation and little guidance as to how research and innovation may be conducted and disseminated responsibly. While other fields can be used for guidance, robotics has its own needs and specificities which have to be taken into account. The robotics community should therefore work toward its own set of sector-specific guidance and possibly regulation. To that end, we propose a roadmap focusing on four practices: a) education in responsible robotics; b) incentivizing risk assessment; c) moderating the diffusion of high-risk material; and d) developing red lines.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u673a\u5668\u4eba\u9886\u57df\u5f00\u653e\u83b7\u53d6\u5e26\u6765\u7684\u53cc\u91cd\u7528\u9014\u98ce\u9669\uff0c\u63d0\u51fa\u5236\u5b9a\u884c\u4e1a\u7279\u5b9a\u6307\u5bfc\u65b9\u9488\u548c\u76d1\u7ba1\u7684\u8def\u7ebf\u56fe", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u7684\u5f00\u653e\u83b7\u53d6\u867d\u7136\u964d\u4f4e\u4e86\u8fdb\u5165\u95e8\u69db\u3001\u652f\u6301\u53ef\u91cd\u590d\u79d1\u5b66\u5e76\u52a0\u901f\u7cfb\u7edf\u5f00\u53d1\uff0c\u4f46\u4e5f\u52a0\u5267\u4e86\u53cc\u91cd\u7528\u9014\u98ce\u9669\uff0c\u4f7f\u56fd\u5bb6\u548c\u975e\u56fd\u5bb6\u884c\u4e3a\u8005\u66f4\u5bb9\u6613\u5c06\u673a\u5668\u4eba\u6280\u672f\u7528\u4e8e\u519b\u4e8b\u548c\u6709\u5bb3\u76ee\u7684\u3002\u4e0e\u5176\u4ed6\u5de5\u7a0b\u9886\u57df\u76f8\u6bd4\uff0c\u673a\u5668\u4eba\u9886\u57df\u7f3a\u4e4f\u5177\u4f53\u7684\u76d1\u7ba1\u548c\u8d1f\u8d23\u4efb\u7814\u7a76\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u5b9e\u8df5\u9886\u57df\u7684\u8def\u7ebf\u56fe\uff1aa) \u8d1f\u8d23\u4efb\u673a\u5668\u4eba\u6559\u80b2\uff1bb) \u6fc0\u52b1\u98ce\u9669\u8bc4\u4f30\uff1bc) \u8c03\u8282\u9ad8\u98ce\u9669\u6750\u6599\u7684\u4f20\u64ad\uff1bd) \u5236\u5b9a\u7ea2\u7ebf\u6807\u51c6\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u793e\u533a\u5e94\u5236\u5b9a\u884c\u4e1a\u7279\u5b9a\u6307\u5bfc\u65b9\u9488\u548c\u53ef\u80fd\u76d1\u7ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u5b9e\u8df5\u6765\u5e94\u5bf9\u5f00\u653e\u83b7\u53d6\u5e26\u6765\u7684\u53cc\u91cd\u7528\u9014\u98ce\u9669\u3002", "conclusion": "\u673a\u5668\u4eba\u793e\u533a\u9700\u8981\u5236\u5b9a\u81ea\u5df1\u7684\u884c\u4e1a\u7279\u5b9a\u6307\u5bfc\u65b9\u9488\u548c\u76d1\u7ba1\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u80b2\u3001\u98ce\u9669\u8bc4\u4f30\u3001\u6750\u6599\u4f20\u64ad\u63a7\u5236\u548c\u7ea2\u7ebf\u5236\u5b9a\u7b49\u63aa\u65bd\uff0c\u5728\u4fdd\u6301\u5f00\u653e\u83b7\u53d6\u4f18\u52bf\u7684\u540c\u65f6\u7ba1\u7406\u53cc\u91cd\u7528\u9014\u98ce\u9669\u3002"}}
{"id": "2601.10930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10930", "abs": "https://arxiv.org/abs/2601.10930", "authors": ["Zhixian Xie", "Yu Xiang", "Michael Posa", "Wanxin Jin"], "title": "Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation", "comment": "13 Pages, Plan to submit RSS", "summary": "A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42RL-MPC\u6846\u67b6\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u95ee\u9898\uff1a\u9ad8\u5c42RL\u9884\u6d4b\u63a5\u89e6\u610f\u56fe\uff0c\u5e95\u5c42MPC\u4f18\u5316\u63a5\u89e6\u6a21\u5f0f\uff0c\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u3001\u9c81\u68d2\u6027\u5f3a\u3001\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb", "motivation": "\u63a5\u89e6\u4e30\u5bcc\u7684\u7075\u5de7\u64cd\u4f5c\u9700\u8981\u540c\u65f6\u8003\u8651\u51e0\u4f55\u3001\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u975e\u5149\u6ed1\u63a5\u89e6\u52a8\u529b\u5b66\uff0c\u7aef\u5230\u7aef\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u9700\u8981\u5927\u91cf\u6570\u636e\u3001\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u5f31", "method": "\u5206\u5c42RL-MPC\u6846\u67b6\uff1a\u9ad8\u5c42RL\u7b56\u7565\u9884\u6d4b\u63a5\u89e6\u610f\u56fe\uff08\u63a5\u89e6\u4f4d\u7f6e\u548c\u5b50\u76ee\u6807\u59ff\u6001\uff09\uff0c\u5e95\u5c42\u63a5\u89e6\u9690\u5f0fMPC\u4f18\u5316\u5c40\u90e8\u63a5\u89e6\u6a21\u5f0f\u5e76\u91cd\u65b0\u89c4\u5212\u63a5\u89e6\u52a8\u529b\u5b66", "result": "\u5728\u975e\u6293\u53d6\u4efb\u52a1\uff08\u51e0\u4f55\u6cdb\u5316\u63a8\u52a8\u548c\u7269\u4f533D\u91cd\u5b9a\u5411\uff09\u4e2d\u5b9e\u73b0\u63a5\u8fd1100%\u6210\u529f\u7387\uff0c\u6570\u636e\u91cf\u51cf\u5c1110\u500d\uff0c\u6027\u80fd\u9ad8\u5ea6\u9c81\u68d2\uff0c\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb", "conclusion": "\u5206\u5c42\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u51e0\u4f55/\u8fd0\u52a8\u5b66\u89c4\u5212\u548c\u63a5\u89e6\u52a8\u529b\u5b66\u6267\u884c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u7684\u6570\u636e\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u8fc1\u79fb\u6027\u95ee\u9898"}}
{"id": "2601.11026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11026", "abs": "https://arxiv.org/abs/2601.11026", "authors": ["HyoJae Kang", "SunWoo Ahn", "InGyu Choi", "GeonYeong Go", "KunWoo Son", "Min-Sung Kang"], "title": "Crane Lowering Guidance Using a Attachable Camera Module for Driver Vision Support", "comment": "Presented at ICCR 2025(International COnference on Control and Robotics 2025). Submitted to the IEEE for possible publication", "summary": "Cranes have long been essential equipment for lifting and placing heavy loads in construction projects. This study focuses on the lowering phase of crane operation, the stage in which the load is moved to the desired location. During this phase, a constant challenge exists: the load obstructs the operator's view of the landing point. As a result, operators traditionally have to rely on verbal or gestural instructions from ground personnel, which significantly impacts site safety. To alleviate this constraint, the proposed system incorporates a attachable camera module designed to be attached directly to the load via a suction cup. This module houses a single-board computer, battery, and compact camera. After installation, it streams and processes images of the ground directly below the load in real time to generate installation guidance. Simultaneously, this guidance is transmitted to and monitored by a host computer. Preliminary experiments were conducted by attaching this module to a test object, confirming the feasibility of real-time image acquisition and transmission. This approach has the potential to significantly improve safety on construction sites by providing crane operators with an instant visual reference of hidden landing zones.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u9644\u7740\u5728\u8d77\u91cd\u673a\u540a\u94a9\u4e0a\u7684\u6444\u50cf\u5934\u6a21\u5757\uff0c\u901a\u8fc7\u5b9e\u65f6\u56fe\u50cf\u4f20\u8f93\u89e3\u51b3\u64cd\u4f5c\u5458\u5728\u540a\u88c5\u4e0b\u964d\u9636\u6bb5\u65e0\u6cd5\u770b\u5230\u7740\u9646\u70b9\u7684\u95ee\u9898\u3002", "motivation": "\u8d77\u91cd\u673a\u5728\u540a\u88c5\u4e0b\u964d\u9636\u6bb5\uff0c\u64cd\u4f5c\u5458\u89c6\u7ebf\u88ab\u8d1f\u8f7d\u906e\u6321\uff0c\u65e0\u6cd5\u76f4\u63a5\u770b\u5230\u7740\u9646\u70b9\uff0c\u4f20\u7edf\u4e0a\u4f9d\u8d56\u5730\u9762\u4eba\u5458\u53e3\u5934\u6216\u624b\u52bf\u6307\u6325\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u53ef\u9644\u7740\u5f0f\u6444\u50cf\u5934\u6a21\u5757\uff0c\u5305\u542b\u5355\u677f\u8ba1\u7b97\u673a\u3001\u7535\u6c60\u548c\u7d27\u51d1\u578b\u6444\u50cf\u5934\uff0c\u901a\u8fc7\u5438\u76d8\u76f4\u63a5\u5b89\u88c5\u5728\u8d1f\u8f7d\u4e0a\uff0c\u5b9e\u65f6\u91c7\u96c6\u548c\u5904\u7406\u8d1f\u8f7d\u4e0b\u65b9\u5730\u9762\u56fe\u50cf\uff0c\u751f\u6210\u5b89\u88c5\u6307\u5bfc\u5e76\u4f20\u8f93\u5230\u4e3b\u673a\u8ba1\u7b97\u673a\u76d1\u63a7\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u901a\u8fc7\u5c06\u6a21\u5757\u9644\u7740\u5728\u6d4b\u8bd5\u7269\u4f53\u4e0a\uff0c\u786e\u8ba4\u4e86\u5b9e\u65f6\u56fe\u50cf\u91c7\u96c6\u548c\u4f20\u8f93\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6f5c\u529b\u901a\u8fc7\u4e3a\u8d77\u91cd\u673a\u64cd\u4f5c\u5458\u63d0\u4f9b\u9690\u85cf\u7740\u9646\u533a\u57df\u7684\u5373\u65f6\u89c6\u89c9\u53c2\u8003\uff0c\u663e\u8457\u63d0\u9ad8\u65bd\u5de5\u73b0\u573a\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.11063", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.11063", "abs": "https://arxiv.org/abs/2601.11063", "authors": ["Haishan Zeng", "Peng Li"], "title": "H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning", "comment": null, "summary": "In embodied artificial intelligence, enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions remains a critical challenge. While large language models (LLMs) show promise in instruction parsing and preliminary planning, they exhibit limitations in long-term reasoning and dynamic multi-robot coordination. We propose Hierarchical Autonomous Intelligent Multi-Robot Planning(H-AIM), a novel embodied multi-robot task planning framework that addresses these issues through a three-stage cascaded architecture: 1) It leverages an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, thereby transforming commands into formal planning problems; 2) It combines the semantic reasoning of LLMs with the search capabilities of a classical planner to produce optimized action sequences; 3) It compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization. To validate our approach, we introduce the MACE-THOR benchmark dataset, comprising 42 complex tasks across 8 distinct household layouts. Experimental results demonstrate that H-AIM achieves a remarkable performance improvement, elevating the task success rate from 12% to 55% and boosting the goal condition recall from 32% to 72% against the strongest baseline, LaMMA-P.", "AI": {"tldr": "H-AIM\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u6267\u884c\u957f\u671f\u4efb\u52a1\u7684\u5c42\u6b21\u5316\u81ea\u4e3b\u667a\u80fd\u591a\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7LLM\u89e3\u6790\u6307\u4ee4\u3001\u7ecf\u5178\u89c4\u5212\u5668\u4f18\u5316\u884c\u52a8\u5e8f\u5217\u3001\u884c\u4e3a\u6811\u5b9e\u73b0\u53cd\u5e94\u63a7\u5236\uff0c\u5c06\u4efb\u52a1\u6210\u529f\u7387\u4ece12%\u63d0\u5347\u523055%\u3002", "motivation": "\u5728\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u4e2d\uff0c\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u6267\u884c\u9ad8\u7ea7\u6307\u4ee4\u7684\u957f\u671f\u4efb\u52a1\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u89e3\u6790\u548c\u521d\u6b65\u89c4\u5212\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u957f\u671f\u63a8\u7406\u548c\u52a8\u6001\u591a\u673a\u5668\u4eba\u534f\u8c03\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faH-AIM\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u7ea7\u7ea7\u8054\u67b6\u6784\uff1a1) \u5229\u7528LLM\u89e3\u6790\u6307\u4ee4\u5e76\u751f\u6210PDDL\u95ee\u9898\u63cf\u8ff0\uff1b2) \u7ed3\u5408LLM\u8bed\u4e49\u63a8\u7406\u548c\u7ecf\u5178\u89c4\u5212\u5668\u641c\u7d22\u80fd\u529b\u751f\u6210\u4f18\u5316\u884c\u52a8\u5e8f\u5217\uff1b3) \u5c06\u89c4\u5212\u7ed3\u679c\u7f16\u8bd1\u4e3a\u884c\u4e3a\u6811\u8fdb\u884c\u53cd\u5e94\u63a7\u5236\u3002\u901a\u8fc7\u5171\u4eab\u9ed1\u677f\u673a\u5236\u652f\u6301\u52a8\u6001\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u901a\u4fe1\u548c\u72b6\u6001\u540c\u6b65\u3002", "result": "\u5728MACE-THOR\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u542b8\u79cd\u4e0d\u540c\u5bb6\u5ead\u5e03\u5c40\u768442\u4e2a\u590d\u6742\u4efb\u52a1\uff09\u4e0a\u9a8c\u8bc1\uff0cH-AIM\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebfLaMMA-P\uff0c\u5c06\u4efb\u52a1\u6210\u529f\u7387\u4ece12%\u63d0\u5347\u523055%\uff0c\u76ee\u6807\u6761\u4ef6\u53ec\u56de\u7387\u4ece32%\u63d0\u5347\u523072%\u3002", "conclusion": "H-AIM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u7ecf\u5178\u89c4\u5212\u5668\u7684\u641c\u7d22\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u957f\u671f\u4efb\u52a1\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4efb\u52a1\u6267\u884c\u6027\u80fd\u3002"}}
{"id": "2601.11076", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11076", "abs": "https://arxiv.org/abs/2601.11076", "authors": ["Jiaqi Liang", "Yue Chen", "Qize Yu", "Yan Shen", "Haipeng Zhang", "Hao Dong", "Ruihai Wu"], "title": "A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation", "comment": "AAAI2026 oral", "summary": "Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.", "AI": {"tldr": "A3D\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u81ea\u9002\u5e94\u53ef\u4f9b\u6027\u6765\u8bc6\u522b\u5bb6\u5177\u96f6\u4ef6\u4e0a\u7684\u6700\u4f73\u652f\u6491\u548c\u7a33\u5b9a\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u53cc\u81c2\u534f\u4f5c\u7684\u5bb6\u5177\u88c5\u914d\u4efb\u52a1", "motivation": "\u5bb6\u5177\u88c5\u914d\u5bf9\u673a\u5668\u4eba\u6765\u8bf4\u662f\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u7cbe\u786e\u7684\u53cc\u81c2\u534f\u8c03\uff0c\u5176\u4e2d\u4e00\u53ea\u624b\u81c2\u64cd\u4f5c\u96f6\u4ef6\uff0c\u53e6\u4e00\u53ea\u63d0\u4f9b\u534f\u4f5c\u652f\u6491\u548c\u7a33\u5b9a\u3002\u673a\u5668\u4eba\u9700\u8981\u5728\u957f\u671f\u88c5\u914d\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u9002\u5e94\u652f\u6491\u7b56\u7565\uff0c\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u96f6\u4ef6\u51e0\u4f55\u5f62\u72b6\u3002", "method": "\u63d0\u51faA3D\u6846\u67b6\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u53ef\u4f9b\u6027\u6765\u8bc6\u522b\u5bb6\u5177\u96f6\u4ef6\u4e0a\u7684\u6700\u4f73\u652f\u6491\u548c\u7a33\u5b9a\u4f4d\u7f6e\u3002\u65b9\u6cd5\u91c7\u7528\u5bc6\u96c6\u70b9\u7ea7\u51e0\u4f55\u8868\u793a\u6765\u5efa\u6a21\u96f6\u4ef6\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7684\u6cdb\u5316\u3002\u5f15\u5165\u81ea\u9002\u5e94\u6a21\u5757\uff0c\u4f7f\u7528\u4ea4\u4e92\u53cd\u9988\u5728\u88c5\u914d\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u5148\u524d\u4ea4\u4e92\u52a8\u6001\u8c03\u6574\u652f\u6491\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b8\u79cd\u5bb6\u5177\u7c7b\u578b\u300150\u4e2a\u4e0d\u540c\u96f6\u4ef6\u7684\u4eff\u771f\u73af\u5883\uff0c\u7528\u4e8e\u53cc\u81c2\u534f\u4f5c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u96f6\u4ef6\u51e0\u4f55\u5f62\u72b6\u548c\u5bb6\u5177\u7c7b\u522b\u3002", "conclusion": "A3D\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u53ef\u4f9b\u6027\u5b66\u4e60\u548c\u52a8\u6001\u7b56\u7565\u8c03\u6574\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u53cc\u81c2\u534f\u4f5c\u5bb6\u5177\u88c5\u914d\u4e2d\u7684\u51e0\u4f55\u6cdb\u5316\u548c\u957f\u671f\u9002\u5e94\u6027\u95ee\u9898\u3002"}}
{"id": "2601.11231", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.11231", "abs": "https://arxiv.org/abs/2601.11231", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "title": "Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control", "comment": "2025 IEEE 64th Conference on Decision and Control (CDC)", "summary": "We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u76d1\u6d4b\u91ce\u706b\u706b\u7ebf\u7684\u79fb\u52a8\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u611f\u77e5\u3001\u4f30\u8ba1\u548c\u63a7\u5236\u96c6\u6210\u5728\u4e00\u4e2a\u968f\u673a\u6700\u4f18\u63a7\u5236\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u4fe1\u606f\u5bfb\u6c42\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u5b9e\u73b0\u6700\u4f18\u76d1\u6d4b\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u76d1\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u611f\u77e5\u3001\u4f30\u8ba1\u548c\u63a7\u5236\u5206\u5f00\u5904\u7406\uff0c\u8981\u4e48\u65bd\u52a0\u7ebf\u6027\u9ad8\u65af\u5047\u8bbe\u6765\u5efa\u7acb\u6700\u4f18\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u8fd1\u4f3c\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6027\u80fd\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u91ce\u706b\u6f14\u5316\u7684\u968f\u673a\u6027\u3002", "method": "1. \u5c06\u706b\u7ebf\u76d1\u6d4b\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff1b2. \u4e3a\u968f\u673a\u975e\u7ebf\u6027\u692d\u5706\u589e\u957f\u706b\u7ebf\u6a21\u578b\u63a8\u5bfc\u6700\u4f18\u9012\u5f52\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff1b3. \u5c06\u975e\u7ebf\u6027\u968f\u673a\u63a7\u5236\u95ee\u9898\u8f6c\u5316\u4e3a\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff1b4. \u8bbe\u8ba1\u57fa\u4e8e\u4e0b\u7f6e\u4fe1\u754c\u81ea\u9002\u5e94\u641c\u7d22\u7b97\u6cd5\u7684\u4fe1\u606f\u5bfb\u6c42\u9884\u6d4b\u63a7\u5236\u5f8b\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u6e10\u8fd1\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u7684\u7279\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u968f\u673a\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u611f\u77e5\u3001\u4f30\u8ba1\u548c\u63a7\u5236\u7ec4\u4ef6\uff0c\u4e3a\u91ce\u706b\u706b\u7ebf\u76d1\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u6e10\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u706b\u7ebf\u76d1\u6d4b\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u5177\u6709\u6e10\u8fd1\u6536\u655b\u4fdd\u8bc1\u7684\u4fe1\u606f\u5bfb\u6c42\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u96c6\u6210\u611f\u77e5\u3001\u4f30\u8ba1\u548c\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u91ce\u706b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11250", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11250", "abs": "https://arxiv.org/abs/2601.11250", "authors": ["Tobias J\u00fclg", "Khaled Gamal", "Nisarga Nilavadi", "Pierre Krack", "Seongjin Bien", "Michael Krawez", "Florian Walter", "Wolfram Burgard"], "title": "VLAgents: A Policy Server for Efficient VLA Inference", "comment": null, "summary": "The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents", "AI": {"tldr": "VLAgents\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u7b56\u7565\u670d\u52a1\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u7684Gymnasium\u98ce\u683c\u534f\u8bae\u62bd\u8c61\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63a8\u7406\uff0c\u652f\u6301\u96f6\u62f7\u8d1d\u5171\u4eab\u5185\u5b58\u548c\u538b\u7f29\u6d41\u4f20\u8f93\uff0c\u5728\u672c\u5730\u548c\u8fdc\u7a0b\u901a\u4fe1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u90e8\u7f72\u590d\u6742\uff0c\u5b58\u5728\u63a5\u53e3\u788e\u7247\u5316\u548c\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7b80\u5316\u90e8\u7f72\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51faVLAgents\u6a21\u5757\u5316\u7b56\u7565\u670d\u52a1\u5668\uff0c\u91c7\u7528\u7edf\u4e00\u7684Gymnasium\u98ce\u683c\u534f\u8bae\u62bd\u8c61VLA\u63a8\u7406\uff0c\u901a\u4fe1\u5c42\u6839\u636e\u4e0a\u4e0b\u6587\u900f\u660e\u9002\u914d\uff0c\u652f\u6301\u96f6\u62f7\u8d1d\u5171\u4eab\u5185\u5b58\uff08\u9ad8\u901f\u4eff\u771f\uff09\u548c\u538b\u7f29\u6d41\u4f20\u8f93\uff08\u8fdc\u7a0b\u786c\u4ef6\uff09\u3002", "result": "\u6210\u529f\u96c6\u6210\u4e03\u4e2a\u7b56\u7565\uff08\u5305\u62ecOpenVLA\u548cPi Zero\uff09\uff0c\u5728\u672c\u5730\u548c\u8fdc\u7a0b\u901a\u4fe1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8eOpenVLA\u3001OpenPi\u548cLeRobot\u7684\u9ed8\u8ba4\u7b56\u7565\u670d\u52a1\u5668\u3002", "conclusion": "VLAgents\u901a\u8fc7\u7edf\u4e00\u7684\u534f\u8bae\u548c\u81ea\u9002\u5e94\u901a\u4fe1\u5c42\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u90e8\u7f72\u7684\u590d\u6742\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11335", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.11335", "abs": "https://arxiv.org/abs/2601.11335", "authors": ["Tyler Paine", "Brendan Long", "Jeremy Wenger", "Michael DeFilippo", "James Usevitch", "Michael Benjamin"], "title": "Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets", "comment": "8 pages, 10 figures", "summary": "Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u6784\u65e0\u4eba\u8239\u961f\u78b0\u649e\u907f\u514d\u7684\u5206\u5e03\u5f0f\u5b89\u5168\u63a7\u5236\u8fc7\u6ee4\u5668\uff0c\u7ed3\u5408\u4e86\u6700\u574f\u60c5\u51b5\u63a7\u5236\u969c\u788d\u51fd\u6570\u548c\u907f\u78b0\u89c4\u5219\u884c\u4e3a\u65b9\u6cd5\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f02\u6784\u65e0\u4eba\u8239\u961f\u78b0\u649e\u907f\u514d\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u4e0d\u540c\u5e73\u53f0\u7684\u51b3\u7b56\u8fc7\u7a0b\u548c\u63a7\u5236\u5668\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u5b9e\u65f6\u5171\u4eab\u8f68\u8ff9\u548c\u63a7\u5236\u503c\u5b58\u5728\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7279\u522b\u662f\u8981\u5e94\u5bf9\u5305\u62ec\u6709\u4eba\u9a7e\u9a76\u8239\u8236\u5728\u5185\u7684\u5176\u4ed6\u63a5\u89e6\u5bf9\u8c61\u7684\u6700\u574f\u60c5\u51b5\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u5b89\u5168\u63a7\u5236\u8fc7\u6ee4\u5668\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e2a\u81ea\u4e3b\u8f66\u8f86\u4e0a\u6dfb\u52a0\u63a7\u5236\u8fc7\u6ee4\u5668\uff0c\u5047\u8bbe\u5176\u4ed6\u63a5\u89e6\u5bf9\u8c61\uff08\u5305\u62ec\u6709\u4eba\u9a7e\u9a76\u8239\u8236\uff09\u91c7\u53d6\u6700\u574f\u60c5\u51b5\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u63a7\u5236\u969c\u788d\u51fd\u6570\u7406\u8bba\u5f00\u53d1\uff0c\u5e76\u4e0e\u907f\u78b0\u89c4\u5219\u884c\u4e3a\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6a21\u62df\u906d\u9047\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u6700\u574f\u60c5\u51b5CBF\u65b9\u6cd5\u548c\u907f\u78b0\u89c4\u5219\u884c\u4e3a\u65b9\u6cd5\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u65e0\u4eba\u8239\u548c\u4eba\u7c7b\u64cd\u4f5c\u8239\u8236\u8fdb\u884c\uff0c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u6709\u6548\uff0c\u5e76\u5bf9\u4eba\u7c7b\u64cd\u4f5c\u8005\u7684\u4e0d\u5408\u4f5c\u884c\u4e3a\u5177\u6709\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u663e\u793a\u7ed3\u5408CBF\u65b9\u6cd5\u548c\u907f\u78b0\u89c4\u5219\u884c\u4e3a\u80fd\u5b9e\u73b0\u6700\u4f73\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408\u63a7\u5236\u969c\u788d\u51fd\u6570\u65b9\u6cd5\u548c\u907f\u78b0\u89c4\u5219\u884c\u4e3a\u65b9\u6cd5\u5728\u5f02\u6784\u65e0\u4eba\u8239\u961f\u78b0\u649e\u907f\u514d\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3001\u5e73\u53f0\u65e0\u5173\u6027\u548c\u5bf9\u4e0d\u5408\u4f5c\u884c\u4e3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.11404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11404", "abs": "https://arxiv.org/abs/2601.11404", "authors": ["Linqing Zhong", "Yi Liu", "Yifei Wei", "Ziyu Xiong", "Maoqing Yao", "Si Liu", "Guanghui Ren"], "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACoT-VLA\u7684\u65b0\u67b6\u6784\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u76f4\u63a5\u6784\u5efa\u4e3a\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u5316\u5e8f\u5217\uff0c\u901a\u8fc7\u663e\u5f0f\u548c\u9690\u5f0f\u52a8\u4f5c\u63a8\u7406\u5668\u5171\u540c\u5f62\u6210\u52a8\u4f5c\u601d\u7ef4\u94fe\uff0c\u6307\u5bfc\u6700\u7ec8\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e2d\u95f4\u63a8\u7406\uff08\u5982\u5b50\u4efb\u52a1\u9884\u6d4b\u6216\u76ee\u6807\u56fe\u50cf\u5408\u6210\uff09\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4f20\u9012\u7684\u4fe1\u606f\u6709\u9650\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u52a8\u4f5c\u6267\u884c\u6240\u9700\u7684\u5b8c\u6574\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002\u4f5c\u8005\u8ba4\u4e3a\u6700\u6709\u6548\u7684\u63a8\u7406\u5f62\u5f0f\u5e94\u8be5\u662f\u76f4\u63a5\u5728\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u601d\u8003\u3002", "method": "\u63d0\u51faACoT-VLA\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a\u663e\u5f0f\u52a8\u4f5c\u63a8\u7406\u5668\uff08EAR\uff09\u63d0\u51fa\u7c97\u7565\u53c2\u8003\u8f68\u8ff9\u4f5c\u4e3a\u663e\u5f0f\u52a8\u4f5c\u7ea7\u63a8\u7406\u6b65\u9aa4\uff1b\u9690\u5f0f\u52a8\u4f5c\u63a8\u7406\u5668\uff08IAR\uff09\u4ece\u591a\u6a21\u6001\u8f93\u5165\u7684\u5185\u90e8\u8868\u793a\u4e2d\u63d0\u53d6\u6f5c\u5728\u52a8\u4f5c\u5148\u9a8c\u3002\u4e24\u8005\u5171\u540c\u5f62\u6210\u52a8\u4f5c\u601d\u7ef4\u94fe\uff0c\u4e3a\u4e0b\u6e38\u52a8\u4f5c\u5934\u63d0\u4f9b\u6761\u4ef6\uff0c\u5b9e\u73b0\u57fa\u4e8e\u7b56\u7565\u7684\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u4eff\u771f\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LIBERO\u3001LIBERO-Plus\u548cVLABench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u5230\u4e8698.5%\u300184.1%\u548c47.4%\u7684\u6027\u80fd\u3002", "conclusion": "\u52a8\u4f5c\u601d\u7ef4\u94fe\u8303\u5f0f\u901a\u8fc7\u5728\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u76f4\u63a5\u63a8\u7406\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u5b8c\u6574\u3001\u7ec6\u7c92\u5ea6\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u901a\u7528\u7b56\u7565\u3002"}}
{"id": "2601.11421", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11421", "abs": "https://arxiv.org/abs/2601.11421", "authors": ["Ziyu Wang", "Chenyuan Liu", "Yushun Xiang", "Runhao Zhang", "Qingbo Hao", "Hongliang Lu", "Houyu Chen", "Zhizhong Feng", "Kaiyue Zheng", "Dehao Ye", "Xianchao Zeng", "Xinyu Zhou", "Boran Wen", "Jiaxin Li", "Mingyu Zhang", "Kecheng Zheng", "Qian Zhu", "Ran Cheng", "Yong-Lu Li"], "title": "The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents", "comment": null, "summary": "Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.", "AI": {"tldr": "GM-100\u662f\u9996\u4e2a\u9762\u5411\u673a\u5668\u4eba\u5b66\u4e60\u7684\"\u5965\u6797\u5339\u514b\"\u57fa\u51c6\uff0c\u5305\u542b100\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u6db5\u76d6\u5e7f\u6cdb\u4ea4\u4e92\u548c\u957f\u5c3e\u884c\u4e3a\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u673a\u5668\u4eba\u667a\u80fd\u4f53\u80fd\u529b\u5e76\u4fc3\u8fdb\u4efb\u52a1\u8bbe\u8ba1\u7684\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u96c6\u53ca\u4efb\u52a1\u8bbe\u8ba1\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u539f\u5219\u6027\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4e0d\u540c\u65b9\u6cd5\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u73b0\u6709\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4eba-\u7269\u4f53\u4ea4\u4e92\u57fa\u5143\u548c\u7269\u4f53\u53ef\u4f9b\u6027\u7406\u8bba\uff0c\u8bbe\u8ba1\u5e76\u6269\u5c55\u51fa100\u4e2a\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6536\u96c6\u5927\u91cf\u8f68\u8ff9\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1) GM-100\u4efb\u52a1\u5177\u6709\u53ef\u6267\u884c\u6027\uff1b2) \u4efb\u52a1\u8db3\u591f\u5177\u6709\u6311\u6218\u6027\uff0c\u80fd\u6709\u6548\u533a\u5206\u5f53\u524dVLA\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "GM-100\u4f5c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u5965\u6797\u5339\u514b\u7684\u7b2c\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u96c6\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u673a\u5668\u4eba\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u4fc3\u8fdb\u673a\u5668\u4eba\u6570\u636e\u96c6\u4efb\u52a1\u8bbe\u8ba1\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u53d1\u5c55\u3002"}}
