<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Uncertainty Quantification for Visual Object Pose Estimation](https://arxiv.org/abs/2511.21666)
*Lorenzo Shaikewitz,Charis Georgiou,Luca Carlone*

Main category: cs.RO

TL;DR: SLUE方法通过凸优化生成包含真实物体姿态的高概率椭球不确定性边界，仅需2D语义关键点检测的噪声边界，无需初始猜测或严格分布假设。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术中，量化物体姿态估计的不确定性对于鲁棒控制和规划至关重要，但现有方法通常需要严格的分布假设，缺乏统计上严谨的不确定性评估。

Method: 开发了SLUE（S-Lemma不确定性估计）凸优化程序，将非凸的位姿不确定性约束集简化为单个椭球不确定性边界，并扩展到平方和松弛层次以获得更紧密的边界。

Result: 在两个姿态估计数据集和真实无人机跟踪场景上的评估显示，SLUE生成显著更小的平移边界和具有竞争力的方向边界。

Conclusion: SLUE提供了一种分布自由的方法来量化姿态估计的不确定性，仅需2D关键点检测的噪声边界，能够生成统计上严谨且紧凑的不确定性边界。

Abstract: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.

</details>


### [2] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: TraceGen是一个从少量演示中学习机器人任务的世界模型，通过统一的3D轨迹空间表示实现跨平台、跨环境和跨任务的学习，仅需5个目标机器人视频即可达到80%成功率。


<details>
  <summary>Details</summary>
Motivation: 解决从少量演示学习新机器人任务的挑战，利用丰富的跨平台视频资源（人类和不同机器人），克服平台差异、相机差异和环境差异带来的障碍。

Method: 提出TraceGen世界模型，在3D轨迹空间而非像素空间预测未来运动，保留几何结构同时抽象外观差异；开发TraceForge数据管道将异构视频转换为一致的3D轨迹，构建包含123K视频和1.8M三元组的数据集。

Result: 在四个任务上仅需5个目标机器人视频即可达到80%成功率，推理速度比最先进的视频世界模型快50-600倍；在仅有5个手持手机拍摄的人类演示视频时，仍能在真实机器人上达到67.5%成功率。

Conclusion: TraceGen通过3D轨迹空间表示实现了高效的跨平台任务学习，无需依赖物体检测器或繁重的像素空间生成，在小样本学习场景下表现出色。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>
