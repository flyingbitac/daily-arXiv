{"id": "2601.16242", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16242", "abs": "https://arxiv.org/abs/2601.16242", "authors": ["S. Yaqubi", "J. Mattila"], "title": "Scalable Screw-Theoretic Synthesis for PDE-Based Dynamic Modeling of Multibody Flexible Manipulators", "comment": null, "summary": "This paper presents a novel and scalable screw-theoretic multibody synthesis framework for PDE-based dynamic modeling of serial robotic manipulators with an arbitrary number of flexible links in three-dimensional space. The proposed approach systematically constructs screw-theoretic PDE models for individual flexible links and rigorously enforces holonomic joint constraints through interaction forces. The dynamics of each link are formulated using a set of dual screws expressed in body-fixed coordinates: one describing the motion of the body-fixed frame relative to the inertial frame, a second relating the body-fixed frame to the undeformed configuration, and a third capturing elastic deformations. By expressing the system energy and applying variational principles, the governing dynamics of each link had been previously derived in a unified manner. Synthesizing the individual link models yields an infinitely scalable multibody representation capable of capturing both local (subsystem-level) and global (system-level) dynamics. The framework explicitly recovers all dynamic states, including the motion of each body-fixed frame and the distributed deformation fields of the flexible links. For computational tractability and mathematical rigor, the resulting governing equations are formulated as a semi-explicit index-1 differential-algebraic system. Furthermore, by applying separation of variables, the PDE model is recast as an abstract Cauchy problem, and well-posedness of the resulting system is established.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u7684\u3001\u53ef\u6269\u5c55\u7684\u591a\u4f53\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u4efb\u610f\u6570\u91cf\u67d4\u6027\u8fde\u6746\u4e32\u8054\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7684PDE\u52a8\u6001\u5efa\u6a21\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u5efa\u6a21\u5177\u6709\u67d4\u6027\u8fde\u6746\u7684\u4e32\u8054\u673a\u5668\u4eba\u673a\u68b0\u81c2\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u6570\u91cf\u7684\u67d4\u6027\u8fde\u6746\uff0c\u5e76\u540c\u65f6\u6355\u6349\u5c40\u90e8\uff08\u5b50\u7cfb\u7edf\u7ea7\uff09\u548c\u5168\u5c40\uff08\u7cfb\u7edf\u7ea7\uff09\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u87ba\u65cb\u7406\u8bba\u6784\u5efa\u5355\u4e2a\u67d4\u6027\u8fde\u6746\u7684PDE\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4e92\u4f5c\u7528\u529b\u4e25\u683c\u5f3a\u5236\u6267\u884c\u5b8c\u6574\u5173\u8282\u7ea6\u675f\u3002\u6bcf\u4e2a\u8fde\u6746\u7684\u52a8\u529b\u5b66\u4f7f\u7528\u56fa\u5b9a\u5728\u7269\u4f53\u5750\u6807\u7cfb\u4e2d\u7684\u4e00\u7ec4\u5bf9\u5076\u87ba\u65cb\u8868\u793a\uff1a\u4e00\u4e2a\u63cf\u8ff0\u56fa\u5b9a\u5728\u7269\u4f53\u5750\u6807\u7cfb\u76f8\u5bf9\u4e8e\u60ef\u6027\u5750\u6807\u7cfb\u7684\u8fd0\u52a8\uff0c\u7b2c\u4e8c\u4e2a\u5c06\u56fa\u5b9a\u5728\u7269\u4f53\u5750\u6807\u7cfb\u4e0e\u672a\u53d8\u5f62\u914d\u7f6e\u76f8\u5173\u8054\uff0c\u7b2c\u4e09\u4e2a\u6355\u6349\u5f39\u6027\u53d8\u5f62\u3002\u901a\u8fc7\u8868\u8fbe\u7cfb\u7edf\u80fd\u91cf\u548c\u5e94\u7528\u53d8\u5206\u539f\u7406\uff0c\u4ee5\u7edf\u4e00\u65b9\u5f0f\u63a8\u5bfc\u6bcf\u4e2a\u8fde\u6746\u7684\u652f\u914d\u52a8\u529b\u5b66\u3002\u5408\u6210\u5355\u4e2a\u8fde\u6746\u6a21\u578b\u5f97\u5230\u4e00\u4e2a\u65e0\u9650\u53ef\u6269\u5c55\u7684\u591a\u4f53\u8868\u793a\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u663e\u5f0f\u6062\u590d\u6240\u6709\u52a8\u6001\u72b6\u6001\uff0c\u5305\u62ec\u6bcf\u4e2a\u56fa\u5b9a\u5728\u7269\u4f53\u5750\u6807\u7cfb\u7684\u8fd0\u52a8\u548c\u67d4\u6027\u8fde\u6746\u7684\u5206\u5e03\u5f0f\u53d8\u5f62\u573a\u3002\u4e3a\u4e86\u8ba1\u7b97\u53ef\u5904\u7406\u6027\u548c\u6570\u5b66\u4e25\u8c28\u6027\uff0c\u5f97\u5230\u7684\u652f\u914d\u65b9\u7a0b\u88ab\u8868\u8ff0\u4e3a\u534a\u663e\u5f0f\u6307\u6570-1\u5fae\u5206\u4ee3\u6570\u7cfb\u7edf\u3002\u901a\u8fc7\u5e94\u7528\u53d8\u91cf\u5206\u79bb\uff0cPDE\u6a21\u578b\u88ab\u91cd\u65b0\u8868\u8ff0\u4e3a\u62bd\u8c61\u67ef\u897f\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u6240\u5f97\u7cfb\u7edf\u7684\u9002\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u53ef\u6269\u5c55\u7684\u87ba\u65cb\u7406\u8bba\u591a\u4f53\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u67d4\u6027\u8fde\u6746\u4e32\u8054\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7684PDE\u52a8\u6001\u5efa\u6a21\uff0c\u8be5\u6846\u67b6\u5177\u6709\u6570\u5b66\u4e25\u8c28\u6027\u3001\u8ba1\u7b97\u53ef\u5904\u7406\u6027\uff0c\u5e76\u80fd\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u52a8\u529b\u5b66\u7279\u6027\u3002"}}
{"id": "2601.16327", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16327", "abs": "https://arxiv.org/abs/2601.16327", "authors": ["Zubair Islam", "Mohamed El-Darieby"], "title": "DMV-AVP: Distributed Multi-Vehicle Autonomous Valet Parking using Autoware", "comment": "7 pages, 5 figures, 1 table. Demo videos and source code available", "summary": "This paper presents the DMV-AVP System, a distributed simulation of Multi-Vehicle Autonomous Valet Parking (AVP). The system was implemented as an application of the Distributed Multi-Vehicle Architecture (DMAVA) for synchronized multi-host execution. Most existing simulation approaches rely on centralized or non-distributed designs that constrain scalability and limit fully autonomous control. This work introduces two modules built on top of the DMAVA: 1) a Multi-Vehicle AVP Node that performs state-based coordination, queuing, and reservation management across multiple vehicles, and 2) a Unity-Integrated YOLOv5 Parking Spot Detection Module that provides real-time, vision-based perception within AWSIM Labs. Both modules integrate seamlessly with the DMAVA and extend it specifically for multi-vehicle AVP operation, supported by a Zenoh-based communication layer that ensures low-latency topic synchronization and coordinated behavior across hosts. Experiments conducted on two- and three-host configurations demonstrate deterministic coordination, conflict-free parking behavior, and scalable performance across distributed Autoware instances. The results confirm that the proposed Distributed Multi-Vehicle AVP System supports cooperative AVP simulation and establishes a foundation for future real-world and hardware-in-the-loop validation. Demo videos and source code are available at https://github.com/zubxxr/multi-vehicle-avp", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DMV-AVP\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u591a\u8f66\u81ea\u4e3b\u4ee3\u5ba2\u6cca\u8f66\u4eff\u771f\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5206\u5e03\u5f0f\u591a\u8f66\u67b6\u6784\u5b9e\u73b0\u540c\u6b65\u591a\u4e3b\u673a\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u96c6\u4e2d\u5f0f\u4eff\u771f\u5728\u53ef\u6269\u5c55\u6027\u548c\u81ea\u4e3b\u63a7\u5236\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u96c6\u4e2d\u5f0f\u6216\u975e\u5206\u5e03\u5f0f\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b8c\u5168\u81ea\u4e3b\u63a7\u5236\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u591a\u8f66\u534f\u540c\u6cca\u8f66\u4eff\u771f\u3002", "method": "\u5728DMAVA\u67b6\u6784\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u4e24\u4e2a\u6a21\u5757\uff1a1) \u591a\u8f66AVP\u8282\u70b9\uff0c\u8d1f\u8d23\u72b6\u6001\u534f\u8c03\u3001\u6392\u961f\u548c\u9884\u7ea6\u7ba1\u7406\uff1b2) Unity\u96c6\u6210\u7684YOLOv5\u6cca\u8f66\u4f4d\u68c0\u6d4b\u6a21\u5757\uff0c\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u611f\u77e5\u3002\u7cfb\u7edf\u91c7\u7528Zenoh\u901a\u4fe1\u5c42\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u4e3b\u9898\u540c\u6b65\u548c\u8de8\u4e3b\u673a\u534f\u8c03\u3002", "result": "\u5728\u53cc\u4e3b\u673a\u548c\u4e09\u4e3b\u673a\u914d\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u534f\u8c03\u3001\u65e0\u51b2\u7a81\u6cca\u8f66\u884c\u4e3a\uff0c\u5e76\u5728\u5206\u5e03\u5f0fAutoware\u5b9e\u4f8b\u95f4\u5c55\u73b0\u51fa\u53ef\u6269\u5c55\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u591a\u8f66AVP\u7cfb\u7edf\u652f\u6301\u534f\u540cAVP\u4eff\u771f\uff0c\u4e3a\u672a\u6765\u771f\u5b9e\u4e16\u754c\u548c\u786c\u4ef6\u5728\u73af\u9a8c\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5206\u5e03\u5f0f\u67b6\u6784\u5728\u591a\u8f66\u81ea\u4e3b\u6cca\u8f66\u4eff\u771f\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.16336", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16336", "abs": "https://arxiv.org/abs/2601.16336", "authors": ["Zubair Islam", "Mohamed El-Darieby"], "title": "DMAVA: Distributed Multi-Autonomous Vehicle Architecture Using Autoware", "comment": "9 pages, 4 figures, 5 tables, Submitted to IEEE IV 2026, Demo videos and source code available", "summary": "Simulating and validating coordination among multiple autonomous vehicles (AVs) is a challenging task as most existing simulation architectures are limited to single-vehicle operation or rely on centralized control. This paper presents a Distributed Multi-AV Architecture (DMAVA) that enables synchronized, real-time autonomous driving simulation across multiple physical hosts. Each vehicle runs its own complete AV stack and operates independently from other AVs. The vehicles in the simulation maintain synchronized coordination through a low-latency data-centric communication layer. The proposed system integrates ROS 2 Humble, Autoware Universe, AWSIM Labs, and Zenoh to support concurrent execution of multiple Autoware stacks within a shared Unity-based environment. Experiments conducted on multiple-host configurations demonstrate stable localization, reliable inter-host communication, and fully synchronized closed-loop control. The DMAVA also serves as a foundation for Multi-Vehicle Autonomous Valet Parking, demonstrating its extensibility toward higher-level cooperative autonomy. Demo videos and source code are available at: https://github.com/zubxxr/distributed-multi-autonomous-vehicle-architecture.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u591a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u67b6\u6784(DMAVA)\uff0c\u652f\u6301\u591a\u7269\u7406\u4e3b\u673a\u95f4\u7684\u5b9e\u65f6\u540c\u6b65\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\uff0c\u6bcf\u8f86\u8f66\u72ec\u7acb\u8fd0\u884c\u5b8c\u6574AV\u6808\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u5c42\u5b9e\u73b0\u534f\u8c03", "motivation": "\u73b0\u6709\u4eff\u771f\u67b6\u6784\u5927\u591a\u5c40\u9650\u4e8e\u5355\u8f66\u8f86\u64cd\u4f5c\u6216\u4f9d\u8d56\u96c6\u4e2d\u63a7\u5236\uff0c\u96be\u4ee5\u6709\u6548\u6a21\u62df\u548c\u9a8c\u8bc1\u591a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u95f4\u7684\u534f\u8c03\uff0c\u9700\u8981\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u5206\u5e03\u5f0f\u591a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u67b6\u6784(DMAVA)\uff0c\u96c6\u6210ROS 2 Humble\u3001Autoware Universe\u3001AWSIM Labs\u548cZenoh\uff0c\u6bcf\u8f86\u8f66\u72ec\u7acb\u8fd0\u884c\u5b8c\u6574Autoware\u6808\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u6570\u636e\u901a\u4fe1\u5c42\u5b9e\u73b0\u591a\u4e3b\u673a\u95f4\u540c\u6b65", "result": "\u5728\u591a\u4e3b\u673a\u914d\u7f6e\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u5b9a\u4f4d\u3001\u53ef\u9760\u7684\u4e3b\u673a\u95f4\u901a\u4fe1\u548c\u5b8c\u5168\u540c\u6b65\u7684\u95ed\u73af\u63a7\u5236\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u591a\u8f66\u8f86\u81ea\u4e3b\u4ee3\u5ba2\u6cca\u8f66\u573a\u666f", "conclusion": "DMAVA\u4e3a\u591a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4eff\u771f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u65f6\u540c\u6b65\u64cd\u4f5c\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u66f4\u9ad8\u7ea7\u522b\u7684\u534f\u540c\u81ea\u4e3b\u6027\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2601.16393", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16393", "abs": "https://arxiv.org/abs/2601.16393", "authors": ["Keidai Iiyama", "Grace Gao"], "title": "GNSS-based Lunar Orbit and Clock Estimation With Stochastic Cloning UD Filter", "comment": "Submitted to the Journal of Guidance, Control, and Dynamics", "summary": "This paper presents a terrestrial GNSS-based orbit and clock estimation framework for lunar navigation satellites. To enable high-precision estimation under the low-observability conditions encountered at lunar distances, we develop a stochastic-cloning UD-factorized filter and delayed-state smoother that provide enhanced numerical stability when processing precise time-differenced carrier phase (TDCP) measurements. A comprehensive dynamics and measurement model is formulated, explicitly accounting for relativistic coupling between orbital and clock states, lunar time-scale transformations, and signal propagation delays including ionospheric, plasmaspheric, and Shapiro effects. The proposed approach is evaluated using high-fidelity Monte-Carlo simulations incorporating realistic multi-constellation GNSS geometry, broadcast ephemeris errors, lunar satellite dynamics, and ionospheric and plasmaspheric delay computed from empirical electron density models. Simulation results demonstrate that combining ionosphere-free pseudorange and TDCP measurements achieves meter-level orbit accuracy and sub-millimeter-per-second velocity accuracy, satisfying the stringent signal-in-space error requirements of future Lunar Augmented Navigation Services (LANS).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5730\u9762GNSS\u7684\u6708\u7403\u5bfc\u822a\u536b\u661f\u8f68\u9053\u4e0e\u949f\u5dee\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u514b\u9686UD\u5206\u89e3\u6ee4\u6ce2\u5668\u548c\u5ef6\u8fdf\u72b6\u6001\u5e73\u6ed1\u5668\u5904\u7406TDCP\u6d4b\u91cf\uff0c\u5b9e\u73b0\u7c73\u7ea7\u8f68\u9053\u7cbe\u5ea6\u548c\u4e9a\u6beb\u7c73/\u79d2\u901f\u5ea6\u7cbe\u5ea6", "motivation": "\u4e3a\u6ee1\u8db3\u672a\u6765\u6708\u7403\u589e\u5f3a\u5bfc\u822a\u670d\u52a1\uff08LANS\uff09\u7684\u4e25\u683c\u4fe1\u53f7\u7a7a\u95f4\u8bef\u5dee\u8981\u6c42\uff0c\u9700\u8981\u5728\u6708\u7403\u8ddd\u79bb\u4f4e\u53ef\u89c2\u6d4b\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8f68\u9053\u548c\u949f\u5dee\u4f30\u8ba1", "method": "\u5f00\u53d1\u968f\u673a\u514b\u9686UD\u5206\u89e3\u6ee4\u6ce2\u5668\u548c\u5ef6\u8fdf\u72b6\u6001\u5e73\u6ed1\u5668\uff0c\u5904\u7406\u65f6\u95f4\u5dee\u5206\u8f7d\u6ce2\u76f8\u4f4d\u6d4b\u91cf\uff1b\u5efa\u7acb\u7efc\u5408\u52a8\u529b\u5b66\u548c\u6d4b\u91cf\u6a21\u578b\uff0c\u8003\u8651\u76f8\u5bf9\u8bba\u8f68\u9053-\u949f\u5dee\u8026\u5408\u3001\u6708\u7403\u65f6\u6807\u53d8\u6362\u3001\u7535\u79bb\u5c42/\u7b49\u79bb\u5b50\u5c42/\u590f\u76ae\u7f57\u6548\u5e94\u7b49\u4f20\u64ad\u5ef6\u8fdf", "result": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\uff0c\u7ed3\u5408\u65e0\u7535\u79bb\u5c42\u4f2a\u8ddd\u548cTDCP\u6d4b\u91cf\u53ef\u5b9e\u73b0\u7c73\u7ea7\u8f68\u9053\u7cbe\u5ea6\u548c\u4e9a\u6beb\u7c73/\u79d2\u901f\u5ea6\u7cbe\u5ea6\uff0c\u6ee1\u8db3LANS\u8981\u6c42", "conclusion": "\u63d0\u51fa\u7684GNSS\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301\u6708\u7403\u5bfc\u822a\u536b\u661f\u7684\u9ad8\u7cbe\u5ea6\u8f68\u9053\u548c\u949f\u5dee\u4f30\u8ba1\uff0c\u4e3a\u672a\u6765\u6708\u7403\u589e\u5f3a\u5bfc\u822a\u670d\u52a1\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.16405", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16405", "abs": "https://arxiv.org/abs/2601.16405", "authors": ["Beining Wu", "Zihao Ding", "Leo Ostigaard", "Jun Huang"], "title": "Reinforcement Learning-Based Energy-Aware Coverage Path Planning for Precision Agriculture", "comment": "Accepted by RACS '25: International Conference on Research in Adaptive and Convergent Systems, November 16-19, 2025, Ho Chi Minh, Vietnam. 10 pages, 5 figures", "summary": "Coverage Path Planning (CPP) is a fundamental capability for agricultural robots; however, existing solutions often overlook energy constraints, resulting in incomplete operations in large-scale or resource-limited environments. This paper proposes an energy-aware CPP framework grounded in Soft Actor-Critic (SAC) reinforcement learning, designed for grid-based environments with obstacles and charging stations. To enable robust and adaptive decision-making under energy limitations, the framework integrates Convolutional Neural Networks (CNNs) for spatial feature extraction and Long Short-Term Memory (LSTM) networks for temporal dynamics. A dedicated reward function is designed to jointly optimize coverage efficiency, energy consumption, and return-to-base constraints. Experimental results demonstrate that the proposed approach consistently achieves over 90% coverage while ensuring energy safety, outperforming traditional heuristic algorithms such as Rapidly-exploring Random Tree (RRT), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO) baselines by 13.4-19.5% in coverage and reducing constraint violations by 59.9-88.3%. These findings validate the proposed SAC-based framework as an effective and scalable solution for energy-constrained CPP in agricultural robotics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSAC\u5f3a\u5316\u5b66\u4e60\u7684\u80fd\u91cf\u611f\u77e5\u8986\u76d6\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u4e1a\u673a\u5668\u4eba\uff0c\u5728\u80fd\u91cf\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8986\u76d6\u3002", "motivation": "\u73b0\u6709\u8986\u76d6\u8def\u5f84\u89c4\u5212\u65b9\u6848\u5e38\u5ffd\u7565\u80fd\u91cf\u7ea6\u675f\uff0c\u5bfc\u81f4\u5728\u5927\u89c4\u6a21\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u64cd\u4f5c\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u80fd\u91cf\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eSoft Actor-Critic\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0cLSTM\u5904\u7406\u65f6\u5e8f\u52a8\u6001\uff0c\u8bbe\u8ba1\u4e13\u95e8\u5956\u52b1\u51fd\u6570\u8054\u5408\u4f18\u5316\u8986\u76d6\u6548\u7387\u3001\u80fd\u8017\u548c\u8fd4\u56de\u57fa\u7ad9\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u6301\u7eed\u5b9e\u73b0\u8d85\u8fc790%\u8986\u76d6\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u80fd\u91cf\u5b89\u5168\uff0c\u6bd4RRT\u3001PSO\u3001ACO\u7b49\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u8986\u76d6\u7387\u4e0a\u63d0\u534713.4-19.5%\uff0c\u7ea6\u675f\u8fdd\u53cd\u51cf\u5c1159.9-88.3%\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8eSAC\u7684\u6846\u67b6\u662f\u519c\u4e1a\u673a\u5668\u4eba\u80fd\u91cf\u7ea6\u675f\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.16424", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16424", "abs": "https://arxiv.org/abs/2601.16424", "authors": ["Mingi Jeong", "Alberto Quattrini Li"], "title": "RENEW: Risk- and Energy-Aware Navigation in Dynamic Waterways", "comment": "9 pages, 10 figure, 4 tables, AAAI 2026 (main track; oral acceptance)", "summary": "We present RENEW, a global path planner for Autonomous Surface Vehicle (ASV) in dynamic environments with external disturbances (e.g., water currents). RENEW introduces a unified risk- and energy-aware strategy that ensures safety by dynamically identifying non-navigable regions and enforcing adaptive safety constraints. Inspired by maritime contingency planning, it employs a best-effort strategy to maintain control under adverse conditions. The hierarchical architecture combines high-level constrained triangulation for topological diversity with low-level trajectory optimization within safe corridors. Validated with real-world ocean data, RENEW is the first framework to jointly address adaptive non-navigability and topological path diversity for robust maritime navigation.", "AI": {"tldr": "RENEW\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u5668\uff0c\u5728\u52a8\u6001\u73af\u5883\u548c\u5916\u90e8\u5e72\u6270\u4e0b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u98ce\u9669\u548c\u80fd\u91cf\u611f\u77e5\u7b56\u7565\u786e\u4fdd\u5b89\u5168\uff0c\u91c7\u7528\u5206\u5c42\u67b6\u6784\u7ed3\u5408\u9ad8\u5c42\u7ea6\u675f\u4e09\u89d2\u5256\u5206\u548c\u4f4e\u5c42\u8f68\u8ff9\u4f18\u5316\u3002", "motivation": "\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\u5728\u52a8\u6001\u6d77\u6d0b\u73af\u5883\u4e2d\u9762\u4e34\u5916\u90e8\u5e72\u6270\uff08\u5982\u6c34\u6d41\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u786e\u4fdd\u5b89\u5168\u3001\u9002\u5e94\u975e\u53ef\u822a\u884c\u533a\u57df\u5e76\u4fdd\u6301\u62d3\u6251\u8def\u5f84\u591a\u6837\u6027\u7684\u9c81\u68d2\u5bfc\u822a\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u4f7f\u7528\u7ea6\u675f\u4e09\u89d2\u5256\u5206\u786e\u4fdd\u62d3\u6251\u591a\u6837\u6027\uff0c\u4f4e\u5c42\u5728\u5b89\u5168\u8d70\u5eca\u5185\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\uff1b\u5f15\u5165\u7edf\u4e00\u7684\u98ce\u9669\u548c\u80fd\u91cf\u611f\u77e5\u7b56\u7565\uff0c\u52a8\u6001\u8bc6\u522b\u975e\u53ef\u822a\u884c\u533a\u57df\u5e76\u5b9e\u65bd\u81ea\u9002\u5e94\u5b89\u5168\u7ea6\u675f\uff1b\u53d7\u6d77\u4e8b\u5e94\u6025\u89c4\u5212\u542f\u53d1\uff0c\u91c7\u7528\u5c3d\u529b\u800c\u4e3a\u7b56\u7565\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u4fdd\u6301\u63a7\u5236\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u6d77\u6d0b\u6570\u636e\u9a8c\u8bc1\uff0cRENEW\u662f\u9996\u4e2a\u540c\u65f6\u89e3\u51b3\u81ea\u9002\u5e94\u975e\u53ef\u822a\u884c\u6027\u548c\u62d3\u6251\u8def\u5f84\u591a\u6837\u6027\u7684\u6846\u67b6\uff0c\u4e3a\u9c81\u68d2\u6d77\u4e8b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "RENEW\u4e3a\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\u5728\u52a8\u6001\u73af\u5883\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u98ce\u9669\u548c\u80fd\u91cf\u611f\u77e5\u7b56\u7565\u786e\u4fdd\u4e86\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.16578", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16578", "abs": "https://arxiv.org/abs/2601.16578", "authors": ["Julius Beerwerth", "Jianye Xu", "Simon Sch\u00e4fer", "Fynn Belderink", "Bassam Alrifaee"], "title": "Zero-Shot MARL Benchmark in the Cyber-Physical Mobility Lab", "comment": null, "summary": "We present a reproducible benchmark for evaluating sim-to-real transfer of Multi-Agent Reinforcement Learning (MARL) policies for Connected and Automated Vehicles (CAVs). The platform, based on the Cyber-Physical Mobility Lab (CPM Lab) [1], integrates simulation, a high-fidelity digital twin, and a physical testbed, enabling structured zero-shot evaluation of MARL motion-planning policies. We demonstrate its use by deploying a SigmaRL-trained policy [2] across all three domains, revealing two complementary sources of performance degradation: architectural differences between simulation and hardware control stacks, and the sim-to-real gap induced by increasing environmental realism. The open-source setup enables systematic analysis of sim-to-real challenges in MARL under realistic, reproducible conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u53ef\u590d\u73b0\u57fa\u51c6\u5e73\u53f0\uff0c\u57fa\u4e8eCyber-Physical Mobility Lab\uff0c\u96c6\u6210\u4e86\u4eff\u771f\u3001\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u548c\u7269\u7406\u6d4b\u8bd5\u5e8a\u3002", "motivation": "\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u7f51\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8eCyber-Physical Mobility Lab\u6784\u5efa\u4e86\u4e00\u4e2a\u96c6\u6210\u5e73\u53f0\uff0c\u5305\u542b\u4eff\u771f\u73af\u5883\u3001\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u548c\u7269\u7406\u6d4b\u8bd5\u5e8a\uff0c\u652f\u6301\u96f6\u6837\u672c\u8bc4\u4f30MARL\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u5e76\u4f7f\u7528SigmaRL\u8bad\u7ec3\u7684\u7b56\u7565\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u90e8\u7f72SigmaRL\u8bad\u7ec3\u7684\u7b56\u7565\u63ed\u793a\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u6027\u80fd\u4e0b\u964d\u6765\u6e90\uff1a\u4eff\u771f\u4e0e\u786c\u4ef6\u63a7\u5236\u67b6\u6784\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u73af\u5883\u771f\u5b9e\u6027\u589e\u52a0\u5bfc\u81f4\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u5f00\u6e90\u5e73\u53f0\u80fd\u591f\u5728\u771f\u5b9e\u3001\u53ef\u590d\u73b0\u7684\u6761\u4ef6\u4e0b\u7cfb\u7edf\u5206\u6790MARL\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u6311\u6218\uff0c\u4e3a\u7f51\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.16638", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16638", "abs": "https://arxiv.org/abs/2601.16638", "authors": ["Philip Tobuschat", "Simon Duenser", "Markus Bambach", "Ivo Aschwanden"], "title": "A Unified Calibration Framework for High-Accuracy Articulated Robot Kinematics", "comment": null, "summary": "Researchers have identified various sources of tool positioning errors for articulated industrial robots and have proposed dedicated compensation strategies. However, these typically require individual, specialized experiments with separate models and identification procedures. This article presents a unified approach to the static calibration of industrial robots that identifies a robot model, including geometric and non-geometric effects (compliant bending, thermal deformation, gear transmission errors), using only a single, straightforward experiment for data collection. The model augments the kinematic chain with virtual joints for each modeled effect and realizes the identification using Gauss-Newton optimization with analytic gradients. Fisher information spectra show that the estimation is well-conditioned and the parameterization near-minimal, whereas systematic temporal cross-validation and model ablations demonstrate robustness of the model identification. The resulting model is very accurate and its identification robust, achieving a mean position error of 26.8 $\u03bcm$ on a KUKA KR30 industrial robot compared to 102.3 $\u03bcm$ for purely geometric calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5de5\u4e1a\u673a\u5668\u4eba\u9759\u6001\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u7b80\u5355\u5b9e\u9a8c\u540c\u65f6\u8bc6\u522b\u51e0\u4f55\u548c\u975e\u51e0\u4f55\u8bef\u5dee\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u673a\u5668\u4eba\u5de5\u5177\u5b9a\u4f4d\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u8bef\u5dee\u6e90\u8fdb\u884c\u4e13\u95e8\u7684\u5b9e\u9a8c\u548c\u72ec\u7acb\u7684\u6a21\u578b\u8bc6\u522b\uff0c\u8fc7\u7a0b\u590d\u6742\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u5173\u8282\u6269\u5c55\u8fd0\u52a8\u5b66\u94fe\u6765\u5efa\u6a21\u51e0\u4f55\u548c\u975e\u51e0\u4f55\u6548\u5e94\uff08\u67d4\u6027\u5f2f\u66f2\u3001\u70ed\u53d8\u5f62\u3001\u9f7f\u8f6e\u4f20\u52a8\u8bef\u5dee\uff09\uff0c\u4f7f\u7528\u5177\u6709\u89e3\u6790\u68af\u5ea6\u7684Gauss-Newton\u4f18\u5316\u8fdb\u884c\u53c2\u6570\u8bc6\u522b\u3002", "result": "\u5728KUKA KR30\u5de5\u4e1a\u673a\u5668\u4eba\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u4ece\u7eaf\u51e0\u4f55\u6807\u5b9a\u7684102.3\u03bcm\u964d\u4f4e\u523026.8\u03bcm\uff0c\u6a21\u578b\u8bc6\u522b\u9c81\u68d2\u4e14\u53c2\u6570\u5316\u63a5\u8fd1\u6700\u5c0f\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6807\u5b9a\u65b9\u6cd5\u901a\u8fc7\u5355\u6b21\u5b9e\u9a8c\u5c31\u80fd\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u8bef\u5dee\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4e1a\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.16667", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16667", "abs": "https://arxiv.org/abs/2601.16667", "authors": ["Zhuohao Li", "Yinghao Li", "Jian-Jian Jiang", "Lang Zhou", "Tianyu Zhang", "Wei-Shi Zheng"], "title": "ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance", "comment": null, "summary": "Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations.", "AI": {"tldr": "ReViP\u901a\u8fc7\u89c6\u89c9-\u672c\u4f53\u611f\u77e5\u518d\u5e73\u8861\u673a\u5236\u89e3\u51b3VLA\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u51cf\u5c11\u72b6\u6001\u4e3b\u5bfc\u504f\u5dee\u548c\u865a\u5047\u5b8c\u6210\u9519\u8bef\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u878d\u5408\u672c\u4f53\u611f\u77e5\u4fe1\u53f7\u65f6\u5b58\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u5ea6\u4f9d\u8d56\u5185\u90e8\u72b6\u6001\u800c\u5ffd\u89c6\u89c6\u89c9\u8bc1\u636e\uff0c\u4ea7\u751f\u72b6\u6001\u4e3b\u5bfc\u504f\u5dee\u548c\u865a\u5047\u5b8c\u6210\u9519\u8bef\uff0c\u5373\u4f7f\u6267\u884c\u5931\u8d25\u4e5f\u9519\u8bef\u9884\u6d4b\u5b8c\u6210\u3002", "method": "\u63d0\u51faReViP\u6846\u67b6\uff0c\u5f15\u5165\u4efb\u52a1\u611f\u77e5\u73af\u5883\u5148\u9a8c\u6765\u81ea\u9002\u5e94\u8c03\u8282\u8bed\u4e49\u611f\u77e5\u4e0e\u672c\u4f53\u611f\u77e5\u52a8\u6001\u7684\u8026\u5408\u3002\u4f7f\u7528\u5916\u90e8VLM\u4f5c\u4e3a\u4efb\u52a1\u9636\u6bb5\u89c2\u5bdf\u5668\u63d0\u53d6\u5b9e\u65f6\u4efb\u52a1\u4e2d\u5fc3\u89c6\u89c9\u7ebf\u7d22\uff0c\u9a71\u52a8\u89c6\u89c9-\u672c\u4f53\u611f\u77e5\u7279\u5f81\u7ea7\u7ebf\u6027\u8c03\u5236\uff0c\u589e\u5f3a\u73af\u5883\u611f\u77e5\u5e76\u51cf\u5c11\u72b6\u6001\u9a71\u52a8\u9519\u8bef\u3002", "result": "\u5728\u57fa\u4e8eLIBERO\u6784\u5efa\u7684\u9996\u4e2a\u865a\u5047\u5b8c\u6210\u57fa\u51c6\u5957\u4ef6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cReViP\u6709\u6548\u964d\u4f4e\u4e86\u865a\u5047\u5b8c\u6210\u7387\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5728LIBERO\u3001RoboTwin 2.0\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709VLA\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReViP\u901a\u8fc7\u89c6\u89c9-\u672c\u4f53\u611f\u77e5\u518d\u5e73\u8861\u673a\u5236\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u865a\u5047\u5b8c\u6210\u9519\u8bef\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u3002"}}
{"id": "2601.16677", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16677", "abs": "https://arxiv.org/abs/2601.16677", "authors": ["Luc\u00eda G\u00fcitta-L\u00f3pez", "Lionel G\u00fcitta-L\u00f3pez", "Jaime Boal", "\u00c1lvaro Jes\u00fas L\u00f3pez-L\u00f3pez"], "title": "Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation", "comment": null, "summary": "The sample efficiency challenge in Deep Reinforcement Learning (DRL) compromises its industrial adoption due to the high cost and time demands of real-world training. Virtual environments offer a cost-effective alternative for training DRL agents, but the transfer of learned policies to real setups is hindered by the sim-to-real gap. Achieving zero-shot transfer, where agents perform directly in real environments without additional tuning, is particularly desirable for its efficiency and practical value. This work proposes a novel domain adaptation approach relying on a Style-Identified Cycle Consistent Generative Adversarial Network (StyleID-CycleGAN or SICGAN), an original Cycle Consistent Generative Adversarial Network (CycleGAN) based model. SICGAN translates raw virtual observations into real-synthetic images, creating a hybrid domain for training DRL agents that combines virtual dynamics with real-like visual inputs. Following virtual training, the agent can be directly deployed, bypassing the need for real-world training. The pipeline is validated with two distinct industrial robots in the approaching phase of a pick-and-place operation. In virtual environments agents achieve success rates of 90 to 100\\%, and real-world deployment confirms robust zero-shot transfer (i.e., without additional training in the physical environment) with accuracies above 95\\% for most workspace regions. We use augmented reality targets to improve the evaluation process efficiency, and experimentally demonstrate that the agent successfully generalizes to real objects of varying colors and shapes, including LEGO\\textsuperscript{\\textregistered}~cubes and a mug. These results establish the proposed pipeline as an efficient, scalable solution to the sim-to-real problem.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eStyleID-CycleGAN\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06\u865a\u62df\u89c2\u6d4b\u8f6c\u6362\u4e3a\u771f\u5b9e\u98ce\u683c\u56fe\u50cf\uff0c\u5b9e\u73b0DRL\u7b56\u7565\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u62fe\u653e\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\uff0c\u865a\u62df\u73af\u5883\u8bad\u7ec3\u6210\u672c\u4f4e\u4f46\u5b58\u5728\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u4ee5\u63d0\u9ad8\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51faStyleID-CycleGAN\u6a21\u578b\uff0c\u57fa\u4e8eCycleGAN\u6846\u67b6\uff0c\u5c06\u539f\u59cb\u865a\u62df\u89c2\u6d4b\u8f6c\u6362\u4e3a\u771f\u5b9e\u98ce\u683c\u5408\u6210\u56fe\u50cf\uff0c\u521b\u5efa\u6df7\u5408\u57df\u8bad\u7ec3DRL\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u865a\u62df\u52a8\u529b\u5b66\u548c\u771f\u5b9e\u89c6\u89c9\u8f93\u5165\u3002", "result": "\u865a\u62df\u73af\u5883\u4e2d\u667a\u80fd\u4f53\u6210\u529f\u738790-100%\uff0c\u771f\u5b9e\u90e8\u7f72\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5927\u591a\u6570\u5de5\u4f5c\u533a\u57df\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u989c\u8272\u548c\u5f62\u72b6\u7684\u771f\u5b9e\u7269\u4f53\uff08\u5982\u4e50\u9ad8\u79ef\u6728\u548c\u676f\u5b50\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ba1\u9053\u662f\u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u95ee\u9898\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\u76ee\u6807\u6539\u8fdb\u8bc4\u4f30\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.16686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16686", "abs": "https://arxiv.org/abs/2601.16686", "authors": ["Ning Liu", "Sen Shen", "Zheng Li", "Matthew D'Souza", "Jen Jen Chung", "Thomas Braunl"], "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation", "comment": null, "summary": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at https://github.com/21ning/ARMS.git.", "AI": {"tldr": "ARMS\u662f\u4e00\u4e2a\u6df7\u5408\u5b66\u4e60\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u7528\u4e8e\u79fb\u52a8\u534f\u4f5c\u673a\u5668\u4eba\u7684\u4eba\u5bfc\u5bfc\u822a\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u7ea6\u675f\u7684\u540c\u65f6\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u52a8\u6027\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u534f\u4f5c\u673a\u5668\u4eba\u5728\u540c\u65f6\u6ee1\u8db3\u63a5\u8fd1\u5ea6\u8c03\u8282\u548c\u5b89\u5168\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u4eba\u5bfc\u5bfc\u822a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u548c\u975e\u5e73\u7a33\u4eba\u7c7b\u8fd0\u52a8\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u611f\u77e5\u548c\u63a7\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faARMS\u6df7\u5408\u6846\u67b6\uff1a1) \u4f7f\u7528PPO\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u8ddf\u968f\u5668\uff1b2) \u4f5c\u4e3a\u5b89\u5168\u6ee4\u6ce2\u5668\u7684\u5355\u6b65\u6a21\u578b\u9884\u6d4b\u63a7\u5236QP\uff1b3) \u89e3\u8026\u611f\u77e5\u67b6\u6784\uff08LSTM\u65f6\u95f4\u7f16\u7801\u5668\u5904\u7406\u4eba\u673a\u76f8\u5bf9\u72b6\u6001\uff0c\u7a7a\u95f4\u7f16\u7801\u5668\u5904\u7406360\u5ea6LiDAR\u626b\u63cf\uff09\uff1b4) \u81ea\u9002\u5e94\u795e\u7ecf\u5207\u6362\u5668\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8f6f\u52a8\u4f5c\u878d\u5408\u3002", "result": "\u5728\u9ad8\u5ea6\u6742\u4e71\u73af\u5883\u4e2d\u8fbe\u523082.5%\u7684\u6210\u529f\u7387\uff0c\u6bd4DWA\u548c\u7eafRL\u65b9\u6cd5\u5206\u522b\u63d0\u9ad87.1%\u548c3.1%\uff1b\u8ba1\u7b97\u5ef6\u8fdf\u964d\u4f4e33%\u81f35.2\u6beb\u79d2\uff1bGazebo\u4eff\u771f\u548c\u521d\u6b65\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ARMS\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u6df7\u5408\u63a7\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u7ea6\u675f\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u79fb\u52a8\u534f\u4f5c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.16691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16691", "abs": "https://arxiv.org/abs/2601.16691", "authors": ["Siyuan Sun", "Eugene H. Lin", "Nathan Brown", "Hsin-Yi Hung", "Andrew Gordus", "Jochen Mueller", "Chen Li"], "title": "Creating a biologically more accurate spider robot to study active vibration sensing", "comment": "8 pages, 12 figures", "summary": "Orb-weaving spiders detect prey on a web using vibration sensors at leg joints. They often dynamically crouch their legs during prey sensing, likely an active sensing strategy. However, how leg crouching enhances sensing is poorly understood, because measuring system vibrations in behaving animals is difficult. We use robophysical modeling to study this problem. Our previous spider robot had only four legs, simplified leg morphology, and a shallow crouching range of motion. Here, we developed a new spider robot, with eight legs, each with four joints that better approximated spider leg morphology. Leg exoskeletons were 3-D printed and joint stiffness was tuned using integrated silicone molding with variable materials and geometry. Tendon-driven actuation allowed a motor in the body to crouch all eight legs deeply as spiders do, while accelerometers at leg joints record leg vibrations. Experiments showed that our new spider robot reproduced key vibration features observed in the previous robot while improving biological accuracy. Our new robot provides a biologically more accurate robophysical model for studying how leg behaviors modulate vibration sensing on a web.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u516b\u817f\u8718\u86db\u673a\u5668\u4eba\uff0c\u5177\u6709\u66f4\u63a5\u8fd1\u771f\u5b9e\u8718\u86db\u7684\u817f\u90e8\u5f62\u6001\u548c\u66f4\u6df1\u7684\u8e72\u4f0f\u8fd0\u52a8\u8303\u56f4\uff0c\u7528\u4e8e\u7814\u7a76\u8718\u86db\u5982\u4f55\u901a\u8fc7\u817f\u90e8\u8e72\u4f0f\u884c\u4e3a\u589e\u5f3a\u632f\u52a8\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5706\u7f51\u8718\u86db\u901a\u8fc7\u817f\u90e8\u5173\u8282\u7684\u632f\u52a8\u4f20\u611f\u5668\u68c0\u6d4b\u730e\u7269\uff0c\u5b83\u4eec\u7ecf\u5e38\u5728\u611f\u77e5\u730e\u7269\u65f6\u52a8\u6001\u8e72\u4f0f\u817f\u90e8\uff0c\u8fd9\u5f88\u53ef\u80fd\u662f\u4e00\u79cd\u4e3b\u52a8\u611f\u77e5\u7b56\u7565\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5728\u884c\u4e3a\u52a8\u7269\u4e2d\u6d4b\u91cf\u7cfb\u7edf\u632f\u52a8\u5f88\u56f0\u96be\uff0c\u817f\u90e8\u8e72\u4f0f\u5982\u4f55\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u91c7\u7528\u673a\u5668\u4eba\u7269\u7406\u5efa\u6a21\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u516b\u817f\u8718\u86db\u673a\u5668\u4eba\uff0c\u6bcf\u6761\u817f\u6709\u56db\u4e2a\u5173\u8282\uff0c\u66f4\u63a5\u8fd1\u8718\u86db\u817f\u90e8\u5f62\u6001\u3002\u817f\u90e8\u5916\u9aa8\u9abc\u91c7\u75283D\u6253\u5370\uff0c\u5173\u8282\u521a\u5ea6\u901a\u8fc7\u96c6\u6210\u7845\u80f6\u6a21\u5851\u548c\u53ef\u53d8\u6750\u6599\u51e0\u4f55\u5f62\u72b6\u8fdb\u884c\u8c03\u8282\u3002\u808c\u8171\u9a71\u52a8\u6267\u884c\u5668\u4f7f\u8eab\u4f53\u5185\u7684\u7535\u673a\u80fd\u591f\u50cf\u8718\u86db\u4e00\u6837\u6df1\u5ea6\u8e72\u4f0f\u6240\u6709\u516b\u6761\u817f\uff0c\u540c\u65f6\u817f\u90e8\u5173\u8282\u7684\u52a0\u901f\u5ea6\u8ba1\u8bb0\u5f55\u817f\u90e8\u632f\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u7684\u8718\u86db\u673a\u5668\u4eba\u518d\u73b0\u4e86\u5148\u524d\u673a\u5668\u4eba\u89c2\u5bdf\u5230\u7684\u5173\u952e\u632f\u52a8\u7279\u5f81\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u751f\u7269\u51c6\u786e\u6027\u3002\u65b0\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u673a\u5668\u4eba\u7269\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u7814\u7a76\u817f\u90e8\u884c\u4e3a\u5982\u4f55\u8c03\u8282\u7f51\u4e0a\u7684\u632f\u52a8\u611f\u77e5\u3002", "conclusion": "\u65b0\u7684\u516b\u817f\u8718\u86db\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u751f\u7269\u5b66\u4e0a\u66f4\u51c6\u786e\u7684\u673a\u5668\u4eba\u7269\u7406\u6a21\u578b\uff0c\u4e3a\u7814\u7a76\u8718\u86db\u5982\u4f55\u901a\u8fc7\u817f\u90e8\u884c\u4e3a\uff08\u7279\u522b\u662f\u8e72\u4f0f\uff09\u8c03\u8282\u7f51\u4e0a\u632f\u52a8\u611f\u77e5\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.16866", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16866", "abs": "https://arxiv.org/abs/2601.16866", "authors": ["Luc\u00eda G\u00fcitta-L\u00f3pez", "Vincenzo Suriani", "Jaime Boal", "\u00c1lvaro J. L\u00f3pez-L\u00f3pez", "Daniele Nardi"], "title": "Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators", "comment": null, "summary": "Deep Reinforcement Learning (DRL) is a powerful framework for solving complex sequential decision-making problems, particularly in robotic control. However, its practical deployment is often hindered by the substantial amount of experience required for learning, which results in high computational and time costs. In this work, we propose a novel integration of DRL with semantic knowledge in the form of Knowledge Graph Embeddings (KGEs), aiming to enhance learning efficiency by providing contextual information to the agent. Our architecture combines KGEs with visual observations, enabling the agent to exploit environmental knowledge during training. Experimental validation with robotic manipulators in environments featuring both fixed and randomized target attributes demonstrates that our method achieves up to {60}{\\%} reduction in learning time and improves task accuracy by approximately 15 percentage points, without increasing training time or computational complexity. These results highlight the potential of semantic knowledge to reduce sample complexity and improve the effectiveness of DRL in robotic applications.", "AI": {"tldr": "\u5c06\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u8bed\u4e49\u77e5\u8bc6\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u7387\uff0c\u51cf\u5c1160%\u5b66\u4e60\u65f6\u95f4\u5e76\u63d0\u9ad815%\u4efb\u52a1\u51c6\u786e\u7387\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u9700\u8981\u5927\u91cf\u7ecf\u9a8c\u6570\u636e\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u548c\u65f6\u95f4\u6210\u672c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u77e5\u8bc6\u6765\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5c06\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u7684\u65b0\u67b6\u6784\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e0e\u89c6\u89c9\u89c2\u5bdf\u76f8\u7ed3\u5408\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528\u73af\u5883\u77e5\u8bc6\u3002", "result": "\u5728\u5177\u6709\u56fa\u5b9a\u548c\u968f\u673a\u76ee\u6807\u5c5e\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe60%\u7684\u5b66\u4e60\u65f6\u95f4\u51cf\u5c11\uff0c\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u9ad8\u7ea615\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u4e0d\u589e\u52a0\u8bad\u7ec3\u65f6\u95f4\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8bed\u4e49\u77e5\u8bc6\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u548c\u4efb\u52a1\u6548\u679c\uff0c\u5c55\u793a\u4e86\u77e5\u8bc6\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.16870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16870", "abs": "https://arxiv.org/abs/2601.16870", "authors": ["Guangping Liu", "Nicholas Hawkins", "Billy Madden", "Tipu Sultan", "Flavio Esposito", "Madi Babaiasl"], "title": "A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study", "comment": null, "summary": "Integrated control of wheelchairs and wheelchair-mounted robotic arms (WMRAs) has strong potential to increase independence for users with severe motor limitations, yet existing interfaces often lack the flexibility needed for intuitive assistive interaction. Although data-driven AI methods show promise, progress is limited by the lack of multimodal datasets that capture natural Human-Robot Interaction (HRI), particularly conversational ambiguity in dialogue-driven control. To address this gap, we propose a multimodal data collection framework that employs a dialogue-based interaction protocol and a two-room Wizard-of-Oz (WoZ) setup to simulate robot autonomy while eliciting natural user behavior. The framework records five synchronized modalities: RGB-D video, conversational audio, inertial measurement unit (IMU) signals, end-effector Cartesian pose, and whole-body joint states across five assistive tasks. Using this framework, we collected a pilot dataset of 53 trials from five participants and validated its quality through motion smoothness analysis and user feedback. The results show that the framework effectively captures diverse ambiguity types and supports natural dialogue-driven interaction, demonstrating its suitability for scaling to a larger dataset for learning, benchmarking, and evaluation of ambiguity-aware assistive control.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u8f6e\u6905\u53ca\u8f6e\u6905\u673a\u68b0\u81c2\u96c6\u6210\u63a7\u5236\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u534f\u8bae\u548c\u53cc\u623f\u95f4Wizard-of-Oz\u8bbe\u7f6e\u6765\u6a21\u62df\u673a\u5668\u4eba\u81ea\u4e3b\u6027\uff0c\u8bb0\u5f55\u4e94\u79cd\u540c\u6b65\u6a21\u6001\u6570\u636e\uff0c\u65e8\u5728\u89e3\u51b3\u8f85\u52a9\u4ea4\u4e92\u4e2d\u5bf9\u8bdd\u6b67\u4e49\u7684\u6570\u636e\u7f3a\u4e4f\u95ee\u9898\u3002", "motivation": "\u8f6e\u6905\u548c\u8f6e\u6905\u673a\u68b0\u81c2\u7684\u96c6\u6210\u63a7\u5236\u5bf9\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u7528\u6237\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u63a5\u53e3\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u6570\u636e\u9a71\u52a8\u7684AI\u65b9\u6cd5\u8fdb\u5c55\u53d7\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u6355\u6349\u81ea\u7136\u4eba\u673a\u4ea4\u4e92\uff08\u7279\u522b\u662f\u5bf9\u8bdd\u9a71\u52a8\u63a7\u5236\u4e2d\u7684\u6b67\u4e49\uff09\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u5bf9\u8bdd\u7684\u4ea4\u4e92\u534f\u8bae\u548c\u53cc\u623f\u95f4Wizard-of-Oz\u8bbe\u7f6e\u6765\u6a21\u62df\u673a\u5668\u4eba\u81ea\u4e3b\u6027\uff0c\u540c\u65f6\u8bb0\u5f55\u4e94\u79cd\u540c\u6b65\u6a21\u6001\uff1aRGB-D\u89c6\u9891\u3001\u5bf9\u8bdd\u97f3\u9891\u3001IMU\u4fe1\u53f7\u3001\u672b\u7aef\u6267\u884c\u5668\u7b1b\u5361\u5c14\u4f4d\u59ff\u548c\u5168\u8eab\u5173\u8282\u72b6\u6001\uff0c\u6db5\u76d6\u4e94\u4e2a\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u6536\u96c6\u4e86\u6765\u81ea5\u540d\u53c2\u4e0e\u8005\u768453\u6b21\u8bd5\u9a8c\u7684\u8bd5\u70b9\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u5206\u6790\u548c\u7528\u6237\u53cd\u9988\u9a8c\u8bc1\u4e86\u6570\u636e\u8d28\u91cf\u3002\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u591a\u79cd\u6b67\u4e49\u7c7b\u578b\uff0c\u652f\u6301\u81ea\u7136\u7684\u5bf9\u8bdd\u9a71\u52a8\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u5408\u6269\u5c55\u4e3a\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b66\u4e60\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u5177\u6709\u6b67\u4e49\u611f\u77e5\u80fd\u529b\u7684\u8f85\u52a9\u63a7\u5236\uff0c\u4e3a\u89e3\u51b3\u8f85\u52a9\u4ea4\u4e92\u4e2d\u7684\u5bf9\u8bdd\u6b67\u4e49\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002"}}
