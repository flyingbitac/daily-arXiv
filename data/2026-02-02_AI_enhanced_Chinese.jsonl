{"id": "2601.22198", "categories": ["cs.RO", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.22198", "abs": "https://arxiv.org/abs/2601.22198", "authors": ["Judith Vilella-Cantos", "M\u00f3nica Ballesta", "David Valiente", "Mar\u00eda Flores", "Luis Pay\u00e1"], "title": "Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey", "comment": null, "summary": "An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.", "AI": {"tldr": "\u672c\u6587\u662f\u7b2c\u4e00\u7bc7\u4e13\u6ce8\u4e8e\u519c\u4e1a\u73af\u5883\u4e2d\u57fa\u4e8eLiDAR\u5b9a\u4f4d\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u548cLiDAR\u5730\u70b9\u8bc6\u522b\u6280\u672f\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7cbe\u51c6\u519c\u4e1a\u662f\u80fd\u591f\u4ece\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u83b7\u76ca\u6700\u591a\u7684\u9886\u57df\u4e4b\u4e00\uff0c\u4f46\u519c\u4e1a\u73af\u5883\u7f3a\u4e4f\u663e\u8457\u7279\u5f81\u4e14\u7ed3\u6784\u975e\u7ed3\u6784\u5316\uff0c\u4f7f\u5f97\u5730\u70b9\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\u3002\u76ee\u524dLiDAR\u5730\u70b9\u8bc6\u522b\u6280\u672f\u4e3b\u8981\u5e94\u7528\u4e8e\u57ce\u5e02\u73af\u5883\uff0c\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u7814\u7a76\u76f8\u5bf9\u7f3a\u4e4f\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7efc\u8ff0\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5168\u9762\u56de\u987e\u4e86\u519c\u4e1a\u73af\u5883\u4e2d\u6700\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u548cLiDAR\u5730\u70b9\u8bc6\u522b\u6280\u672f\u3002\u91cd\u70b9\u5173\u6ce8\u8fd9\u4e9b\u73af\u5883\u4e2d\u51fa\u73b0\u7684\u6311\u6218\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u672c\u6587\u662f\u7b2c\u4e00\u7bc7\u4e13\u6ce8\u4e8e\u519c\u4e1a\u73af\u5883\u4e2d\u57fa\u4e8eLiDAR\u5b9a\u4f4d\u7684\u7efc\u8ff0\uff0c\u63d0\u4f9b\u4e86\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u7406\u89e3\u3002\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u4fc3\u8fdb\u8fd9\u4e00\u4e13\u4e1a\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u519c\u4e1a\u73af\u5883\u4e2d\u7684LiDAR\u5b9a\u4f4d\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7814\u7a76\u65b9\u6cd5\u3002\u672c\u6587\u586b\u8865\u4e86\u8be5\u9886\u57df\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u53c2\u8003\u6846\u67b6\uff0c\u6709\u671b\u63a8\u52a8\u7cbe\u51c6\u519c\u4e1a\u4e2d\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.22289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22289", "abs": "https://arxiv.org/abs/2601.22289", "authors": ["Jeeho Ahn", "Christoforos Mavrogiannis"], "title": "ReloPush-BOSS: Optimization-guided Nonmonotone Rearrangement Planning for a Car-like Robot Pusher", "comment": "Preprint of final version, accepted to RA-L 2026", "summary": "We focus on multi-object rearrangement planning in densely cluttered environments using a car-like robot pusher. The combination of kinematic, geometric and physics constraints underlying this domain results in challenging nonmonotone problem instances which demand breaking each manipulation action into multiple parts to achieve a desired object rearrangement. Prior work tackles such instances by planning prerelocations, temporary object displacements that enable constraint satisfaction, but deciding where to prerelocate remains difficult due to local minima leading to infeasible or high-cost paths. Our key insight is that these minima can be avoided by steering a prerelocation optimization toward low-cost regions informed by Dubins path classification. These optimized prerelocations are integrated into an object traversability graph that encodes kinematic, geometric, and pushing constraints. Searching this graph in a depth-first fashion results in efficient, feasible rearrangement sequences. Across a series of densely cluttered scenarios with up to 13 objects, our framework, ReloPush-BOSS, exhibits consistently highest success rates and shortest pushing paths compared to state-of-the-art baselines. Hardware experiments on a 1/10 car-like pusher demonstrate the robustness of our approach. Code and footage from our experiments can be found at: https://fluentrobotics.com/relopushboss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReloPush-BOSS\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u4f7f\u7528\u7c7b\u8f66\u673a\u5668\u4eba\u63a8\u52a8\u5668\u8fdb\u884c\u591a\u7269\u4f53\u91cd\u6392\u89c4\u5212\uff0c\u901a\u8fc7\u4f18\u5316\u9884\u91cd\u5b9a\u4f4d\u6765\u907f\u514d\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u884c\u7684\u91cd\u6392\u5e8f\u5217\u3002", "motivation": "\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u4f7f\u7528\u7c7b\u8f66\u673a\u5668\u4eba\u63a8\u52a8\u5668\u8fdb\u884c\u591a\u7269\u4f53\u91cd\u6392\u9762\u4e34\u8fd0\u52a8\u5b66\u3001\u51e0\u4f55\u548c\u7269\u7406\u7ea6\u675f\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u975e\u5355\u8c03\u95ee\u9898\u5b9e\u4f8b\uff0c\u9700\u8981\u5c06\u64cd\u4f5c\u52a8\u4f5c\u5206\u89e3\u4e3a\u591a\u4e2a\u90e8\u5206\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u89c4\u5212\u9884\u91cd\u5b9a\u4f4d\u6765\u89e3\u51b3\uff0c\u4f46\u51b3\u5b9a\u9884\u91cd\u5b9a\u4f4d\u4f4d\u7f6e\u65f6\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u5bfc\u81f4\u4e0d\u53ef\u884c\u6216\u9ad8\u6210\u672c\u8def\u5f84\u3002", "method": "\u63d0\u51faReloPush-BOSS\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eDubins\u8def\u5f84\u5206\u7c7b\u5f15\u5bfc\u9884\u91cd\u5b9a\u4f4d\u4f18\u5316\uff0c\u907f\u514d\u5c40\u90e8\u6700\u5c0f\u503c\uff1b2\uff09\u5c06\u4f18\u5316\u540e\u7684\u9884\u91cd\u5b9a\u4f4d\u96c6\u6210\u5230\u7f16\u7801\u8fd0\u52a8\u5b66\u3001\u51e0\u4f55\u548c\u63a8\u52a8\u7ea6\u675f\u7684\u5bf9\u8c61\u53ef\u904d\u5386\u56fe\u4e2d\uff1b3\uff09\u4ee5\u6df1\u5ea6\u4f18\u5148\u65b9\u5f0f\u641c\u7d22\u8be5\u56fe\uff0c\u751f\u6210\u9ad8\u6548\u53ef\u884c\u7684\u91cd\u6392\u5e8f\u5217\u3002", "result": "\u5728\u6700\u591a\u5305\u542b13\u4e2a\u7269\u4f53\u7684\u5bc6\u96c6\u6742\u4e71\u573a\u666f\u4e2d\uff0cReloPush-BOSS\u6846\u67b6\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u6700\u9ad8\u7684\u6210\u529f\u7387\u548c\u6700\u77ed\u7684\u63a8\u52a8\u8def\u5f84\u3002\u57281/10\u6bd4\u4f8b\u7c7b\u8f66\u63a8\u52a8\u5668\u4e0a\u7684\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u9884\u91cd\u5b9a\u4f4d\u5e76\u96c6\u6210\u5230\u7ea6\u675f\u7f16\u7801\u56fe\u4e2d\u8fdb\u884c\u641c\u7d22\uff0cReloPush-BOSS\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u591a\u7269\u4f53\u91cd\u6392\u89c4\u5212\u95ee\u9898\uff0c\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.22381", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22381", "abs": "https://arxiv.org/abs/2601.22381", "authors": ["Victor Nikhil Antony", "Zhili Gong", "Guanchen Li", "Clara Jeon", "Chien-Ming Huang"], "title": "Lantern: A Minimalist Robotic Object Platform", "comment": null, "summary": "Robotic objects are simple actuated systems that subtly blend into human environments. We design and introduce Lantern, a minimalist robotic object platform to enable building simple robotic artifacts. We conducted in-depth design and engineering iterations of Lantern's mechatronic architecture to meet specific design goals while maintaining a low build cost (~40 USD). As an extendable, open-source platform, Lantern aims to enable exploration of a range of HRI scenarios by leveraging human tendency to assign social meaning to simple forms. To evaluate Lantern's potential for HRI, we conducted a series of explorations: 1) a co-design workshop, 2) a sensory room case study, 3) distribution to external HRI labs, 4) integration into a graduate-level HRI course, and 5) public exhibitions with older adults and children. Our findings show that Lantern effectively evokes engagement, can support versatile applications ranging from emotion regulation to focused work, and serves as a viable platform for lowering barriers to HRI as a field.", "AI": {"tldr": "Lantern\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u7b80\u7ea6\u7684\u673a\u5668\u4eba\u5bf9\u8c61\u5e73\u53f0\uff0c\u65e8\u5728\u964d\u4f4e\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u95e8\u69db\uff0c\u901a\u8fc7\u7b80\u5355\u5f62\u5f0f\u6fc0\u53d1\u4eba\u7c7b\u793e\u4ea4\u610f\u4e49\u6295\u5c04\uff0c\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u7b80\u7ea6\u7684\u673a\u5668\u4eba\u5bf9\u8c61\u5e73\u53f0\uff0c\u964d\u4f4e\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u95e8\u69db\uff0c\u5229\u7528\u4eba\u7c7b\u5bf9\u7b80\u5355\u5f62\u5f0f\u8d4b\u4e88\u793e\u4ea4\u610f\u4e49\u7684\u503e\u5411\uff0c\u63a2\u7d22\u591a\u6837\u5316\u7684\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u7684\u673a\u7535\u67b6\u6784\u8bbe\u8ba1\u548c\u5de5\u7a0b\u8fed\u4ee3\uff0c\u5f00\u53d1Lantern\u5e73\u53f0\uff08\u6210\u672c\u7ea640\u7f8e\u5143\uff09\uff1b\u901a\u8fc7\u4e94\u4e2a\u63a2\u7d22\u6027\u7814\u7a76\u8bc4\u4f30\u5176HRI\u6f5c\u529b\uff1a1)\u534f\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c2)\u611f\u5b98\u5ba4\u6848\u4f8b\u7814\u7a76\uff0c3)\u5916\u90e8HRI\u5b9e\u9a8c\u5ba4\u5206\u53d1\uff0c4)\u7814\u7a76\u751fHRI\u8bfe\u7a0b\u6574\u5408\uff0c5)\u8001\u5e74\u4eba\u548c\u513f\u7ae5\u7684\u516c\u5171\u5c55\u89c8\u3002", "result": "Lantern\u80fd\u6709\u6548\u6fc0\u53d1\u53c2\u4e0e\u5ea6\uff0c\u652f\u6301\u4ece\u60c5\u7eea\u8c03\u8282\u5230\u4e13\u6ce8\u5de5\u4f5c\u7b49\u591a\u79cd\u5e94\u7528\uff0c\u4f5c\u4e3a\u964d\u4f4eHRI\u9886\u57df\u95e8\u69db\u7684\u53ef\u884c\u5e73\u53f0\u3002", "conclusion": "Lantern\u4f5c\u4e3a\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u7684\u7b80\u7ea6\u673a\u5668\u4eba\u5bf9\u8c61\u5e73\u53f0\uff0c\u6210\u529f\u5c55\u793a\u4e86\u5176\u5728\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u3001\u964d\u4f4e\u9886\u57df\u95e8\u69db\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6fc0\u53d1\u4eba\u7c7b\u53c2\u4e0e\u5e76\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.22387", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22387", "abs": "https://arxiv.org/abs/2601.22387", "authors": ["Victor Nikhil Antony", "Adithya R N", "Sarah Derrick", "Zhili Gong", "Peter M. Donley", "Chien-Ming Huang"], "title": "Plant-Inspired Robot Design Metaphors for Ambient HRI", "comment": null, "summary": "Plants offer a paradoxical model for interaction: they are ambient, low-demand presences that nonetheless shape atmosphere, routines, and relationships through temporal rhythms and subtle expressions. In contrast, most human-robot interaction (HRI) has been grounded in anthropomorphic and zoomorphic paradigms, producing overt, high-demand forms of engagement. Using a Research through Design (RtD) methodology, we explore plants as metaphoric inspiration for HRI; we conducted iterative cycles of ideation, prototyping, and reflection to investigate what design primitives emerge from plant metaphors and morphologies, and how these primitives can be combined into expressive robotic forms. We present a suite of speculative, open-source prototypes that help probe plant-inspired presence, temporality, form, and gestures. We deepened our learnings from design and prototyping through prototype-centered workshops that explored people's perceptions and imaginaries of plant-inspired robots. This work contributes: (1) Set of plant-inspired robotic artifacts; (2) Designerly insights on how people perceive plant-inspired robots; and (3) Design consideration to inform how to use plant metaphors to reshape HRI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4ee5\u690d\u7269\u4e3a\u9690\u55bb\u7075\u611f\u8bbe\u8ba1\u4eba\u673a\u4ea4\u4e92\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7814\u7a76\u5f00\u53d1\u690d\u7269\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u539f\u578b\uff0c\u63a2\u8ba8\u4f4e\u9700\u6c42\u3001\u73af\u5883\u5316\u7684\u4ea4\u4e92\u8303\u5f0f", "motivation": "\u5f53\u524d\u4eba\u673a\u4ea4\u4e92\u4e3b\u8981\u57fa\u4e8e\u62df\u4eba\u5316\u548c\u62df\u52a8\u7269\u5316\u8303\u5f0f\uff0c\u4ea7\u751f\u9ad8\u9700\u6c42\u3001\u663e\u6027\u7684\u4ea4\u4e92\u5f62\u5f0f\u3002\u690d\u7269\u4f5c\u4e3a\u73af\u5883\u5316\u3001\u4f4e\u9700\u6c42\u7684\u5b58\u5728\uff0c\u901a\u8fc7\u65f6\u95f4\u8282\u594f\u548c\u5fae\u5999\u8868\u8fbe\u5851\u9020\u6c1b\u56f4\u3001\u65e5\u5e38\u548c\u5173\u7cfb\uff0c\u4e3aHRI\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u7075\u611f", "method": "\u91c7\u7528\u8bbe\u8ba1\u7814\u7a76\u65b9\u6cd5\uff0c\u8fdb\u884c\u8fed\u4ee3\u7684\u6784\u601d\u3001\u539f\u578b\u5236\u4f5c\u548c\u53cd\u601d\u5faa\u73af\u3002\u901a\u8fc7\u539f\u578b\u4e2d\u5fc3\u5de5\u4f5c\u574a\u63a2\u7d22\u4eba\u4eec\u5bf9\u690d\u7269\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u7684\u611f\u77e5\u548c\u60f3\u8c61", "result": "\u5f00\u53d1\u4e86\u4e00\u5957\u63a8\u6d4b\u6027\u3001\u5f00\u6e90\u7684\u539f\u578b\uff0c\u63a2\u7d22\u690d\u7269\u542f\u53d1\u7684\u5b58\u5728\u611f\u3001\u65f6\u95f4\u6027\u3001\u5f62\u6001\u548c\u624b\u52bf\u3002\u901a\u8fc7\u5de5\u4f5c\u574a\u83b7\u5f97\u4e86\u4eba\u4eec\u5bf9\u690d\u7269\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u6d1e\u5bdf", "conclusion": "\u8d21\u732e\u5305\u62ec\uff1a(1)\u690d\u7269\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u539f\u578b\u96c6\uff1b(2)\u4eba\u4eec\u5bf9\u690d\u7269\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u611f\u77e5\u7684\u8bbe\u8ba1\u6d1e\u5bdf\uff1b(3)\u4f7f\u7528\u690d\u7269\u9690\u55bb\u91cd\u5851\u4eba\u673a\u4ea4\u4e92\u7684\u8bbe\u8ba1\u8003\u91cf"}}
{"id": "2601.22406", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22406", "abs": "https://arxiv.org/abs/2601.22406", "authors": ["Shahar Dubiner", "Peng Ren", "Roberto Manduchi"], "title": "Accurate Pedestrian Tracking in Urban Canyons: A Multi-Modal Fusion Approach", "comment": null, "summary": "The contribution describes a pedestrian navigation approach designed to improve localization accuracy in urban environments where GNSS performance is degraded, a problem that is especially critical for blind or low-vision users who depend on precise guidance such as identifying the correct side of a street. To address GNSS limitations and the impracticality of camera-based visual positioning, the work proposes a particle filter based fusion of GNSS and inertial data that incorporates spatial priors from maps, such as impassable buildings and unlikely walking areas, functioning as a probabilistic form of map matching. Inertial localization is provided by the RoNIN machine learning method, and fusion with GNSS is achieved by weighting particles based on their consistency with GNSS estimates and uncertainty. The system was evaluated on six challenging walking routes in downtown San Francisco using three metrics related to sidewalk correctness and localization error. Results show that the fused approach (GNSS+RoNIN+PF) significantly outperforms GNSS only localization on most metrics, while inertial-only localization with particle filtering also surpasses GNSS alone for critical measures such as sidewalk assignment and across street error.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408GNSS\u3001\u60ef\u6027\u6570\u636e\u548c\u5730\u56fe\u7a7a\u95f4\u5148\u9a8c\u7684\u7c92\u5b50\u6ee4\u6ce2\u884c\u4eba\u5bfc\u822a\u65b9\u6cd5\uff0c\u5728GNSS\u6027\u80fd\u53d7\u9650\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u7279\u522b\u4e3a\u89c6\u969c\u7528\u6237\u63d0\u4f9b\u7cbe\u786e\u7684\u8857\u9053\u4fa7\u5411\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u73af\u5883\u4e2dGNSS\u6027\u80fd\u4e0b\u964d\u5bfc\u81f4\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u95ee\u9898\uff0c\u8fd9\u5bf9\u4f9d\u8d56\u7cbe\u786e\u5bfc\u822a\uff08\u5982\u8bc6\u522b\u6b63\u786e\u8857\u9053\u4fa7\uff09\u7684\u76f2\u4eba\u6216\u4f4e\u89c6\u529b\u7528\u6237\u5c24\u4e3a\u5173\u952e\u3002\u4f20\u7edf\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4e0d\u5b9e\u7528\u3002", "method": "\u91c7\u7528\u7c92\u5b50\u6ee4\u6ce2\u878d\u5408GNSS\u548c\u60ef\u6027\u6570\u636e\uff0c\u5e76\u878d\u5165\u5730\u56fe\u7a7a\u95f4\u5148\u9a8c\uff08\u5982\u4e0d\u53ef\u901a\u884c\u7684\u5efa\u7b51\u7269\u548c\u4e0d\u592a\u53ef\u80fd\u7684\u6b65\u884c\u533a\u57df\uff09\uff0c\u4f5c\u4e3a\u6982\u7387\u5f62\u5f0f\u7684\u5730\u56fe\u5339\u914d\u3002\u60ef\u6027\u5b9a\u4f4d\u7531RoNIN\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\uff0c\u901a\u8fc7\u4e0eGNSS\u4f30\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u7c92\u5b50\u52a0\u6743\u5b9e\u73b0\u878d\u5408\u3002", "result": "\u5728\u65e7\u91d1\u5c71\u5e02\u4e2d\u5fc36\u6761\u5177\u6709\u6311\u6218\u6027\u7684\u6b65\u884c\u8def\u7ebf\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e0e\u884c\u4eba\u9053\u6b63\u786e\u6027\u548c\u5b9a\u4f4d\u8bef\u5dee\u76f8\u5173\u7684\u4e09\u4e2a\u6307\u6807\u3002\u7ed3\u679c\u663e\u793a\uff1a\u878d\u5408\u65b9\u6cd5\uff08GNSS+RoNIN+PF\uff09\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528GNSS\u7684\u5b9a\u4f4d\uff1b\u4ec5\u4f7f\u7528\u60ef\u6027\u5b9a\u4f4d\u52a0\u7c92\u5b50\u6ee4\u6ce2\u5728\u5173\u952e\u6307\u6807\uff08\u5982\u884c\u4eba\u9053\u5206\u914d\u548c\u8de8\u8857\u9053\u8bef\u5dee\uff09\u4e0a\u4e5f\u4f18\u4e8e\u4ec5\u4f7f\u7528GNSS\u3002", "conclusion": "\u63d0\u51fa\u7684\u878d\u5408GNSS\u3001\u60ef\u6027\u6570\u636e\u548c\u5730\u56fe\u5148\u9a8c\u7684\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u884c\u4eba\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u89c6\u969c\u7528\u6237\u7684\u7cbe\u786e\u5bfc\u822a\u9700\u6c42\uff0c\u5728GNSS\u53d7\u9650\u60c5\u51b5\u4e0b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2601.22445", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22445", "abs": "https://arxiv.org/abs/2601.22445", "authors": ["Leaf Jiang", "Matthew Holzel", "Bernhard Kaplan", "Hsiou-Yuan Liu", "Sabyasachi Paul", "Karen Rankin", "Piotr Swierczynski"], "title": "High-Definition 5MP Stereo Vision Sensing for Robotics", "comment": null, "summary": "High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf95MP+\u9ad8\u5206\u8fa8\u7387\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u7684\u65b0\u578b\u6821\u51c6\u548c\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u5904\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u50cf\u7d20\u76f8\u673a\u9700\u8981\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u6821\u51c6\u624d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u70b9\u4e91\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\uff085MP+\uff09\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u5bf9\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u8fdc\u8ddd\u79bb\u7684\u64cd\u4f5c\u5e76\u751f\u6210\u66f4\u5bc6\u96c6\u3001\u66f4\u51c6\u786e\u76843D\u70b9\u4e91\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u89d2\u5206\u8fa8\u7387\u4f20\u611f\u5668\u6240\u9700\u7684\u9ad8\u6821\u51c6\u7cbe\u5ea6\u548c\u5feb\u901f\u5904\u7406\u8981\u6c42\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u5e27\u5230\u5e27\u6821\u51c6\u548c\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u5904\u74065MP\u76f8\u673a\u56fe\u50cf\uff0c\u540c\u65f6\u5f15\u5165\u65b0\u7684\u5b9e\u65f6\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5b9e\u65f6\u89c6\u5dee\u56fe\u4e0e\u8ba1\u7b97\u5bc6\u96c6\u578b\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u751f\u6210\u7684\u57fa\u51c6\u771f\u503c\u89c6\u5dee\u56fe\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u50cf\u7d20\u76f8\u673a\u53ea\u6709\u901a\u8fc7\u5b9e\u65bd\u9ad8\u7cbe\u5ea6\u6821\u51c6\u624d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u70b9\u4e91\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u5904\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9ad8\u5206\u8fa8\u7387\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u7684\u6f5c\u529b\u9700\u8981\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u6821\u51c6\u548c\u9ad8\u6548\u5904\u7406\u65b9\u6cd5\u6765\u5145\u5206\u5b9e\u73b0\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.22467", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22467", "abs": "https://arxiv.org/abs/2601.22467", "authors": ["Jiaqi Shi", "Xulong Zhang", "Xiaoyang Qu", "Jianzong Wang"], "title": "CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control", "comment": "Accepted to 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.", "AI": {"tldr": "CARE\u6846\u67b6\u901a\u8fc7\u4ec5\u4f7f\u7528\u89c6\u9891-\u6587\u672c\u5bf9\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u65e0\u9700\u52a8\u4f5c\u6807\u6ce8\uff0c\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u7136\u540e\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0a\u5fae\u8c03\u52a8\u4f5c\u5934\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u52a8\u4f5c\u76d1\u7763\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u52a8\u4f5c\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u63a7\u5236\u6a21\u578b\u3002", "method": "CARE\u6846\u67b6\u4ec5\u4f7f\u7528\u89c6\u9891-\u6587\u672c\u5bf9\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u76ee\u6807\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u7136\u540e\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0a\u5fae\u8c03\u52a8\u4f5c\u5934\u7528\u4e8e\u63a7\u5236\u3002", "result": "\u5728\u591a\u4e2a\u4eff\u771f\u4efb\u52a1\u4e2d\uff0cCARE\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u597d\u7684\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u907f\u514d\u6377\u5f84\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5f31\u76d1\u7763\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "CARE\u6846\u67b6\u901a\u8fc7\u6d88\u9664\u5bf9\u52a8\u4f5c\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u5f31\u76d1\u7763\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.22517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22517", "abs": "https://arxiv.org/abs/2601.22517", "authors": ["Kangning Yin", "Zhe Cao", "Wentao Dong", "Weishuai Zeng", "Tianyi Zhang", "Qiang Zhang", "Jingbo Wang", "Jiangmiao Pang", "Ming Zhou", "Weinan Zhang"], "title": "RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing", "comment": null, "summary": "Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.", "AI": {"tldr": "RoboStriker\uff1a\u4e00\u4e2a\u4e09\u9636\u6bb5\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u6218\u7565\u63a8\u7406\u4e0e\u4f4e\u5c42\u7269\u7406\u6267\u884c\uff0c\u5b9e\u73b0\u5168\u81ea\u4e3b\u4eba\u5f62\u673a\u5668\u4eba\u62f3\u51fb\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u8fd0\u52a8\u6280\u80fd\u5b66\u4e60\u3001\u6f5c\u5728\u7a7a\u95f4\u84b8\u998f\u548c\u6f5c\u5728\u7a7a\u95f4\u795e\u7ecf\u865a\u62df\u81ea\u535a\u5f08\uff0c\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u9ad8\u7ef4\u63a5\u89e6\u52a8\u529b\u5b66\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u4e14\u9ad8\u5ea6\u52a8\u6001\u7684\u4efb\u52a1\uff08\u5982\u62f3\u51fb\uff09\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u7ade\u4e89\u667a\u80fd\u548c\u8eab\u4f53\u654f\u6377\u6027\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u4e3a\u6218\u7565\u4ea4\u4e92\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u4eba\u5f62\u63a7\u5236\u53d7\u5230\u9ad8\u7ef4\u63a5\u89e6\u52a8\u529b\u5b66\u548c\u7f3a\u4e4f\u5f3a\u7269\u7406\u8fd0\u52a8\u5148\u9a8c\u7684\u963b\u788d\u3002", "method": "1. \u901a\u8fc7\u5355\u667a\u80fd\u4f53\u8fd0\u52a8\u8ddf\u8e2a\u5668\u5728\u4eba\u7c7b\u52a8\u4f5c\u6355\u6349\u6570\u636e\u4e0a\u5b66\u4e60\u5168\u9762\u7684\u62f3\u51fb\u6280\u80fd\u5e93\uff1b2. \u5c06\u8fd9\u4e9b\u6280\u80fd\u84b8\u998f\u5230\u7ed3\u6784\u5316\u6f5c\u5728\u6d41\u5f62\u4e2d\uff0c\u901a\u8fc7\u5c06\u9ad8\u65af\u53c2\u6570\u5316\u5206\u5e03\u6295\u5f71\u5230\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u8fdb\u884c\u6b63\u5219\u5316\uff1b3. \u63d0\u51fa\u6f5c\u5728\u7a7a\u95f4\u795e\u7ecf\u865a\u62df\u81ea\u535a\u5f08\uff0c\u8ba9\u7ade\u4e89\u667a\u80fd\u4f53\u5728\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u800c\u975e\u539f\u59cb\u7535\u673a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4ea4\u4e92\uff0c\u663e\u8457\u7a33\u5b9a\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRoboStriker\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u7ade\u4e89\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "RoboStriker\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u62f3\u51fb\u4e2d\u7684\u6218\u7565\u63a8\u7406\u4e0e\u7269\u7406\u6267\u884c\u8026\u5408\u95ee\u9898\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.22545", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22545", "abs": "https://arxiv.org/abs/2601.22545", "authors": ["Feng Tao", "Luca Paparusso", "Chenyi Gu", "Robin Koehler", "Chenxu Wu", "Xinyu Huang", "Christian Juette", "David Paz", "Ren Liu"], "title": "Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios", "comment": null, "summary": "Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3\u505c\u8f66\u573a\u7b49\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7ecf\u5178\u89c4\u5212\u5668\u5728\u5b8c\u7f8e\u611f\u77e5\u5047\u8bbe\u4e0b\u6709\u6548\uff0c\u4f46\u5bf9\u5b9e\u9645\u611f\u77e5\u7ea6\u675f\u654f\u611f\uff0c\u4e14\u5728\u7ebf\u641c\u7d22\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u5efa\u6a21\u4e3a\u57fa\u4e8e\u81ea\u884c\u8f66\u6a21\u578b\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u76f4\u63a5\u5b66\u4e60\u7b26\u5408\u8f66\u8f86\u8fd0\u52a8\u5b66\u548c\u73af\u5883\u7ea6\u675f\u7684\u5bfc\u822a\u7b56\u7565\u3002\u5f00\u53d1\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u76f8\u6bd4\u7ecf\u5178\u89c4\u5212\u5668\u57fa\u7ebf\uff0c\u6210\u529f\u7387\u63d0\u5347+96%\uff0c\u6548\u7387\u63d0\u5347+52%\u3002\u5f00\u53d1\u7684\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u63d0\u51fa\u7684DRL\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\uff0c\u4e0d\u9700\u8981\u7406\u60f3\u5316\u611f\u77e5\u7ed3\u6784\uff0c\u907f\u514d\u4e86\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u7b49\u989d\u5916\u6a21\u5757\uff0c\u7b80\u5316\u4e86\u5b9e\u9645\u90e8\u7f72\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5f00\u6e90\u8d44\u6e90\u3002"}}
{"id": "2601.22550", "categories": ["cs.RO", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22550", "abs": "https://arxiv.org/abs/2601.22550", "authors": ["Geonho Leem", "Jaedong Lee", "Jehee Lee", "Seungmoon Song", "Jungdam Won"], "title": "Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation", "comment": "10 pages, 9 figures, ICLR 2026 accepted", "summary": "Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.", "AI": {"tldr": "Exo-plore\uff1a\u7ed3\u5408\u795e\u7ecf\u529b\u5b66\u6a21\u62df\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4eff\u771f\u6846\u67b6\uff0c\u65e0\u9700\u771f\u4eba\u5b9e\u9a8c\u5373\u53ef\u4f18\u5316\u9acb\u5173\u8282\u5916\u9aa8\u9abc\u8f85\u52a9", "motivation": "\u5f53\u524d\u5916\u9aa8\u9abc\u63a7\u5236\u5668\u4f18\u5316\u9700\u8981\u5927\u91cf\u4eba\u4f53\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u9700\u884c\u8d70\u6570\u5c0f\u65f6\uff0c\u8fd9\u5bf9\u884c\u52a8\u4e0d\u4fbf\u8005\u5c24\u5176\u56f0\u96be\uff0c\u5f62\u6210\u4e86\u53d7\u76ca\u8005\u65e0\u6cd5\u53c2\u4e0e\u5b9e\u9a8c\u7684\u77db\u76fe", "method": "\u7ed3\u5408\u795e\u7ecf\u529b\u5b66\u6a21\u62df\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u521b\u5efa\u4eff\u771f\u6846\u67b6\u4f18\u5316\u9acb\u5173\u8282\u5916\u9aa8\u9abc\u8f85\u52a9\uff0c\u65e0\u9700\u771f\u5b9e\u4eba\u4f53\u5b9e\u9a8c", "result": "1) \u751f\u6210\u6355\u6349\u4eba\u7c7b\u5bf9\u8f85\u52a9\u529b\u9002\u5e94\u7684\u771f\u5b9e\u6b65\u6001\u6570\u636e\uff1b2) \u5728\u6b65\u6001\u968f\u673a\u6027\u4e0b\u4ecd\u80fd\u4ea7\u751f\u53ef\u9760\u4f18\u5316\u7ed3\u679c\uff1b3) \u53ef\u6cdb\u5316\u5230\u75c5\u7406\u6b65\u6001\uff0c\u75c5\u7406\u4e25\u91cd\u7a0b\u5ea6\u4e0e\u6700\u4f18\u8f85\u52a9\u5448\u5f3a\u7ebf\u6027\u5173\u7cfb", "conclusion": "Exo-plore\u6846\u67b6\u4e3a\u5916\u9aa8\u9abc\u8f85\u52a9\u4f18\u5316\u63d0\u4f9b\u9ad8\u6548\u4eff\u771f\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u884c\u52a8\u4e0d\u4fbf\u8005\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u7684\u53ef\u53ca\u6027\u95ee\u9898"}}
{"id": "2601.22672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22672", "abs": "https://arxiv.org/abs/2601.22672", "authors": ["Theodora Kastritsi", "Marta Lagomarsino", "Arash Ajoudani"], "title": "Postural Virtual Fixtures for Ergonomic Physical Interactions with Supernumerary Robotic Bodies", "comment": "Published in The International Journal of Robotics Research", "summary": "Conjoined collaborative robots, functioning as supernumerary robotic bodies (SRBs), can enhance human load tolerance abilities. However, in tasks involving physical interaction with humans, users may still adopt awkward, non-ergonomic postures, which can lead to discomfort or injury over time. In this paper, we propose a novel control framework that provides kinesthetic feedback to SRB users when a non-ergonomic posture is detected, offering resistance to discourage such behaviors. This approach aims to foster long-term learning of ergonomic habits and promote proper posture during physical interactions. To achieve this, a virtual fixture method is developed, integrated with a continuous, online ergonomic posture assessment framework. Additionally, to improve coordination between the operator and the SRB, which consists of a robotic arm mounted on a floating base, the position of the floating base is adjusted as needed. Experimental results demonstrate the functionality and efficacy of the ergonomics-driven control framework, including two user studies involving practical loco-manipulation tasks with 14 subjects, comparing the proposed framework with a baseline control framework that does not account for human ergonomics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8d85\u6570\u673a\u5668\u4eba\u8eab\u4f53\u7684\u65b0\u578b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u5939\u5177\u548c\u5728\u7ebf\u59ff\u6001\u8bc4\u4f30\u63d0\u4f9b\u52a8\u89c9\u53cd\u9988\uff0c\u9632\u6b62\u7528\u6237\u91c7\u7528\u975e\u4eba\u4f53\u5de5\u5b66\u59ff\u52bf\uff0c\u540c\u65f6\u8c03\u6574\u6d6e\u52a8\u57fa\u5ea7\u4f4d\u7f6e\u4ee5\u6539\u5584\u534f\u8c03\u6027\u3002", "motivation": "\u8d85\u6570\u673a\u5668\u4eba\u8eab\u4f53\u53ef\u4ee5\u589e\u5f3a\u4eba\u7c7b\u8d1f\u8f7d\u80fd\u529b\uff0c\u4f46\u5728\u4e0e\u4eba\u7c7b\u7269\u7406\u4ea4\u4e92\u7684\u4efb\u52a1\u4e2d\uff0c\u7528\u6237\u4ecd\u53ef\u80fd\u91c7\u7528\u975e\u4eba\u4f53\u5de5\u5b66\u59ff\u52bf\uff0c\u957f\u671f\u53ef\u80fd\u5bfc\u81f4\u4e0d\u9002\u6216\u4f24\u5bb3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fc3\u8fdb\u4eba\u4f53\u5de5\u5b66\u4e60\u60ef\u7684\u5b66\u4e60\u3002", "method": "\u5f00\u53d1\u4e86\u865a\u62df\u5939\u5177\u65b9\u6cd5\u4e0e\u8fde\u7eed\u5728\u7ebf\u4eba\u4f53\u5de5\u5b66\u59ff\u6001\u8bc4\u4f30\u6846\u67b6\u96c6\u6210\uff0c\u5f53\u68c0\u6d4b\u5230\u975e\u4eba\u4f53\u5de5\u5b66\u59ff\u52bf\u65f6\u63d0\u4f9b\u52a8\u89c9\u53cd\u9988\u4ee5\u963b\u6b62\u6b64\u7c7b\u884c\u4e3a\u3002\u540c\u65f6\u8c03\u6574\u6d6e\u52a8\u57fa\u5ea7\u4f4d\u7f6e\u4ee5\u6539\u5584\u64cd\u4f5c\u8005\u4e0e\u673a\u5668\u4eba\u7684\u534f\u8c03\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u4eba\u4f53\u5de5\u5b66\u9a71\u52a8\u63a7\u5236\u6846\u67b6\u7684\u529f\u80fd\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u4e24\u4e2a\u6d89\u53ca14\u540d\u53d7\u8bd5\u8005\u7684\u5b9e\u9645\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u7528\u6237\u7814\u7a76\uff0c\u4e0e\u4e0d\u8003\u8651\u4eba\u4f53\u5de5\u5b66\u7684\u57fa\u7ebf\u63a7\u5236\u6846\u67b6\u76f8\u6bd4\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u6846\u67b6\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u8d85\u6570\u673a\u5668\u4eba\u8eab\u4f53\u7528\u6237\u91c7\u7528\u4eba\u4f53\u5de5\u5b66\u59ff\u52bf\uff0c\u901a\u8fc7\u52a8\u89c9\u53cd\u9988\u57f9\u517b\u957f\u671f\u7684\u4eba\u4f53\u5de5\u5b66\u4e60\u60ef\uff0c\u6539\u5584\u4eba\u673a\u534f\u8c03\u6027\uff0c\u51cf\u5c11\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u4f24\u5bb3\u98ce\u9669\u3002"}}
{"id": "2601.22686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22686", "abs": "https://arxiv.org/abs/2601.22686", "authors": ["Biyu Ye", "Na Fan", "Zhengping Fan", "Weiliang Deng", "Hongming Chen", "Qifeng Chen", "Ximin Lyu"], "title": "FlyAware: Inertia-Aware Aerial Manipulation via Vision-Based Estimation and Post-Grasp Adaptation", "comment": "8 pages, 10 figures", "summary": "Aerial manipulators (AMs) are gaining increasing attention in automated transportation and emergency services due to their superior dexterity compared to conventional multirotor drones. However, their practical deployment is challenged by the complexity of time-varying inertial parameters, which are highly sensitive to payload variations and manipulator configurations. Inspired by human strategies for interacting with unknown objects, this letter presents a novel onboard framework for robust aerial manipulation. The proposed system integrates a vision-based pre-grasp inertia estimation module with a post-grasp adaptation mechanism, enabling real-time estimation and adaptation of inertial dynamics. For control, we develop an inertia-aware adaptive control strategy based on gain scheduling, and assess its robustness via frequency-domain system identification. Our study provides new insights into post-grasp control for AMs, and real-world experiments validate the effectiveness and feasibility of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u673a\u8f7d\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u9884\u6293\u53d6\u60ef\u6027\u4f30\u8ba1\u548c\u6293\u53d6\u540e\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u5b9e\u65f6\u60ef\u6027\u52a8\u529b\u5b66\u4f30\u8ba1\u4e0e\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u65f6\u53d8\u60ef\u6027\u53c2\u6570\u5e26\u6765\u7684\u63a7\u5236\u6311\u6218\u3002", "motivation": "\u7a7a\u4e2d\u673a\u68b0\u81c2\u5728\u81ea\u52a8\u5316\u8fd0\u8f93\u548c\u7d27\u6025\u670d\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u65f6\u53d8\u60ef\u6027\u53c2\u6570\u7684\u590d\u6742\u6027\u6311\u6218\uff0c\u8fd9\u4e9b\u53c2\u6570\u5bf9\u8d1f\u8f7d\u53d8\u5316\u548c\u673a\u68b0\u81c2\u914d\u7f6e\u9ad8\u5ea6\u654f\u611f\u3002", "method": "1) \u89c6\u89c9\u9884\u6293\u53d6\u60ef\u6027\u4f30\u8ba1\u6a21\u5757\uff1b2) \u6293\u53d6\u540e\u9002\u5e94\u673a\u5236\uff1b3) \u57fa\u4e8e\u589e\u76ca\u8c03\u5ea6\u7684\u60ef\u6027\u611f\u77e5\u81ea\u9002\u5e94\u63a7\u5236\u7b56\u7565\uff1b4) \u901a\u8fc7\u9891\u57df\u7cfb\u7edf\u8fa8\u8bc6\u8bc4\u4f30\u9c81\u68d2\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u6293\u53d6\u540e\u63a7\u5236\u89c1\u89e3\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u9c81\u68d2\u64cd\u63a7\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u89c6\u89c9\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65f6\u53d8\u60ef\u6027\u53c2\u6570\u7684\u6709\u6548\u5904\u7406\u3002"}}
{"id": "2601.22849", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.22849", "abs": "https://arxiv.org/abs/2601.22849", "authors": ["Christian Dietz", "Sebastian Albrecht", "Gianluca Frison", "Moritz Diehl", "Armin Nurkanovi\u0107"], "title": "Robust Rigid Body Assembly via Contact-Implicit Optimal Control with Exact Second-Order Derivatives", "comment": "Submitted to Transactions on Robotics", "summary": "Efficient planning of assembly motions is a long standing challenge in the field of robotics that has been primarily tackled with reinforcement learning and sampling-based methods by using extensive physics simulations. This paper proposes a sample-efficient robust optimal control approach for the determination of assembly motions, which requires significantly less physics simulation steps during planning through the efficient use of derivative information. To this end, a differentiable physics simulation is constructed that provides second-order analytic derivatives to the numerical solver and allows one to traverse seamlessly from informative derivatives to accurate contact simulation. The solution of the physics simulation problem is made differentiable by using smoothing inspired by interior-point methods applied to both the collision detection as well as the contact resolution problem. We propose a modified variant of an optimization-based formulation of collision detection formulated as a linear program and present an efficient implementation for the nominal evaluation and corresponding first- and second-order derivatives. Moreover, a multi-scenario-based trajectory optimization problem that ensures robustness with respect to sim-to-real mismatches is derived. The capability of the considered formulation is illustrated by results where over 99\\% successful executions are achieved in real-world experiments. Thereby, we carefully investigate the effect of smooth approximations of the contact dynamics and robust modeling on the success rates. Furthermore, the method's capability is tested on different peg-in-hole problems in simulation to show the benefit of using exact Hessians over commonly used Hessian approximations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u7269\u7406\u4eff\u771f\u7684\u9ad8\u6548\u88c5\u914d\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u9636\u5bfc\u6570\u4fe1\u606f\u51cf\u5c11\u4eff\u771f\u6b65\u6570\uff0c\u5e76\u5f15\u5165\u9c81\u68d2\u4f18\u5316\u786e\u4fdd\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u8fd0\u52a8\u89c4\u5212\u4e3b\u8981\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u5927\u91cf\u7269\u7406\u4eff\u771f\u6b65\u9aa4\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u51cf\u5c11\u4eff\u771f\u8ba1\u7b97\u91cf\u3002", "method": "\u6784\u5efa\u53ef\u5fae\u7269\u7406\u4eff\u771f\u5668\uff0c\u63d0\u4f9b\u4e8c\u9636\u89e3\u6790\u5bfc\u6570\u7ed9\u6570\u503c\u6c42\u89e3\u5668\uff1b\u91c7\u7528\u57fa\u4e8e\u5185\u70b9\u6cd5\u7684\u5e73\u6ed1\u6280\u672f\u4f7f\u78b0\u649e\u68c0\u6d4b\u548c\u63a5\u89e6\u89e3\u6790\u53ef\u5fae\uff1b\u63d0\u51fa\u6539\u8fdb\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u78b0\u649e\u68c0\u6d4b\u7ebf\u6027\u89c4\u5212\u516c\u5f0f\uff1b\u8bbe\u8ba1\u591a\u573a\u666f\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u6210\u529f\u6267\u884c\u7387\uff1b\u9a8c\u8bc1\u4e86\u63a5\u89e6\u52a8\u529b\u5b66\u5e73\u6ed1\u8fd1\u4f3c\u548c\u9c81\u68d2\u5efa\u6a21\u5bf9\u6210\u529f\u7387\u7684\u5f71\u54cd\uff1b\u5728\u6a21\u62df\u4e2d\u6d4b\u8bd5\u4e0d\u540c\u5b54\u8f74\u914d\u5408\u95ee\u9898\uff0c\u8bc1\u660e\u4f7f\u7528\u7cbe\u786eHessian\u77e9\u9635\u4f18\u4e8e\u5e38\u7528\u8fd1\u4f3c\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u5fae\u7269\u7406\u4eff\u771f\u7684\u9c81\u68d2\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u89c4\u5212\u88c5\u914d\u8fd0\u52a8\uff0c\u663e\u8457\u51cf\u5c11\u4eff\u771f\u6b65\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6210\u529f\u7387\uff0c\u4e3a\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.22927", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.22927", "abs": "https://arxiv.org/abs/2601.22927", "authors": ["Lars Ullrich", "Michael Buchholz", "Klaus Dietmayer", "Knut Graichen"], "title": "Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs", "comment": "Published in IEEE Access, 29 January 2026", "summary": "Automated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\uff0c\u5206\u6790\u4e86AI\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u5c40\u9650\u6027\u548c\u672a\u6765\u6280\u672f\u53ef\u80fd\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5411\u5b8c\u5168\u81ea\u4e3b\u9a7e\u9a76\u7684\u8fc7\u6e21\u9762\u4e34\u73b0\u5b9e\u5f00\u653e\u4e16\u754c\u7684\u6311\u6218\uff0c\u800cAI\u5728\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3001\u5904\u7406\u66f4\u9ad8\u590d\u6742\u6027\u548c\u5b9e\u73b0\u65b0\u81ea\u4e3b\u6c34\u5e73\u7684\u80fd\u529b\uff0c\u4f46\u540c\u65f6AI\u4e5f\u5e26\u6765\u4e86\u5b89\u5168\u548c\u53ef\u8fc1\u79fb\u6027\u7b49\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u73b0\u72b6\uff0c\u6982\u8ff0\u4e86\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u4e86\u53ef\u9884\u89c1\u7684\u6280\u672f\u53ef\u80fd\u6027\uff0c\u5e76\u5728\u524d\u77bb\u6027\u53d1\u5c55\u80cc\u666f\u4e0b\u8003\u5bdf\u4e86\u5404\u79cd\u6311\u6218\u3002", "result": "\u8bc6\u522b\u4e86AI\u5728\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u65b9\u9762\u5e26\u6765\u7684\u6311\u6218\u548c\u673a\u9047\uff0c\u91cd\u65b0\u5ba1\u89c6\u4e86\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u9700\u6c42\u548c\u7531\u6b64\u4ea7\u751f\u7684\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5206\u6790AI\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u91cd\u65b0\u601d\u8003\u4e86\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\uff0c\u5e76\u660e\u786e\u4e86\u76f8\u5173\u9700\u6c42\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.22930", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22930", "abs": "https://arxiv.org/abs/2601.22930", "authors": ["Xidong Li", "Mingyu Guo", "Chenchao Xu", "Bailin Li", "Wenjing Zhu", "Yangang Zou", "Rui Chen", "Zehuan Wang"], "title": "MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing \"long-tail\" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.", "AI": {"tldr": "MTDrive\u662f\u4e00\u4e2a\u591a\u8f6e\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7MLLMs\u4e0eRL\u7ed3\u5408\uff0c\u5229\u7528\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u590d\u6742\u573a\u666f\uff0c\u76f8\u6bd4\u5355\u8f6e\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709MLLMs\u4e0eRL\u7ed3\u5408\u7684\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u8f6e\u63a8\u7406\uff0c\u65e0\u6cd5\u5904\u7406\u9700\u8981\u8fed\u4ee3\u4f18\u5316\u7684\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff0c\u9650\u5236\u4e86\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faMTDrive\u591a\u8f6e\u6846\u67b6\uff0c\u5f15\u5165mtGRPO\u7b97\u6cd5\u7f13\u89e3\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u8f6e\u6b21\u76f8\u5bf9\u4f18\u52bf\u8ba1\u7b97\uff0c\u5e76\u6784\u5efa\u4ea4\u4e92\u5f0f\u8f68\u8ff9\u7406\u89e3\u6570\u636e\u96c6\u652f\u6301\u591a\u8f6e\u8bad\u7ec3\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u8f6e\u63a8\u7406\u8303\u5f0f\u7684\u6709\u6548\u6027\uff1b\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u5b9e\u73b0\u4e862.5\u500d\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "MTDrive\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u4e3aMLLMs\u4e0eRL\u7684\u6df1\u5ea6\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.22965", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22965", "abs": "https://arxiv.org/abs/2601.22965", "authors": ["Runhua Zhang", "Junyi Hou", "Changxu Cheng", "Qiyi Chen", "Tao Wang", "Wuyue Zhao"], "title": "Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation", "comment": "Preprint", "summary": "Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive \"generate-then-filter\" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.", "AI": {"tldr": "SIDP\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6a21\u4eff\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u7684\u81ea\u6a21\u4eff\u673a\u5236\u9009\u62e9\u6027\u6a21\u4eff\u81ea\u8eab\u91c7\u6837\u8f68\u8ff9\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u91c7\u6837\u548c\u540e\u8fc7\u6ee4\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u7b56\u7565\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\uff0c\u7ee7\u627f\u4e86\u4e13\u5bb6\u6f14\u793a\u7684\u6b21\u4f18\u6027\u548c\u5197\u4f59\u6027\uff0c\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u7684\"\u751f\u6210-\u8fc7\u6ee4\"\u6d41\u7a0b\u548c\u8f85\u52a9\u9009\u62e9\u5668\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u81ea\u6a21\u4eff\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u5305\u542b\u5956\u52b1\u5f15\u5bfc\u7684\u81ea\u6a21\u4eff\u673a\u5236\uff0c\u9f13\u52b1\u7b56\u7565\u6301\u7eed\u4ea7\u751f\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff1b\u91c7\u7528\u5956\u52b1\u9a71\u52a8\u7684\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\u7f13\u89e3\u6570\u636e\u5229\u7528\u6548\u7387\u95ee\u9898\uff0c\u4ee5\u53ca\u76ee\u6807\u65e0\u5173\u7684\u63a2\u7d22\u8fdb\u884c\u8f68\u8ff9\u589e\u5f3a\u3002", "result": "\u5728\u7efc\u5408\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u6709\u6548\u6027\uff1b\u5728Jetson Orin Nano\u4e0a\u63a8\u7406\u901f\u5ea6\u6bd4\u57fa\u7ebfNavDP\u5feb2.5\u500d\uff08110ms vs 273ms\uff09\u3002", "conclusion": "SIDP\u901a\u8fc7\u81ea\u6a21\u4eff\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u7b56\u7565\u7684\u6b21\u4f18\u6027\u548c\u5197\u4f59\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b9e\u65f6\u90e8\u7f72\uff0c\u4e3a\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.22988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22988", "abs": "https://arxiv.org/abs/2601.22988", "authors": ["Di Zhang", "Weicheng Duan", "Dasen Gu", "Hongye Lu", "Hai Zhang", "Hang Yu", "Junqiao Zhao", "Guang Chen"], "title": "Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation", "comment": null, "summary": "Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.", "AI": {"tldr": "MethodName\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a-\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u89c6\u56fe3D\u9884\u8bad\u7ec3\u548c\u591a\u6b65\u84b8\u998f\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c6\u56fe\u6cdb\u5316\uff0c\u5728RLBench\u4efb\u52a1\u4e0a\u5e73\u5747\u6210\u529f\u7387\u63d0\u534712.7%\uff0c\u5728\u89c6\u89d2\u53d8\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u5177\u5907\u5f3a\u5927\u7a7a\u95f4\u573a\u666f\u7406\u89e3\u548c\u8de8\u591a\u6837\u5316\u76f8\u673a\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002\u73b0\u67093D\u611f\u77e5\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u63a8\u7406\u65f6\u4f9d\u8d56\u591a\u89c6\u89d2\u89c2\u5bdf\uff08\u5728\u5355\u89c6\u56fe\u53d7\u9650\u573a\u666f\u4e2d\u4e0d\u5b9e\u7528\uff09\u3001\u4e0d\u5b8c\u6574\u7684\u573a\u666f\u5efa\u6a21\uff08\u65e0\u6cd5\u6355\u6349\u7cbe\u786e\u64cd\u4f5c\u6240\u9700\u7684\u5168\u8c8c\u548c\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7ed3\u6784\uff09\u3001\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u7684\u7b56\u7565\u8bad\u7ec3\u7b56\u7565\u6765\u4fdd\u7559\u548c\u5229\u7528\u83b7\u5f97\u76843D\u77e5\u8bc6\u3002", "method": "MethodName\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a-\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u5355\u89c6\u56fe3D\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u70b9\u4e91\u91cd\u5efa\u548c\u524d\u9988\u9ad8\u65af\u6e85\u5c04\u5728\u591a\u89c6\u56fe\u76d1\u7763\u4e0b\u5b66\u4e60\u5168\u8c8c\u51e0\u4f55\u8868\u793a\uff1b2\uff09\u7b56\u7565\u5b66\u4e60\u9636\u6bb5\uff0c\u6267\u884c\u591a\u6b65\u84b8\u998f\u4ee5\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u51e0\u4f55\u7406\u89e3\uff0c\u5e76\u5c06\u5176\u6709\u6548\u8f6c\u79fb\u5230\u64cd\u4f5c\u6280\u80fd\u4e2d\u3002", "result": "\u572812\u4e2aRLBench\u4efb\u52a1\u4e0a\uff0cMethodName\u6bd4\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5e73\u5747\u6210\u529f\u7387\u9ad8\u51fa12.7%\u3002\u57286\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u7684\u96f6\u6837\u672c\u89c6\u56fe\u6cdb\u5316\u8bc4\u4f30\u4e2d\uff0c\u5728\u4e2d\u7b49\u548c\u5927\u5e45\u89c6\u89d2\u53d8\u5316\u4e0b\uff0cMethodName\u7684\u6210\u529f\u7387\u4ec5\u4e0b\u964d22.0%\u548c29.7%\uff0c\u800c\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4e0b\u964d\u5e45\u5ea6\u66f4\u5927\uff0c\u5206\u522b\u4e3a41.6%\u548c51.5%\u3002", "conclusion": "MethodName\u901a\u8fc7\u7edf\u4e00\u7684\u8868\u793a-\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u611f\u77e5\u65b9\u6cd5\u5728\u5355\u89c6\u56fe\u573a\u666f\u3001\u51e0\u4f55\u5efa\u6a21\u5b8c\u6574\u6027\u548c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u89c6\u56fe\u6cdb\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2601.23080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.23080", "abs": "https://arxiv.org/abs/2601.23080", "authors": ["Yubiao Ma", "Han Yu", "Jiayin Xie", "Changtai Lv", "Qiang Luo", "Chi Zhang", "Yunpeng Yin", "Boyang Xing", "Xuemei Ren", "Dongdong Zheng"], "title": "Robust and Generalized Humanoid Motion Tracking", "comment": null, "summary": "Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u529b\u5b66\u6761\u4ef6\u547d\u4ee4\u805a\u5408\u6846\u67b6\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u56e0\u679c\u65f6\u95f4\u7f16\u7801\u5668\u548c\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u53c2\u8003\u8fd0\u52a8\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u7ed3\u5408\u8dcc\u5012\u6062\u590d\u8bfe\u7a0b\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u4ec5\u97003.5\u5c0f\u65f6\u8fd0\u52a8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3", "motivation": "\u5b66\u4e60\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u5668\u9762\u4e34\u6311\u6218\uff1a\u53c2\u8003\u8fd0\u52a8\u8f6c\u79fb\u5230\u673a\u5668\u4eba\u57df\u540e\u53ef\u80fd\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u95ed\u73af\u6267\u884c\u4f1a\u653e\u5927\u5c40\u90e8\u7f3a\u9677\uff0c\u5bfc\u81f4\u9ad8\u52a8\u6001\u548c\u63a5\u89e6\u4e30\u5bcc\u884c\u4e3a\u4e2d\u7684\u6f02\u79fb\u6216\u5931\u8d25", "method": "\u63d0\u51fa\u52a8\u529b\u5b66\u6761\u4ef6\u547d\u4ee4\u805a\u5408\u6846\u67b6\uff1a1) \u4f7f\u7528\u56e0\u679c\u65f6\u95f4\u7f16\u7801\u5668\u603b\u7ed3\u8fd1\u671f\u672c\u4f53\u611f\u77e5\uff1b2) \u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u547d\u4ee4\u7f16\u7801\u5668\u57fa\u4e8e\u5f53\u524d\u52a8\u529b\u5b66\u9009\u62e9\u6027\u5730\u805a\u5408\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b3) \u96c6\u6210\u8dcc\u5012\u6062\u590d\u8bfe\u7a0b\uff0c\u5305\u542b\u968f\u673a\u4e0d\u7a33\u5b9a\u521d\u59cb\u5316\u548c\u9000\u706b\u5411\u4e0a\u8f85\u52a9\u529b", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700\u7ea63.5\u5c0f\u65f6\u8fd0\u52a8\u6570\u636e\uff0c\u652f\u6301\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u8bad\u7ec3\u65e0\u9700\u84b8\u998f\u3002\u5728\u591a\u6837\u5316\u53c2\u8003\u8f93\u5165\u548c\u6311\u6218\u6027\u8fd0\u52a8\u673a\u5236\u4e0b\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u672a\u89c1\u8fd0\u52a8\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u7269\u7406\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u9c81\u68d2\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb", "conclusion": "\u63d0\u51fa\u7684\u52a8\u529b\u5b66\u6761\u4ef6\u547d\u4ee4\u805a\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u53c2\u8003\u8fd0\u52a8\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u7ed3\u5408\u8dcc\u5012\u6062\u590d\u8bfe\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u5668\u7684\u9c81\u68d2\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u548c\u53ef\u9760\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb"}}
{"id": "2601.23087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.23087", "abs": "https://arxiv.org/abs/2601.23087", "authors": ["Wu Songwei", "Jiang Zhiduo", "Xie Guanghu", "Liu Yang", "Liu Hong"], "title": "Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation", "comment": "8 pages, 8 figures", "summary": "Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space.\n  We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution.\n  LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.", "AI": {"tldr": "LG-Flow Policy\u662f\u4e00\u79cd\u8f68\u8ff9\u7ea7\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u6d41\u5339\u914d\uff0c\u5b9e\u73b0\u5feb\u901f\u5355\u6b65\u63a8\u7406\u548c\u7a33\u5b9a\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548c\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u6d41\u5339\u914d\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u7b56\u7565\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u6311\u6218\uff1a\u6269\u6563\u6a21\u578b\u5efa\u6a21\u80fd\u529b\u5f3a\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u6d41\u5339\u914d\u53ef\u5b9e\u73b0\u5feb\u901f\u5355\u6b65\u751f\u6210\u4f46\u5728\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u76f4\u63a5\u5e94\u7528\u65f6\u6267\u884c\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u8868\u8fbe\u6027\u884c\u4e3a\u5efa\u6a21\u3001\u5b9e\u65f6\u63a8\u7406\u548c\u7a33\u5b9a\u6267\u884c\u3002", "method": "\u63d0\u51faLG-Flow Policy\u6846\u67b6\uff1a1\uff09\u5c06\u52a8\u4f5c\u5e8f\u5217\u7f16\u7801\u4e3a\u65f6\u95f4\u6b63\u5219\u5316\u7684\u6f5c\u5728\u8f68\u8ff9\uff1b2\uff09\u5728\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u5b66\u4e60\u663e\u5f0f\u6d41\u5339\u914d\uff1b3\uff09\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u70b9\u4e91\u6761\u4ef6\uff1b4\uff09\u6267\u884c\u65f6\u591a\u6a21\u6001\u8c03\u5236\uff08\u4ee5\u89c6\u89c9\u7ebf\u7d22\u4e3a\u4ee3\u8868\uff09\u3002\u8be5\u65b9\u6cd5\u5c06\u5168\u5c40\u8fd0\u52a8\u7ed3\u6784\u4e0e\u4f4e\u7ea7\u63a7\u5236\u566a\u58f0\u89e3\u8026\u3002", "result": "\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aLG-Flow Policy\u5b9e\u73b0\u63a5\u8fd1\u5355\u6b65\u63a8\u7406\uff0c\u76f8\u6bd4\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u7684\u6d41\u5339\u914d\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e14\u6bd4\u6269\u6563\u7b56\u7565\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "LG-Flow Policy\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6d41\u5339\u914d\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5efa\u6a21\u80fd\u529b\u3001\u63a8\u7406\u901f\u5ea6\u548c\u6267\u884c\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.23285", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.23285", "abs": "https://arxiv.org/abs/2601.23285", "authors": ["MH Farhadi", "Ali Rabiee", "Sima Ghafoori", "Anna Cetera", "Andrew Fisher", "Reza Abiri"], "title": "End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms", "comment": null, "summary": "Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.", "AI": {"tldr": "BRACE\u6846\u67b6\u901a\u8fc7\u7aef\u5230\u7aef\u68af\u5ea6\u6d41\u6574\u5408\u8d1d\u53f6\u65af\u610f\u56fe\u63a8\u65ad\u548c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u8f85\u52a9\uff0c\u5728\u590d\u6742\u3001\u76ee\u6807\u6a21\u7cca\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u5171\u4eab\u81ea\u4e3b\u7cfb\u7edf\u7684\u6027\u80fd", "motivation": "\u5171\u4eab\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u4ece\u7528\u6237\u610f\u56fe\u63a8\u65ad\u5230\u8f85\u52a9\u6c34\u5e73\u786e\u5b9a\u7684\u7edf\u4e00\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6df7\u5408\u6bd4\u4f8b\u6216\u5c06\u76ee\u6807\u63a8\u65ad\u4e0e\u8f85\u52a9\u4ef2\u88c1\u5206\u79bb\uff0c\u5bfc\u81f4\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6027\u80fd\u4e0d\u4f73", "method": "\u63d0\u51faBRACE\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u68af\u5ea6\u6d41\u67b6\u6784\u5fae\u8c03\u8d1d\u53f6\u65af\u610f\u56fe\u63a8\u65ad\u548c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u8f85\u52a9\uff0c\u5c06\u534f\u4f5c\u63a7\u5236\u7b56\u7565\u57fa\u4e8e\u73af\u5883\u4e0a\u4e0b\u6587\u548c\u5b8c\u6574\u76ee\u6807\u6982\u7387\u5206\u5e03", "result": "\u76f8\u6bd4SOTA\u65b9\u6cd5\uff08IDA\u3001DQN\uff09\uff0c\u5728\u4e09\u4e2a\u9010\u6b65\u5206\u79bb\u7684\u8bc4\u4f30\u4e2d\uff1a2D\u5149\u6807\u4efb\u52a1\u3001\u673a\u5668\u4eba\u624b\u81c2\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3001\u76ee\u6807\u6a21\u7cca\u548c\u73af\u5883\u7ea6\u675f\u4e0b\u7684\u96c6\u6210\u64cd\u4f5c\uff0c\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u5206\u522b\u63d0\u53476.3%\u548c41%\uff0c\u76f8\u6bd4\u65e0\u8f85\u52a9\u63a7\u5236\u63d0\u534736.3%\u548c87%", "conclusion": "\u96c6\u6210\u4f18\u5316\u5728\u590d\u6742\u3001\u76ee\u6807\u6a21\u7cca\u573a\u666f\u4e2d\u6700\u4e3a\u6709\u76ca\uff0c\u53ef\u63a8\u5e7f\u5230\u9700\u8981\u76ee\u6807\u5bfc\u5411\u8f85\u52a9\u7684\u673a\u5668\u4eba\u9886\u57df\uff0c\u63a8\u8fdb\u4e86\u81ea\u9002\u5e94\u5171\u4eab\u81ea\u4e3b\u7cfb\u7edf\u7684\u6280\u672f\u524d\u6cbf"}}
