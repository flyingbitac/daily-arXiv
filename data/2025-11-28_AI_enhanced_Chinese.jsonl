{"id": "2511.21666", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21666", "abs": "https://arxiv.org/abs/2511.21666", "authors": ["Lorenzo Shaikewitz", "Charis Georgiou", "Luca Carlone"], "title": "Uncertainty Quantification for Visual Object Pose Estimation", "comment": "18 pages, 9 figures. Code available: https://github.com/MIT-SPARK/PoseUncertaintySets", "summary": "Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.", "AI": {"tldr": "SLUE\u65b9\u6cd5\u901a\u8fc7\u51f8\u4f18\u5316\u751f\u6210\u5305\u542b\u771f\u5b9e\u7269\u4f53\u59ff\u6001\u7684\u9ad8\u6982\u7387\u692d\u7403\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u4ec5\u97002D\u8bed\u4e49\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u566a\u58f0\u8fb9\u754c\uff0c\u65e0\u9700\u521d\u59cb\u731c\u6d4b\u6216\u4e25\u683c\u5206\u5e03\u5047\u8bbe\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\uff0c\u91cf\u5316\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u9c81\u68d2\u63a7\u5236\u548c\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e25\u683c\u7684\u5206\u5e03\u5047\u8bbe\uff0c\u7f3a\u4e4f\u7edf\u8ba1\u4e0a\u4e25\u8c28\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86SLUE\uff08S-Lemma\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff09\u51f8\u4f18\u5316\u7a0b\u5e8f\uff0c\u5c06\u975e\u51f8\u7684\u4f4d\u59ff\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u96c6\u7b80\u5316\u4e3a\u5355\u4e2a\u692d\u7403\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u5e76\u6269\u5c55\u5230\u5e73\u65b9\u548c\u677e\u5f1b\u5c42\u6b21\u4ee5\u83b7\u5f97\u66f4\u7d27\u5bc6\u7684\u8fb9\u754c\u3002", "result": "\u5728\u4e24\u4e2a\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSLUE\u751f\u6210\u663e\u8457\u66f4\u5c0f\u7684\u5e73\u79fb\u8fb9\u754c\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u65b9\u5411\u8fb9\u754c\u3002", "conclusion": "SLUE\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u5e03\u81ea\u7531\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u59ff\u6001\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ec5\u97002D\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u566a\u58f0\u8fb9\u754c\uff0c\u80fd\u591f\u751f\u6210\u7edf\u8ba1\u4e0a\u4e25\u8c28\u4e14\u7d27\u51d1\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u3002"}}
{"id": "2511.21690", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21690", "abs": "https://arxiv.org/abs/2511.21690", "authors": ["Seungjae Lee", "Yoonkyo Jung", "Inkook Chun", "Yao-Chih Lee", "Zikui Cai", "Hongjia Huang", "Aayush Talreja", "Tan Dat Dao", "Yongyuan Liang", "Jia-Bin Huang", "Furong Huang"], "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos", "comment": null, "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.", "AI": {"tldr": "TraceGen\u662f\u4e00\u4e2a\u4ece\u5c11\u91cf\u6f14\u793a\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u4efb\u52a1\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u76843D\u8f68\u8ff9\u7a7a\u95f4\u8868\u793a\u5b9e\u73b0\u8de8\u5e73\u53f0\u3001\u8de8\u73af\u5883\u548c\u8de8\u4efb\u52a1\u7684\u5b66\u4e60\uff0c\u4ec5\u97005\u4e2a\u76ee\u6807\u673a\u5668\u4eba\u89c6\u9891\u5373\u53ef\u8fbe\u523080%\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u4ece\u5c11\u91cf\u6f14\u793a\u5b66\u4e60\u65b0\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5229\u7528\u4e30\u5bcc\u7684\u8de8\u5e73\u53f0\u89c6\u9891\u8d44\u6e90\uff08\u4eba\u7c7b\u548c\u4e0d\u540c\u673a\u5668\u4eba\uff09\uff0c\u514b\u670d\u5e73\u53f0\u5dee\u5f02\u3001\u76f8\u673a\u5dee\u5f02\u548c\u73af\u5883\u5dee\u5f02\u5e26\u6765\u7684\u969c\u788d\u3002", "method": "\u63d0\u51faTraceGen\u4e16\u754c\u6a21\u578b\uff0c\u57283D\u8f68\u8ff9\u7a7a\u95f4\u800c\u975e\u50cf\u7d20\u7a7a\u95f4\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\uff0c\u4fdd\u7559\u51e0\u4f55\u7ed3\u6784\u540c\u65f6\u62bd\u8c61\u5916\u89c2\u5dee\u5f02\uff1b\u5f00\u53d1TraceForge\u6570\u636e\u7ba1\u9053\u5c06\u5f02\u6784\u89c6\u9891\u8f6c\u6362\u4e3a\u4e00\u81f4\u76843D\u8f68\u8ff9\uff0c\u6784\u5efa\u5305\u542b123K\u89c6\u9891\u548c1.8M\u4e09\u5143\u7ec4\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\u4e0a\u4ec5\u97005\u4e2a\u76ee\u6807\u673a\u5668\u4eba\u89c6\u9891\u5373\u53ef\u8fbe\u523080%\u6210\u529f\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u6700\u5148\u8fdb\u7684\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5feb50-600\u500d\uff1b\u5728\u4ec5\u67095\u4e2a\u624b\u6301\u624b\u673a\u62cd\u6444\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u65f6\uff0c\u4ecd\u80fd\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fbe\u523067.5%\u6210\u529f\u7387\u3002", "conclusion": "TraceGen\u901a\u8fc73D\u8f68\u8ff9\u7a7a\u95f4\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u5e73\u53f0\u4efb\u52a1\u5b66\u4e60\uff0c\u65e0\u9700\u4f9d\u8d56\u7269\u4f53\u68c0\u6d4b\u5668\u6216\u7e41\u91cd\u7684\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\uff0c\u5728\u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
