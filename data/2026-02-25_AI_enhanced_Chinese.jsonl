{"id": "2602.20200", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20200", "abs": "https://arxiv.org/abs/2602.20200", "authors": ["Zaijing Li", "Bing Hu", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Pengwei Xie", "Jianye Hao", "Liqiang Nie"], "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation", "comment": "17 pages, 8 figures", "summary": "Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.", "AI": {"tldr": "OptimusVLA\uff1a\u4e00\u79cd\u5177\u6709\u5168\u5c40\u5148\u9a8c\u8bb0\u5fc6\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u8bb0\u5fc6\u7684\u53cc\u8bb0\u5fc6VLA\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7ea7\u5148\u9a8c\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u7ea6\u675f\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u63a8\u7406\u6548\u7387\u4f4e\u548c\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a1\uff09\u63a8\u7406\u6548\u7387\u4f4e\uff0c\u5404\u5411\u540c\u6027\u566a\u58f0\u5148\u9a8c\u4e0e\u76ee\u6807\u52a8\u4f5c\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5206\u5e03\u5dee\u8ddd\uff0c\u5bfc\u81f4\u53bb\u566a\u6b65\u9aa4\u589e\u591a\u548c\u4e0d\u53ef\u884c\u6837\u672c\u589e\u52a0\uff1b2\uff09\u9c81\u68d2\u6027\u5dee\uff0c\u73b0\u6709\u7b56\u7565\u4ec5\u57fa\u4e8e\u5f53\u524d\u89c2\u6d4b\uff0c\u5ffd\u7565\u4e86\u5386\u53f2\u5e8f\u5217\u7ea6\u675f\uff0c\u7f3a\u4e4f\u4efb\u52a1\u8fdb\u5ea6\u610f\u8bc6\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faOptimusVLA\u53cc\u8bb0\u5fc6\u6846\u67b6\uff1a1\uff09\u5168\u5c40\u5148\u9a8c\u8bb0\u5fc6\uff08GPM\uff09\uff1a\u7528\u4ece\u8bed\u4e49\u76f8\u4f3c\u8f68\u8ff9\u4e2d\u68c0\u7d22\u7684\u4efb\u52a1\u7ea7\u5148\u9a8c\u66ff\u4ee3\u9ad8\u65af\u566a\u58f0\uff0c\u7f29\u77ed\u751f\u6210\u8def\u5f84\u5e76\u51cf\u5c11\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff1b2\uff09\u5c40\u90e8\u4e00\u81f4\u6027\u8bb0\u5fc6\uff08LCM\uff09\uff1a\u52a8\u6001\u5efa\u6a21\u5df2\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u4ee5\u63a8\u65ad\u4efb\u52a1\u8fdb\u5ea6\uff0c\u5e76\u6ce8\u5165\u5b66\u4e60\u5230\u7684\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u589e\u5f3a\u8f68\u8ff9\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u5e73\u6ed1\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aLIBERO\u4e0a\u8fbe\u523098.6%\u5e73\u5747\u6210\u529f\u7387\uff0cCALVIN\u4e0a\u6bd4pi_0\u63d0\u534713.5%\uff0cRoboTwin 2.0 Hard\u4e0a\u8fbe\u523038%\u5e73\u5747\u6210\u529f\u7387\u3002\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0c\u5728\u6cdb\u5316\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5206\u522b\u8d85\u8d8api_0 42.9%\u548c52.4%\uff0c\u540c\u65f6\u5b9e\u73b02.9\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "OptimusVLA\u901a\u8fc7\u5f15\u5165\u53cc\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u63a8\u7406\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u5c55\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5206\u5c42VLA\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20215", "abs": "https://arxiv.org/abs/2602.20215", "authors": ["Jiyuan Zhao", "Zhengyu Shi", "Wentong Tian", "Tianliang Yao", "Dong Liu", "Tao Liu", "Yizhe Wu", "Peng Qi"], "title": "Vision-Based Reasoning with Topology-Encoded Graphs for Anatomical Path Disambiguation in Robot-Assisted Endovascular Navigation", "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 3 figures, 3 tables", "summary": "Robotic-assisted percutaneous coronary intervention (PCI) is constrained by the inherent limitations of 2D Digital Subtraction Angiography (DSA). Unlike physicians, who can directly manipulate guidewires and integrate tactile feedback with their prior anatomical knowledge, teleoperated robotic systems must rely solely on 2D projections. This mode of operation, simultaneously lacking spatial context and tactile sensation, may give rise to projection-induced ambiguities at vascular bifurcations. To address this challenge, we propose a two-stage framework (SCAR-UNet-GAT) for real-time robotic path planning. In the first stage, SCAR-UNet, a spatial-coordinate-attention-regularized U-Net, is employed for accurate coronary vessel segmentation. The integration of multi-level attention mechanisms enhances the delineation of thin, tortuous vessels and improves robustness against imaging noise. From the resulting binary masks, vessel centerlines and bifurcation points are extracted, and geometric descriptors (e.g., branch diameter, intersection angles) are fused with local DSA patches to construct node features. In the second stage, a Graph Attention Network (GAT) reasons over the vessel graph to identify anatomically consistent and clinically feasible trajectories, effectively distinguishing true bifurcations from projection-induced false crossings. On a clinical DSA dataset, SCAR-UNet achieved a Dice coefficient of 93.1%. For path disambiguation, the proposed GAT-based method attained a success rate of 95.0% and a target-arrival success rate of 90.0%, substantially outperforming conventional shortest-path planning (60.0% and 55.0%) and heuristic-based planning (75.0% and 70.0%). Validation on a robotic platform further confirmed the practical feasibility and robustness of the proposed framework.", "AI": {"tldr": "\u63d0\u51faSCAR-UNet-GAT\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9PCI\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\uff0c\u89e3\u51b32D DSA\u6295\u5f71\u5bfc\u81f4\u7684\u8840\u7ba1\u5206\u53c9\u6b67\u4e49\u95ee\u9898", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9PCI\u53d7\u9650\u4e8e2D DSA\u6295\u5f71\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u5bfc\u81f4\u8840\u7ba1\u5206\u53c9\u5904\u51fa\u73b0\u6295\u5f71\u6b67\u4e49\uff0c\u9700\u8981\u667a\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528SCAR-UNet\u8fdb\u884c\u51a0\u72b6\u52a8\u8109\u8840\u7ba1\u5206\u5272\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528GAT\u5728\u56fe\u7ed3\u6784\u4e0a\u63a8\u7406\uff0c\u8bc6\u522b\u89e3\u5256\u4e00\u81f4\u4e14\u4e34\u5e8a\u53ef\u884c\u7684\u8f68\u8ff9", "result": "SCAR-UNet Dice\u7cfb\u657093.1%\uff1bGAT\u8def\u5f84\u6d88\u6b67\u6210\u529f\u738795.0%\uff0c\u76ee\u6807\u5230\u8fbe\u6210\u529f\u738790.0%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6700\u77ed\u8def\u5f84\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684SCAR-UNet-GAT\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b32D DSA\u6295\u5f71\u6b67\u4e49\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9PCI\u63d0\u4f9b\u53ef\u9760\u8def\u5f84\u89c4\u5212"}}
{"id": "2602.20216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20216", "abs": "https://arxiv.org/abs/2602.20216", "authors": ["Hao Wang", "Tianliang Yao", "Bo Lu", "Zhiqiang Pei", "Liu Dong", "Lei Ma", "Peng Qi"], "title": "Sample-Efficient Learning with Online Expert Correction for Autonomous Catheter Steering in Endovascular Bifurcation Navigation", "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 5 figures, 1 table", "summary": "Robot-assisted endovascular intervention offers a safe and effective solution for remote catheter manipulation, reducing radiation exposure while enabling precise navigation. Reinforcement learning (RL) has recently emerged as a promising approach for autonomous catheter steering; however, conventional methods suffer from sparse reward design and reliance on static vascular models, limiting their sample efficiency and generalization to intraoperative variations. To overcome these challenges, this paper introduces a sample-efficient RL framework with online expert correction for autonomous catheter steering in endovascular bifurcation navigation. The proposed framework integrates three key components: (1) A segmentation-based pose estimation module for accurate real-time state feedback, (2) A fuzzy controller for bifurcation-aware orientation adjustment, and (3) A structured reward generator incorporating expert priors to guide policy learning. By leveraging online expert correction, the framework reduces exploration inefficiency and enhances policy robustness in complex vascular structures. Experimental validation on a robotic platform using a transparent vascular phantom demonstrates that the proposed approach achieves convergence in 123 training episodes -- a 25.9% reduction compared to the baseline Soft Actor-Critic (SAC) algorithm -- while reducing average positional error to 83.8% of the baseline. These results indicate that combining sample-efficient RL with online expert correction enables reliable and accurate catheter steering, particularly in anatomically challenging bifurcation scenarios critical for endovascular navigation.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5728\u7ebf\u4e13\u5bb6\u6821\u6b63\u7684\u6837\u672c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8840\u7ba1\u5206\u53c9\u5bfc\u822a\u4e2d\u7684\u81ea\u4e3b\u5bfc\u7ba1\u64cd\u63a7\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8bad\u7ec3\u6536\u655b\u66f4\u5feb\u3001\u5b9a\u4f4d\u8bef\u5dee\u66f4\u5c0f\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u4e3b\u5bfc\u7ba1\u64cd\u63a7\u4e2d\u5b58\u5728\u5956\u52b1\u7a00\u758f\u3001\u4f9d\u8d56\u9759\u6001\u8840\u7ba1\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u9002\u5e94\u672f\u4e2d\u53d8\u5316\u3002\u9700\u8981\u5f00\u53d1\u80fd\u9ad8\u6548\u5b66\u4e60\u3001\u9002\u5e94\u590d\u6742\u8840\u7ba1\u7ed3\u6784\u7684\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u7ec4\u4ef6\u6846\u67b6\uff1a1)\u57fa\u4e8e\u5206\u5272\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u63d0\u4f9b\u5b9e\u65f6\u72b6\u6001\u53cd\u9988\uff1b2)\u6a21\u7cca\u63a7\u5236\u5668\u8fdb\u884c\u5206\u53c9\u611f\u77e5\u7684\u65b9\u5411\u8c03\u6574\uff1b3)\u7ed3\u5408\u4e13\u5bb6\u5148\u9a8c\u7684\u7ed3\u6784\u5316\u5956\u52b1\u751f\u6210\u5668\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u901a\u8fc7\u5728\u7ebf\u4e13\u5bb6\u6821\u6b63\u51cf\u5c11\u63a2\u7d22\u4f4e\u6548\u6027\u3002", "result": "\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u4f7f\u7528\u900f\u660e\u8840\u7ba1\u6a21\u578b\u9a8c\u8bc1\uff0c\u4ec5\u9700123\u4e2a\u8bad\u7ec3\u56de\u5408\u5373\u53ef\u6536\u655b\uff0c\u6bd4\u57fa\u7ebfSAC\u7b97\u6cd5\u51cf\u5c1125.9%\u8bad\u7ec3\u91cf\uff0c\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u964d\u81f3\u57fa\u7ebf\u768483.8%\u3002", "conclusion": "\u7ed3\u5408\u6837\u672c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u4e0e\u5728\u7ebf\u4e13\u5bb6\u6821\u6b63\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u51c6\u786e\u7684\u5bfc\u7ba1\u64cd\u63a7\uff0c\u7279\u522b\u662f\u5728\u8840\u7ba1\u5206\u53c9\u7b49\u89e3\u5256\u7ed3\u6784\u590d\u6742\u7684\u573a\u666f\u4e2d\uff0c\u4e3a\u8840\u7ba1\u5185\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20219", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20219", "abs": "https://arxiv.org/abs/2602.20219", "authors": ["Guanting Shen", "Zi Tian"], "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction", "comment": "Preprint currently under revision", "summary": "Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u8bed\u97f3\u5904\u7406\u548c\u6a21\u7cca\u903b\u8f91\u7684\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u63a7\u5236Dobot\u673a\u68b0\u81c2\uff0c\u901a\u8fc7\u8bed\u97f3\u547d\u4ee4\u5b9e\u73b0\u7269\u4f53\u64cd\u4f5c\uff0c\u5b9e\u9a8c\u663e\u793a75%\u7684\u547d\u4ee4\u6267\u884c\u51c6\u786e\u7387\u3002", "motivation": "\u51c6\u786e\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u662f\u4eba\u673a\u4ea4\u4e92\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e5f\u662f\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u76f4\u89c2\u7684\u4eba\u673a\u534f\u4f5c\u7684\u5173\u952e\u8981\u6c42\u3002\u5f53\u524d\u9700\u8981\u66f4\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u7cfb\u7edf\u6765\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u81ea\u7136\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001HRI\u6846\u67b6\uff0c\u7ed3\u5408Florence-2\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0cLlama 3.1\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff0cWhisper\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0c\u5e76\u4f7f\u7528\u6a21\u7cca\u903b\u8f91\u5b9e\u73b0\u7cbe\u786e\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002", "result": "\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e8675%\u7684\u547d\u4ee4\u6267\u884c\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u672a\u6765HRI\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u7684\u8bed\u97f3\u548c\u89c6\u89c9\u8bed\u8a00\u5904\u7406\uff0c\u4e3a\u5b9e\u73b0\u66f4\u590d\u6742\u3001\u66f4\u81ea\u7136\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2602.20220", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20220", "abs": "https://arxiv.org/abs/2602.20220", "authors": ["Yarden As", "Dhruva Tirumala", "Ren\u00e9 Zurbr\u00fcgg", "Chenhao Li", "Stelian Coros", "Andreas Krause", "Markus Wulfmeier"], "title": "What Matters for Simulation to Online Reinforcement Learning on Real Robots", "comment": null, "summary": "We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7100\u6b21\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u6210\u529f\u8bbe\u8ba1\u9009\u62e9\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u5e38\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u7684\u5371\u5bb3\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u4e00\u5957\u7a33\u5065\u7684\u8bbe\u8ba1\u65b9\u6848", "motivation": "\u7814\u7a76\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5b9e\u65bd\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5177\u4f53\u8bbe\u8ba1\u9009\u62e9\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u901a\u5e38\u9690\u542b\u7684\u8bbe\u8ba1\u51b3\u7b56\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d", "method": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86100\u6b21\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6027\u5730\u6d88\u878d\u5206\u6790\u4e86\u7b97\u6cd5\u3001\u7cfb\u7edf\u548c\u5b9e\u9a8c\u51b3\u7b56\uff0c\u8fd9\u4e9b\u51b3\u7b56\u5728\u5148\u524d\u5de5\u4f5c\u4e2d\u901a\u5e38\u88ab\u9690\u542b\u5904\u7406", "result": "\u53d1\u73b0\u4e00\u4e9b\u5e7f\u6cdb\u4f7f\u7528\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u53ef\u80fd\u6709\u5bb3\uff0c\u800c\u6807\u51c6RL\u5b9e\u8df5\u4e2d\u4e00\u5957\u7a33\u5065\u4e14\u6613\u4e8e\u91c7\u7528\u7684\u8bbe\u8ba1\u9009\u62e9\u80fd\u591f\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u786c\u4ef6\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u5b66\u4e60", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5bf9\u6b64\u7c7b\u8bbe\u8ba1\u9009\u62e9\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u5de5\u7a0b\u52aa\u529b\u90e8\u7f72\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60"}}
{"id": "2602.20225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20225", "abs": "https://arxiv.org/abs/2602.20225", "authors": ["Yichang Feng", "Xiao Liang", "Minghui Zheng"], "title": "FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators", "comment": null, "summary": "This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.", "AI": {"tldr": "FACTO\u662f\u4e00\u79cd\u7528\u4e8e\u5355\u81c2\u548c\u591a\u81c2\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728\u7cfb\u6570\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u7528\u6b63\u4ea4\u57fa\u51fd\u6570\u53c2\u6570\u5316\u8f68\u8ff9\uff0c\u901a\u8fc7\u9ad8\u65af-\u725b\u987f\u8fd1\u4f3c\u548c\u81ea\u9002\u5e94\u7ea6\u675f\u66f4\u65b0\u89e3\u51b3\u975e\u7ebf\u6027\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u89e3\u51b3\u5355\u81c2\u548c\u591a\u81c2\u673a\u68b0\u81c2\u7684\u7ea6\u675f\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u65f6\uff0c\u5728\u89e3\u7684\u8d28\u91cf\u548c\u53ef\u884c\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7ea6\u675f\u573a\u666f\u4e0b\u9700\u8981\u66f4\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u6b63\u4ea4\u57fa\u51fd\u6570\u7ebf\u6027\u7ec4\u5408\u53c2\u6570\u5316\u8f68\u8ff9\u8868\u793a\uff0c\u76f4\u63a5\u5728\u7cfb\u6570\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\u3002\u91c7\u7528\u9ad8\u65af-\u725b\u987f\u8fd1\u4f3c\u914d\u5408\u6307\u6570\u79fb\u52a8\u5e73\u5747\u5904\u7406\u975e\u7ebf\u6027\uff0c\u4f7f\u7528\u7cfb\u6570\u7a7a\u95f4\u6620\u5c04\u5904\u7406\u8f68\u8ff9\u8303\u56f4\u7ea6\u675f\uff0c\u5728\u6d3b\u52a8\u7ea6\u675f\u7684\u96f6\u7a7a\u95f4\u4e2d\u4f7f\u7528Levenberg-Marquardt\u7b97\u6cd5\u8fdb\u884c\u81ea\u9002\u5e94\u7ea6\u675f\u66f4\u65b0\u3002", "result": "\u4e0e\u4f18\u5316\u578b\u89c4\u5212\u5668\uff08CHOMP\u3001TrajOpt\u3001GPMP2\uff09\u548c\u91c7\u6837\u578b\u89c4\u5212\u5668\uff08RRT-Connect\u3001RRT*\u3001PRM\uff09\u76f8\u6bd4\uff0cFACTO\u5728\u89e3\u7684\u8d28\u91cf\u548c\u53ef\u884c\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u7ea6\u675f\u5355\u81c2\u548c\u591a\u81c2\u573a\u666f\u4e2d\u3002\u5728Franka\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "FACTO\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5355\u81c2\u548c\u591a\u81c2\u673a\u68b0\u81c2\u7684\u590d\u6742\u7ea6\u675f\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u5728\u89e3\u7684\u8d28\u91cf\u548c\u53ef\u884c\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.20231", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20231", "abs": "https://arxiv.org/abs/2602.20231", "authors": ["Manish Kumar Govind", "Dominick Reilly", "Pu Wang", "Srijan Das"], "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models", "comment": "https://manishgovind.github.io/unilact-vla/", "summary": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.", "AI": {"tldr": "UniLACT\uff1a\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6f5c\u5728\u9884\u8bad\u7ec3\u5c06\u51e0\u4f55\u7ed3\u6784\u878d\u5165\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u63d0\u5347\u63a5\u89e6\u5f0f\u64cd\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u4e3b\u8981\u7f16\u7801\u5916\u89c2\u9a71\u52a8\u7684\u52a8\u6001\uff0c\u7f3a\u4e4f\u660e\u786e\u76843D\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd9\u5bf9\u4e8e\u7cbe\u786e\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51faUniLACT\uff08\u57fa\u4e8eTransformer\u7684VLA\u6a21\u578b\uff09\u548cUniLARN\uff08\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u6846\u67b6\uff09\uff0c\u901a\u8fc7\u9006\u52a8\u529b\u5b66\u548c\u6b63\u52a8\u529b\u5b66\u76ee\u6807\u5b66\u4e60RGB\u548c\u6df1\u5ea6\u7684\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\uff0c\u663e\u5f0f\u5efa\u6a21\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u4e3aUniLACT\u63d0\u4f9b\u6df1\u5ea6\u611f\u77e5\u9884\u8bad\u7ec3", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cUniLACT\u5728\u57df\u5185\u548c\u57df\u5916\u9884\u8bad\u7ec3\u673a\u5236\u4e0b\uff0c\u4ee5\u53ca\u5728\u5df2\u89c1\u548c\u672a\u89c1\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0c\u5747\u4f18\u4e8e\u57fa\u4e8eRGB\u7684\u6f5c\u5728\u52a8\u4f5c\u57fa\u7ebf", "conclusion": "\u6df1\u5ea6\u611f\u77e5\u7684\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u611f\u77e5\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d"}}
{"id": "2602.20304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20304", "abs": "https://arxiv.org/abs/2602.20304", "authors": ["Onur Beker", "Andreas Ren\u00e9 Geist", "Anselm Paulus", "Nico G\u00fcrtler", "Ji Shi", "Sylvain Calinon", "Georg Martius"], "title": "Smoothly Differentiable and Efficiently Vectorizable Contact Manifold Generation", "comment": null, "summary": "Simulating rigid-body dynamics with contact in a fast, massively vectorizable, and smoothly differentiable manner is highly desirable in robotics. An important bottleneck faced by existing differentiable simulation frameworks is contact manifold generation: representing the volume of intersection between two colliding geometries via a discrete set of properly distributed contact points. A major factor contributing to this bottleneck is that the related routines of commonly used robotics simulators were not designed with vectorization and differentiability as a primary concern, and thus rely on logic and control flow that hinder these goals. We instead propose a framework designed from the ground up with these goals in mind, by trying to strike a middle ground between: i) convex primitive based approaches used by common robotics simulators (efficient but not differentiable), and ii) mollified vertex-face and edge-edge unsigned distance-based approaches used by barrier methods (differentiable but inefficient). Concretely, we propose: i) a representative set of smooth analytical signed distance primitives to implement vertex-face collisions, and ii) a novel differentiable edge-edge collision routine that can provide signed distances and signed contact normals. The proposed framework is evaluated via a set of didactic experiments and benchmarked against the collision detection routine of the well-established Mujoco XLA framework, where we observe a significant speedup. Supplementary videos can be found at https://github.com/bekeronur/contax, where a reference implementation in JAX will also be made available at the conclusion of the review process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u521a\u4f53\u52a8\u529b\u5b66\u63a5\u89e6\u6a21\u62df\u7684\u5feb\u901f\u3001\u53ef\u5411\u91cf\u5316\u3001\u5e73\u6ed1\u53ef\u5fae\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u53ef\u5fae\u6a21\u62df\u6846\u67b6\u4e2d\u63a5\u89e6\u6d41\u5f62\u751f\u6210\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6a21\u62df\u5668\u7684\u63a5\u89e6\u68c0\u6d4b\u4f8b\u7a0b\u5728\u8bbe\u8ba1\u65f6\u672a\u5145\u5206\u8003\u8651\u5411\u91cf\u5316\u548c\u53ef\u5fae\u6027\uff0c\u5bfc\u81f4\u5728\u53ef\u5fae\u6a21\u62df\u4e2d\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u6027\u53c8\u80fd\u5b9e\u73b0\u5e73\u6ed1\u53ef\u5fae\u7684\u63a5\u89e6\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\uff1a1\uff09\u4f7f\u7528\u5e73\u6ed1\u89e3\u6790\u7b26\u53f7\u8ddd\u79bb\u57fa\u5143\u5b9e\u73b0\u9876\u70b9-\u9762\u78b0\u649e\uff1b2\uff09\u63d0\u51fa\u65b0\u9896\u7684\u53ef\u5fae\u8fb9-\u8fb9\u78b0\u649e\u4f8b\u7a0b\uff0c\u80fd\u591f\u63d0\u4f9b\u7b26\u53f7\u8ddd\u79bb\u548c\u7b26\u53f7\u63a5\u89e6\u6cd5\u7ebf\u3002", "result": "\u901a\u8fc7\u6559\u5b66\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u6210\u719f\u7684Mujoco XLA\u6846\u67b6\u7684\u78b0\u649e\u68c0\u6d4b\u4f8b\u7a0b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89c2\u5bdf\u5230\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u53ef\u5fae\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u521a\u4f53\u52a8\u529b\u5b66\u63a5\u89e6\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u5411\u91cf\u5316\u3001\u5e73\u6ed1\u53ef\u5fae\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20323", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20323", "abs": "https://arxiv.org/abs/2602.20323", "authors": ["Haoyang Li", "Yang You", "Hao Su", "Leonidas Guibas"], "title": "Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory", "comment": null, "summary": "Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.", "AI": {"tldr": "PhysMem\u662f\u4e00\u4e2a\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u673a\u5668\u4eba\u89c4\u5212\u5668\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u7269\u7406\u539f\u7406\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5047\u8bbe\u800c\u975e\u76f4\u63a5\u5e94\u7528\u7ecf\u9a8c\u6765\u9002\u5e94\u53d8\u5316\u7684\u7269\u7406\u6761\u4ef6\u3002", "motivation": "\u53ef\u9760\u7684\u7269\u4f53\u64cd\u4f5c\u9700\u8981\u7406\u89e3\u968f\u7269\u4f53\u548c\u73af\u5883\u53d8\u5316\u7684\u7269\u7406\u5c5e\u6027\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u5668\u867d\u7136\u80fd\u5728\u4e00\u822c\u610f\u4e49\u4e0a\u63a8\u7406\u6469\u64e6\u548c\u7a33\u5b9a\u6027\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u7279\u5b9a\u7269\u4f53\u5728\u5177\u4f53\u8868\u9762\u4e0a\u7684\u884c\u4e3a\uff0c\u6216\u5224\u65ad\u54ea\u4e2a\u77f3\u5934\u80fd\u63d0\u4f9b\u7a33\u5b9a\u57fa\u7840\uff0c\u7f3a\u4e4f\u76f4\u63a5\u7ecf\u9a8c\u3002", "method": "PhysMem\u6846\u67b6\u8bb0\u5f55\u4ea4\u4e92\u7ecf\u9a8c\uff0c\u751f\u6210\u5019\u9009\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u4ea4\u4e92\u9a8c\u8bc1\u8fd9\u4e9b\u5047\u8bbe\uff0c\u7136\u540e\u5c06\u9a8c\u8bc1\u540e\u7684\u77e5\u8bc6\u63d0\u5347\u4e3a\u6307\u5bfc\u672a\u6765\u51b3\u7b56\u3002\u6838\u5fc3\u8bbe\u8ba1\u662f\"\u5148\u9a8c\u8bc1\u540e\u5e94\u7528\"\uff1a\u7cfb\u7edf\u7528\u65b0\u89c2\u5bdf\u6d4b\u8bd5\u5047\u8bbe\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5e94\u7528\u68c0\u7d22\u5230\u7684\u7ecf\u9a8c\uff0c\u51cf\u5c11\u7269\u7406\u6761\u4ef6\u53d8\u5316\u65f6\u5bf9\u5148\u524d\u7ecf\u9a8c\u7684\u50f5\u5316\u4f9d\u8d56\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u548c\u56db\u4e2aVLM\u9aa8\u5e72\u7684\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30PhysMem\u3002\u5728\u53d7\u63a7\u7684\u7816\u5757\u63d2\u5165\u4efb\u52a1\u4e2d\uff0c\u539f\u5219\u6027\u62bd\u8c61\u8fbe\u523076%\u6210\u529f\u7387\uff0c\u800c\u76f4\u63a5\u7ecf\u9a8c\u68c0\u7d22\u53ea\u670923%\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\u572830\u5206\u949f\u90e8\u7f72\u4f1a\u8bdd\u4e2d\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "PhysMem\u6846\u67b6\u4f7fVLM\u673a\u5668\u4eba\u89c4\u5212\u5668\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u7269\u7406\u539f\u7406\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5047\u8bbe\u800c\u975e\u76f4\u63a5\u5e94\u7528\u7ecf\u9a8c\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u53d8\u5316\u7269\u7406\u6761\u4ef6\u4e0b\u7684\u64cd\u4f5c\u6210\u529f\u7387\u3002"}}
{"id": "2602.20362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20362", "abs": "https://arxiv.org/abs/2602.20362", "authors": ["Robin Jeanne Kirschner", "Anna Huber", "Carina M. Micheler", "Dirk M\u00fcller", "Nader Rajaei", "Rainer Burgkart", "Sami Haddadin"], "title": "Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates", "comment": "9 pages, 11 figures", "summary": "While robotics research continues to propose strategies for collision avoidance in human-robot interaction, the reality of constrained environments and future humanoid systems makes contact inevitable. To mitigate injury risks, energy-constraining control approaches are commonly used, often relying on safety thresholds derived from blunt impact data in EN ISO 10218-2:2025. However, this dataset does not extend to edged or pointed collisions. Without scalable, clinically grounded datasets covering diverse contact scenarios, safety validation remains limited. Previous studies have laid the groundwork by assessing surrogate-based velocity and mass limits across various geometries, focusing on perpendicular impacts. This study expands those datasets by including shearing contact scenarios in unconstrained collisions, revealing that collision angle significantly affects injury outcomes. Notably, unconstrained shearing contacts result in fewer injuries than perpendicular ones. By reevaluating all prior porcine surrogate data, we establish energy thresholds across geometries and contact types, forming the first energy-based Injury Protection Database. This enables the development of meaningful energy-limiting controllers that ensure safety across a wide range of realistic collision events.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u673a\u5668\u4eba\u78b0\u649e\u5b89\u5168\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5efa\u7acb\u4e86\u57fa\u4e8e\u80fd\u91cf\u7684\u4f24\u5bb3\u4fdd\u62a4\u6570\u636e\u5e93\uff0c\u7279\u522b\u5173\u6ce8\u526a\u5207\u63a5\u89e6\u573a\u666f\uff0c\u53d1\u73b0\u78b0\u649e\u89d2\u5ea6\u663e\u8457\u5f71\u54cd\u4f24\u5bb3\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5b89\u5168\u9a8c\u8bc1\u4e3b\u8981\u4f9d\u8d56EN ISO 10218-2:2025\u7684\u949d\u5668\u78b0\u649e\u6570\u636e\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8fb9\u7f18\u6216\u5c16\u9510\u78b0\u649e\u7684\u53ef\u6269\u5c55\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5b89\u5168\u9a8c\u8bc1\u7684\u6709\u6548\u6027\u3002", "method": "\u6269\u5c55\u5148\u524d\u6570\u636e\u96c6\uff0c\u7eb3\u5165\u65e0\u7ea6\u675f\u78b0\u649e\u4e2d\u7684\u526a\u5207\u63a5\u89e6\u573a\u666f\uff0c\u91cd\u65b0\u8bc4\u4f30\u6240\u6709\u5148\u524d\u7684\u732a\u66ff\u4ee3\u7269\u6570\u636e\uff0c\u5efa\u7acb\u8de8\u51e0\u4f55\u5f62\u72b6\u548c\u63a5\u89e6\u7c7b\u578b\u7684\u80fd\u91cf\u9608\u503c\u3002", "result": "\u53d1\u73b0\u78b0\u649e\u89d2\u5ea6\u663e\u8457\u5f71\u54cd\u4f24\u5bb3\u7ed3\u679c\uff0c\u65e0\u7ea6\u675f\u526a\u5207\u63a5\u89e6\u6bd4\u5782\u76f4\u78b0\u649e\u9020\u6210\u66f4\u5c11\u4f24\u5bb3\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u80fd\u91cf\u7684\u4f24\u5bb3\u4fdd\u62a4\u6570\u636e\u5e93\u3002", "conclusion": "\u8be5\u6570\u636e\u5e93\u652f\u6301\u5f00\u53d1\u6709\u610f\u4e49\u7684\u80fd\u91cf\u9650\u5236\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u5728\u5404\u79cd\u73b0\u5b9e\u78b0\u649e\u4e8b\u4ef6\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2602.20375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20375", "abs": "https://arxiv.org/abs/2602.20375", "authors": ["Jiashun Wang", "M. Eva Mungai", "He Li", "Jean Pierre Sleiman", "Jessica Hodgins", "Farbod Farshidian"], "title": "Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework", "comment": null, "summary": "Learning agile humanoid behaviors from human motion offers a powerful route to natural, coordinated control, but existing approaches face a persistent trade-off: reference-tracking policies are often brittle outside the demonstration dataset, while purely task-driven Reinforcement Learning (RL) can achieve adaptability at the cost of motion quality. We introduce a unified multi-task RL framework that bridges this gap by treating reference motion as a prior for behavioral shaping rather than a deployment-time constraint. A single goal-conditioned policy is trained jointly on two tasks that share the same observation and action spaces, but differ in their initialization schemes, command spaces, and reward structures: (i) a reference-guided imitation task in which reference trajectories define dense imitation rewards but are not provided as policy inputs, and (ii) a goal-conditioned generalization task in which goals are sampled independently of any reference and where rewards reflect only task success. By co-optimizing these objectives within a shared formulation, the policy acquires structured, human-like motor skills from dense reference supervision while learning to adapt these skills to novel goals and initial conditions. This is achieved without adversarial objectives, explicit trajectory tracking, phase variables, or reference-dependent inference. We evaluate the method on a challenging box-based parkour playground that demands diverse athletic behaviors (e.g., jumping and climbing), and show that the learned controller transfers beyond the reference distribution while preserving motion naturalness. Finally, we demonstrate long-horizon behavior generation by composing multiple learned skills, illustrating the flexibility of the learned polices in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53c2\u8003\u8fd0\u52a8\u4f5c\u4e3a\u884c\u4e3a\u5851\u9020\u7684\u5148\u9a8c\u800c\u975e\u90e8\u7f72\u65f6\u7ea6\u675f\uff0c\u5b9e\u73b0\u81ea\u7136\u8fd0\u52a8\u8d28\u91cf\u4e0e\u4efb\u52a1\u9002\u5e94\u6027\u7684\u5e73\u8861", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6743\u8861\uff1a\u53c2\u8003\u8ddf\u8e2a\u7b56\u7565\u5728\u6f14\u793a\u6570\u636e\u96c6\u5916\u5f80\u5f80\u8106\u5f31\uff0c\u800c\u7eaf\u4efb\u52a1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u9002\u5e94\u6027\u5f3a\u4f46\u8fd0\u52a8\u8d28\u91cf\u5dee\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b66\u4e60\u81ea\u7136\u534f\u8c03\u8fd0\u52a8\u53c8\u80fd\u9002\u5e94\u65b0\u76ee\u6807\u548c\u6761\u4ef6\u7684\u7edf\u4e00\u65b9\u6cd5", "method": "\u63d0\u51fa\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u5355\u4e00\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u540c\u65f6\u4f18\u5316\u4e24\u4e2a\u4efb\u52a1\uff1a1)\u53c2\u8003\u5f15\u5bfc\u7684\u6a21\u4eff\u4efb\u52a1\uff0c\u4f7f\u7528\u53c2\u8003\u8f68\u8ff9\u5b9a\u4e49\u5bc6\u96c6\u6a21\u4eff\u5956\u52b1\u4f46\u4e0d\u4f5c\u4e3a\u7b56\u7565\u8f93\u5165\uff1b2)\u76ee\u6807\u6761\u4ef6\u6cdb\u5316\u4efb\u52a1\uff0c\u72ec\u7acb\u91c7\u6837\u76ee\u6807\u4e14\u5956\u52b1\u4ec5\u53cd\u6620\u4efb\u52a1\u6210\u529f\u3002\u901a\u8fc7\u5171\u4eab\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u8054\u5408\u4f18\u5316\uff0c\u5b66\u4e60\u7ed3\u6784\u5316\u7684\u4eba\u7c7b\u8fd0\u52a8\u6280\u80fd", "result": "\u5728\u57fa\u4e8e\u7bb1\u5b50\u7684\u8dd1\u9177\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u63a7\u5236\u5668\u80fd\u591f\u8d85\u8d8a\u53c2\u8003\u5206\u5e03\u8fdb\u884c\u6cdb\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u81ea\u7136\u6027\u3002\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u5b66\u4e60\u6280\u80fd\u5c55\u793a\u4e86\u957f\u65f6\u7a0b\u884c\u4e3a\u751f\u6210\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u8fd0\u52a8\u4e2d\u5b66\u4e60\u654f\u6377\u4eba\u5f62\u884c\u4e3a\uff0c\u5728\u4fdd\u6301\u8fd0\u52a8\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f97\u5bf9\u65b0\u76ee\u6807\u548c\u6761\u4ef6\u7684\u9002\u5e94\u6027\uff0c\u65e0\u9700\u5bf9\u6297\u76ee\u6807\u3001\u663e\u5f0f\u8f68\u8ff9\u8ddf\u8e2a\u3001\u76f8\u4f4d\u53d8\u91cf\u6216\u53c2\u8003\u4f9d\u8d56\u63a8\u7406"}}
{"id": "2602.20466", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20466", "abs": "https://arxiv.org/abs/2602.20466", "authors": ["Harsh Gupta", "Mohammad Amin Mirzaee", "Wenzhen Yuan"], "title": "Grasp to Act: Dexterous Grasping for Tool Use in Dynamic Settings", "comment": "Result videos can be found at https://grasp2act.github.io/", "summary": "Achieving robust grasping with dexterous hands remains challenging, especially when manipulation involves dynamic forces such as impacts, torques, and continuous resistance--situations common in real-world tool use. Existing methods largely optimize grasps for static geometric stability and often fail once external forces arise during manipulation. We present Grasp-to-Act, a hybrid system that combines physics-based grasp optimization with reinforcement-learning-based grasp adaptation to maintain stable grasps throughout functional manipulation tasks. Our method synthesizes robust grasp configurations informed by human demonstrations and employs an adaptive controller that residually issues joint corrections to prevent in-hand slip while tracking the object trajectory. Grasp-to-Act enables robust zero-shot sim-to-real transfer across five dynamic tool-use tasks--hammering, sawing, cutting, stirring, and scooping--consistently outperforming baselines. Across simulation and real-world hardware trials with a 16-DoF dexterous hand, our method reduces translational and rotational in-hand slip and achieves the highest task completion rates, demonstrating stable functional grasps under dynamic, contact-rich conditions.", "AI": {"tldr": "\u63d0\u51faGrasp-to-Act\u6df7\u5408\u7cfb\u7edf\uff0c\u7ed3\u5408\u7269\u7406\u6293\u53d6\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\uff0c\u5b9e\u73b0\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6293\u63e1", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u51e0\u4f55\u7a33\u5b9a\u6027\u4f18\u5316\u6293\u63e1\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u4e2d\u9047\u5230\u51b2\u51fb\u3001\u626d\u77e9\u548c\u6301\u7eed\u963b\u529b\u7b49\u52a8\u6001\u529b\u65f6\u5bb9\u6613\u5931\u6548", "method": "\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u6293\u53d6\u4f18\u5316\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6293\u53d6\u9002\u5e94\uff0c\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u4fe1\u606f\u5408\u6210\u9c81\u68d2\u6293\u53d6\u914d\u7f6e\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u63a7\u5236\u5668\u53d1\u51fa\u5173\u8282\u4fee\u6b63\u6765\u9632\u6b62\u624b\u5185\u6ed1\u52a8", "result": "\u5728\u4e94\u79cd\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\uff08\u9524\u51fb\u3001\u952f\u5207\u3001\u5207\u5272\u3001\u6405\u62cc\u3001\u8200\u53d6\uff09\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8f6c\u79fb\uff0c\u51cf\u5c11\u5e73\u79fb\u548c\u65cb\u8f6c\u6ed1\u52a8\uff0c\u83b7\u5f97\u6700\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387", "conclusion": "Grasp-to-Act\u7cfb\u7edf\u80fd\u591f\u5728\u52a8\u6001\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u7684\u529f\u80fd\u6027\u6293\u63e1\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5"}}
{"id": "2602.20500", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20500", "abs": "https://arxiv.org/abs/2602.20500", "authors": ["Keyu Zhou", "Peisen Xu", "Yahao Wu", "Jiming Chen", "Gaofeng Li", "Shunlei Li"], "title": "Strategy-Supervised Autonomous Laparoscopic Camera Control via Event-Driven Graph Mining", "comment": "Submitted to IEEE Transactions on Robotics (T-RO). 19 pages, 9 figures", "summary": "Autonomous laparoscopic camera control must maintain a stable and safe surgical view under rapid tool-tissue interactions while remaining interpretable to surgeons. We present a strategy-grounded framework that couples high-level vision-language inference with low-level closed-loop control. Offline, raw surgical videos are parsed into camera-relevant temporal events (e.g., interaction, working-distance deviation, and view-quality degradation) and structured as attributed event graphs. Mining these graphs yields a compact set of reusable camera-handling strategy primitives, which provide structured supervision for learning. Online, a fine-tuned Vision-Language Model (VLM) processes the live laparoscopic view to predict the dominant strategy and discrete image-based motion commands, executed by an IBVS-RCM controller under strict safety constraints; optional speech input enables intuitive human-in-the-loop conditioning. On a surgeon-annotated dataset, event parsing achieves reliable temporal localization (F1-score 0.86), and the mined strategies show strong semantic alignment with expert interpretation (cluster purity 0.81). Extensive ex vivo experiments on silicone phantoms and porcine tissues demonstrate that the proposed system outperforms junior surgeons in standardized camera-handling evaluations, reducing field-of-view centering error by 35.26% and image shaking by 62.33%, while preserving smooth motion and stable working-distance regulation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7b56\u7565\u7684\u81ea\u4e3b\u8179\u8154\u955c\u76f8\u673a\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u5c42\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4e0e\u4f4e\u5c42\u95ed\u73af\u63a7\u5236\uff0c\u901a\u8fc7\u4e8b\u4ef6\u56fe\u6316\u6398\u53ef\u91cd\u7528\u7b56\u7565\u539f\u8bed\uff0c\u5728\u79bb\u4f53\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u521d\u7ea7\u5916\u79d1\u533b\u751f", "motivation": "\u81ea\u4e3b\u8179\u8154\u955c\u76f8\u673a\u63a7\u5236\u9700\u8981\u5728\u5feb\u901f\u5668\u68b0-\u7ec4\u7ec7\u4ea4\u4e92\u4e2d\u4fdd\u6301\u7a33\u5b9a\u5b89\u5168\u7684\u624b\u672f\u89c6\u91ce\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5916\u79d1\u533b\u751f\u7684\u53ef\u89e3\u91ca\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7a33\u5b9a\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u7b56\u7565\u57fa\u7840\u6846\u67b6\uff1a\u79bb\u7ebf\u9636\u6bb5\u5c06\u539f\u59cb\u624b\u672f\u89c6\u9891\u89e3\u6790\u4e3a\u76f8\u673a\u76f8\u5173\u65f6\u95f4\u4e8b\u4ef6\uff08\u4ea4\u4e92\u3001\u5de5\u4f5c\u8ddd\u79bb\u504f\u5dee\u3001\u89c6\u91ce\u8d28\u91cf\u9000\u5316\u7b49\uff09\uff0c\u6784\u5efa\u5c5e\u6027\u4e8b\u4ef6\u56fe\uff0c\u6316\u6398\u53ef\u91cd\u7528\u76f8\u673a\u64cd\u4f5c\u7b56\u7565\u539f\u8bed\uff1b\u5728\u7ebf\u9636\u6bb5\u4f7f\u7528\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5b9e\u65f6\u8179\u8154\u955c\u89c6\u56fe\u9884\u6d4b\u4e3b\u5bfc\u7b56\u7565\u548c\u79bb\u6563\u56fe\u50cf\u8fd0\u52a8\u6307\u4ee4\uff0c\u7531IBVS-RCM\u63a7\u5236\u5668\u5728\u4e25\u683c\u5b89\u5168\u7ea6\u675f\u4e0b\u6267\u884c\uff0c\u652f\u6301\u8bed\u97f3\u8f93\u5165\u5b9e\u73b0\u4eba\u673a\u534f\u540c\u3002", "result": "\u4e8b\u4ef6\u89e3\u6790\u5b9e\u73b0\u53ef\u9760\u65f6\u95f4\u5b9a\u4f4d\uff08F1\u5206\u65700.86\uff09\uff0c\u6316\u6398\u7b56\u7565\u4e0e\u4e13\u5bb6\u89e3\u91ca\u8bed\u4e49\u5bf9\u9f50\u5f3a\uff08\u805a\u7c7b\u7eaf\u5ea60.81\uff09\u3002\u5728\u7845\u80f6\u6a21\u578b\u548c\u732a\u7ec4\u7ec7\u79bb\u4f53\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u5728\u6807\u51c6\u5316\u76f8\u673a\u64cd\u4f5c\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u521d\u7ea7\u5916\u79d1\u533b\u751f\uff0c\u89c6\u91ce\u4e2d\u5fc3\u8bef\u5dee\u51cf\u5c1135.26%\uff0c\u56fe\u50cf\u6296\u52a8\u51cf\u5c1162.33%\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u6ed1\u8fd0\u52a8\u548c\u7a33\u5b9a\u5de5\u4f5c\u8ddd\u79bb\u8c03\u8282\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u57fa\u7840\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u5b89\u5168\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u4e3b\u8179\u8154\u955c\u76f8\u673a\u63a7\u5236\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u548c\u4f4e\u5c42\u95ed\u73af\u63a7\u5236\uff0c\u5728\u79bb\u4f53\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u4eba\u7c7b\u64cd\u4f5c\u8005\u7684\u6027\u80fd\uff0c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u81ea\u4e3b\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20566", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20566", "abs": "https://arxiv.org/abs/2602.20566", "authors": ["Haosheng Li", "Weixin Mao", "Zihan Lan", "Hongwei Xiong", "Hongan Wang", "Chenyang Si", "Ziwei Liu", "Xiaoming Deng", "Hua Chen"], "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model", "comment": "9 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the \u03c00 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.", "AI": {"tldr": "BFA++\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bbe\u8ba1\u7684\u52a8\u6001\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u526a\u679d\u7b56\u7565\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4fdd\u6301\u64cd\u4f5c\u6210\u529f\u7387\u7684\u540c\u65f6\u5b9e\u73b01.5-1.8\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5904\u7406\u591a\u89c6\u89d2\u89c6\u89c9\u8f93\u5165\u65f6\u9762\u4e34\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5e26\u6765\u7684\u5b9e\u65f6\u6027\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u4ee4\u724c\u526a\u679d\u6280\u672f\u5728\u76f4\u63a5\u5e94\u7528\u4e8eVLA\u6a21\u578b\u65f6\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u4e0d\u540c\u89c6\u89d2\u4e4b\u95f4\u7684\u5173\u7cfb\u4ee5\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u52a8\u6001\u548c\u4efb\u52a1\u7279\u5b9a\u7279\u6027\u3002", "method": "\u63d0\u51faBFA++\u52a8\u6001\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u526a\u679d\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u7ea7\u522b\u7684\u91cd\u8981\u6027\u9884\u6d4b\u5668\uff1a1\uff09\u89c6\u56fe\u5185\u9884\u6d4b\u5668\u7a81\u51fa\u663e\u793a\u6bcf\u5f20\u56fe\u50cf\u4e2d\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u533a\u57df\u4ee5\u6291\u5236\u7a7a\u95f4\u566a\u58f0\uff1b2\uff09\u89c6\u56fe\u95f4\u9884\u6d4b\u5668\u8bc6\u522b\u4e0d\u540c\u64cd\u4f5c\u9636\u6bb5\u7684\u5173\u952e\u76f8\u673a\u89c6\u89d2\u4ee5\u51cf\u5c11\u8de8\u89c6\u89d2\u5197\u4f59\u3002", "result": "\u5728RoboTwin\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cBFA++\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u03c00\u548cRDT\u6a21\u578b\u4e0a\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u7ea610%\uff0c\u5206\u522b\u5b9e\u73b01.8\u500d\u548c1.5\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u654f\u611f\u548c\u4efb\u52a1\u611f\u77e5\u7684\u4ee4\u724c\u526a\u679d\u6bd4\u5b8c\u6574\u7684\u89c6\u89c9\u5904\u7406\u66f4\u6709\u6548\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u64cd\u4f5c\u7cbe\u5ea6\u3002"}}
{"id": "2602.20596", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20596", "abs": "https://arxiv.org/abs/2602.20596", "authors": ["Zongyuan Zhang", "Christopher Lehnert", "Will N. Browne", "Jonathan M. Roberts"], "title": "Acoustic Feedback for Closed-Loop Force Control in Robotic Grinding", "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2026. 8 pages, 10 figures", "summary": "Acoustic feedback is a critical indicator for assessing the contact condition between the tool and the workpiece when humans perform grinding tasks with rotary tools. In contrast, robotic grinding systems typically rely on force sensing, with acoustic information largely ignored. This reliance on force sensors is costly and difficult to adapt to different grinding tools, whereas audio sensors (microphones) are low-cost and can be mounted on any medium that conducts grinding sound.\n  This paper introduces a low-cost Acoustic Feedback Robotic Grinding System (AFRG) that captures audio signals with a contact microphone, estimates grinding force from the audio in real time, and enables closed-loop force control of the grinding process. Compared with conventional force-sensing approaches, AFRG achieves a 4-fold improvement in consistency across different grinding disc conditions. AFRG relies solely on a low-cost microphone, which is approximately 200-fold cheaper than conventional force sensors, as the sensing modality, providing an easily deployable, cost-effective robotic grinding solution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u58f0\u5b66\u53cd\u9988\u673a\u5668\u4eba\u78e8\u524a\u7cfb\u7edf(AFRG)\uff0c\u4f7f\u7528\u63a5\u89e6\u5f0f\u9ea6\u514b\u98ce\u91c7\u96c6\u97f3\u9891\u4fe1\u53f7\uff0c\u5b9e\u65f6\u4f30\u8ba1\u78e8\u524a\u529b\uff0c\u5b9e\u73b0\u95ed\u73af\u529b\u63a7\u5236\uff0c\u76f8\u6bd4\u4f20\u7edf\u529b\u4f20\u611f\u5668\u65b9\u6cd5\u6210\u672c\u964d\u4f4e200\u500d\uff0c\u5728\u4e0d\u540c\u78e8\u524a\u76d8\u6761\u4ef6\u4e0b\u4e00\u81f4\u6027\u63d0\u9ad84\u500d\u3002", "motivation": "\u4eba\u7c7b\u5728\u78e8\u524a\u4efb\u52a1\u4e2d\u4f9d\u8d56\u58f0\u5b66\u53cd\u9988\u5224\u65ad\u5de5\u5177\u4e0e\u5de5\u4ef6\u63a5\u89e6\u72b6\u6001\uff0c\u800c\u673a\u5668\u4eba\u78e8\u524a\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u6602\u8d35\u7684\u529b\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u78e8\u524a\u5de5\u5177\u3002\u97f3\u9891\u4f20\u611f\u5668\u6210\u672c\u4f4e\u4e14\u53ef\u5b89\u88c5\u5728\u4efb\u4f55\u4f20\u5bfc\u78e8\u524a\u58f0\u97f3\u7684\u4ecb\u8d28\u4e0a\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u58f0\u5b66\u53cd\u9988\u7684\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAFRG\u7cfb\u7edf\uff1a\u4f7f\u7528\u63a5\u89e6\u5f0f\u9ea6\u514b\u98ce\u91c7\u96c6\u97f3\u9891\u4fe1\u53f7\uff0c\u4ece\u97f3\u9891\u4e2d\u5b9e\u65f6\u4f30\u8ba1\u78e8\u524a\u529b\uff0c\u5b9e\u73b0\u78e8\u524a\u8fc7\u7a0b\u7684\u95ed\u73af\u529b\u63a7\u5236\u3002\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u4f4e\u6210\u672c\u9ea6\u514b\u98ce\u4f5c\u4e3a\u4f20\u611f\u65b9\u5f0f\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u529b\u4f20\u611f\u65b9\u6cd5\uff0cAFRG\u5728\u4e0d\u540c\u78e8\u524a\u76d8\u6761\u4ef6\u4e0b\u4e00\u81f4\u6027\u63d0\u9ad8\u4e864\u500d\u3002\u7cfb\u7edf\u4f7f\u7528\u7684\u9ea6\u514b\u98ce\u6210\u672c\u7ea6\u4e3a\u4f20\u7edf\u529b\u4f20\u611f\u5668\u76841/200\uff0c\u63d0\u4f9b\u4e86\u6613\u4e8e\u90e8\u7f72\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u673a\u5668\u4eba\u78e8\u524a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AFRG\u7cfb\u7edf\u901a\u8fc7\u4f4e\u6210\u672c\u58f0\u5b66\u53cd\u9988\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u673a\u5668\u4eba\u78e8\u524a\u529b\u63a7\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u529b\u4f20\u611f\u5668\u6210\u672c\u9ad8\u3001\u9002\u5e94\u6027\u5dee\u7684\u7f3a\u70b9\uff0c\u4e3a\u673a\u5668\u4eba\u78e8\u524a\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.20645", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20645", "abs": "https://arxiv.org/abs/2602.20645", "authors": ["Keisuke Takeshita", "Takahiro Yamazaki", "Tomohiro Ono", "Takashi Yamamoto"], "title": "Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments", "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2025. Project Page: https://toyotafrc.github.io/RobotLocalPlanner-Proj/", "summary": "The objective of this study is to enable fast and safe manipulation tasks in home environments. Specifically, we aim to develop a system that can recognize its surroundings and identify target objects while in motion, enabling it to plan and execute actions accordingly. We propose a periodic sampling-based whole-body trajectory planning method, called the \"Robot Local Planner (RLP).\" This method leverages unique features of home environments to enhance computational efficiency, motion optimality, and robustness against recognition and control errors, all while ensuring safety. The RLP minimizes computation time by planning with minimal waypoints and generating safe trajectories. Furthermore, overall motion optimality is improved by periodically executing trajectory planning to select more optimal motions. This approach incorporates inverse kinematics that are robust to base position errors, further enhancing robustness. Evaluation experiments demonstrated that the RLP outperformed existing methods in terms of motion planning time, motion duration, and robustness, confirming its effectiveness in home environments. Moreover, application experiments using a tidy-up task achieved high success rates and short operation times, thereby underscoring its practical feasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u673a\u5668\u4eba\u5c40\u90e8\u89c4\u5212\u5668(RLP)\"\u7684\u5468\u671f\u6027\u91c7\u6837\u5168\u8eab\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5feb\u901f\u5b89\u5168\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc6\u522b\u5468\u56f4\u73af\u5883\u3001\u8bc6\u522b\u76ee\u6807\u7269\u4f53\u5e76\u5728\u8fd0\u52a8\u4e2d\u89c4\u5212\u6267\u884c\u52a8\u4f5c\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5468\u671f\u6027\u91c7\u6837\u5168\u8eab\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5(RLP)\uff0c\u5229\u7528\u5bb6\u5ead\u73af\u5883\u7684\u72ec\u7279\u7279\u5f81\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3001\u8fd0\u52a8\u6700\u4f18\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u8def\u5f84\u70b9\u89c4\u5212\u3001\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3001\u5468\u671f\u6027\u6267\u884c\u8f68\u8ff9\u89c4\u5212\u9009\u62e9\u66f4\u4f18\u8fd0\u52a8\uff0c\u5e76\u91c7\u7528\u5bf9\u57fa\u5ea7\u4f4d\u7f6e\u8bef\u5dee\u9c81\u68d2\u7684\u9006\u8fd0\u52a8\u5b66\u3002", "result": "\u8bc4\u4f30\u5b9e\u9a8c\u8868\u660eRLP\u5728\u8fd0\u52a8\u89c4\u5212\u65f6\u95f4\u3001\u8fd0\u52a8\u6301\u7eed\u65f6\u95f4\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5e94\u7528\u5b9e\u9a8c\u5728\u6574\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u548c\u77ed\u64cd\u4f5c\u65f6\u95f4\u3002", "conclusion": "RLP\u65b9\u6cd5\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u6709\u6548\u4e14\u5b9e\u7528\uff0c\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2602.20715", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20715", "abs": "https://arxiv.org/abs/2602.20715", "authors": ["Zhian Su", "Weijie Kong", "Haonan Dong", "Huixu Dong"], "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To address these issues, we propose IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system designed for flow-based VLA models. Firstly, to facilitate effective policy optimization, we introduce Interaction-Guided Advantage Weighted Regression (IG-AWR), an RL algorithm that dynamically modulates exploration intensity based on the robot's interaction status. Furthermore, to address the limitations of sparse or task-specific rewards, we design a novel hybrid dense reward function that integrates the trajectory-level reward and the subtask-level reward. Finally, we construct a three-stage RL system comprising SFT, Offline RL, and Human-in-the-Loop RL for fine-tuning VLA models. Extensive real-world experiments on four challenging long-horizon tasks demonstrate that IG-RFT achieves an average success rate of 85.0%, significantly outperforming SFT (18.8%) and standard Offline RL baselines (40.0%). Ablation studies confirm the critical contributions of IG-AWR and hybrid reward shaping. In summary, our work establishes and validates a novel reinforced fine-tuning system for VLA models in real-world robotic manipulation.", "AI": {"tldr": "IG-RFT\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u4ea4\u4e92\u5f15\u5bfc\u5f3a\u5316\u5fae\u8c03\u7cfb\u7edf\uff0c\u901a\u8fc7IG-AWR\u7b97\u6cd5\u548c\u6df7\u5408\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "VLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u9ad8\u8d28\u91cf\u6f14\u793a\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u6539\u8fdb\u7b56\u7565\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754cVLA\u5fae\u8c03\u4e2d\u5b58\u5728\u63a2\u7d22\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6210\u672c\u7b49\u6311\u6218", "method": "\u63d0\u51faIG-RFT\u7cfb\u7edf\uff1a1) IG-AWR\u7b97\u6cd5\u6839\u636e\u673a\u5668\u4eba\u4ea4\u4e92\u72b6\u6001\u52a8\u6001\u8c03\u8282\u63a2\u7d22\u5f3a\u5ea6\uff1b2) \u8bbe\u8ba1\u6df7\u5408\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u8f68\u8ff9\u7ea7\u548c\u5b50\u4efb\u52a1\u7ea7\u5956\u52b1\uff1b3) \u6784\u5efa\u4e09\u9636\u6bb5RL\u7cfb\u7edf\uff08SFT\u3001\u79bb\u7ebfRL\u3001\u4eba\u5728\u73afRL\uff09\u5fae\u8c03VLA\u6a21\u578b", "result": "\u5728\u56db\u4e2a\u6311\u6218\u6027\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\uff0cIG-RFT\u5e73\u5747\u6210\u529f\u738785.0%\uff0c\u663e\u8457\u4f18\u4e8eSFT\uff0818.8%\uff09\u548c\u6807\u51c6\u79bb\u7ebfRL\u57fa\u7ebf\uff0840.0%\uff09\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86IG-AWR\u548c\u6df7\u5408\u5956\u52b1\u5851\u5f62\u7684\u5173\u952e\u8d21\u732e", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684VLA\u6a21\u578b\u5f3a\u5316\u5fae\u8c03\u7cfb\u7edf\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.20768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20768", "abs": "https://arxiv.org/abs/2602.20768", "authors": ["Marius Schaab", "Alisha Kiefer", "Thomas Wiedemann", "Patrick Hinsen", "Achim J. Lilienthal"], "title": "Visual Cooperative Drone Tracking for Open-Path Gas Measurements", "comment": null, "summary": "Open-path Tunable Diode Laser Absorption Spectroscopy offers an effective method for measuring, mapping, and monitoring gas concentrations, such as leaking CO2 or methane. Compared to spatial sampling of gas distributions using in-situ sensors, open-path sensors in combination with gas tomography algorithms can cover large outdoor environments faster in a non-invasive way. However, the requirement of a dedicated reflection surface for the open-path laser makes automating the spatial sampling process challenging. This publication presents a robotic system for collecting open-path measurements, making use of a sensor mounted on a ground-based pan-tilt unit and a small drone carrying a reflector. By means of a zoom camera, the ground unit visually tracks red LED markers mounted on the drone and aligns the sensor's laser beam with the reflector. Incorporating GNSS position information provided by the drone's flight controller further improves the tracking approach. Outdoor experiments validated the system's performance, demonstrating successful autonomous tracking and valid CO2 measurements at distances up to 60 meters. Furthermore, the system successfully measured a CO2 plume without interference from the drone's propulsion system, demonstrating its superiority compared to flying in-situ sensors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f00\u653e\u8def\u5f84\u6fc0\u5149\u5438\u6536\u5149\u8c31\u6d4b\u91cf\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5730\u9762\u4e91\u53f0\u5355\u5143\u548c\u643a\u5e26\u53cd\u5c04\u5668\u7684\u65e0\u4eba\u673a\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u8fdc\u8ddd\u79bb\u6c14\u4f53\u6d53\u5ea6\u7684\u81ea\u4e3b\u6d4b\u91cf\u3002", "motivation": "\u5f00\u653e\u8def\u5f84\u53ef\u8c03\u8c10\u4e8c\u6781\u7ba1\u6fc0\u5149\u5438\u6536\u5149\u8c31\u6280\u672f\u80fd\u591f\u6709\u6548\u6d4b\u91cf\u6c14\u4f53\u6d53\u5ea6\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e13\u7528\u53cd\u5c04\u8868\u9762\uff0c\u4f7f\u5f97\u7a7a\u95f4\u91c7\u6837\u8fc7\u7a0b\u81ea\u52a8\u5316\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5305\u62ec\u5730\u9762\u4e91\u53f0\u5355\u5143\uff08\u642d\u8f7d\u4f20\u611f\u5668\u548c\u53d8\u7126\u76f8\u673a\uff09\u548c\u5c0f\u578b\u65e0\u4eba\u673a\uff08\u643a\u5e26\u53cd\u5c04\u5668\u548c\u7ea2\u8272LED\u6807\u8bb0\uff09\u3002\u901a\u8fc7\u89c6\u89c9\u8ddf\u8e2a\u65e0\u4eba\u673a\u4e0a\u7684LED\u6807\u8bb0\uff0c\u7ed3\u5408GNSS\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4f7f\u6fc0\u5149\u675f\u4e0e\u53cd\u5c04\u5668\u5bf9\u9f50\u3002", "result": "\u5ba4\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u6210\u529f\u5b9e\u73b0\u4e8660\u7c73\u8ddd\u79bb\u5185\u7684\u81ea\u4e3b\u8ddf\u8e2a\u548c\u6709\u6548\u7684CO2\u6d4b\u91cf\u3002\u7cfb\u7edf\u80fd\u591f\u6d4b\u91cfCO2\u7fbd\u6d41\u800c\u4e0d\u53d7\u65e0\u4eba\u673a\u63a8\u8fdb\u7cfb\u7edf\u5e72\u6270\uff0c\u4f18\u4e8e\u98de\u884c\u539f\u4f4d\u4f20\u611f\u5668\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u5f00\u653e\u8def\u5f84\u6fc0\u5149\u5438\u6536\u5149\u8c31\u6d4b\u91cf\u7684\u81ea\u52a8\u5316\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u7684\u6c14\u4f53\u6d53\u5ea6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20850", "abs": "https://arxiv.org/abs/2602.20850", "authors": ["Lei Ye", "Haibo Gao", "Huaiguang Yang", "Peng Xu", "Haoyu Wang", "Tie Liu", "Junqi Shan", "Zongquan Deng", "Liang Ding"], "title": "KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion", "comment": null, "summary": "Legged robots face significant challenges in navigating complex environments, as they require precise real-time decisions for foothold selection and contact planning. While existing research has explored methods to select footholds based on terrain geometry or kinematics, a critical gap remains: few existing methods efficiently validate the existence of a non-collision swing trajectory. This paper addresses this gap by introducing KCFRC, a novel approach for efficient foothold reachability analysis. We first formally define the foothold reachability problem and establish a sufficient condition for foothold reachability. Based on this condition, we develop the KCFRC algorithm, which enables robots to validate foothold reachability in real time. Our experimental results demonstrate that KCFRC achieves remarkable time efficiency, completing foothold reachability checks for a single leg across 900 potential footholds in an average of 2 ms. Furthermore, we show that KCFRC can accelerate trajectory optimization and is particularly beneficial for contact planning in confined spaces, enhancing the adaptability and robustness of legged robots in challenging environments.", "AI": {"tldr": "KCFRC\u7b97\u6cd5\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u7684\u843d\u811a\u70b9\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u80fd\u57282\u6beb\u79d2\u5185\u9a8c\u8bc1900\u4e2a\u6f5c\u5728\u843d\u811a\u70b9\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u7cbe\u786e\u7684\u5b9e\u65f6\u51b3\u7b56\u6765\u9009\u62e9\u843d\u811a\u70b9\u548c\u89c4\u5212\u63a5\u89e6\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5730\u5f62\u51e0\u4f55\u6216\u8fd0\u52a8\u5b66\u9009\u62e9\u843d\u811a\u70b9\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u9a8c\u8bc1\u65e0\u78b0\u649e\u6446\u52a8\u8f68\u8ff9\u5b58\u5728\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faKCFRC\u7b97\u6cd5\uff1a\u9996\u5148\u6b63\u5f0f\u5b9a\u4e49\u843d\u811a\u70b9\u53ef\u8fbe\u6027\u95ee\u9898\u5e76\u5efa\u7acb\u843d\u811a\u70b9\u53ef\u8fbe\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u57fa\u4e8e\u6b64\u6761\u4ef6\u5f00\u53d1KCFRC\u7b97\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5b9e\u65f6\u9a8c\u8bc1\u843d\u811a\u70b9\u53ef\u8fbe\u6027\u3002", "result": "KCFRC\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u5355\u817f\u5bf9900\u4e2a\u6f5c\u5728\u843d\u811a\u70b9\u7684\u53ef\u8fbe\u6027\u68c0\u67e5\u5e73\u5747\u4ec5\u97002\u6beb\u79d2\u3002\u8be5\u7b97\u6cd5\u80fd\u52a0\u901f\u8f68\u8ff9\u4f18\u5316\uff0c\u7279\u522b\u6709\u5229\u4e8e\u53d7\u9650\u7a7a\u95f4\u4e2d\u7684\u63a5\u89e6\u89c4\u5212\u3002", "conclusion": "KCFRC\u586b\u8865\u4e86\u8db3\u5f0f\u673a\u5668\u4eba\u843d\u811a\u70b9\u53ef\u8fbe\u6027\u5206\u6790\u7684\u5173\u952e\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u65f6\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20871", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20871", "abs": "https://arxiv.org/abs/2602.20871", "authors": ["Wenbo Yu", "Wenke Xia", "Weitao Zhang", "Di Hu"], "title": "GeCo-SRT: Geometry-aware Continual Adaptation for Robotic Cross-Task Sim-to-Real Transfer", "comment": "Accepted By CVPR 2026", "summary": "Bridging the sim-to-real gap is important for applying low-cost simulation data to real-world robotic systems. However, previous methods are severely limited by treating each transfer as an isolated endeavor, demanding repeated, costly tuning and wasting prior transfer experience.To move beyond isolated sim-to-real, we build a continual cross-task sim-to-real transfer paradigm centered on knowledge accumulation across iterative transfers, thereby enabling effective and efficient adaptation to novel tasks. Thus, we propose GeCo-SRT, a geometry-aware continual adaptation method. It utilizes domain-invariant and task-invariant knowledge from local geometric features as a transferable foundation to accelerate adaptation during subsequent sim-to-real transfers. This method starts with a geometry-aware mixture-of-experts module, which dynamically activates experts to specialize in distinct geometric knowledge to bridge observation sim-to-real gap. Further, the geometry-expert-guided prioritized experience replay module preferentially samples from underutilized experts, refreshing specialized knowledge to combat forgetting and maintain robust cross-task performance. Leveraging knowledge accumulated during iterative transfer, GeCo-SRT method not only achieves 52% average performance improvement over the baseline, but also demonstrates significant data efficiency for new task adaptation with only 1/6 data.We hope this work inspires approaches for efficient, low-cost cross-task sim-to-real transfer.", "AI": {"tldr": "GeCo-SRT\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u6301\u7eed\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u79ef\u7d2f\u8de8\u4efb\u52a1\u7684\u77e5\u8bc6\u6765\u6865\u63a5\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u8de8\u4efb\u52a1\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u65b9\u6cd5\u5c06\u6bcf\u6b21\u8fc1\u79fb\u89c6\u4e3a\u5b64\u7acb\u4efb\u52a1\uff0c\u9700\u8981\u91cd\u590d\u6602\u8d35\u7684\u8c03\u4f18\uff0c\u6d6a\u8d39\u4e86\u4e4b\u524d\u7684\u8fc1\u79fb\u7ecf\u9a8c\u3002\u9700\u8981\u8d85\u8d8a\u8fd9\u79cd\u5b64\u7acb\u6a21\u5f0f\uff0c\u5efa\u7acb\u6301\u7eed\u8de8\u4efb\u52a1\u7684\u8fc1\u79fb\u8303\u5f0f\u3002", "method": "GeCo-SRT\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u51e0\u4f55\u611f\u77e5\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u52a8\u6001\u6fc0\u6d3b\u4e13\u5bb6\u6765\u4e13\u95e8\u5904\u7406\u4e0d\u540c\u7684\u51e0\u4f55\u77e5\u8bc6\uff1b2\uff09\u51e0\u4f55\u4e13\u5bb6\u5f15\u5bfc\u7684\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u6a21\u5757\uff0c\u4f18\u5148\u4ece\u672a\u5145\u5206\u5229\u7528\u7684\u4e13\u5bb6\u4e2d\u91c7\u6837\uff0c\u9632\u6b62\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u6027\u80fd\u63d0\u534752%\uff0c\u5e76\u4e14\u5728\u65b0\u4efb\u52a1\u9002\u5e94\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\uff0c\u4ec5\u97001/6\u7684\u6570\u636e\u91cf\u3002", "conclusion": "GeCo-SRT\u901a\u8fc7\u79ef\u7d2f\u8fed\u4ee3\u8fc1\u79fb\u4e2d\u7684\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u8de8\u4efb\u52a1\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20915", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20915", "abs": "https://arxiv.org/abs/2602.20915", "authors": ["Dimitrios Dimou", "Jos\u00e9 Santos-Victor", "Plinio Moreno"], "title": "Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning", "comment": null, "summary": "In this paper, we address the problem of task-oriented grasping for humanoid robots, emphasizing the need to align with human social norms and task-specific objectives. Existing methods, employ a variety of open-loop and closed-loop approaches but lack an end-to-end solution that can grasp several objects while taking into account the downstream task's constraints. Our proposed approach employs reinforcement learning to enhance task-oriented grasping, prioritizing the post-grasp intention of the agent. We extract human grasp preferences from the ContactPose dataset, and train a hand synergy model based on the Variational Autoencoder (VAE) to imitate the participant's grasping actions. Based on this data, we train an agent able to grasp multiple objects while taking into account distinct post-grasp intentions that are task-specific. By combining data-driven insights from human grasping behavior with learning by exploration provided by reinforcement learning, we can develop humanoid robots capable of context-aware manipulation actions, facilitating collaboration in human-centered environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u5bfc\u5411\u6293\u53d6\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u6293\u53d6\u504f\u597d\u6570\u636e\u548cVAE\u624b\u90e8\u534f\u540c\u6a21\u578b\uff0c\u5b9e\u73b0\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u7ea6\u675f\u7684\u591a\u7269\u4f53\u6293\u53d6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u6cd5\u5728\u6293\u53d6\u591a\u4e2a\u7269\u4f53\u65f6\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u7684\u7ea6\u675f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7b26\u5408\u4eba\u7c7b\u793e\u4ea4\u89c4\u8303\u548c\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5", "method": "1. \u4eceContactPose\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u4eba\u7c7b\u6293\u53d6\u504f\u597d\uff1b2. \u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u8bad\u7ec3\u624b\u90e8\u534f\u540c\u6a21\u578b\u6765\u6a21\u4eff\u4eba\u7c7b\u6293\u53d6\u52a8\u4f5c\uff1b3. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u591f\u8003\u8651\u4efb\u52a1\u7279\u5b9a\u540e\u6293\u53d6\u610f\u56fe\u7684\u667a\u80fd\u4f53", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u6293\u53d6\u591a\u4e2a\u7269\u4f53\u5e76\u8003\u8651\u4e0d\u540c\u4efb\u52a1\u7279\u5b9a\u540e\u6293\u53d6\u610f\u56fe\u7684\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u4eba\u7c7b\u6293\u53d6\u884c\u4e3a\u7684\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u5b66\u4e60\u80fd\u529b", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6293\u53d6\u884c\u4e3a\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u5177\u6709\u60c5\u5883\u611f\u77e5\u64cd\u4f5c\u80fd\u529b\u7684\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u4fc3\u8fdb\u5728\u4ee5\u4eba\u4e3a\u672c\u73af\u5883\u4e2d\u7684\u534f\u4f5c"}}
{"id": "2602.20920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20920", "abs": "https://arxiv.org/abs/2602.20920", "authors": ["Daniel Huczala", "Severinas Zube", "Martin Pfurner", "Johannes Siegele", "Frank C. Park"], "title": "Computer-Aided Design of Rational Motions for 4R and 6R Spatial Mechanism Synthesis", "comment": null, "summary": "This paper focuses on geometric methods for generating rational motions used in the design of single-loop rational linkages, 1-degree-of-freedom mechanisms that can execute prescribed spatial tasks. Building on established rational motion synthesis methods, we introduce a new interpolation scheme for seven 3D points based on cubic quaternionic Bezier curves. The resulting motion admits factorization, i.e. the synthesis of a spatial six-bar mechanism whose tool frame passes the specified seven points. To support engineering practice, we provide open-source CAD tools that implement also the other methods and provide fast visual evaluation of motion generation and mechanism synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u4e09\u6b21\u56db\u5143\u6570\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u4e03\u70b9\u63d2\u503c\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u6709\u7406\u8fd0\u52a8\u5e76\u5408\u6210\u7a7a\u95f4\u516d\u6746\u673a\u6784", "motivation": "\u4e3a\u5355\u56de\u8def\u6709\u7406\u8fde\u6746\u673a\u6784\u8bbe\u8ba1\u63d0\u4f9b\u51e0\u4f55\u65b9\u6cd5\uff0c\u4f7f\u673a\u6784\u80fd\u591f\u6267\u884c\u89c4\u5b9a\u7684\u7a7a\u95f4\u4efb\u52a1\uff0c\u652f\u6301\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u8fd0\u52a8\u751f\u6210\u548c\u673a\u6784\u5408\u6210", "method": "\u57fa\u4e8e\u73b0\u6709\u6709\u7406\u8fd0\u52a8\u5408\u6210\u65b9\u6cd5\uff0c\u5f15\u5165\u57fa\u4e8e\u4e09\u6b21\u56db\u5143\u6570\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u4e03\u70b93D\u63d2\u503c\u65b9\u6848\uff0c\u5b9e\u73b0\u8fd0\u52a8\u5206\u89e3\u548c\u7a7a\u95f4\u516d\u6746\u673a\u6784\u5408\u6210", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90CAD\u5de5\u5177\uff0c\u5b9e\u73b0\u8fd0\u52a8\u751f\u6210\u548c\u673a\u6784\u5408\u6210\u7684\u5feb\u901f\u53ef\u89c6\u5316\u8bc4\u4f30\uff0c\u652f\u6301\u5de5\u7a0b\u5b9e\u8df5\u5e94\u7528", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u6709\u7406\u8fd0\u52a8\u5e76\u5408\u6210\u7a7a\u95f4\u516d\u6746\u673a\u6784\uff0c\u4e3a\u5355\u56de\u8def\u6709\u7406\u8fde\u6746\u673a\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u7a0b\u5de5\u5177"}}
{"id": "2602.20923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20923", "abs": "https://arxiv.org/abs/2602.20923", "authors": ["Jiarong Wei", "Anna Rehr", "Christian Feist", "Abhinav Valada"], "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models", "comment": "ICRA 2026 Camera Ready Version", "summary": "Automated parking is a challenging operational domain for advanced driver assistance systems, requiring robust scene understanding and interaction reasoning. The key challenge is twofold: (i) predict multiple plausible ego intentions according to context and (ii) for each intention, predict the joint responses of surrounding agents, enabling effective what-if decision-making. However, existing methods often fall short, typically treating these interdependent problems in isolation. We propose ParkDiffusion++, which jointly learns a multi-modal ego intention predictor and an ego-conditioned multi-agent joint trajectory predictor for automated parking. Our approach makes several key contributions. First, we introduce an ego intention tokenizer that predicts a small set of discrete endpoint intentions from agent histories and vectorized map polylines. Second, we perform ego-intention-conditioned joint prediction, yielding socially consistent predictions of the surrounding agents for each possible ego intention. Third, we employ a lightweight safety-guided denoiser with different constraints to refine joint scenes during training, thus improving accuracy and safety. Fourth, we propose counterfactual knowledge distillation, where an EMA teacher refined by a frozen safety-guided denoiser provides pseudo-targets that capture how agents react to alternative ego intentions. Extensive evaluations demonstrate that ParkDiffusion++ achieves state-of-the-art performance on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Importantly, qualitative what-if visualizations show that other agents react appropriately to different ego intentions.", "AI": {"tldr": "ParkDiffusion++\uff1a\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u6cca\u8f66\u7684\u8054\u5408\u591a\u6a21\u6001\u610f\u56fe\u9884\u6d4b\u548c\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u610f\u56fe\u6807\u8bb0\u5316\u3001\u6761\u4ef6\u9884\u6d4b\u3001\u5b89\u5168\u5f15\u5bfc\u53bb\u566a\u548c\u53cd\u4e8b\u5b9e\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u6cca\u8f66\u662f\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u7684\u6311\u6218\u6027\u9886\u57df\uff0c\u9700\u8981\u5f3a\u5927\u7684\u573a\u666f\u7406\u89e3\u548c\u4ea4\u4e92\u63a8\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u76f8\u4e92\u4f9d\u8d56\u7684\u610f\u56fe\u9884\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\u5b64\u7acb\u5904\u7406\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\"what-if\"\u51b3\u7b56\u5236\u5b9a\u3002", "method": "1. \u5f15\u5165\u81ea\u6211\u610f\u56fe\u6807\u8bb0\u5668\u9884\u6d4b\u79bb\u6563\u7ec8\u70b9\u610f\u56fe\uff1b2. \u6267\u884c\u81ea\u6211\u610f\u56fe\u6761\u4ef6\u8054\u5408\u9884\u6d4b\uff1b3. \u4f7f\u7528\u8f7b\u91cf\u7ea7\u5b89\u5168\u5f15\u5bfc\u53bb\u566a\u5668\u4f18\u5316\u8054\u5408\u573a\u666f\uff1b4. \u63d0\u51fa\u53cd\u4e8b\u5b9e\u77e5\u8bc6\u84b8\u998f\uff0c\u901a\u8fc7EMA\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u4f2a\u76ee\u6807\u3002", "result": "\u5728Dragon Lake Parking (DLP)\u6570\u636e\u96c6\u548cIntersections Drone (inD)\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5b9a\u6027\u53ef\u89c6\u5316\u663e\u793a\u5176\u4ed6\u667a\u80fd\u4f53\u5bf9\u4e0d\u540c\u81ea\u6211\u610f\u56fe\u505a\u51fa\u9002\u5f53\u53cd\u5e94\u3002", "conclusion": "ParkDiffusion++\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u591a\u6a21\u6001\u81ea\u6211\u610f\u56fe\u9884\u6d4b\u548c\u81ea\u6211\u6761\u4ef6\u591a\u667a\u80fd\u4f53\u8054\u5408\u8f68\u8ff9\u9884\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u6cca\u8f66\u4e2d\u7684\u573a\u666f\u7406\u89e3\u548c\u4ea4\u4e92\u63a8\u7406\u6311\u6218\uff0c\u652f\u6301\u66f4\u597d\u7684what-if\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2602.20925", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20925", "abs": "https://arxiv.org/abs/2602.20925", "authors": ["Zeyu Jiang", "Kuan Xu", "Changhao Chen"], "title": "LST-SLAM: A Stereo Thermal SLAM System for Kilometer-Scale Dynamic Environments", "comment": "ICRA 2026", "summary": "Thermal cameras offer strong potential for robot perception under challenging illumination and weather conditions. However, thermal Simultaneous Localization and Mapping (SLAM) remains difficult due to unreliable feature extraction, unstable motion tracking, and inconsistent global pose and map construction, particularly in dynamic large-scale outdoor environments. To address these challenges, we propose LST-SLAM, a novel large-scale stereo thermal SLAM system that achieves robust performance in complex, dynamic scenes. Our approach combines self-supervised thermal feature learning, stereo dual-level motion tracking, and geometric pose optimization. We also introduce a semantic-geometric hybrid constraint that suppresses potentially dynamic features lacking strong inter-frame geometric consistency. Furthermore, we develop an online incremental bag-of-words model for loop closure detection, coupled with global pose optimization to mitigate accumulated drift. Extensive experiments on kilometer-scale dynamic thermal datasets show that LST-SLAM significantly outperforms recent representative SLAM systems, including AirSLAM and DROID-SLAM, in both robustness and accuracy.", "AI": {"tldr": "LST-SLAM\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u6237\u5916\u73af\u5883\u7684\u65b0\u578b\u7acb\u4f53\u70ed\u6210\u50cfSLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u3001\u53cc\u7ea7\u8fd0\u52a8\u8ddf\u8e2a\u548c\u51e0\u4f55\u59ff\u6001\u4f18\u5316\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\u3002", "motivation": "\u70ed\u6210\u50cf\u76f8\u673a\u5728\u6076\u52a3\u5149\u7167\u548c\u5929\u6c14\u6761\u4ef6\u4e0b\u5177\u6709\u611f\u77e5\u4f18\u52bf\uff0c\u4f46\u70ed\u6210\u50cfSLAM\u9762\u4e34\u7279\u5f81\u63d0\u53d6\u4e0d\u53ef\u9760\u3001\u8fd0\u52a8\u8ddf\u8e2a\u4e0d\u7a33\u5b9a\u3001\u5168\u5c40\u59ff\u6001\u548c\u5730\u56fe\u6784\u5efa\u4e0d\u4e00\u81f4\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u70ed\u6210\u50cf\u7279\u5f81\u5b66\u4e60\u3001\u7acb\u4f53\u53cc\u7ea7\u8fd0\u52a8\u8ddf\u8e2a\u548c\u51e0\u4f55\u59ff\u6001\u4f18\u5316\uff1b\u5f15\u5165\u8bed\u4e49-\u51e0\u4f55\u6df7\u5408\u7ea6\u675f\u6291\u5236\u7f3a\u4e4f\u5e27\u95f4\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u52a8\u6001\u7279\u5f81\uff1b\u5f00\u53d1\u5728\u7ebf\u589e\u91cf\u8bcd\u888b\u6a21\u578b\u8fdb\u884c\u56de\u73af\u68c0\u6d4b\uff0c\u914d\u5408\u5168\u5c40\u59ff\u6001\u4f18\u5316\u51cf\u5c11\u7d2f\u79ef\u6f02\u79fb\u3002", "result": "\u5728\u5343\u7c73\u7ea7\u52a8\u6001\u70ed\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLST-SLAM\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u8fd1\u7684\u4ee3\u8868\u6027SLAM\u7cfb\u7edf\uff0c\u5305\u62ecAirSLAM\u548cDROID-SLAM\u3002", "conclusion": "LST-SLAM\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u70ed\u6210\u50cfSLAM\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u5b66\u4e60\u3001\u8fd0\u52a8\u8ddf\u8e2a\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u6237\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2602.20958", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20958", "abs": "https://arxiv.org/abs/2602.20958", "authors": ["Luka \u0160iktar", "Branimir \u0106aran", "Bojan \u0160ekoranja", "Marko \u0160vaco"], "title": "EKF-Based Depth Camera and Deep Learning Fusion for UAV-Person Distance Estimation and Following in SAR Operations", "comment": null, "summary": "Search and rescue (SAR) operations require rapid responses to save lives or property. Unmanned Aerial Vehicles (UAVs) equipped with vision-based systems support these missions through prior terrain investigation or real-time assistance during the mission itself. Vision-based UAV frameworks aid human search tasks by detecting and recognizing specific individuals, then tracking and following them while maintaining a safe distance. A key safety requirement for UAV following is the accurate estimation of the distance between camera and target object under real-world conditions, achieved by fusing multiple image modalities. UAVs with deep learning-based vision systems offer a new approach to the planning and execution of SAR operations. As part of the system for automatic people detection and face recognition using deep learning, in this paper we present the fusion of depth camera measurements and monocular camera-to-body distance estimation for robust tracking and following. Deep learning-based filtering of depth camera data and estimation of camera-to-body distance from a monocular camera are achieved with YOLO-pose, enabling real-time fusion of depth information using the Extended Kalman Filter (EKF) algorithm. The proposed subsystem, designed for use in drones, estimates and measures the distance between the depth camera and the human body keypoints, to maintain the safe distance between the drone and the human target. Our system provides an accurate estimated distance, which has been validated against motion capture ground truth data. The system has been tested in real time indoors, where it reduces the average errors, root mean square error (RMSE) and standard deviations of distance estimation up to 15,3\\% in three tested scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u641c\u6551\u4efb\u52a1\u7684\u8ddd\u79bb\u4f30\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408\u6df1\u5ea6\u76f8\u673a\u6570\u636e\u548c\u5355\u76ee\u76f8\u673a\u7684\u4eba\u4f53\u8ddd\u79bb\u4f30\u8ba1\uff0c\u4f7f\u7528YOLO-pose\u548c\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b9e\u73b0\u5b9e\u65f6\u8ddd\u79bb\u6d4b\u91cf\uff0c\u4ee5\u4fdd\u6301\u65e0\u4eba\u673a\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684\u5b89\u5168\u8ddd\u79bb\u3002", "motivation": "\u641c\u6551\u4efb\u52a1\u9700\u8981\u5feb\u901f\u54cd\u5e94\uff0c\u65e0\u4eba\u673a\u914d\u5907\u89c6\u89c9\u7cfb\u7edf\u53ef\u4ee5\u8f85\u52a9\u641c\u6551\u4efb\u52a1\u3002\u5173\u952e\u7684\u5b89\u5168\u8981\u6c42\u662f\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u51c6\u786e\u4f30\u8ba1\u76f8\u673a\u4e0e\u76ee\u6807\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u8fd9\u9700\u8981\u878d\u5408\u591a\u79cd\u56fe\u50cf\u6a21\u6001\u6765\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6df1\u5ea6\u76f8\u673a\u6d4b\u91cf\u548c\u5355\u76ee\u76f8\u673a\u5230\u4eba\u4f53\u8ddd\u79bb\u4f30\u8ba1\u7684\u7cfb\u7edf\u3002\u4f7f\u7528YOLO-pose\u8fdb\u884c\u6df1\u5ea6\u76f8\u673a\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u6ee4\u6ce2\u548c\u5355\u76ee\u76f8\u673a\u8ddd\u79bb\u4f30\u8ba1\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7b97\u6cd5\u5b9e\u65f6\u878d\u5408\u6df1\u5ea6\u4fe1\u606f\u3002", "result": "\u7cfb\u7edf\u5728\u5ba4\u5185\u5b9e\u65f6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e09\u79cd\u6d4b\u8bd5\u573a\u666f\u4e0b\u5c06\u8ddd\u79bb\u4f30\u8ba1\u7684\u5e73\u5747\u8bef\u5dee\u3001\u5747\u65b9\u6839\u8bef\u5dee\u548c\u6807\u51c6\u5dee\u964d\u4f4e\u4e8615.3%\u3002\u7cfb\u7edf\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u4f30\u8ba1\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u5730\u9762\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b50\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u6df1\u5ea6\u76f8\u673a\u4e0e\u4eba\u4f53\u5173\u952e\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u4e3a\u65e0\u4eba\u673a\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u8ddf\u8e2a\u548c\u8ddf\u968f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ddd\u79bb\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20963", "abs": "https://arxiv.org/abs/2602.20963", "authors": ["Ang", "Li", "Alexander Yin", "Alexander White", "Sahib Sandhu", "Matthew Francoeur", "Victor Jimenez-Santiago", "Van Remenar", "Codrin Tugui", "Mihai Duduta"], "title": "A Robotic Testing Platform for Pipelined Discovery of Resilient Soft Actuators", "comment": null, "summary": "Short lifetime under high electrical fields hinders the widespread robotic application of linear dielectric elastomer actuators (DEAs). Systematic scanning is difficult due to time-consuming per-sample testing and the high-dimensional parameter space affecting performance. To address this, we propose an optimization pipeline enabled by a novel testing robot capable of scanning DEA lifetime. The robot integrates electro-mechanical property measurement, programmable voltage input, and multi-channel testing capacity. Using it, we scanned the lifetime of Elastosil-based linear actuators across parameters including input voltage magnitude, frequency, electrode material concentration, and electrical connection filler. The optimal parameter combinations improved operational lifetime under boundary operating conditions by up to 100% and were subsequently scaled up to achieve higher force and displacement output. The final product demonstrated resilience on a modular, scalable quadruped walking robot with payload carrying capacity (>100% of its untethered body weight, and >700% of combined actuator weight). This work is the first to introduce a self-driving lab approach into robotic actuator design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f18\u5316\u7ebf\u6027\u4ecb\u7535\u5f39\u6027\u4f53\u81f4\u52a8\u5668\uff08DEA\uff09\u5bff\u547d\u7684\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u578b\u6d4b\u8bd5\u673a\u5668\u4eba\u626b\u63cf\u591a\u53c2\u6570\u7a7a\u95f4\uff0c\u5c06DEA\u5bff\u547d\u63d0\u5347100%\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u6a21\u5757\u5316\u56db\u8db3\u673a\u5668\u4eba\u3002", "motivation": "\u7ebf\u6027\u4ecb\u7535\u5f39\u6027\u4f53\u81f4\u52a8\u5668\u5728\u9ad8\u7535\u573a\u4e0b\u7684\u77ed\u5bff\u547d\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u626b\u63cf\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u6837\u672c\u6d4b\u8bd5\u8017\u65f6\u4e14\u53c2\u6570\u4f17\u591a\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u65b0\u578b\u6d4b\u8bd5\u673a\u5668\u4eba\u652f\u6301\u7684\u4f18\u5316\u6d41\u7a0b\uff0c\u8be5\u673a\u5668\u4eba\u96c6\u6210\u4e86\u673a\u7535\u6027\u80fd\u6d4b\u91cf\u3001\u53ef\u7f16\u7a0b\u7535\u538b\u8f93\u5165\u548c\u591a\u901a\u9053\u6d4b\u8bd5\u80fd\u529b\u3002\u4f7f\u7528\u8be5\u673a\u5668\u4eba\u626b\u63cf\u4e86\u57fa\u4e8eElastosil\u7684\u7ebf\u6027\u81f4\u52a8\u5668\u5728\u4e0d\u540c\u53c2\u6570\uff08\u5305\u62ec\u8f93\u5165\u7535\u538b\u5e45\u503c\u3001\u9891\u7387\u3001\u7535\u6781\u6750\u6599\u6d53\u5ea6\u548c\u7535\u8fde\u63a5\u586b\u6599\uff09\u4e0b\u7684\u5bff\u547d\u3002", "result": "\u6700\u4f18\u53c2\u6570\u7ec4\u5408\u5728\u8fb9\u754c\u64cd\u4f5c\u6761\u4ef6\u4e0b\u5c06\u8fd0\u884c\u5bff\u547d\u63d0\u9ad8\u4e86100%\u3002\u968f\u540e\u5c06\u4f18\u5316\u7ed3\u679c\u653e\u5927\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u529b\u548c\u4f4d\u79fb\u8f93\u51fa\u3002\u6700\u7ec8\u4ea7\u54c1\u5728\u6a21\u5757\u5316\u53ef\u6269\u5c55\u7684\u56db\u8db3\u884c\u8d70\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u627f\u8f7d\u80fd\u529b\u8d85\u8fc7\u5176\u65e0\u7ef3\u4f53\u91cd\u7684100%\uff0c\u4ee5\u53ca\u8d85\u8fc7\u81f4\u52a8\u5668\u7ec4\u5408\u91cd\u91cf\u7684700%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u9996\u6b21\u5c06\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\u65b9\u6cd5\u5f15\u5165\u673a\u5668\u4eba\u81f4\u52a8\u5668\u8bbe\u8ba1\uff0c\u4e3aDEA\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81f4\u52a8\u5668\u5bff\u547d\u548c\u673a\u5668\u4eba\u6027\u80fd\u3002"}}
{"id": "2602.21028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21028", "abs": "https://arxiv.org/abs/2602.21028", "authors": ["Gayatri Indukumar", "Muhammad Awais", "Diana Cafiso", "Matteo Lo Preti", "Lucia Beccai"], "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing", "comment": "6 pages, 6 figures, 1 table, to be published in RoboSoft 2026 proceedings", "summary": "There is a growing need for soft robotic platforms that perform gentle, precise handling of a wide variety of objects. Existing surface-based manipulation systems, however, lack the compliance and tactile feedback needed for delicate handling. This work introduces the COmpliant Porous-Elastic Soft Sensing (COPESS) integrated with inductive sensors for adaptive object manipulation and localised sensing. The design features a tunable lattice layer that simultaneously modulates mechanical compliance and sensing performance. By adjusting lattice geometry, both stiffness and sensor response can be tailored to handle objects with varying mechanical properties. Experiments demonstrate that by easily adjusting one parameter, the lattice density, from 7 % to 20 %, it is possible to significantly alter the sensitivity and operational force range (about -23x and 9x, respectively). This approach establishes a blueprint for creating adaptive, sensorized surfaces where mechanical and sensory properties are co-optimized, enabling passive, yet programmable, delicate manipulation.", "AI": {"tldr": "COPESS\u7cfb\u7edf\u901a\u8fc7\u53ef\u8c03\u6676\u683c\u5c42\u540c\u65f6\u8c03\u63a7\u673a\u68b0\u67d4\u987a\u6027\u548c\u4f20\u611f\u6027\u80fd\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7269\u4f53\u64cd\u4f5c\u548c\u5c40\u90e8\u4f20\u611f", "motivation": "\u73b0\u6709\u8868\u9762\u64cd\u4f5c\u7cfb\u7edf\u7f3a\u4e4f\u5904\u7406\u5404\u79cd\u7269\u4f53\u6240\u9700\u7684\u67d4\u987a\u6027\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u8f7b\u67d4\u3001\u7cbe\u786e\u64cd\u4f5c\u7684\u8f6f\u673a\u5668\u4eba\u5e73\u53f0", "method": "\u5f15\u5165COPESS\u7cfb\u7edf\uff0c\u96c6\u6210\u611f\u5e94\u4f20\u611f\u5668\uff0c\u91c7\u7528\u53ef\u8c03\u6676\u683c\u5c42\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8c03\u6574\u6676\u683c\u51e0\u4f55\u5f62\u72b6\u540c\u65f6\u8c03\u5236\u673a\u68b0\u67d4\u987a\u6027\u548c\u4f20\u611f\u6027\u80fd", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u7b80\u5355\u8c03\u6574\u6676\u683c\u5bc6\u5ea6\uff08\u4ece7%\u523020%\uff09\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u7075\u654f\u5ea6\u548c\u64cd\u4f5c\u529b\u8303\u56f4\uff08\u5206\u522b\u7ea6-23\u500d\u548c9\u500d\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u521b\u5efa\u81ea\u9002\u5e94\u4f20\u611f\u8868\u9762\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u5b9e\u73b0\u4e86\u673a\u68b0\u548c\u611f\u5b98\u7279\u6027\u7684\u534f\u540c\u4f18\u5316\uff0c\u652f\u6301\u88ab\u52a8\u4f46\u53ef\u7f16\u7a0b\u7684\u7cbe\u7ec6\u64cd\u4f5c"}}
{"id": "2602.21119", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21119", "abs": "https://arxiv.org/abs/2602.21119", "authors": ["Rui Zhao", "Xihui Li", "Yizheng Zhang", "Yuzhen Liu", "Zhong Zhang", "Yufeng Zhang", "Cheng Zhou", "Zhengyou Zhang", "Lei Han"], "title": "Cooperative-Competitive Team Play of Real-World Craft Robots", "comment": "Accepted by 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria", "summary": "Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement learning techniques designed for efficient training of cooperative and competitive policies on this platform. To address the challenges of multi-agent sim-to-real transfer, we introduce Out of Distribution State Initialization (OODSI) to mitigate the impact of the sim-to-real gap. In the experiments, OODSI improves the Sim2Real performance by 20%. We demonstrate the effectiveness of our approach through experiments with a multi-robot car competitive game and a cooperative task in real-world settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5e73\u53f0\uff0c\u5e76\u63d0\u51faOODSI\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u6e38\u620f\u667a\u80fd\u4f53\u5f00\u53d1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u8bad\u7ec3\u96c6\u4f53\u673a\u5668\u4eba\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8fc1\u79fb\u5230\u73b0\u5b9e\u5e94\u7528\u4e2d\u4ecd\u7136\u662f\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "\u9996\u5148\u5f00\u53d1\u4e86\u5305\u542b\u4eff\u771f\u3001\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\u548c\u7269\u7406\u673a\u5668\u4eba\u7ec4\u4ef6\u7684\u7efc\u5408\u673a\u5668\u4eba\u7cfb\u7edf\uff1b\u7136\u540e\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e13\u95e8\u4e3a\u6b64\u5e73\u53f0\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff1b\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u5f15\u5165\u4e86OODSI\uff08\u5206\u5e03\u5916\u72b6\u6001\u521d\u59cb\u5316\uff09\u65b9\u6cd5\u6765\u51cf\u5c11\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u5f71\u54cd\u3002", "result": "OODSI\u65b9\u6cd5\u5c06Sim2Real\u6027\u80fd\u63d0\u5347\u4e8620%\uff1b\u901a\u8fc7\u591a\u673a\u5668\u4eba\u6c7d\u8f66\u7ade\u4e89\u6e38\u620f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u4efb\u52a1\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7efc\u5408\u673a\u5668\u4eba\u7cfb\u7edf\u548cOODSI\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u95ee\u9898\uff0c\u4e3a\u96c6\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21148", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21148", "abs": "https://arxiv.org/abs/2602.21148", "authors": ["Davis S. Catherman", "Carlo Pinciroli"], "title": "A Micro-Macro Model of Encounter-Driven Information Diffusion in Robot Swarms", "comment": "10 pages, 5 figures, published at ANTS 2026", "summary": "In this paper, we propose the problem of Encounter-Driven Information Diffusion (EDID). In EDID, robots are allowed to exchange information only upon meeting. Crucially, EDID assumes that the robots are not allowed to schedule their meetings. As such, the robots have no means to anticipate when, where, and who they will meet. As a step towards the design of storage and routing algorithms for EDID, in this paper we propose a model of information diffusion that captures the essential dynamics of EDID. The model is derived from first principles and is composed of two levels: a micro model, based on a generalization of the concept of `mean free path'; and a macro model, which captures the global dynamics of information diffusion. We validate the model through extensive robot simulations, in which we consider swarm size, communication range, environment size, and different random motion regimes. We conclude the paper with a discussion of the implications of this model on the algorithms that best support information diffusion according to the parameters of interest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76f8\u9047\u9a71\u52a8\u4fe1\u606f\u6269\u6563\uff08EDID\uff09\u95ee\u9898\uff0c\u7814\u7a76\u673a\u5668\u4eba\u5728\u53ea\u80fd\u76f8\u9047\u65f6\u4ea4\u6362\u4fe1\u606f\u4e14\u65e0\u6cd5\u5b89\u6392\u4f1a\u9762\u7684\u573a\u666f\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u5fae\u89c2\u548c\u5b8f\u89c2\u6a21\u578b\u7684\u4fe1\u606f\u6269\u6563\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u5728\u673a\u5668\u4eba\u53ea\u80fd\u901a\u8fc7\u76f8\u9047\u4ea4\u6362\u4fe1\u606f\u4e14\u65e0\u6cd5\u9884\u6d4b\u4f1a\u9762\u65f6\u95f4\u3001\u5730\u70b9\u548c\u5bf9\u8c61\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u4fe1\u606f\u5b58\u50a8\u548c\u8def\u7531\u7b97\u6cd5\uff0c\u8fd9\u662f\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53cc\u5c42\u6a21\u578b\uff1a\u5fae\u89c2\u6a21\u578b\u57fa\u4e8e\"\u5e73\u5747\u81ea\u7531\u7a0b\"\u6982\u5ff5\u7684\u63a8\u5e7f\uff0c\u63cf\u8ff0\u4e2a\u4f53\u76f8\u9047\u6982\u7387\uff1b\u5b8f\u89c2\u6a21\u578b\u6355\u6349\u4fe1\u606f\u6269\u6563\u7684\u5168\u5c40\u52a8\u6001\u3002\u901a\u8fc7\u5927\u91cf\u673a\u5668\u4eba\u4eff\u771f\u9a8c\u8bc1\u6a21\u578b\uff0c\u8003\u8651\u7fa4\u4f53\u89c4\u6a21\u3001\u901a\u4fe1\u8303\u56f4\u3001\u73af\u5883\u5927\u5c0f\u548c\u4e0d\u540c\u968f\u673a\u8fd0\u52a8\u673a\u5236\u3002", "result": "\u5efa\u7acb\u4e86\u80fd\u591f\u51c6\u786e\u6355\u6349EDID\u57fa\u672c\u52a8\u6001\u7684\u4fe1\u606f\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bbe\u8ba1\u652f\u6301\u4fe1\u606f\u6269\u6563\u7684\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aEDID\u95ee\u9898\u4e2d\u7684\u5b58\u50a8\u548c\u8def\u7531\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u4f9d\u636e\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u53c2\u6570\u5bf9\u4fe1\u606f\u6269\u6563\u6548\u7387\u7684\u5f71\u54cd\uff0c\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4fe1\u606f\u4f20\u64ad\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.21157", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21157", "abs": "https://arxiv.org/abs/2602.21157", "authors": ["Quanxin Shou", "Fangqi Zhu", "Shawn Chen", "Puxin Yan", "Zhengyang Yan", "Yikun Miao", "Xiaoyi Pang", "Zicong Hong", "Ruikai Shi", "Hao Huang", "Jie Zhang", "Song Guo"], "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.", "AI": {"tldr": "HALO\u6a21\u578b\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u957f\u89c6\u91ce\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\u63d0\u534734.1%", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u957f\u89c6\u91ce\u6216\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u52a8\u4f5c\u5f71\u54cd\u9884\u6d4b\u673a\u5236\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u5f15\u5165\u6587\u672c\u601d\u7ef4\u94fe\u6216\u89c6\u89c9\u5b50\u76ee\u6807\u9884\u6d4b\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u7edf\u4e00\u7684\u4eba\u7c7b\u5f0f\u63a8\u7406\u6846\u67b6\u6765\u6574\u5408\u6587\u672c\u63a8\u7406\u3001\u89c6\u89c9\u9884\u89c1\u548c\u52a8\u4f5c\u9884\u6d4b\u3002", "method": "\u63d0\u51faHALO\u6a21\u578b\uff0c\u901a\u8fc7\u5177\u8eab\u591a\u6a21\u6001\u601d\u7ef4\u94fe(EM-CoT)\u63a8\u7406\u5b9e\u73b0\u6587\u672c\u4efb\u52a1\u63a8\u7406\u3001\u89c6\u89c9\u5b50\u76ee\u6807\u9884\u6d4b\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\u3002\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5c06\u8bed\u4e49\u63a8\u7406\u3001\u89c6\u89c9\u9884\u89c1\u548c\u52a8\u4f5c\u9884\u6d4b\u89e3\u8026\u4e3a\u4e13\u5bb6\u6a21\u5757\uff0c\u540c\u65f6\u652f\u6301\u8de8\u4e13\u5bb6\u534f\u4f5c\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u7684EM-CoT\u8bad\u7ec3\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u5747\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u5728RoboTwin\u57fa\u51c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u7b56\u756534.1%\u3002\u6240\u6709\u63d0\u51fa\u7684\u8bad\u7ec3\u65b9\u6848\u548cEM-CoT\u8bbe\u8ba1\u7ec4\u4ef6\u90fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002\u5728\u6fc0\u8fdb\u7684\u672a\u89c1\u73af\u5883\u968f\u673a\u5316\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HALO\u901a\u8fc7\u7edf\u4e00\u7684\u5177\u8eab\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u957f\u89c6\u91ce\u548c\u5206\u5e03\u5916\u573a\u666f\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.21161", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21161", "abs": "https://arxiv.org/abs/2602.21161", "authors": ["Guangming Wang", "Qizhen Ying", "Yixiong Jing", "Olaf Wysocki", "Brian Sheil"], "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking", "comment": "8 pages, 5 figures, accepted by the 2026 IEEE International Conference on Robotics and Automation", "summary": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faActionReasoning\u6846\u67b6\uff0c\u5229\u7528LLM\u8fdb\u884c\u663e\u5f0f\u52a8\u4f5c\u63a8\u7406\uff0c\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u52a8\u4f5c\u51b3\u7b56\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7816\u5757\u5806\u53e0\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u5b9a\u5236\u89c4\u5212\u5668\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff1b\u73b0\u6709VLA\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bed\u8a00token\u5bf9\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u8868\u793a\u80fd\u529b\uff0c\u9700\u8981\u7269\u7406\u63a8\u7406\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faActionReasoning\u6846\u67b6\uff0c\u5229\u7528LLM\u4e2d\u5df2\u6709\u7684\u7269\u7406\u5148\u9a8c\u548c\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u73af\u5883\u72b6\u6001\u5e8f\u5217\u5316\u540e\u8f93\u5165LLM\u751f\u6210\u7269\u7406\u611f\u77e5\u7684\u52a8\u4f5c\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u7684\u7816\u5757\u653e\u7f6e\uff0c\u5c06\u5de5\u4f5c\u91cd\u5fc3\u4ece\u5e95\u5c42\u7f16\u7801\u8f6c\u79fb\u5230\u9ad8\u5c42\u5de5\u5177\u8c03\u7528\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u5c55\u73b0\u51fa\u6cdb\u5316\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c06\u7269\u7406\u63a8\u7406\u4e0eLLM\u7ed3\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u4e0e\u6267\u884c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6865\u6881\u65b9\u6cd5\u3002"}}
{"id": "2602.21174", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21174", "abs": "https://arxiv.org/abs/2602.21174", "authors": ["Victor Reijgwart", "Cesar Cadena", "Roland Siegwart", "Lionel Ott"], "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids", "comment": "12 pages, 9 figures, 4 tables, accepted to RSS 2025, code is open-source: https://github.com/ethz-asl/wavestar", "summary": "Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the underpinning of the navigation system. For such applications, any-angle planning methods, which find optimal paths by connecting corners of obstacles with straight-line segments, provide a simple and efficient solution. In this paper, we present a method that has the optimality and completeness properties of any-angle planners while overcoming computational tractability issues common to search-based methods by exploiting multi-resolution representations. Extensive experiments on real and synthetic environments demonstrate the proposed approach's solution quality and speed, outperforming even sampling-based methods. The framework is open-sourced to allow the robotics and planning community to build on our research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u5730\u56fe\u7684\u4efb\u610f\u89d2\u5ea6\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4efb\u610f\u89d2\u5ea6\u89c4\u5212\u7684\u6700\u4f18\u6027\u548c\u5b8c\u5907\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u8868\u793a\u89e3\u51b3\u4e86\u641c\u7d22\u65b9\u6cd5\u7684\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u91c7\u6837\u548c\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u4e0d\u5229\u7528\u5730\u56fe\u7684\u663e\u5f0f\u8fde\u901a\u6027\u4fe1\u606f\uff0c\u800c\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\uff08\u5982A*\uff09\u5728\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u7387\u5730\u56fe\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u591a\u5206\u8fa8\u7387\u5730\u56fe\u8fde\u901a\u6027\u4fe1\u606f\uff0c\u53c8\u80fd\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4efb\u610f\u89d2\u5ea6\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u63a5\u969c\u788d\u7269\u89d2\u843d\u7684\u76f4\u7ebf\u6bb5\u6765\u5bfb\u627e\u6700\u4f18\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u5730\u56fe\u8868\u793a\uff0c\u7ed3\u5408\u4e86\u4efb\u610f\u89d2\u5ea6\u89c4\u5212\u7684\u6700\u4f18\u6027\u548c\u5b8c\u5907\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u8868\u793a\u514b\u670d\u4e86\u641c\u7d22\u65b9\u6cd5\u7684\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89e3\u8d28\u91cf\u548c\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u4f18\u4e8e\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6700\u4f18\u6027\u548c\u5b8c\u5907\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u4efb\u610f\u89d2\u5ea6\u89c4\u5212\u7684\u6700\u4f18\u6027\u548c\u5b8c\u5907\u6027\u4e0e\u591a\u5206\u8fa8\u7387\u8868\u793a\u7684\u8ba1\u7b97\u6548\u7387\u76f8\u7ed3\u5408\uff0c\u4e3a\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u6846\u67b6\u5df2\u5f00\u6e90\uff0c\u4f9b\u673a\u5668\u4eba\u5b66\u548c\u89c4\u5212\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2602.21203", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21203", "abs": "https://arxiv.org/abs/2602.21203", "authors": ["Abdulaziz Almuzairee", "Henrik I. Christensen"], "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics", "comment": "For website and code, see https://aalmuzairee.github.io/squint", "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.", "AI": {"tldr": "Squint\u662f\u4e00\u79cd\u89c6\u89c9\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u4eff\u771f\u3001\u5206\u5e03\u8bc4\u8bba\u5bb6\u3001\u5206\u8fa8\u7387\u538b\u7f29\u7b49\u6280\u672f\uff0c\u5728\u5355GPU\u4e0a15\u5206\u949f\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u6bd4\u73b0\u6709\u89c6\u89c9\u79bb\u7b56\u7565\u548c\u540c\u7b56\u7565\u65b9\u6cd5\u8bad\u7ec3\u66f4\u5feb\u3002", "motivation": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5f88\u6709\u524d\u666f\u4f46\u6210\u672c\u9ad8\u6602\uff1a\u79bb\u7b56\u7565\u65b9\u6cd5\u6837\u672c\u6548\u7387\u9ad8\u4f46\u8bad\u7ec3\u6162\uff1b\u540c\u7b56\u7565\u65b9\u6cd5\u53ef\u5e76\u884c\u5316\u4f46\u6d6a\u8d39\u6837\u672c\u3002\u73b0\u6709\u5de5\u4f5c\u8868\u660e\u79bb\u7b56\u7565\u65b9\u6cd5\u5728\u72b6\u6001\u63a7\u5236\u4e2d\u6bd4\u540c\u7b56\u7565\u65b9\u6cd5\u8bad\u7ec3\u66f4\u5feb\uff0c\u4f46\u6269\u5c55\u5230\u89c6\u89c9\u9886\u57df\u4ecd\u9762\u4e34\u9ad8\u7ef4\u56fe\u50cf\u8f93\u5165\u5e26\u6765\u7684\u8bad\u7ec3\u52a8\u6001\u590d\u6742\u3001\u5b58\u50a8\u548c\u7f16\u7801\u5f00\u9500\u5927\u7684\u6311\u6218\u3002", "method": "Squint\u65b9\u6cd5\u5305\u542b\uff1a1) \u5e76\u884c\u4eff\u771f\uff1b2) \u5206\u5e03\u8bc4\u8bba\u5bb6\uff1b3) \u5206\u8fa8\u7387\u538b\u7f29\uff08\u964d\u4f4e\u56fe\u50cf\u5206\u8fa8\u7387\uff09\uff1b4) \u5c42\u5f52\u4e00\u5316\uff1b5) \u4f18\u5316\u7684\u66f4\u65b0-\u6570\u636e\u6bd4\u7387\uff1b6) \u4f18\u5316\u7684\u5b9e\u73b0\u3002\u5728SO-101\u4efb\u52a1\u96c6\uff08ManiSkill3\u4e2d\u76848\u4e2a\u64cd\u4f5c\u4efb\u52a1\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u91c7\u7528\u91cd\u5ea6\u57df\u968f\u673a\u5316\u3002", "result": "\u5728\u5355RTX 3090 GPU\u4e0a15\u5206\u949f\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u5927\u591a\u6570\u4efb\u52a1\u57286\u5206\u949f\u5185\u6536\u655b\u3002\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9eSO-101\u673a\u5668\u4eba\u7684\u8fc1\u79fb\uff0c\u6bd4\u73b0\u6709\u89c6\u89c9\u79bb\u7b56\u7565\u548c\u540c\u7b56\u7565\u65b9\u6cd5\u8bad\u7ec3\u66f4\u5feb\u3002", "conclusion": "Squint\u901a\u8fc7\u591a\u79cd\u4f18\u5316\u6280\u672f\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8bad\u7ec3\u548c\u4eff\u771f\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
