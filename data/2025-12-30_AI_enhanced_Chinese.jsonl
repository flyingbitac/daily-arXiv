{"id": "2512.22342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22342", "abs": "https://arxiv.org/abs/2512.22342", "authors": ["Wensi Huang", "Shaohao Zhu", "Meng Wei", "Jinming Xu", "Xihui Liu", "Hanqing Wang", "Tai Wang", "Feng Zhao", "Jiangmiao Pang"], "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs", "comment": null, "summary": "In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4ea4\u4e92\u5f0f\u5b9e\u4f8b\u5bf9\u8c61\u5bfc\u822a\uff08IION\uff09\u4efb\u52a1\u548c\u89c6\u89c9\u8bed\u8a00-\u8bed\u8a00\u5bfc\u822a\uff08VL-LN\uff09\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u6a21\u7cca\u5bfc\u822a\u6307\u4ee4\u7684\u95ee\u9898\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u901a\u8fc7\u4e3b\u52a8\u5bf9\u8bdd\u6765\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6307\u4ee4\u901a\u5e38\u662f\u660e\u786e\u65e0\u6b67\u4e49\u7684\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u5bfc\u822a\u6307\u4ee4\u5f80\u5f80\u6a21\u7cca\u4e0d\u6e05\uff0c\u9700\u8981\u667a\u80fd\u4f53\u901a\u8fc7\u4e3b\u52a8\u5bf9\u8bdd\u6765\u89e3\u6790\u4e0d\u786e\u5b9a\u6027\u548c\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3002", "method": "\u63d0\u51faIION\u4efb\u52a1\uff0c\u6269\u5c55\u5b9e\u4f8b\u5bf9\u8c61\u5bfc\u822a\uff08ION\uff09\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7528\u81ea\u7136\u8bed\u8a00\u81ea\u7531\u54a8\u8be2oracle\u3002\u57fa\u4e8e\u6b64\u4efb\u52a1\u6784\u5efaVL-LN\u57fa\u51c6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u81ea\u52a8\u751f\u6210\u7684\u6570\u636e\u96c6\u548c\u7efc\u5408\u8bc4\u4f30\u534f\u8bae\u3002", "result": "VL-LN\u5305\u542b\u8d85\u8fc741k\u4e2a\u957f\u89c6\u91ce\u5bf9\u8bdd\u589e\u5f3a\u8f68\u8ff9\u7528\u4e8e\u8bad\u7ec3\uff0c\u4ee5\u53ca\u5e26\u6709\u80fd\u591f\u54cd\u5e94\u667a\u80fd\u4f53\u67e5\u8be2\u7684oracle\u7684\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u3002\u8bad\u7ec3\u51fa\u7684\u5177\u5907\u5bf9\u8bdd\u80fd\u529b\u7684\u5bfc\u822a\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VL-LN\u57fa\u51c6\u4e3a\u5bf9\u8bdd\u4f7f\u80fd\u7684\u5177\u8eab\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.22408", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22408", "abs": "https://arxiv.org/abs/2512.22408", "authors": ["Amro Gamar", "Ahmed Abduljalil", "Alargam Mohammed", "Ali Elhenidy", "Abeer Tawakol"], "title": "A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot", "comment": null, "summary": "This paper presents the development of a fully autonomous delivery robot integrating mechanical engineering, embedded systems, and artificial intelligence. The platform employs a heterogeneous computing architecture, with RPi 5 and ROS 2 handling AI-based perception and path planning, while ESP32 running FreeRTOS ensures real-time motor control. The mechanical design was optimized for payload capacity and mobility through precise motor selection and material engineering. Key technical challenges addressed include optimizing computationally intensive AI algorithms on a resource-constrained platform and implementing a low-latency, reliable communication link between the ROS 2 host and embedded controller. Results demonstrate deterministic, PID-based motor control through rigorous memory and task management, and enhanced system reliability via AWS IoT monitoring and a firmware-level motor shutdown failsafe. This work highlights a unified, multi-disciplinary methodology, resulting in a robust and operational autonomous delivery system capable of real-world deployment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u673a\u68b0\u5de5\u7a0b\u3001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u4eba\u5de5\u667a\u80fd\u4e8e\u4e00\u4f53\u7684\u5168\u81ea\u4e3b\u9001\u8d27\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u91c7\u7528\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86AI\u611f\u77e5\u4e0e\u5b9e\u65f6\u63a7\u5236\u7684\u534f\u540c\u5de5\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u7684\u81ea\u4e3b\u9001\u8d27\u7cfb\u7edf\uff0c\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8ba1\u7b97\u5bc6\u96c6\u578bAI\u7b97\u6cd5\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u53caROS 2\u4e3b\u673a\u4e0e\u5d4c\u5165\u5f0f\u63a7\u5236\u5668\u4e4b\u95f4\u4f4e\u5ef6\u8fdf\u53ef\u9760\u901a\u4fe1\u7684\u6280\u672f\u6311\u6218\u3002", "method": "\u91c7\u7528\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\uff1aRPi 5\u548cROS 2\u5904\u7406\u57fa\u4e8eAI\u7684\u611f\u77e5\u548c\u8def\u5f84\u89c4\u5212\uff0cESP32\u8fd0\u884cFreeRTOS\u786e\u4fdd\u5b9e\u65f6\u7535\u673a\u63a7\u5236\u3002\u901a\u8fc7\u7cbe\u786e\u7684\u7535\u673a\u9009\u62e9\u548c\u6750\u6599\u5de5\u7a0b\u4f18\u5316\u673a\u68b0\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u6709\u6548\u8f7d\u8377\u80fd\u529b\u548c\u79fb\u52a8\u6027\u3002\u91c7\u7528PID\u7535\u673a\u63a7\u5236\u3001\u4e25\u683c\u7684\u5185\u5b58\u548c\u4efb\u52a1\u7ba1\u7406\uff0c\u4ee5\u53caAWS IoT\u76d1\u63a7\u548c\u56fa\u4ef6\u7ea7\u7535\u673a\u505c\u673a\u5b89\u5168\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u7684PID\u7535\u673a\u63a7\u5236\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u5185\u5b58\u548c\u4efb\u52a1\u7ba1\u7406\u786e\u4fdd\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002AWS IoT\u76d1\u63a7\u548c\u56fa\u4ef6\u7ea7\u7535\u673a\u505c\u673a\u5b89\u5168\u673a\u5236\u589e\u5f3a\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002\u6700\u7ec8\u5f00\u53d1\u51fa\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u64cd\u4f5c\u7684\u81ea\u4e3b\u9001\u8d27\u7cfb\u7edf\uff0c\u5177\u5907\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u5b66\u79d1\u65b9\u6cd5\u8bba\uff0c\u6210\u529f\u5f00\u53d1\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u7684\u7a33\u5065\u81ea\u4e3b\u9001\u8d27\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u6280\u672f\u6311\u6218\uff0c\u5b9e\u73b0\u4e86AI\u611f\u77e5\u4e0e\u5b9e\u65f6\u63a7\u5236\u7684\u534f\u540c\u5de5\u4f5c\u3002"}}
{"id": "2512.22414", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22414", "abs": "https://arxiv.org/abs/2512.22414", "authors": ["Simar Kareer", "Karl Pertsch", "James Darpinian", "Judy Hoffman", "Danfei Xu", "Sergey Levine", "Chelsea Finn", "Suraj Nair"], "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u53ef\u80fd\u6027\uff0c\u53d1\u73b0\u5f53\u6a21\u578b\u5728\u8db3\u591f\u591a\u6837\u5316\u7684\u573a\u666f\u3001\u4efb\u52a1\u548c\u4f53\u73b0\u5f62\u5f0f\u4e0a\u9884\u8bad\u7ec3\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u4ece\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u7684\u6280\u80fd\u8fc1\u79fb\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u6570\u636e\u6765\u5b9e\u73b0\u5f00\u653e\u4e16\u754c\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4eba\u7c7b\u89c6\u9891\u8986\u76d6\u4e86\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e14\u6613\u4e8e\u83b7\u53d6\uff0c\u4f46\u76f4\u63a5\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u8bad\u7ec3VLA\u6a21\u578b\u9762\u4e34\u4eba\u7c7b-\u673a\u5668\u4eba\u6620\u5c04\u7684\u5de5\u7a0b\u6311\u6218\u3002\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u591a\u6837\u5316\u76d1\u7763\u4e2d\u5b66\u4e60\u80fd\u529b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u63a2\u7d22VLA\u6a21\u578b\u662f\u5426\u4e5f\u80fd\u901a\u8fc7\u89c4\u6a21\u6548\u5e94\u5b9e\u73b0\u7c7b\u4f3c\u80fd\u529b\u3002", "method": "\u5f15\u5165\u7b80\u5355\u7684\u534f\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u573a\u666f\u3001\u4efb\u52a1\u548c\u4f53\u73b0\u5f62\u5f0f\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u4f7fVLA\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u4f53\u73b0\u65e0\u5173\u7684\u8868\u5f81\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u6a21\u578b\u4ece\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u5c06\u5176\u77e5\u8bc6\u8fc1\u79fb\u5230\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53VLA\u6a21\u578b\u5728\u8db3\u591f\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u5b9e\u73b0\u4ece\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u7684\u6280\u80fd\u8fc1\u79fb\u3002\u5206\u6790\u8868\u660e\u8fd9\u79cd\u6d8c\u73b0\u80fd\u529b\u6e90\u4e8e\u591a\u6837\u5316\u9884\u8bad\u7ec3\u4ea7\u751f\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u6570\u636e\u7684\u4f53\u73b0\u65e0\u5173\u8868\u5f81\u3002\u5728\u4ec5\u6709\u4eba\u7c7b\u6570\u636e\u7684\u6cdb\u5316\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u6027\u80fd\u63d0\u5347\u8fd1\u4e00\u500d\u3002", "conclusion": "\u901a\u8fc7\u8db3\u591f\u591a\u6837\u5316\u7684\u9884\u8bad\u7ec3\uff0cVLA\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u4f53\u73b0\u65e0\u5173\u7684\u8868\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u4eba\u7c7b\u89c6\u9891\u5230\u673a\u5668\u4eba\u63a7\u5236\u7684\u6280\u80fd\u8fc1\u79fb\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5229\u7528\u4e30\u5bcc\u7684\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u51cf\u5c11\u4e86\u624b\u52a8\u5de5\u7a0b\u6620\u5c04\u7684\u9700\u6c42\u3002"}}
{"id": "2512.22448", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22448", "abs": "https://arxiv.org/abs/2512.22448", "authors": ["Peleg Shefi", "Amir Ayali", "Gal A. Kaminka"], "title": "Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature", "comment": null, "summary": "In collective motion, perceptually-limited individuals move in an ordered manner, without centralized control. The perception of each individual is highly localized, as is its ability to interact with others. While natural collective motion is robust, most artificial swarms are brittle. This particularly occurs when vision is used as the sensing modality, due to ambiguities and information-loss inherent in visual perception. This paper presents mechanisms for robust collective motion inspired by studies of locusts. First, we develop a robust distance estimation method that combines visually perceived horizontal and vertical sizes of neighbors. Second, we introduce intermittent locomotion as a mechanism that allows robots to reliably detect peers that fail to keep up, and disrupt the motion of the swarm. We show how such faulty robots can be avoided in a manner that is robust to errors in classifying them as faulty. Through extensive physics-based simulation experiments, we show dramatic improvements to swarm resilience when using these techniques. We show these are relevant to both distance-based Avoid-Attract models, as well as to models relying on Alignment, in a wide range of experiment settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u673a\u5236\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u673a\u5668\u4eba\u96c6\u7fa4\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\uff1a\u7ed3\u5408\u6c34\u5e73\u548c\u5782\u76f4\u5c3a\u5bf8\u7684\u90bb\u5c45\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u53ca\u95f4\u6b47\u6027\u8fd0\u52a8\u6765\u68c0\u6d4b\u548c\u907f\u514d\u6545\u969c\u673a\u5668\u4eba", "motivation": "\u81ea\u7136\u7fa4\u4f53\u8fd0\u52a8\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u4eba\u5de5\u96c6\u7fa4\u7cfb\u7edf\uff08\u7279\u522b\u662f\u57fa\u4e8e\u89c6\u89c9\u611f\u77e5\u7684\uff09\u901a\u5e38\u5f88\u8106\u5f31\uff0c\u4e3b\u8981\u7531\u4e8e\u89c6\u89c9\u611f\u77e5\u56fa\u6709\u7684\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u635f\u5931\u95ee\u9898", "method": "1. \u5f00\u53d1\u7ed3\u5408\u90bb\u5c45\u6c34\u5e73\u548c\u5782\u76f4\u5c3a\u5bf8\u7684\u9c81\u68d2\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff1b2. \u5f15\u5165\u95f4\u6b47\u6027\u8fd0\u52a8\u673a\u5236\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u53ef\u9760\u68c0\u6d4b\u843d\u540e\u540c\u4f34\u5e76\u4e2d\u65ad\u96c6\u7fa4\u8fd0\u52a8\uff1b3. \u63d0\u51fa\u5bf9\u6545\u969c\u673a\u5668\u4eba\u7684\u9c81\u68d2\u907f\u514d\u7b56\u7565", "result": "\u901a\u8fc7\u5927\u91cf\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u5b9e\u9a8c\uff0c\u663e\u793a\u8fd9\u4e9b\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u96c6\u7fa4\u7684\u97e7\u6027\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u8ddd\u79bb\u7684\u907f\u8ba9-\u5438\u5f15\u6a21\u578b\u548c\u5bf9\u9f50\u6a21\u578b\uff0c\u5728\u591a\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u90fd\u6709\u6548", "conclusion": "\u53d7\u8757\u866b\u7814\u7a76\u542f\u53d1\u7684\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u673a\u5668\u4eba\u96c6\u7fa4\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u4eba\u5de5\u96c6\u7fa4\u7cfb\u7edf\u5728\u89c6\u89c9\u611f\u77e5\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898"}}
{"id": "2512.22484", "categories": ["cs.RO", "math.DG"], "pdf": "https://arxiv.org/pdf/2512.22484", "abs": "https://arxiv.org/abs/2512.22484", "authors": ["Ross L. Hatton", "Yousef Salaman", "Shai Revzen"], "title": "Asymmetric Friction in Geometric Locomotion", "comment": "23 pages, 15 figures", "summary": "Geometric mechanics models of locomotion have provided insight into how robots and animals use environmental interactions to convert internal shape changes into displacement through the world, encoding this relationship in a ``motility map''. A key class of such motility maps arises from (possibly anisotropic) linear drag acting on the system's individual body parts, formally described via Riemannian metrics on the motions of the system's individual body parts. The motility map can then be generated by invoking a sub-Riemannian constraint on the aggregate system motion under which the position velocity induced by a given shape velocity is that which minimizes the power dissipated via friction. The locomotion of such systems is ``geometric'' in the sense that the final position reached by the system depends only on the sequence of shapes that the system passes through, but not on the rate with which the shape changes are made.\n  In this paper, we consider a far more general class of systems in which the drag may be not only anisotropic (with different coefficients for forward/backward and left/right motions), but also asymmetric (with different coefficients for forward and backward motions). Formally, including asymmetry in the friction replaces the Riemannian metrics on the body parts with Finsler metrics. We demonstrate that the sub-Riemannian approach to constructing the system motility map extends naturally to a sub-Finslerian approach and identify system properties analogous to the constraint curvature of sub-Riemannian systems that allow for the characterization of the system motion capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u51e0\u4f55\u529b\u5b66\u4e2d\u7684\u8fd0\u52a8\u6027\u6620\u5c04\u4ece\u9ece\u66fc\u5ea6\u91cf\u63a8\u5e7f\u5230\u82ac\u65af\u52d2\u5ea6\u91cf\uff0c\u8003\u8651\u4e86\u5404\u5411\u5f02\u6027\u4e14\u4e0d\u5bf9\u79f0\u7684\u963b\u529b\uff0c\u5efa\u7acb\u4e86\u5b50\u82ac\u65af\u52d2\u6846\u67b6\u6765\u5206\u6790\u673a\u5668\u4eba\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u51e0\u4f55\u529b\u5b66\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u9ece\u66fc\u5ea6\u91cf\u63cf\u8ff0\u5404\u5411\u540c\u6027\u6216\u5404\u5411\u5f02\u6027\u963b\u529b\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u4e0d\u5bf9\u79f0\u963b\u529b\uff08\u5982\u524d\u8fdb\u548c\u540e\u9000\u963b\u529b\u4e0d\u540c\uff09\u3002\u5b9e\u9645\u751f\u7269\u548c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5e38\u5b58\u5728\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\uff0c\u9700\u8981\u66f4\u4e00\u822c\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5c06\u9ece\u66fc\u5ea6\u91cf\u63a8\u5e7f\u5230\u82ac\u65af\u52d2\u5ea6\u91cf\u6765\u63cf\u8ff0\u4e0d\u5bf9\u79f0\u963b\u529b\uff0c\u5c06\u5b50\u9ece\u66fc\u6784\u9020\u65b9\u6cd5\u6269\u5c55\u5230\u5b50\u82ac\u65af\u52d2\u6846\u67b6\uff0c\u5efa\u7acb\u7cfb\u7edf\u8fd0\u52a8\u6027\u6620\u5c04\uff0c\u5e76\u8bc6\u522b\u7c7b\u4f3c\u4e8e\u5b50\u9ece\u66fc\u7cfb\u7edf\u4e2d\u7ea6\u675f\u66f2\u7387\u7684\u7cfb\u7edf\u7279\u6027\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u5305\u542b\u4e0d\u5bf9\u79f0\u963b\u529b\u7684\u5b50\u82ac\u65af\u52d2\u8fd0\u52a8\u6027\u6620\u5c04\u7406\u8bba\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u8868\u5f81\u7cfb\u7edf\u8fd0\u52a8\u80fd\u529b\u7684\u5173\u952e\u7279\u6027\uff0c\u6269\u5c55\u4e86\u51e0\u4f55\u529b\u5b66\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u51e0\u4f55\u529b\u5b66\u4e2d\u7684\u8fd0\u52a8\u6027\u6620\u5c04\u4ece\u5bf9\u79f0\u963b\u529b\u60c5\u51b5\u63a8\u5e7f\u5230\u4e0d\u5bf9\u79f0\u963b\u529b\u60c5\u51b5\uff0c\u4e3a\u5206\u6790\u66f4\u5e7f\u6cdb\u7684\u751f\u7269\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4e00\u822c\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5b50\u82ac\u65af\u52d2\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u63cf\u8ff0\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u963b\u529b\u7279\u6027\u3002"}}
{"id": "2512.22502", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.22502", "abs": "https://arxiv.org/abs/2512.22502", "authors": ["Shen Changqing", "Xu Bingzhou", "Qi Bosong", "Zhang Xiaojian", "Yan Sijie", "Ding Han"], "title": "Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces", "comment": "24Pages,12Figures", "summary": "Ball-end milling path planning on multiply connected freeform surfaces is pivotal for high-quality and efficient machining of components in automotive and aerospace manufacturing. Although scalar-field-based optimization provides a unified framework for multi-objective toolpath generation, maintaining boundary conformity while eliminating zero-gradient singularities that cause iso-curve branching or termination and disrupt toolpath continuity remains challenging on multiply connected surfaces. We propose an efficient strategy to robustly enforce these constraints throughout optimization. Conformal slit mapping is employed to construct a feasible, singularity-free initial scalar field. The optimization is reformulated as a topology-preserving mesh deformation governed by boundary-synchronous updates, enabling globally optimized spacing, scallop-height uniformity, and smooth trajectory transitions. Consequently, the toolpaths are continuous, boundary-conforming, and free of self-intersections. Milling experiments demonstrate that, compared with a state-of-the-art conformal slit mapping-based method, the proposed approach increases machining efficiency by 14.24%, improves scallop-height uniformity by 5.70%, and reduces milling impact-induced vibrations by over 10%. The strategy offers broad applicability in high-performance machining scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u7f1d\u6620\u5c04\u548c\u62d3\u6251\u4fdd\u6301\u7f51\u683c\u53d8\u5f62\u7684\u7403\u5934\u94e3\u524a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u8fde\u901a\u81ea\u7531\u66f2\u9762\u52a0\u5de5\u4e2d\u7684\u8fb9\u754c\u4e00\u81f4\u6027\u548c\u68af\u5ea6\u5947\u5f02\u70b9\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u8fde\u901a\u81ea\u7531\u66f2\u9762\u4e0a\u8fdb\u884c\u7403\u5934\u94e3\u524a\u8def\u5f84\u89c4\u5212\u65f6\uff0c\u73b0\u6709\u6807\u91cf\u573a\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u8fb9\u754c\u4e00\u81f4\u6027\u5e76\u6d88\u9664\u5bfc\u81f4\u7b49\u503c\u7ebf\u5206\u652f\u6216\u7ec8\u6b62\u7684\u96f6\u68af\u5ea6\u5947\u5f02\u70b9\uff0c\u8fd9\u4f1a\u7834\u574f\u5200\u5177\u8def\u5f84\u7684\u8fde\u7eed\u6027\u3002", "method": "\u91c7\u7528\u4fdd\u5f62\u7f1d\u6620\u5c04\u6784\u5efa\u65e0\u5947\u5f02\u70b9\u7684\u521d\u59cb\u6807\u91cf\u573a\uff0c\u5c06\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u8fb9\u754c\u540c\u6b65\u66f4\u65b0\u63a7\u5236\u7684\u62d3\u6251\u4fdd\u6301\u7f51\u683c\u53d8\u5f62\uff0c\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u7684\u95f4\u8ddd\u3001\u6b8b\u7559\u9ad8\u5ea6\u5747\u5300\u6027\u548c\u5e73\u6ed1\u8f68\u8ff9\u8fc7\u6e21\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u4fdd\u5f62\u7f1d\u6620\u5c04\u7684\u65b9\u6cd5\uff0c\u52a0\u5de5\u6548\u7387\u63d0\u9ad814.24%\uff0c\u6b8b\u7559\u9ad8\u5ea6\u5747\u5300\u6027\u6539\u55845.70%\uff0c\u94e3\u524a\u51b2\u51fb\u5f15\u8d77\u7684\u632f\u52a8\u964d\u4f4e\u8d85\u8fc710%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u8fde\u7eed\u3001\u8fb9\u754c\u4e00\u81f4\u4e14\u65e0\u81ea\u76f8\u4ea4\u7684\u5200\u5177\u8def\u5f84\uff0c\u5728\u9ad8\u6027\u80fd\u52a0\u5de5\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2512.22519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22519", "abs": "https://arxiv.org/abs/2512.22519", "authors": ["Khoa Vo", "Taisei Hanyu", "Yuki Ikebe", "Trong Thang Pham", "Nhat Chung", "Minh Nhat Vu", "Duy Nguyen Ho Minh", "Anh Nguyen", "Anthony Gunderman", "Chase Rainwater", "Ngan Le"], "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding", "comment": "Under review. Project website: https://uark-aicv.github.io/OBEYED_VLA", "summary": "Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.\n  To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects.\n  On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.", "AI": {"tldr": "OBEYED-VLA\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u611f\u77e5\u4e0e\u52a8\u4f5c\u63a8\u7406\uff0c\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u5f15\u5165\u7269\u4f53\u4e2d\u5fc3\u548c\u51e0\u4f55\u611f\u77e5\u7684\u89c2\u5bdf\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5c06\u611f\u77e5\u4e0e\u63a7\u5236\u8026\u5408\u5728\u5355\u4e00\u7ba1\u9053\u4e2d\uff0c\u4e3b\u8981\u9488\u5bf9\u52a8\u4f5c\u9884\u6d4b\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u4f1a\u524a\u5f31\u8bed\u8a00\u6761\u4ef6\u7684\u57fa\u7840\u3002\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0c\u7b56\u7565\u5728\u76ee\u6807\u7f3a\u5931\u65f6\u8fc7\u5ea6\u6293\u53d6\u3001\u88ab\u6742\u7269\u5e72\u6270\u3001\u5e76\u5bf9\u80cc\u666f\u5916\u89c2\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faOBEYED-VLA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u57fa\u4e8eVLM\u7684\u7269\u4f53\u4e2d\u5fc3\u57fa\u7840\u9636\u6bb5\uff0c\u8de8\u76f8\u673a\u89c6\u89d2\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u533a\u57df\uff1b2) \u51e0\u4f55\u57fa\u7840\u9636\u6bb5\uff0c\u5f3a\u8c03\u7269\u4f53\u76843D\u7ed3\u6784\u800c\u975e\u5916\u89c2\u3002\u7136\u540e\u5c06\u57fa\u7840\u540e\u7684\u89c6\u56fe\u8f93\u5165\u9884\u8bad\u7ec3\u7684VLA\u7b56\u7565\uff0c\u4ec5\u4f7f\u7528\u65e0\u73af\u5883\u6742\u7269\u548c\u975e\u76ee\u6807\u7269\u4f53\u7684\u5355\u7269\u4f53\u6f14\u793a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cUR10e\u684c\u9762\u8bbe\u7f6e\u4e2d\uff0cOBEYED-VLA\u5728\u56db\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u5f3aVLA\u57fa\u7ebf\uff1a\u5e72\u6270\u7269\u4f53\u3001\u76ee\u6807\u7f3a\u5931\u62d2\u7edd\u3001\u80cc\u666f\u5916\u89c2\u53d8\u5316\u3001\u4ee5\u53ca\u672a\u89c1\u7269\u4f53\u7684\u6742\u4e71\u64cd\u4f5c\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u8bed\u4e49\u57fa\u7840\u548c\u51e0\u4f55\u611f\u77e5\u57fa\u7840\u90fd\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5c06\u611f\u77e5\u4f5c\u4e3a\u663e\u5f0f\u7684\u3001\u7269\u4f53\u4e2d\u5fc3\u7684\u7ec4\u4ef6\u662f\u589e\u5f3a\u548c\u6cdb\u5316\u57fa\u4e8eVLA\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6709\u6548\u65b9\u6cd5\u3002\u611f\u77e5\u4e0e\u52a8\u4f5c\u63a8\u7406\u7684\u663e\u5f0f\u89e3\u8026\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.22539", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22539", "abs": "https://arxiv.org/abs/2512.22539", "authors": ["Borong Zhang", "Jiahao Li", "Jiachen Shen", "Yishuai Cai", "Yuhao Zhang", "Yuanpei Chen", "Juntao Dai", "Jiaming Ji", "Yaodong Yang"], "title": "VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models", "comment": null, "summary": "While Vision-Language-Action models (VLAs) are rapidly advancing towards generalist robot policies, it remains difficult to quantitatively understand their limits and failure modes. To address this, we introduce a comprehensive benchmark called VLA-Arena. We propose a novel structured task design framework to quantify difficulty across three orthogonal axes: (1) Task Structure, (2) Language Command, and (3) Visual Observation. This allows us to systematically design tasks with fine-grained difficulty levels, enabling a precise measurement of model capability frontiers. For Task Structure, VLA-Arena's 170 tasks are grouped into four dimensions: Safety, Distractor, Extrapolation, and Long Horizon. Each task is designed with three difficulty levels (L0-L2), with fine-tuning performed exclusively on L0 to assess general capability. Orthogonal to this, language (W0-W4) and visual (V0-V4) perturbations can be applied to any task to enable a decoupled analysis of robustness. Our extensive evaluation of state-of-the-art VLAs reveals several critical limitations, including a strong tendency toward memorization over generalization, asymmetric robustness, a lack of consideration for safety constraints, and an inability to compose learned skills for long-horizon tasks. To foster research addressing these challenges and ensure reproducibility, we provide the complete VLA-Arena framework, including an end-to-end toolchain from task definition to automated evaluation and the VLA-Arena-S/M/L datasets for fine-tuning. Our benchmark, data, models, and leaderboard are available at https://vla-arena.github.io.", "AI": {"tldr": "VLA-Arena\u662f\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u8bbe\u8ba1\u6846\u67b6\u4ece\u4e09\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\u91cf\u5316\u96be\u5ea6\uff0c\u63ed\u793aVLA\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u91cf\u5316\u7406\u89e3\u5176\u9650\u5236\u548c\u5931\u8d25\u6a21\u5f0f\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u7cbe\u786e\u6d4b\u91cf\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u4efb\u52a1\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\u91cf\u5316\u96be\u5ea6\uff1a(1)\u4efb\u52a1\u7ed3\u6784\uff08\u5b89\u5168\u3001\u5e72\u6270\u3001\u5916\u63a8\u3001\u957f\u89c6\u91ce\uff09\uff0c(2)\u8bed\u8a00\u6307\u4ee4\uff0c(3)\u89c6\u89c9\u89c2\u5bdf\u3002\u8bbe\u8ba1\u4e86170\u4e2a\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u67093\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u5e94\u7528\u8bed\u8a00\u548c\u89c6\u89c9\u6270\u52a8\u8fdb\u884c\u89e3\u8026\u5206\u6790\u3002", "result": "\u5bf9\u6700\u5148\u8fdbVLA\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u63ed\u793a\u4e86\u591a\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5f3a\u8bb0\u5fc6\u5316\u800c\u975e\u6cdb\u5316\u503e\u5411\u3001\u4e0d\u5bf9\u79f0\u9c81\u68d2\u6027\u3001\u7f3a\u4e4f\u5b89\u5168\u7ea6\u675f\u8003\u8651\u3001\u65e0\u6cd5\u7ec4\u5408\u5b66\u4e60\u6280\u80fd\u5904\u7406\u957f\u89c6\u91ce\u4efb\u52a1\u3002", "conclusion": "VLA-Arena\u4e3a\u7cfb\u7edf\u8bc4\u4f30VLA\u6a21\u578b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u6846\u67b6\u3001\u6570\u636e\u96c6\u548c\u5de5\u5177\u94fe\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2512.22588", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22588", "abs": "https://arxiv.org/abs/2512.22588", "authors": ["Max Beffert", "Andreas Zell"], "title": "Modeling of UAV Tether Aerodynamics for Real-Time Simulation", "comment": null, "summary": "One of the main limitations of multirotor UAVs is their short flight time due to battery constraints. A practical solution for continuous operation is to power the drone from the ground via a tether. While this approach has been demonstrated for stationary systems, scenarios with a fast-moving base vehicle or strong wind conditions require modeling the tether forces, including aerodynamic effects. In this work, we propose two complementary approaches for real-time quasi-static tether modeling with aerodynamics. The first is an analytical method based on catenary theory with a uniform drag assumption, achieving very fast solve times below 1ms. The second is a numerical method that discretizes the tether into segments and lumped masses, solving the equilibrium equations using CasADi and IPOPT. By leveraging initialization strategies, such as warm starting and analytical initialization, real-time performance was achieved with a solve time of 5ms, while allowing for flexible force formulations. Both approaches were validated in real-world tests using a load cell to measure the tether force. The results show that the analytical method provides sufficient accuracy for most tethered UAV applications with minimal computational cost, while the numerical method offers higher flexibility and physical accuracy when required. These approaches form a lightweight and extensible framework for real-time tether simulation, applicable to both offline optimization and online tasks such as simulation, control, and trajectory planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5b9e\u65f6\u51c6\u9759\u6001\u7cfb\u7559\u5efa\u6a21\u65b9\u6cd5\uff1a\u57fa\u4e8e\u60ac\u94fe\u7ebf\u7406\u8bba\u7684\u89e3\u6790\u65b9\u6cd5\uff08<1ms\uff09\u548c\u57fa\u4e8e\u5206\u6bb5\u79bb\u6563\u5316\u7684\u6570\u503c\u65b9\u6cd5\uff085ms\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u79fb\u52a8\u57fa\u5ea7\u6216\u5f3a\u98ce\u6761\u4ef6\u4e0b\u7684\u7cfb\u7559\u65e0\u4eba\u673a\u529b\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u53d7\u7535\u6c60\u9650\u5236\u98de\u884c\u65f6\u95f4\u77ed\uff0c\u901a\u8fc7\u7cfb\u7559\u4ece\u5730\u9762\u4f9b\u7535\u53ef\u5b9e\u73b0\u8fde\u7eed\u4f5c\u4e1a\u3002\u4f46\u5728\u5feb\u901f\u79fb\u52a8\u57fa\u5ea7\u6216\u5f3a\u98ce\u6761\u4ef6\u4e0b\uff0c\u9700\u8981\u8003\u8651\u7cfb\u7559\u7ef3\u7684\u529b\u5efa\u6a21\uff08\u5305\u62ec\u7a7a\u6c14\u52a8\u529b\u5b66\u6548\u5e94\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u60ac\u94fe\u7ebf\u7406\u8bba\u548c\u5747\u5300\u963b\u529b\u5047\u8bbe\u7684\u89e3\u6790\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6781\u5feb\uff08<1ms\uff09\uff1b2\uff09\u5c06\u7cfb\u7559\u7ef3\u79bb\u6563\u4e3a\u5206\u6bb5\u548c\u96c6\u4e2d\u8d28\u91cf\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u4f7f\u7528CasADi\u548cIPOPT\u6c42\u89e3\u5e73\u8861\u65b9\u7a0b\uff0c\u901a\u8fc7\u70ed\u542f\u52a8\u548c\u89e3\u6790\u521d\u59cb\u5316\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff085ms\uff09\u3002", "result": "\u901a\u8fc7\u8d1f\u8f7d\u4f20\u611f\u5668\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u3002\u89e3\u6790\u65b9\u6cd5\u5bf9\u5927\u591a\u6570\u7cfb\u7559\u65e0\u4eba\u673a\u5e94\u7528\u5177\u6709\u8db3\u591f\u7cbe\u5ea6\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\uff1b\u6570\u503c\u65b9\u6cd5\u5728\u9700\u8981\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7075\u6d3b\u6027\u548c\u7269\u7406\u7cbe\u5ea6\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u6784\u6210\u4e86\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u7cfb\u7559\u4eff\u771f\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u79bb\u7ebf\u4f18\u5316\u548c\u5728\u7ebf\u4efb\u52a1\uff08\u4eff\u771f\u3001\u63a7\u5236\u548c\u8f68\u8ff9\u89c4\u5212\uff09\u3002\u89e3\u6790\u65b9\u6cd5\u9002\u5408\u5927\u591a\u6570\u5e94\u7528\uff0c\u6570\u503c\u65b9\u6cd5\u5728\u9700\u8981\u66f4\u9ad8\u7cbe\u5ea6\u65f6\u63d0\u4f9b\u8865\u5145\u3002"}}
{"id": "2512.22734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22734", "abs": "https://arxiv.org/abs/2512.22734", "authors": ["Michelle Valenzuela", "Francisco Leiva", "Javier Ruiz-del-Solar"], "title": "Sistema de navegaci\u00f3n de cobertura para veh\u00edculos no holon\u00f3micos en ambientes de exterior", "comment": "13 pages, in Spanish language, 12 figures, accepted at Tercer Congreso Iberoamericano de Miner\u00eda Subterranea y a Cielo Abierto, UMining 2024", "summary": "In mobile robotics, coverage navigation refers to the deliberate movement of a robot with the purpose of covering a certain area or volume. Performing this task properly is fundamental for the execution of several activities, for instance, cleaning a facility with a robotic vacuum cleaner. In the mining industry, it is required to perform coverage in several unit processes related with material movement using industrial machinery, for example, in cleaning tasks, in dumps, and in the construction of tailings dam walls. The automation of these processes is fundamental to enhance the security associated with their execution. In this work, a coverage navigation system for a non-holonomic robot is presented. This work is intended to be a proof of concept for the potential automation of various unit processes that require coverage navigation like the ones mentioned before. The developed system includes the calculation of routes that allow a mobile platform to cover a specific area, and incorporates recovery behaviors in case that an unforeseen event occurs, such as the arising of dynamic or previously unmapped obstacles in the terrain to be covered, e.g., other machines or pedestrians passing through the area, being able to perform evasive maneuvers and post-recovery to ensure a complete coverage of the terrain. The system was tested in different simulated and real outdoor environments, obtaining results near 90% of coverage in the majority of experiments. The next step of development is to scale up the utilized robot to a mining machine/vehicle whose operation will be validated in a real environment. The result of one of the tests performed in the real world can be seen in the video available in https://youtu.be/gK7_3bK1P5g.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u975e\u5b8c\u6574\u673a\u5668\u4eba\u7684\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e8\u5728\u5b9e\u73b0\u7279\u5b9a\u533a\u57df\u7684\u5b8c\u5168\u8986\u76d6\uff0c\u9002\u7528\u4e8e\u91c7\u77ff\u7b49\u5de5\u4e1a\u573a\u666f\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u8986\u76d6\u5bfc\u822a\u5bf9\u4e8e\u6e05\u6d01\u3001\u91c7\u77ff\u7b49\u5de5\u4e1a\u81ea\u52a8\u5316\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u91c7\u77ff\u884c\u4e1a\u4e2d\u7684\u7269\u6599\u79fb\u52a8\u3001\u6e05\u6d01\u3001\u5c3e\u77ff\u575d\u5efa\u8bbe\u7b49\u5355\u5143\u8fc7\u7a0b\u90fd\u9700\u8981\u8986\u76d6\u5bfc\u822a\uff0c\u81ea\u52a8\u5316\u8fd9\u4e9b\u8fc7\u7a0b\u5bf9\u63d0\u9ad8\u4f5c\u4e1a\u5b89\u5168\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\uff0c\u5305\u62ec\u4e3a\u79fb\u52a8\u5e73\u53f0\u8ba1\u7b97\u8986\u76d6\u7279\u5b9a\u533a\u57df\u7684\u8def\u5f84\uff0c\u5e76\u96c6\u6210\u4e86\u6062\u590d\u884c\u4e3a\u673a\u5236\u3002\u5f53\u9047\u5230\u52a8\u6001\u969c\u788d\u7269\u6216\u672a\u6620\u5c04\u7684\u969c\u788d\u7269\uff08\u5982\u5176\u4ed6\u673a\u5668\u6216\u884c\u4eba\uff09\u65f6\uff0c\u7cfb\u7edf\u80fd\u591f\u6267\u884c\u89c4\u907f\u673a\u52a8\u548c\u6062\u590d\u540e\u64cd\u4f5c\uff0c\u786e\u4fdd\u533a\u57df\u5b8c\u5168\u8986\u76d6\u3002", "result": "\u7cfb\u7edf\u5728\u4e0d\u540c\u6a21\u62df\u548c\u771f\u5b9e\u5ba4\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5927\u591a\u6570\u5b9e\u9a8c\u83b7\u5f97\u4e86\u63a5\u8fd190%\u7684\u8986\u76d6\u7387\u3002\u4e0b\u4e00\u6b65\u8ba1\u5212\u5c06\u7cfb\u7edf\u6269\u5c55\u5230\u91c7\u77ff\u673a\u68b0/\u8f66\u8f86\u4e0a\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\u4e3a\u975e\u5b8c\u6574\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u533a\u57df\u8986\u76d6\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u91c7\u77ff\u7b49\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u5316\u8986\u76d6\u5bfc\u822a\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5de5\u4e1a\u81ea\u52a8\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002"}}
{"id": "2512.22757", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.22757", "abs": "https://arxiv.org/abs/2512.22757", "authors": ["Zheng Qiu", "Chih-Yuan Chiu", "Glen Chou"], "title": "Active Constraint Learning in High Dimensions from Demonstrations", "comment": "Under review, 25 pages, 11 figures", "summary": "We present an iterative active constraint learning (ACL) algorithm, within the learning from demonstrations (LfD) paradigm, which intelligently solicits informative demonstration trajectories for inferring an unknown constraint in the demonstrator's environment. Our approach iteratively trains a Gaussian process (GP) on the available demonstration dataset to represent the unknown constraints, uses the resulting GP posterior to query start/goal states, and generates informative demonstrations which are added to the dataset. Across simulation and hardware experiments using high-dimensional nonlinear dynamics and unknown nonlinear constraints, our method outperforms a baseline, random-sampling based method at accurately performing constraint inference from an iteratively generated set of sparse but informative demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u5f0f\u4e3b\u52a8\u7ea6\u675f\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u8bf7\u6c42\u4fe1\u606f\u4e30\u5bcc\u7684\u6f14\u793a\u8f68\u8ff9\u6765\u63a8\u65ad\u6f14\u793a\u8005\u73af\u5883\u4e2d\u7684\u672a\u77e5\u7ea6\u675f", "motivation": "\u5728\u4ece\u6f14\u793a\u5b66\u4e60\uff08LfD\uff09\u8303\u5f0f\u4e2d\uff0c\u5982\u4f55\u9ad8\u6548\u5730\u4ece\u7a00\u758f\u6f14\u793a\u4e2d\u63a8\u65ad\u672a\u77e5\u7ea6\u675f\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u4f9d\u8d56\u968f\u673a\u91c7\u6837\uff0c\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u65b9\u6cd5\u6765\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u6f14\u793a\u6570\u636e", "method": "\u4f7f\u7528\u8fed\u4ee3\u5f0f\u4e3b\u52a8\u7ea6\u675f\u5b66\u4e60\u7b97\u6cd5\uff1a1\uff09\u5728\u53ef\u7528\u6f14\u793a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u6765\u8868\u793a\u672a\u77e5\u7ea6\u675f\uff1b2\uff09\u5229\u7528GP\u540e\u9a8c\u67e5\u8be2\u8d77\u59cb/\u76ee\u6807\u72b6\u6001\uff1b3\uff09\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u6f14\u793a\u5e76\u6dfb\u52a0\u5230\u6570\u636e\u96c6\u4e2d\u3002\u8be5\u65b9\u6cd5\u5728\u9ad8\u7ef4\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u672a\u77e5\u975e\u7ebf\u6027\u7ea6\u675f\u6761\u4ef6\u4e0b\u8fdb\u884c\u6d4b\u8bd5", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4ece\u8fed\u4ee3\u751f\u6210\u7684\u7a00\u758f\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u6f14\u793a\u96c6\u4e2d\u51c6\u786e\u6267\u884c\u7ea6\u675f\u63a8\u65ad\u65b9\u9762\uff0c\u4f18\u4e8e\u57fa\u4e8e\u968f\u673a\u91c7\u6837\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u8fed\u4ee3\u4e3b\u52a8\u7ea6\u675f\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4ece\u7a00\u758f\u6f14\u793a\u4e2d\u63a8\u65ad\u672a\u77e5\u7ea6\u675f\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u6f14\u793a\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4e86\u7ea6\u675f\u63a8\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387"}}
{"id": "2512.22770", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22770", "abs": "https://arxiv.org/abs/2512.22770", "authors": ["Naoki Kitamura", "Yuichi Sudo", "Koichi Wada"], "title": "Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems", "comment": "23 pages, 3 figures", "summary": "The computational power of autonomous mobile robots under the Look-Compute-Move (LCM) model has been widely studied through an extensive hierarchy of robot models defined by the presence of memory, communication, and synchrony assumptions. While the general n-robot landscape has been largely established, the exact structure for two robots has remained unresolved. This paper presents the first complete characterization of the computational power of two autonomous robots across all major models, namely OBLOT, FSTA, FCOM, and LUMI, under the full spectrum of schedulers (FSYNCH, SSYNCH, ASYNCH, and their atomic variants). Our results reveal a landscape that fundamentally differs from the general case. Most notably, we prove that FSTA^F and LUMI^F coincide under full synchrony, a surprising collapse indicating that perfect synchrony can substitute both memory and communication when only two robots exist. We also show that FSTA and FCOM are orthogonal: there exists a problem solvable in the weakest communication model but impossible even in the strongest finite-state model, completing the bidirectional incomparability. All equivalence and separation results are derived through a novel simulation-free method, providing a unified and constructive view of the two-robot hierarchy. This yields the first complete and exact computational landscape for two robots, highlighting the intrinsic challenges of coordination at the minimal scale.", "AI": {"tldr": "\u9996\u6b21\u5b8c\u6574\u523b\u753b\u4e86\u4e24\u4e2a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5728\u6240\u6709\u4e3b\u8981\u6a21\u578b\u4e0b\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u4e0e\u591a\u673a\u5668\u4eba\u60c5\u51b5\u6839\u672c\u4e0d\u540c\u7684\u8ba1\u7b97\u683c\u5c40\uff0c\u7279\u522b\u662f\u5728\u5b8c\u5168\u540c\u6b65\u4e0bFSTA\u548cLUMI\u6a21\u578b\u7b49\u4ef7\uff0c\u8868\u660e\u5b8c\u7f8e\u540c\u6b65\u53ef\u4ee5\u66ff\u4ee3\u8bb0\u5fc6\u548c\u901a\u4fe1\u3002", "motivation": "\u867d\u7136\u591a\u673a\u5668\u4eba\u573a\u666f\u4e0b\u7684\u8ba1\u7b97\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\u5df2\u57fa\u672c\u5efa\u7acb\uff0c\u4f46\u4e24\u4e2a\u673a\u5668\u4eba\u7684\u7cbe\u786e\u8ba1\u7b97\u7ed3\u6784\u4e00\u76f4\u672a\u89e3\u51b3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5168\u9762\u5206\u6790\u4e24\u4e2a\u673a\u5668\u4eba\u5728\u6240\u6709\u4e3b\u8981\u6a21\u578b\u4e0b\u7684\u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u91c7\u7528\u65e0\u6a21\u62df\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e24\u4e2a\u673a\u5668\u4eba\u5728OBLOT\u3001FSTA\u3001FCOM\u548cLUMI\u6a21\u578b\u4e0b\uff0c\u9762\u5bf9\u5b8c\u6574\u8c03\u5ea6\u5668\u8c31\u7cfb\uff08FSYNCH\u3001SSYNCH\u3001ASYNCH\u53ca\u5176\u539f\u5b50\u53d8\u4f53\uff09\u7684\u8ba1\u7b97\u80fd\u529b\u3002\u901a\u8fc7\u6784\u9020\u6027\u8bc1\u660e\u5efa\u7acb\u7b49\u4ef7\u548c\u5206\u79bb\u5173\u7cfb\u3002", "result": "1. \u8bc1\u660e\u4e86\u5728\u5b8c\u5168\u540c\u6b65\u4e0bFSTA^F\u548cLUMI^F\u6a21\u578b\u7b49\u4ef7\uff0c\u8868\u660e\u5b8c\u7f8e\u540c\u6b65\u53ef\u4ee5\u66ff\u4ee3\u8bb0\u5fc6\u548c\u901a\u4fe1\uff1b2. \u8bc1\u660e\u4e86FSTA\u548cFCOM\u6a21\u578b\u6b63\u4ea4\u4e14\u4e0d\u53ef\u6bd4\u8f83\uff1b3. \u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u4e24\u4e2a\u673a\u5668\u4eba\u8ba1\u7b97\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\uff0c\u4e0e\u591a\u673a\u5668\u4eba\u60c5\u51b5\u6709\u6839\u672c\u5dee\u5f02\u3002", "conclusion": "\u9996\u6b21\u83b7\u5f97\u4e86\u4e24\u4e2a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5b8c\u6574\u7cbe\u786e\u8ba1\u7b97\u683c\u5c40\uff0c\u63ed\u793a\u4e86\u6700\u5c0f\u89c4\u6a21\u534f\u8c03\u7684\u5185\u5728\u6311\u6218\uff0c\u4e3a\u7406\u89e3\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.22868", "categories": ["cs.RO", "cs.AI", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.22868", "abs": "https://arxiv.org/abs/2512.22868", "authors": ["Matej Hoffmann"], "title": "The body is not there to compute: Comment on \"Informational embodiment: Computational role of information structure in codes and robots\" by Pitti et al", "comment": "Comment on Pitti, A., Austin, M., Nakajima, K., & Kuniyoshi, Y. (2025). Informational Embodiment: Computational role of information structure in codes and robots. Physics of Life Reviews 53, 262-276. https://doi.org/10.1016/j.plrev.2025.03.018. Also available as arXiv:2408.12950", "summary": "Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.", "AI": {"tldr": "\u8be5\u8bc4\u8bba\u6587\u7ae0\u8ba4\u4e3a\u8eab\u4f53\u7684\u4e3b\u8981\u529f\u80fd\u4e0d\u662f\u8ba1\u7b97\uff0c\u53cd\u5bf9\u5c06\u8ba1\u7b97\u548c\u4fe1\u606f\u5904\u7406\u4f5c\u4e3a\u7406\u89e3\u751f\u7269\u4f53\u8fdb\u5316\u548c\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u6838\u5fc3\u6846\u67b6\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u6311\u6218\u76ee\u6807\u6587\u7ae0\u4e2d\u63d0\u51fa\u7684\u89c2\u70b9\uff0c\u5373\u8ba1\u7b97\u548c\u4fe1\u606f\u5904\u7406\u662f\u7406\u89e3\u52a8\u7269\u8eab\u4f53\u8fdb\u5316\u548c\u673a\u5668\u4eba\u8eab\u4f53\u8bbe\u8ba1\u4f18\u5316\u7684\u6838\u5fc3\u6846\u67b6\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u8ba1\u7b97\u4e3b\u4e49\u7684\u89c6\u89d2\u8fc7\u4e8e\u7b80\u5316\uff0c\u672a\u80fd\u6355\u6349\u8eab\u4f53\u7684\u672c\u8d28\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u8bc4\u8bba\u7684\u65b9\u5f0f\uff0c\u5bf9\u76ee\u6807\u6587\u7ae0\u7684\u8ba1\u7b97\u4e3b\u4e49\u6846\u67b6\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\uff0c\u63d0\u51fa\u8eab\u4f53\u7684\u4e3b\u8981\u529f\u80fd\u4e0d\u662f\u8ba1\u7b97\uff0c\u800c\u662f\u6267\u884c\u5176\u4ed6\u66f4\u57fa\u672c\u7684\u751f\u7269\u548c\u7269\u7406\u529f\u80fd\u3002", "result": "\u4f5c\u8005\u6210\u529f\u6307\u51fa\u4e86\u8ba1\u7b97\u4e3b\u4e49\u6846\u67b6\u5728\u7406\u89e3\u8eab\u4f53\u529f\u80fd\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u8eab\u4f53\u7684\u4e3b\u8981\u89d2\u8272\u4e0d\u662f\u4fe1\u606f\u5904\u7406\uff0c\u800c\u662f\u5176\u4ed6\u66f4\u57fa\u672c\u7684\u751f\u7269\u529f\u80fd\u3002", "conclusion": "\u867d\u7136\u8ba1\u7b97\u548c\u4fe1\u606f\u7406\u8bba\u5728\u7406\u89e3\u795e\u7ecf\u7cfb\u7edf\u548c\u8ba4\u77e5\u65b9\u9762\u6709\u4ef7\u503c\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u8eab\u4f53\u7684\u7406\u89e3\u548c\u8bbe\u8ba1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8eab\u4f53\u7684\u4e3b\u8981\u529f\u80fd\u4e0d\u662f\u8ba1\u7b97\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u7406\u89e3\u6846\u67b6\u3002"}}
{"id": "2512.22927", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22927", "abs": "https://arxiv.org/abs/2512.22927", "authors": ["Daqian Cao", "Quan Yuan", "Weibang Bai"], "title": "P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach", "comment": "7 pages, 8 figures, and 2 tables", "summary": "Traditional geometric inverse kinematics methods for parallel mechanisms rely on specific spatial geometry constraints. However, their application to redundant parallel mechanisms is challenged due to the increased constraint complexity. Moreover, it will output no solutions and cause unpredictable control problems when the target pose lies outside its workspace. To tackle these challenging issues, this work proposes P-FABRIK, a general, intuitive, and robust inverse kinematics method to find one feasible solution for diverse parallel mechanisms based on the FABRIK algorithm. By decomposing the general parallel mechanism into multiple serial sub-chains using a new topological decomposition strategy, the end targets of each sub-chain can be subsequently revised to calculate the inverse kinematics solutions iteratively. Multiple case studies involving planar, standard, and redundant parallel mechanisms demonstrated the proposed method's generality across diverse parallel mechanisms. Furthermore, numerical simulation studies verified its efficacy and computational efficiency, as well as its robustness ability to handle out-of-workspace targets.", "AI": {"tldr": "\u63d0\u51faP-FABRIK\u65b9\u6cd5\uff0c\u57fa\u4e8eFABRIK\u7b97\u6cd5\u4e3a\u591a\u79cd\u5e76\u8054\u673a\u6784\u63d0\u4f9b\u901a\u7528\u3001\u76f4\u89c2\u3001\u9c81\u68d2\u7684\u9006\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u5904\u7406\u5197\u4f59\u5e76\u8054\u673a\u6784\u548c\u5de5\u4f5c\u7a7a\u95f4\u5916\u76ee\u6807\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u51e0\u4f55\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7a7a\u95f4\u51e0\u4f55\u7ea6\u675f\uff0c\u5e94\u7528\u4e8e\u5197\u4f59\u5e76\u8054\u673a\u6784\u65f6\u9762\u4e34\u7ea6\u675f\u590d\u6742\u6027\u589e\u52a0\u7684\u95ee\u9898\uff0c\u4e14\u5f53\u76ee\u6807\u4f4d\u59ff\u8d85\u51fa\u5de5\u4f5c\u7a7a\u95f4\u65f6\u65e0\u89e3\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u63a7\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faP-FABRIK\u65b9\u6cd5\uff0c\u57fa\u4e8eFABRIK\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u62d3\u6251\u5206\u89e3\u7b56\u7565\u5c06\u901a\u7528\u5e76\u8054\u673a\u6784\u5206\u89e3\u4e3a\u591a\u4e2a\u4e32\u8054\u5b50\u94fe\uff0c\u8fed\u4ee3\u4fee\u6b63\u5404\u5b50\u94fe\u672b\u7aef\u76ee\u6807\u6765\u8ba1\u7b97\u9006\u8fd0\u52a8\u5b66\u89e3\u3002", "result": "\u901a\u8fc7\u5e73\u9762\u3001\u6807\u51c6\u548c\u5197\u4f59\u5e76\u8054\u673a\u6784\u7684\u591a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e76\u8054\u673a\u6784\u4e2d\u7684\u901a\u7528\u6027\uff1b\u6570\u503c\u4eff\u771f\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u5904\u7406\u5de5\u4f5c\u7a7a\u95f4\u5916\u76ee\u6807\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "P-FABRIK\u65b9\u6cd5\u4e3a\u5e76\u8054\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u76f4\u89c2\u3001\u9c81\u68d2\u7684\u9006\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u7684\u5197\u4f59\u673a\u6784\u548c\u5de5\u4f5c\u7a7a\u95f4\u5916\u76ee\u6807\u95ee\u9898\u3002"}}
{"id": "2512.22957", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22957", "abs": "https://arxiv.org/abs/2512.22957", "authors": ["Mengyu Ji", "Shiliang Guo", "Zhengzhen Li", "Jiahao Shen", "Huazi Cao", "Shiyu Zhao"], "title": "PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO", "comment": "12 pages, 6 figures", "summary": "An aerial manipulator, comprising a multirotor base and a robotic arm, is subject to significant dynamic coupling between these two components. Therefore, achieving precise and robust motion control is a challenging yet important objective. Here, we propose a novel prescribed performance motion control framework based on variable-gain extended state observers (ESOs), referred to as PreGME. The method includes variable-gain ESOs for real-time estimation of dynamic coupling and a prescribed performance flight control that incorporates error trajectory constraints. Compared with existing methods, the proposed approach exhibits the following two characteristics. First, the adopted variable-gain ESOs can accurately estimate rapidly varying dynamic coupling. This enables the proposed method to handle manipulation tasks that require aggressive motion of the robotic arm. Second, by prescribing the performance, a preset error trajectory is generated to guide the system evolution along this trajectory. This strategy allows the proposed method to ensure the tracking error remains within the prescribed performance envelope, thereby achieving high-precision control. Experiments on a real platform, including aerial staff twirling, aerial mixology, and aerial cart-pulling experiments, are conducted to validate the effectiveness of the proposed method.\n  Experimental results demonstrate that even under the dynamic coupling caused by rapid robotic arm motion (end-effector velocity: 1.02 m/s, acceleration: 5.10 m/s$^2$), the proposed method achieves high tracking performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u589e\u76ca\u6269\u5f20\u72b6\u6001\u89c2\u6d4b\u5668\u7684\u89c4\u5b9a\u6027\u80fd\u8fd0\u52a8\u63a7\u5236\u6846\u67b6PreGME\uff0c\u7528\u4e8e\u89e3\u51b3\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u52a8\u6001\u8026\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9c81\u68d2\u63a7\u5236\u3002", "motivation": "\u7a7a\u4e2d\u673a\u68b0\u81c2\uff08\u591a\u65cb\u7ffc\u57fa\u5ea7+\u673a\u68b0\u81c2\uff09\u5b58\u5728\u663e\u8457\u52a8\u6001\u8026\u5408\uff0c\u5b9e\u73b0\u7cbe\u786e\u9c81\u68d2\u7684\u8fd0\u52a8\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\u4f46\u5f88\u91cd\u8981\u3002", "method": "\u63d0\u51faPreGME\u6846\u67b6\uff1a1\uff09\u53d8\u589e\u76ca\u6269\u5f20\u72b6\u6001\u89c2\u6d4b\u5668\u5b9e\u65f6\u4f30\u8ba1\u52a8\u6001\u8026\u5408\uff1b2\uff09\u89c4\u5b9a\u6027\u80fd\u98de\u884c\u63a7\u5236\u7ed3\u5408\u8bef\u5dee\u8f68\u8ff9\u7ea6\u675f\uff0c\u9884\u8bbe\u8bef\u5dee\u8f68\u8ff9\u5f15\u5bfc\u7cfb\u7edf\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\uff08\u7a7a\u4e2d\u68cd\u68d2\u65cb\u8f6c\u3001\u7a7a\u4e2d\u8c03\u9152\u3001\u7a7a\u4e2d\u62c9\u8f66\uff09\uff1a\u5373\u4f7f\u5728\u673a\u68b0\u81c2\u5feb\u901f\u8fd0\u52a8\uff08\u672b\u7aef\u901f\u5ea61.02m/s\uff0c\u52a0\u901f\u5ea65.10m/s\u00b2\uff09\u5f15\u8d77\u7684\u52a8\u6001\u8026\u5408\u4e0b\uff0c\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684PreGME\u65b9\u6cd5\u80fd\u51c6\u786e\u4f30\u8ba1\u5feb\u901f\u53d8\u5316\u7684\u52a8\u6001\u8026\u5408\uff0c\u901a\u8fc7\u89c4\u5b9a\u6027\u80fd\u786e\u4fdd\u8ddf\u8e2a\u8bef\u5dee\u5728\u9884\u8bbe\u6027\u80fd\u5305\u7edc\u5185\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u9700\u8981\u673a\u68b0\u81c2\u6fc0\u8fdb\u8fd0\u52a8\u7684\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2512.22983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22983", "abs": "https://arxiv.org/abs/2512.22983", "authors": ["Shuanghao Bai", "Wenxuan Song", "Jiayi Chen", "Yuheng Ji", "Zhide Zhong", "Jin Yang", "Han Zhao", "Wanqi Zhou", "Zhe Li", "Pengxiang Ding", "Cheng Chi", "Chang Xu", "Xiaolong Zheng", "Donglin Wang", "Haoang Li", "Shanghang Zhang", "Badong Chen"], "title": "Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives", "comment": "This work is a re-architected core derived from the full survey (arXiv:2510.10903) , refined to highlight the most central themes and representative studies", "summary": "Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u4ece\u7b97\u6cd5\u89d2\u5ea6\u5ba1\u89c6\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u5c06\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u7ec4\u7ec7\u5230\u9ad8\u5c42\u89c4\u5212\u4e0e\u5e95\u5c42\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u5206\u6790\u4e86\u73b0\u4ee3\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u673a\u5668\u4eba\u64cd\u4f5c\u4ecd\u7136\u662f\u4e00\u4e2a\u6838\u5fc3\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u9700\u8981\u4ece\u7b97\u6cd5\u89d2\u5ea6\u7cfb\u7edf\u68b3\u7406\u548c\u7ec4\u7ec7\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u6e05\u6670\u7684\u8bbe\u8ba1\u6846\u67b6\u3002", "method": "\u8bba\u6587\u91c7\u7528\u5206\u5c42\u5206\u6790\u65b9\u6cd5\uff1a\u5728\u9ad8\u5c42\u89c4\u5212\u5c42\u9762\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u4efb\u52a1\u89c4\u5212\u6982\u5ff5\uff0c\u7eb3\u5165\u8bed\u8a00\u3001\u4ee3\u7801\u3001\u8fd0\u52a8\u3001\u529f\u80fd\u6027\u548c3D\u8868\u793a\u7b49\u63a8\u7406\uff1b\u5728\u5e95\u5c42\u63a7\u5236\u5c42\u9762\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bad\u7ec3\u8303\u5f0f\u7684\u5206\u7c7b\u6cd5\uff0c\u4ece\u8f93\u5165\u5efa\u6a21\u3001\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u548c\u7b56\u7565\u5b66\u4e60\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b97\u6cd5\u6846\u67b6\uff0c\u5c06\u9ad8\u5c42\u89c4\u5212\u4e0e\u5e95\u5c42\u63a7\u5236\u6709\u673a\u7ed3\u5408\uff0c\u4e3a\u7406\u89e3\u73b0\u4ee3\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u89c6\u89d2\uff0c\u5e76\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u8bbe\u8ba1\u7ef4\u5ea6\u3002", "conclusion": "\u8bba\u6587\u660e\u786e\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u6570\u636e\u6548\u7387\u3001\u591a\u6a21\u6001\u7269\u7406\u4ea4\u4e92\u548c\u5b89\u5168\u6027\u7b49\u6311\u6218\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2512.23077", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23077", "abs": "https://arxiv.org/abs/2512.23077", "authors": ["Saraswati Soedarmadji", "Yunyue Wei", "Chen Zhang", "Yisong Yue", "Yanan Sui"], "title": "Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models", "comment": "18 pages, 8 figures", "summary": "Discovering effective reward functions remains a fundamental challenge in motor control of high-dimensional musculoskeletal systems. While humans can describe movement goals explicitly such as \"walking forward with an upright posture,\" the underlying control strategies that realize these goals are largely implicit, making it difficult to directly design rewards from high-level goals and natural language descriptions. We introduce Motion from Vision-Language Representation (MoVLR), a framework that leverages vision-language models (VLMs) to bridge the gap between goal specification and movement control. Rather than relying on handcrafted rewards, MoVLR iteratively explores the reward space through iterative interaction between control optimization and VLM feedback, aligning control policies with physically coordinated behaviors. Our approach transforms language and vision-based assessments into structured guidance for embodied learning, enabling the discovery and refinement of reward functions for high-dimensional musculoskeletal locomotion and manipulation. These results suggest that VLMs can effectively ground abstract motion descriptions in the implicit principles governing physiological motor control.", "AI": {"tldr": "MoVLR\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u9ad8\u7ea7\u8fd0\u52a8\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u4f18\u5316\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7528\u4e8e\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u7684\u8fd0\u52a8\u63a7\u5236", "motivation": "\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u7684\u8fd0\u52a8\u63a7\u5236\u9762\u4e34\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u6839\u672c\u6311\u6218\u3002\u4eba\u7c7b\u80fd\u7528\u8bed\u8a00\u63cf\u8ff0\u8fd0\u52a8\u76ee\u6807\uff08\u5982\"\u76f4\u7acb\u5411\u524d\u884c\u8d70\"\uff09\uff0c\u4f46\u5b9e\u73b0\u8fd9\u4e9b\u76ee\u6807\u7684\u63a7\u5236\u7b56\u7565\u662f\u9690\u5f0f\u7684\uff0c\u96be\u4ee5\u76f4\u63a5\u4ece\u9ad8\u7ea7\u76ee\u6807\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570", "method": "\u63d0\u51faMotion from Vision-Language Representation (MoVLR)\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u89c4\u8303\u548c\u8fd0\u52a8\u63a7\u5236\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u3002\u901a\u8fc7\u63a7\u5236\u4f18\u5316\u548cVLM\u53cd\u9988\u7684\u8fed\u4ee3\u4ea4\u4e92\u63a2\u7d22\u5956\u52b1\u7a7a\u95f4\uff0c\u5c06\u8bed\u8a00\u548c\u89c6\u89c9\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6307\u5bfc\uff0c\u7528\u4e8e\u5177\u8eab\u5b66\u4e60", "result": "MoVLR\u80fd\u591f\u53d1\u73b0\u548c\u4f18\u5316\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u64cd\u4f5c\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u63a7\u5236\u7b56\u7565\u4e0e\u7269\u7406\u534f\u8c03\u884c\u4e3a\u5bf9\u9f50", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5c06\u62bd\u8c61\u8fd0\u52a8\u63cf\u8ff0\u5efa\u7acb\u5728\u751f\u7406\u8fd0\u52a8\u63a7\u5236\u7684\u9690\u5f0f\u539f\u5219\u4e0a\uff0c\u4e3a\u89e3\u51b3\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u63a7\u5236\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2512.23103", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23103", "abs": "https://arxiv.org/abs/2512.23103", "authors": ["Peter Messina", "Daniel Rakita"], "title": "APOLLO Blender: A Robotics Library for Visualization and Animation in Blender", "comment": null, "summary": "High-quality visualizations are an essential part of robotics research, enabling clear communication of results through figures, animations, and demonstration videos. While Blender is a powerful and freely available 3D graphics platform, its steep learning curve and lack of robotics-focused integrations make it difficult and time-consuming for researchers to use effectively. In this work, we introduce a lightweight software library that bridges this gap by providing simple scripting interfaces for common robotics visualization tasks. The library offers three primary capabilities: (1) importing robots and environments directly from standardized descriptions such as URDF; (2) Python-based scripting tools for keyframing robot states and visual attributes; and (3) convenient generation of primitive 3D shapes for schematic figures and animations. Together, these features allow robotics researchers to rapidly create publication-ready images, animations, and explanatory schematics without needing extensive Blender expertise. We demonstrate the library through a series of proof-of-concept examples and conclude with a discussion of current limitations and opportunities for future extensions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8f6f\u4ef6\u5e93\uff0c\u4e3a\u673a\u5668\u4eba\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u7b80\u5355\u7684Blender\u811a\u672c\u63a5\u53e3\uff0c\u7528\u4e8e\u5feb\u901f\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u51fa\u7248\u7269\u53ef\u89c6\u5316\u5185\u5bb9\u3002", "motivation": "Blender\u4f5c\u4e3a\u5f3a\u5927\u7684\u514d\u8d393D\u56fe\u5f62\u5e73\u53f0\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u4e14\u7f3a\u4e4f\u673a\u5668\u4eba\u4e13\u7528\u96c6\u6210\uff0c\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u9ad8\u6548\u4f7f\u7528\u5b83\u6765\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u4eba\u7814\u7a76\u53ef\u89c6\u5316\u5185\u5bb9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8f6f\u4ef6\u5e93\uff0c\u63d0\u4f9b\u4e09\u4e2a\u4e3b\u8981\u529f\u80fd\uff1a1) \u76f4\u63a5\u4eceURDF\u7b49\u6807\u51c6\u5316\u63cf\u8ff0\u5bfc\u5165\u673a\u5668\u4eba\u548c\u73af\u5883\uff1b2) \u57fa\u4e8ePython\u7684\u811a\u672c\u5de5\u5177\uff0c\u7528\u4e8e\u5173\u952e\u5e27\u8bbe\u7f6e\u673a\u5668\u4eba\u72b6\u6001\u548c\u89c6\u89c9\u5c5e\u6027\uff1b3) \u65b9\u4fbf\u76843D\u57fa\u672c\u5f62\u72b6\u751f\u6210\u5de5\u5177\uff0c\u7528\u4e8e\u793a\u610f\u56fe\u548c\u52a8\u753b\u5236\u4f5c\u3002", "result": "\u8be5\u5e93\u4f7f\u673a\u5668\u4eba\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5feb\u901f\u521b\u5efa\u51fa\u7248\u7269\u5c31\u7eea\u7684\u56fe\u50cf\u3001\u52a8\u753b\u548c\u89e3\u91ca\u6027\u793a\u610f\u56fe\uff0c\u65e0\u9700\u6df1\u5165\u7684Blender\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u6982\u5ff5\u9a8c\u8bc1\u793a\u4f8b\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u5e93\u586b\u8865\u4e86\u673a\u5668\u4eba\u7814\u7a76\u4e0eBlender\u53ef\u89c6\u5316\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u521b\u5efa\u9ad8\u8d28\u91cf\u53ef\u89c6\u5316\u5185\u5bb9\u7684\u95e8\u69db\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5f53\u524d\u5c40\u9650\u6027\u548c\u672a\u6765\u6269\u5c55\u673a\u4f1a\u3002"}}
{"id": "2512.23141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23141", "abs": "https://arxiv.org/abs/2512.23141", "authors": ["Wuhao Xie", "Kanji Tanaka"], "title": "Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset", "comment": "3 pages, technical report", "summary": "While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8fdc\u8ddd\u79bb\u6746\u72b6\u7ed3\u6784\u8bc6\u522b\u53ef\u9760\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4ece\u63cf\u8ff0\u7b26\u8bbe\u8ba1\u8f6c\u5411\u7cfb\u7edf\u7814\u7a76\u63cf\u8ff0\u7b26\u9c81\u68d2\u6027\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8eSPL\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u6746\u72b6\u7ed3\u6784\u4f5c\u4e3a\u673a\u5668\u4eba\u957f\u671f\u5b9a\u4f4d\u7684\u7a33\u5b9a\u51e0\u4f55\u951a\u70b9\uff0c\u4f46\u5728\u8fdc\u8ddd\u79bb\u89c2\u6d4b\u65f6\u8bc6\u522b\u53ef\u9760\u6027\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u63cf\u8ff0\u7b26\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "1) \u5efa\u7acb\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8eSPL\u6570\u636e\u96c6\uff1b2) \u4f7f\u7528\u81ea\u52a8\u8ddf\u8e2a\u5173\u8054\u7ba1\u9053\u6784\u5efa\u6570\u636e\u96c6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff1b3) \u5bf9\u6bd4\u5206\u6790\u5bf9\u6bd4\u5b66\u4e60(CL)\u548c\u76d1\u7763\u5b66\u4e60(SL)\u4e24\u79cd\u8303\u5f0f\u3002", "result": "\u5bf9\u6bd4\u5b66\u4e60\u5728\u7a00\u758f\u51e0\u4f55\u7279\u5f81\u7a7a\u95f4\u4e2d\u8bf1\u5bfc\u51fa\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u8868\u793a\uff0c\u57285-10\u7c73\u8ddd\u79bb\u8303\u56f4\u5185\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bc4\u4f30\u6311\u6218\u6027\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5730\u6807\u72ec\u7279\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u53ef\u6269\u5c55\u65b9\u6cd5\u5b66\uff0c\u8868\u660e\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u66f4\u9002\u5408\u5904\u7406\u8fdc\u8ddd\u79bb\u6746\u72b6\u7ed3\u6784\u7684\u8bc6\u522b\u95ee\u9898\u3002"}}
{"id": "2512.23153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23153", "abs": "https://arxiv.org/abs/2512.23153", "authors": ["Seiko Piotr Yamaguchi", "Kentaro Uno", "Yasumaru Fujii", "Masazumi Imai", "Kazuki Takada", "Taku Okawara", "Kazuya Yoshida"], "title": "Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot", "comment": "Author's version of a manuscript accepted at the 2025 IEEE/SICE International Symposium on System Integration (SII). (c) IEEE. The final published version is available at https://doi.org/10.1109/SII59315.2025.10870890", "summary": "This paper presents a feasibility study, including simulations and prototype tests, on the autonomous operation of a multi-limbed intra-vehicular robot (mobile manipulator), shortly MLIVR, designed to assist astronauts with logistical tasks on the International Space Station (ISS). Astronauts spend significant time on tasks such as preparation, close-out, and the collection and transportation of goods, reducing the time available for critical mission activities. Our study explores the potential for a mobile manipulator to support these operations, emphasizing the need for autonomous functionality to minimize crew and ground operator effort while enabling real-time task execution. We focused on the robot's transportation capabilities, simulating its motion planning in 3D space. The actual motion execution was tested with a prototype on a 2D table to mimic a microgravity environment. The results demonstrate the feasibility of performing these tasks with minimal human intervention, offering a promising solution to enhance operational efficiency on the ISS.", "AI": {"tldr": "\u7814\u7a76\u591a\u80a2\u4f53\u8231\u5185\u673a\u5668\u4eba\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u81ea\u4e3b\u6267\u884c\u7269\u6d41\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u539f\u578b\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u8fd0\u8f93\u80fd\u529b", "motivation": "\u5b87\u822a\u5458\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u5904\u7406\u7269\u6d41\u4efb\u52a1\uff08\u51c6\u5907\u3001\u6536\u5c3e\u3001\u8d27\u7269\u6536\u96c6\u548c\u8fd0\u8f93\uff09\uff0c\u51cf\u5c11\u4e86\u6267\u884c\u5173\u952e\u4efb\u52a1\u7684\u65f6\u95f4\u3002\u9700\u8981\u81ea\u4e3b\u79fb\u52a8\u673a\u68b0\u81c2\u6765\u652f\u6301\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u51cf\u5c11\u673a\u7ec4\u4eba\u5458\u548c\u5730\u9762\u64cd\u4f5c\u5458\u7684\u5de5\u4f5c\u91cf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4efb\u52a1\u6267\u884c\u3002", "method": "\u8fdb\u884c\u53ef\u884c\u6027\u7814\u7a76\uff0c\u5305\u62ec\u4eff\u771f\u548c\u539f\u578b\u6d4b\u8bd5\u3002\u4eff\u771f\u4e2d\u6a21\u62df\u673a\u5668\u4eba\u57283D\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u539f\u578b\u6d4b\u8bd5\u57282D\u5e73\u53f0\u4e0a\u8fdb\u884c\u4ee5\u6a21\u62df\u5fae\u91cd\u529b\u73af\u5883\uff0c\u6d4b\u8bd5\u5b9e\u9645\u8fd0\u52a8\u6267\u884c\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u673a\u5668\u4eba\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u5e72\u9884\u6267\u884c\u8fd9\u4e9b\u4efb\u52a1\uff0c\u4e3a\u589e\u5f3a\u56fd\u9645\u7a7a\u95f4\u7ad9\u64cd\u4f5c\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u81ea\u4e3b\u591a\u80a2\u4f53\u8231\u5185\u673a\u5668\u4eba\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u6267\u884c\u7269\u6d41\u4efb\u52a1\u662f\u53ef\u884c\u7684\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\uff0c\u51cf\u5c11\u5b87\u822a\u5458\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002"}}
{"id": "2512.23154", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23154", "abs": "https://arxiv.org/abs/2512.23154", "authors": ["Keigo Torii", "Kentaro Uno", "Shreya Santra", "Kazuya Yoshida"], "title": "A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots", "comment": "Author's version of a manuscript accepted at the 2025 IEEE International Conference on Mechatronics (ICM). (c) IEEE. The final published version is available at https://doi.org/10.1109/ICM62621.2025.10934866", "summary": "Lattice-based modular robot systems are envisioned for large-scale construction in extreme environments, such as space. Coupling mechanisms for heterogeneous structural modules should meet all of the following requirements: single-sided coupling and decoupling, flat surfaces when uncoupled, and coupling to passive coupling interfaces as well as coupling behavior between coupling mechanisms. The design requirements for such a coupling mechanism are complex. We propose a novel shape-matching mechanical coupling mechanism that satisfies these design requirements. This mechanism enables controlled, sequential transitions between male and female states. When uncoupled, all mechanisms are in the female state. To enable single-sided coupling, one side of the mechanisms switches to the male state during the coupling process. Single-sided decoupling is possible not only from the male side but also from the female side by forcibly switching the opposite mechanism's male state to the female state. This coupling mechanism can be applied to various modular robot systems and robot arm tool changers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u5f62\u72b6\u5339\u914d\u673a\u68b0\u8026\u5408\u673a\u5236\uff0c\u6ee1\u8db3\u5355\u4fa7\u8026\u5408/\u89e3\u8026\u3001\u89e3\u8026\u65f6\u8868\u9762\u5e73\u6574\u3001\u80fd\u4e0e\u88ab\u52a8\u63a5\u53e3\u8026\u5408\u7b49\u590d\u6742\u8bbe\u8ba1\u8981\u6c42\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u548c\u673a\u68b0\u81c2\u5de5\u5177\u66f4\u6362\u5668\u3002", "motivation": "\u9762\u5411\u6781\u7aef\u73af\u5883\uff08\u5982\u592a\u7a7a\uff09\u5927\u89c4\u6a21\u5efa\u9020\u7684\u6676\u683c\u578b\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u9700\u8981\u6ee1\u8db3\u5355\u4fa7\u8026\u5408/\u89e3\u8026\u3001\u89e3\u8026\u65f6\u8868\u9762\u5e73\u6574\u3001\u80fd\u4e0e\u88ab\u52a8\u63a5\u53e3\u8026\u5408\u7b49\u591a\u91cd\u590d\u6742\u8bbe\u8ba1\u8981\u6c42\u7684\u8026\u5408\u673a\u5236\u3002", "method": "\u63d0\u51fa\u5f62\u72b6\u5339\u914d\u673a\u68b0\u8026\u5408\u673a\u5236\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u987a\u5e8f\u72b6\u6001\u8f6c\u6362\u5b9e\u73b0\u96c4\u6027\u548c\u96cc\u6027\u72b6\u6001\u5207\u6362\u3002\u89e3\u8026\u65f6\u6240\u6709\u673a\u5236\u5904\u4e8e\u96cc\u6027\u72b6\u6001\uff1b\u8026\u5408\u8fc7\u7a0b\u4e2d\u4e00\u4fa7\u5207\u6362\u4e3a\u96c4\u6027\u72b6\u6001\u5b9e\u73b0\u5355\u4fa7\u8026\u5408\uff1b\u5355\u4fa7\u89e3\u8026\u53ef\u901a\u8fc7\u5f3a\u5236\u5c06\u5bf9\u65b9\u673a\u5236\u4ece\u96c4\u6027\u72b6\u6001\u5207\u6362\u4e3a\u96cc\u6027\u72b6\u6001\u5b9e\u73b0\u3002", "result": "\u8be5\u673a\u5236\u6210\u529f\u6ee1\u8db3\u4e86\u6240\u6709\u8bbe\u8ba1\u8981\u6c42\uff1a\u5355\u4fa7\u8026\u5408\u548c\u89e3\u8026\u3001\u89e3\u8026\u65f6\u8868\u9762\u5e73\u6574\u3001\u80fd\u4e0e\u88ab\u52a8\u8026\u5408\u63a5\u53e3\u8026\u5408\u4ee5\u53ca\u8026\u5408\u673a\u5236\u4e4b\u95f4\u7684\u8026\u5408\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u72b6\u5339\u914d\u673a\u68b0\u8026\u5408\u673a\u5236\u80fd\u591f\u6ee1\u8db3\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u8026\u5408\u9700\u6c42\uff0c\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u548c\u673a\u68b0\u81c2\u5de5\u5177\u66f4\u6362\u5668\u3002"}}
{"id": "2512.23162", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23162", "abs": "https://arxiv.org/abs/2512.23162", "authors": ["Yufan He", "Pengfei Guo", "Mengya Xu", "Zhaoshuo Li", "Andriy Myronenko", "Dillan Imans", "Bingjie Liu", "Dongren Yang", "Mingxue Gu", "Yongnan Ji", "Yueming Jin", "Ren Zhao", "Baiyong Shen", "Daguang Xu"], "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling", "comment": null, "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528SurgWorld\u4e16\u754c\u6a21\u578b\u751f\u6210\u5408\u6210\u624b\u672f\u89c6\u9891\uff0c\u901a\u8fc7\u9006\u52a8\u529b\u5b66\u6a21\u578b\u63a8\u65ad\u4f2a\u8fd0\u52a8\u5b66\u6570\u636e\uff0c\u89e3\u51b3\u624b\u672f\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347VLA\u7b56\u7565\u6027\u80fd", "motivation": "\u624b\u672f\u673a\u5668\u4eba\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u867d\u7136\u6709\u5927\u91cf\u624b\u672f\u89c6\u9891\u4f46\u7f3a\u4e4f\u5bf9\u5e94\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u6570\u636e\uff0c\u9650\u5236\u4e86\u6a21\u4eff\u5b66\u4e60\u548cVLA\u8bad\u7ec3\u7684\u5e94\u7528", "method": "1) \u6784\u5efaSATA\u6570\u636e\u96c6\u5305\u542b\u8be6\u7ec6\u624b\u672f\u673a\u5668\u4eba\u52a8\u4f5c\u63cf\u8ff0\uff1b2) \u57fa\u4e8e\u5148\u8fdb\u7269\u7406AI\u4e16\u754c\u6a21\u578b\u548cSATA\u6784\u5efaSurgWorld\u751f\u6210\u591a\u6837\u5316\u624b\u672f\u89c6\u9891\uff1b3) \u4f7f\u7528\u9006\u52a8\u529b\u5b66\u6a21\u578b\u4ece\u5408\u6210\u89c6\u9891\u63a8\u65ad\u4f2a\u8fd0\u52a8\u5b66\u6570\u636e\uff1b4) \u7528\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u624b\u672fVLA\u7b56\u7565", "result": "\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684\u624b\u672fVLA\u7b56\u7565\u5728\u771f\u5b9e\u624b\u672f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6f14\u793a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b", "conclusion": "\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u624b\u672f\u89c6\u9891\u548c\u751f\u6210\u5f0f\u4e16\u754c\u5efa\u6a21\uff0c\u4e3a\u81ea\u4e3b\u624b\u672f\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u5f00\u542f\u4e86\u901a\u7528\u4e14\u6570\u636e\u9ad8\u6548\u7684\u624b\u672f\u673a\u5668\u4eba\u7b56\u7565\u7684\u5927\u95e8"}}
{"id": "2512.23220", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23220", "abs": "https://arxiv.org/abs/2512.23220", "authors": ["Qin Wang", "Shanmin Pang", "Jianwu Fang", "Shengye Dong", "Fuhao Liu", "Jianru Xue", "Chen Lv"], "title": "A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict", "comment": null, "summary": "Human-vehicle cooperative driving serves as a vital bridge to fully autonomous driving by improving driving flexibility and gradually building driver trust and acceptance of autonomous technology. To establish more natural and effective human-vehicle interaction, we propose a Human-Oriented Cooperative Driving (HOCD) approach that primarily minimizes human-machine conflict by prioritizing driver intention and state. In implementation, we take both tactical and operational levels into account to ensure seamless human-vehicle cooperation. At the tactical level, we design an intention-aware trajectory planning method, using intention consistency cost as the core metric to evaluate the trajectory and align it with driver intention. At the operational level, we develop a control authority allocation strategy based on reinforcement learning, optimizing the policy through a designed reward function to achieve consistency between driver state and authority allocation. The results of simulation and human-in-the-loop experiments demonstrate that our proposed approach not only aligns with driver intention in trajectory planning but also ensures a reasonable authority allocation. Compared to other cooperative driving approaches, the proposed HOCD approach significantly enhances driving performance and mitigates human-machine conflict.The code is available at https://github.com/i-Qin/HOCD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4eba\u7c7b\u7684\u534f\u540c\u9a7e\u9a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6743\u5206\u914d\uff0c\u51cf\u5c11\u4eba\u673a\u51b2\u7a81\uff0c\u63d0\u5347\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u4eba\u8f66\u534f\u540c\u9a7e\u9a76\u662f\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u7684\u91cd\u8981\u6865\u6881\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u673a\u4ea4\u4e92\u81ea\u7136\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u51cf\u5c11\u4eba\u673a\u51b2\u7a81\u5e76\u63d0\u9ad8\u9a7e\u9a76\u5458\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6\u3002", "method": "\u63d0\u51faHOCD\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5c42\u9762\uff1a\u6218\u672f\u5c42\u9762\u8bbe\u8ba1\u610f\u56fe\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u4f7f\u7528\u610f\u56fe\u4e00\u81f4\u6027\u6210\u672c\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\uff1b\u64cd\u4f5c\u5c42\u9762\u5f00\u53d1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6743\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u4f18\u5316\u7b56\u7565\u3002", "result": "\u4eff\u771f\u548c\u4eba\u5728\u73af\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u80fd\u5bf9\u9f50\u9a7e\u9a76\u5458\u610f\u56fe\uff0c\u786e\u4fdd\u5408\u7406\u7684\u63a7\u5236\u6743\u5206\u914d\uff0c\u76f8\u6bd4\u5176\u4ed6\u534f\u540c\u9a7e\u9a76\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u6027\u80fd\u5e76\u51cf\u8f7b\u4e86\u4eba\u673a\u51b2\u7a81\u3002", "conclusion": "HOCD\u65b9\u6cd5\u901a\u8fc7\u4f18\u5148\u8003\u8651\u9a7e\u9a76\u5458\u610f\u56fe\u548c\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u6709\u6548\u7684\u4eba\u8f66\u4ea4\u4e92\uff0c\u4e3a\u4eba\u673a\u534f\u540c\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.23257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23257", "abs": "https://arxiv.org/abs/2512.23257", "authors": ["Socratis Gkelios", "Savvas D. Apostolidis", "Pavlos Ch. Kapoutsis", "Elias B. Kosmatopoulos", "Athanasios Ch. Kapoutsis"], "title": "Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) have revolutionized inspection tasks by offering a safer, more efficient, and flexible alternative to traditional methods. However, battery limitations often constrain their effectiveness, necessitating the development of optimized flight paths and data collection techniques. While existing approaches like coverage path planning (CPP) ensure comprehensive data collection, they can be inefficient, especially when inspecting multiple non connected Regions of Interest (ROIs). This paper introduces the Fast Inspection of Scattered Regions (FISR) problem and proposes a novel solution, the multi UAV Disjoint Areas Inspection (mUDAI) method. The introduced approach implements a two fold optimization procedure, for calculating the best image capturing positions and the most efficient UAV trajectories, balancing data resolution and operational time, minimizing redundant data collection and resource consumption. The mUDAI method is designed to enable rapid, efficient inspections of scattered ROIs, making it ideal for applications such as security infrastructure assessments, agricultural inspections, and emergency site evaluations. A combination of simulated evaluations and real world deployments is used to validate and quantify the method's ability to improve operational efficiency while preserving high quality data capture, demonstrating its effectiveness in real world operations. An open source Python implementation of the mUDAI method can be found on GitHub (https://github.com/soc12/mUDAI) and the collected and processed data from the real world experiments are all hosted on Zenodo (https://zenodo.org/records/13866483). Finally, this online platform (https://sites.google.com/view/mudai-platform/) allows interested readers to interact with the mUDAI method and generate their own multi UAV FISR missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51famUDAI\u65b9\u6cd5\u89e3\u51b3\u65e0\u4eba\u673a\u5bf9\u5206\u6563\u533a\u57df\u5feb\u901f\u5de1\u68c0\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u91c7\u96c6\u4f4d\u7f6e\u548c\u98de\u884c\u8f68\u8ff9\uff0c\u5e73\u8861\u6570\u636e\u5206\u8fa8\u7387\u4e0e\u64cd\u4f5c\u65f6\u95f4\uff0c\u51cf\u5c11\u5197\u4f59\u6570\u636e\u6536\u96c6\u548c\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u65e0\u4eba\u673a\u5de1\u68c0\u867d\u7136\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5b89\u5168\u9ad8\u6548\uff0c\u4f46\u7535\u6c60\u9650\u5236\u5f71\u54cd\u5176\u6709\u6548\u6027\u3002\u73b0\u6709\u8986\u76d6\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u68c0\u67e5\u591a\u4e2a\u975e\u8fde\u63a5\u611f\u5174\u8da3\u533a\u57df\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u4f18\u5316\u7684\u98de\u884c\u8def\u5f84\u548c\u6570\u636e\u91c7\u96c6\u6280\u672f\u3002", "method": "\u63d0\u51fa\u591a\u65e0\u4eba\u673a\u5206\u79bb\u533a\u57df\u5de1\u68c0\u65b9\u6cd5\uff0c\u5b9e\u65bd\u53cc\u91cd\u4f18\u5316\u7a0b\u5e8f\uff1a\u8ba1\u7b97\u6700\u4f73\u56fe\u50cf\u91c7\u96c6\u4f4d\u7f6e\u548c\u6700\u6709\u6548\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\uff0c\u5e73\u8861\u6570\u636e\u5206\u8fa8\u7387\u548c\u64cd\u4f5c\u65f6\u95f4\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u8bc4\u4f30\u548c\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6570\u636e\u91c7\u96c6\uff0c\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u63d0\u4f9b\u4e86\u5f00\u6e90Python\u5b9e\u73b0\u3001\u5b9e\u9a8c\u6570\u636e\u548c\u5728\u7ebf\u4ea4\u4e92\u5e73\u53f0\u3002", "conclusion": "mUDAI\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5206\u6563\u611f\u5174\u8da3\u533a\u57df\u7684\u5feb\u901f\u9ad8\u6548\u5de1\u68c0\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u8bc4\u4f30\u3001\u519c\u4e1a\u68c0\u67e5\u548c\u5e94\u6025\u73b0\u573a\u8bc4\u4f30\u7b49\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.23318", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23318", "abs": "https://arxiv.org/abs/2512.23318", "authors": ["Sheng-Kai Chen", "Jie-Yu Chao", "Jr-Yu Chang", "Po-Lien Wu", "Po-Chiang Lin"], "title": "PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering", "comment": "17 pages, 2 figures, 1 table", "summary": "Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.", "AI": {"tldr": "PCR-ORB\u662f\u4e00\u79cd\u589e\u5f3a\u7684ORB-SLAM3\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u70b9\u4e91\u7cbe\u70bc\u548c\u8bed\u4e49\u5206\u5272\u6765\u51cf\u5c11\u52a8\u6001\u7269\u4f53\u5bf9vSLAM\u7cfb\u7edf\u7684\u5e72\u6270\uff0c\u5728KITTI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6df7\u5408\u4f46\u90e8\u5206\u5e8f\u5217\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u7269\u4f53\u4f1a\u4e25\u91cd\u5f71\u54cd\u89c6\u89c9SLAM\u7cfb\u7edf\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5730\u56fe\u4e00\u81f4\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u52a8\u6001\u7269\u4f53\u5e72\u6270\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8eORB-SLAM3\u6846\u67b6\uff0c\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u70b9\u4e91\u7cbe\u70bc\u6280\u672f\uff0c\u4f7f\u7528YOLOv8\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u7ed3\u5408CUDA\u52a0\u901f\u5904\u7406\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002\u91c7\u7528\u591a\u9636\u6bb5\u8fc7\u6ee4\u7b56\u7565\uff0c\u5305\u62ec\u5730\u9762\u5e73\u9762\u4f30\u8ba1\u3001\u5929\u7a7a\u533a\u57df\u79fb\u9664\u3001\u8fb9\u7f18\u8fc7\u6ee4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u9a8c\u8bc1\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\uff08\u5e8f\u521700-09\uff09\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u8868\u73b0\u56e0\u573a\u666f\u800c\u5f02\u3002\u5e8f\u521704\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff1aATE RMSE\u63d0\u534725.9%\uff0cATE\u4e2d\u503c\u63d0\u534730.4%\u3002\u4f46\u4e0d\u540c\u5e8f\u5217\u7ed3\u679c\u6df7\u5408\uff0c\u8868\u660e\u6548\u679c\u5177\u6709\u573a\u666f\u4f9d\u8d56\u6027\u3002", "conclusion": "PCR-ORB\u6846\u67b6\u4e3a\u52a8\u6001\u7269\u4f53\u8fc7\u6ee4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u663e\u8457\u63d0\u5347SLAM\u6027\u80fd\uff0c\u4f46\u6548\u679c\u53d7\u73af\u5883\u6761\u4ef6\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u52a8\u6001\u73af\u5883\u5bfc\u822a\u7684\u6311\u6218\u548c\u673a\u9047\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u5bfc\u822a\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2512.23482", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23482", "abs": "https://arxiv.org/abs/2512.23482", "authors": ["Marie Bauer", "Julia Gachot", "Matthias Kerzel", "Cornelius Weber", "Stefan Wermter"], "title": "Theory of Mind for Explainable Human-Robot Interaction", "comment": null, "summary": "Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\u89c6\u4e3a\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u901a\u8fc7VXAI\u6846\u67b6\u8bc4\u4f30\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u5fc3\u7406\u7406\u8bba\u5e94\u7528\u7f3a\u4e4f\u5bf9\u673a\u5668\u4eba\u5b9e\u9645\u5185\u90e8\u63a8\u7406\u9a8c\u8bc1\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\u65b9\u6cd5\u5f88\u5c11\u8bc4\u4f30\u89e3\u91ca\u4e0e\u673a\u5668\u4eba\u5b9e\u9645\u5185\u90e8\u63a8\u7406\u7684\u5bf9\u5e94\u7a0b\u5ea6\uff0c\u5b58\u5728\u91cd\u8981\u7814\u7a76\u7a7a\u767d\u3002\u540c\u65f6\uff0c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e3b\u8981\u5173\u6ce8AI\u7cfb\u7edf\u672c\u8eab\uff0c\u7f3a\u4e4f\u7528\u6237\u4e2d\u5fc3\u7684\u89e3\u91ca\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u5c06\u5fc3\u7406\u7406\u8bba\u6574\u5408\u5230\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u4e2d\uff0c\u7279\u522b\u662f\u901a\u8fc7VXAI\u6846\u67b6\u53ca\u5176\u4e03\u4e2a\u671f\u671b\u6807\u51c6\u6765\u8bc4\u4f30\u5fc3\u7406\u7406\u8bba\u3002\u5c06\u5fc3\u7406\u7406\u8bba\u539f\u5219\u5d4c\u5165\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff0c\u5b9e\u73b0\u4ece\u7cfb\u7edf\u4e2d\u5fc3\u5230\u7528\u6237\u4e2d\u5fc3\u7684\u89c6\u89d2\u8f6c\u53d8\u3002", "result": "\u901a\u8fc7\u5c06\u5fc3\u7406\u7406\u8bba\u89c6\u4e3a\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u673a\u5668\u4eba\u89e3\u91ca\u4e0e\u5176\u5b9e\u9645\u5185\u90e8\u63a8\u7406\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4f7f\u89e3\u91ca\u66f4\u52a0\u7528\u6237\u4e2d\u5fc3\u5316\uff0c\u4f18\u5148\u8003\u8651\u7528\u6237\u7684\u4fe1\u606f\u9700\u6c42\u548c\u89c6\u89d2\u3002", "conclusion": "\u5c06\u5fc3\u7406\u7406\u8bba\u6574\u5408\u5230\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u4e2d\u53ef\u4ee5\u5f25\u8865\u5f53\u524d\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u4fc3\u8fdb\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u4e14\u7528\u6237\u4e2d\u5fc3\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u53d1\u5c55\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u6548\u679c\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2512.23505", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23505", "abs": "https://arxiv.org/abs/2512.23505", "authors": ["Mehdi Heydari Shahna"], "title": "Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery", "comment": "Doctoral Dissertation, Tampere University", "summary": "Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.\n  This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.\n  Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.\n  The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u91cd\u578b\u79fb\u52a8\u673a\u68b0\u7684\u7535\u6c14\u5316\u63a7\u5236\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u63a7\u5236\u7b56\u7565\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u524d\u63d0\u4e0b\u90e8\u5206\u96c6\u6210AI\u6280\u672f\u3002", "motivation": "\u91cd\u578b\u79fb\u52a8\u673a\u68b0\u9762\u4e34\u4e24\u5927\u8f6c\u578b\uff1a\u4ece\u67f4\u6cb9\u6db2\u538b\u9a71\u52a8\u8f6c\u5411\u6e05\u6d01\u7535\u529b\u7cfb\u7edf\uff0c\u4ee5\u53ca\u4ece\u4eba\u5de5\u76d1\u7763\u8f6c\u5411\u66f4\u9ad8\u81ea\u4e3b\u6027\u3002\u4f46\u5b8c\u5168\u7535\u6c14\u5316\u9762\u4e34\u6280\u672f\u7ecf\u6d4e\u6311\u6218\uff0cAI\u5728\u91cd\u578b\u673a\u68b0\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u4e25\u683c\u7684\u5b89\u5168\u8981\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u901a\u7528\u6a21\u5757\u5316\u63a7\u5236\u6846\u67b6\uff0c\u91c7\u7528\u80fd\u6e90\u72ec\u7acb\u7684\u901a\u7528\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff0c\u5b9a\u4e49\u5206\u5c42\u63a7\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u90e8\u5206\u96c6\u6210AI\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u6846\u67b6\u5728\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u6db5\u76d6\u4e0d\u540c\u7c7b\u578b\u6267\u884c\u5668\u548c\u5de5\u51b5\u7684\u91cd\u578b\u79fb\u52a8\u673a\u5668\u4eba\u548c\u673a\u68b0\u81c2\uff0c\u6210\u679c\u53d1\u8868\u5728\u4e94\u7bc7\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u548c\u4e00\u7bc7\u672a\u53d1\u8868\u624b\u7a3f\u4e2d\u3002", "conclusion": "\u8be5\u63a7\u5236\u6846\u67b6\u4e3a\u91cd\u578b\u79fb\u52a8\u673a\u68b0\u7684\u7535\u6c14\u5316\u548c\u81ea\u4e3b\u5316\u8f6c\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u8fdb\u4e86\u975e\u7ebf\u6027\u63a7\u5236\u548c\u673a\u5668\u4eba\u6280\u672f\u53d1\u5c55\uff0c\u652f\u6301\u4e24\u5927\u884c\u4e1a\u8f6c\u578b\u3002"}}
{"id": "2512.23541", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23541", "abs": "https://arxiv.org/abs/2512.23541", "authors": ["Pengfei Zhou", "Liliang Chen", "Shengcong Chen", "Di Chen", "Wenzhi Zhao", "Rongjun Jin", "Guanghui Ren", "Jianlan Luo"], "title": "Act2Goal: From World Model To General Goal-conditioned Policy", "comment": null, "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/", "AI": {"tldr": "Act2Goal\u662f\u4e00\u4e2a\u76ee\u6807\u6761\u4ef6\u64cd\u4f5c\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\uff0c\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u89c6\u89c9\u72b6\u6001\u5e8f\u5217\u548c\u5206\u89e3\u8f68\u8ff9\u6765\u5b9e\u73b0\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u9c81\u68d2\u6267\u884c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u76ee\u6807\u7684\u4efb\u52a1\u6307\u5b9a\u65b9\u6cd5\u867d\u7136\u7d27\u51d1\u660e\u786e\uff0c\u4f46\u73b0\u6709\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f9d\u8d56\u5355\u6b65\u52a8\u4f5c\u9884\u6d4b\u800c\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u8fdb\u5ea6\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u63d0\u51faAct2Goal\u6846\u67b6\uff1a1) \u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u751f\u6210\u53ef\u4fe1\u7684\u4e2d\u95f4\u89c6\u89c9\u72b6\u6001\u5e8f\u5217\uff1b2) \u591a\u5c3a\u5ea6\u65f6\u95f4\u54c8\u5e0c(MSTH)\u5c06\u8f68\u8ff9\u5206\u89e3\u4e3a\u5bc6\u96c6\u8fd1\u7aef\u5e27\uff08\u7ec6\u7c92\u5ea6\u95ed\u73af\u63a7\u5236\uff09\u548c\u7a00\u758f\u8fdc\u7aef\u5e27\uff08\u5168\u5c40\u4efb\u52a1\u4e00\u81f4\u6027\uff09\uff1b3) \u901a\u8fc7\u7aef\u5230\u7aef\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u89c6\u89c9\u8868\u793a\u4e0e\u8fd0\u52a8\u63a7\u5236\u8026\u5408\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u65b0\u7269\u4f53\u3001\u7a7a\u95f4\u5e03\u5c40\u548c\u73af\u5883\u7684\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff1b\u901a\u8fc7\u540e\u89c1\u76ee\u6807\u91cd\u6807\u8bb0\u548cLoRA\u5fae\u8c03\u5b9e\u73b0\u65e0\u5956\u52b1\u5728\u7ebf\u9002\u5e94\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6311\u6218\u6027\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\uff0c\u6210\u529f\u7387\u4ece30%\u63d0\u5347\u523090%\uff0c\u4ec5\u9700\u51e0\u5206\u949f\u7684\u81ea\u4e3b\u4ea4\u4e92\u3002", "conclusion": "\u5177\u6709\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\u7684\u76ee\u6807\u6761\u4ef6\u4e16\u754c\u6a21\u578b\u4e3a\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u7ed3\u6784\u5316\u6307\u5bfc\uff0cAct2Goal\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2512.23570", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23570", "abs": "https://arxiv.org/abs/2512.23570", "authors": ["Amy Ingold", "Loong Yi Lee", "Richard Suphapol Diteesawat", "Ajmal Roshan", "Yael Zekaria", "Edith-Clare Hall", "Enrico Werner", "Nahian Rahman", "Elaine Czech", "Jonathan Rossiter"], "title": "Soft Robotic Technological Probe for Speculative Fashion Futures", "comment": null, "summary": "Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.", "AI": {"tldr": "Sumbrella\u662f\u4e00\u6b3e\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u88c5\uff0c\u4f5c\u4e3a\u63a8\u6d4b\u6027\u65f6\u5c1a\u63a2\u9488\u5f00\u53d1\uff0c\u7ed3\u5408\u4e86\u6298\u7eb8\u542f\u53d1\u7684\u53cc\u7a33\u6001\u5355\u5143\u3001\u7ec7\u7269\u6c14\u52a8\u9a71\u52a8\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u7528\u4e8e\u63a2\u7d22\u4eba\u4eec\u5bf9\u8f6f\u4f53\u673a\u5668\u4eba\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u672a\u6765\u5173\u7cfb\u60f3\u8c61\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e0d\u4ec5\u5173\u6ce8\u529f\u80fd\uff0c\u8fd8\u8981\u8003\u8651\u793e\u4f1a\u610f\u4e49\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u88c5\u5982\u4f55\u5f71\u54cd\u4eba\u4eec\u5bf9\u53ef\u7a7f\u6234\u6280\u672f\u7684\u89e3\u8bfb\u3001\u4e92\u52a8\u548c\u672a\u6765\u5173\u7cfb\u60f3\u8c61\u3002", "method": "\u8bbe\u8ba1\u4e86Sumbrella\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u88c5\uff0c\u5305\u542b\u6298\u7eb8\u542f\u53d1\u7684\u53cc\u7a33\u6001\u5355\u5143\u3001\u7ec7\u7269\u6c14\u52a8\u9a71\u52a8\u5ba4\u3001\u7ebf\u7f06\u9a71\u52a8\u5f62\u72b6\u53d8\u5f62\u673a\u5236\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7ec4\u4ef6\uff0c\u4ee5\u53ca\u96c6\u6210\u7535\u6e90\u548c\u63a7\u5236\u7535\u5b50\u8bbe\u5907\u7684\u5e3d\u5b50\u548c\u5939\u514b\u3002\u901a\u8fc712\u540d\u521b\u610f\u6280\u672f\u4e13\u5bb6\u7684\u7126\u70b9\u5c0f\u7ec4\uff0c\u5c06\u5176\u4f5c\u4e3a\u6280\u672f\u63a2\u9488\u6765\u63a2\u7d22\u4eba\u4eec\u5bf9\u8f6f\u4f53\u673a\u5668\u4eba\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u89e3\u8bfb\u548c\u60f3\u8c61\u3002", "result": "\u53c2\u4e0e\u8005\u56f4\u7ed5\u63a8\u6d4b\u6027\u672a\u6765\u548c\u8868\u8fbe\u6f5c\u529b\u8fdb\u884c\u4e86\u4e30\u5bcc\u8ba8\u8bba\uff0c\u4f46\u4e5f\u63d0\u51fa\u4e86\u5bf9\u5265\u524a\u3001\u76d1\u63a7\u4ee5\u53ca\u751f\u7269\u4f20\u611f\u6280\u672f\u5d4c\u5165\u516c\u5171\u751f\u6d3b\u7684\u4e2a\u4eba\u98ce\u9669\u548c\u793e\u4f1a\u4f26\u7406\u7684\u62c5\u5fe7\u3002\u7814\u7a76\u4e3aHRI\u9886\u57df\u63d0\u4f9b\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u88c5\u8bbe\u8ba1\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\u548c\u5efa\u8bae\u3002", "conclusion": "\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6cd5\u4f7fHRI\u7814\u7a76\u4eba\u5458\u4e0d\u4ec5\u80fd\u8003\u8651\u529f\u80fd\u6027\uff0c\u8fd8\u80fd\u63a2\u7d22\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u5982\u4f55\u5f71\u54cd\u516c\u5171\u573a\u5408\u4e2d\u88ab\u8ba4\u4e3a\u53ef\u63a5\u53d7\u6216\u7406\u60f3\u7684\u5b9a\u4e49\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u88c5\u5728\u52a8\u6001\u4ea4\u6d41\u3001\u793e\u4f1a\u52a8\u6001\u5f71\u54cd\u548c\u4f26\u7406\u6307\u5357\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.23593", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23593", "abs": "https://arxiv.org/abs/2512.23593", "authors": ["Nikolai Beving", "Jonas Marxen", "Steffen Mueller", "Johannes Betz"], "title": "A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems", "comment": null, "summary": "Steer-by-Wire systems replace mechanical linkages, which provide benefits like weight reduction, design flexibility, and compatibility with autonomous driving. However, they are susceptible to high-frequency disturbances from unintentional driver torque, known as driver impedance, which can degrade steering performance. Existing approaches either rely on direct torque sensors, which are costly and impractical, or lack the temporal resolution to capture rapid, high-frequency driver-induced disturbances. We address this limitation by designing a Kalman filter-based disturbance observer that estimates high-frequency driver torque using only motor state measurements. We model the drivers passive torque as an extended state using a PT1-lag approximation and integrate it into both linear and nonlinear Steer-by-Wire system models. In this paper, we present the design, implementation and simulation of this disturbance observer with an evaluation of different Kalman filter variants. Our findings indicate that the proposed disturbance observer accurately reconstructs driver-induced disturbances with only minimal delay 14ms. We show that a nonlinear extended Kalman Filter outperforms its linear counterpart in handling frictional nonlinearities, improving estimation during transitions from static to dynamic friction. Given the study's methodology, it was unavoidable to rely on simulation-based validation rather than real-world experimentation. Further studies are needed to investigate the robustness of the observers under real-world driving conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6270\u52a8\u89c2\u6d4b\u5668\uff0c\u4ec5\u4f7f\u7528\u7535\u673a\u72b6\u6001\u6d4b\u91cf\u5373\u53ef\u4f30\u8ba1\u9ad8\u9891\u9a7e\u9a76\u5458\u626d\u77e9\uff0c\u89e3\u51b3\u7ebf\u63a7\u8f6c\u5411\u7cfb\u7edf\u4e2d\u9a7e\u9a76\u5458\u963b\u6297\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4eff\u771f\u663e\u793a\u5ef6\u8fdf\u4ec514ms\u3002", "motivation": "\u7ebf\u63a7\u8f6c\u5411\u7cfb\u7edf\u53d6\u4ee3\u673a\u68b0\u8fde\u63a5\u5e26\u6765\u8bf8\u591a\u4f18\u52bf\uff0c\u4f46\u6613\u53d7\u9a7e\u9a76\u5458\u65e0\u610f\u626d\u77e9\u4ea7\u751f\u7684\u9ad8\u9891\u6270\u52a8\uff08\u9a7e\u9a76\u5458\u963b\u6297\uff09\u5f71\u54cd\uff0c\u964d\u4f4e\u8f6c\u5411\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u76f4\u63a5\u626d\u77e9\u4f20\u611f\u5668\uff0c\u8981\u4e48\u7f3a\u4e4f\u6355\u6349\u5feb\u901f\u9ad8\u9891\u6270\u52a8\u7684\u65f6\u57df\u5206\u8fa8\u7387\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6270\u52a8\u89c2\u6d4b\u5668\uff0c\u4f7f\u7528PT1\u6ede\u540e\u8fd1\u4f3c\u5c06\u9a7e\u9a76\u5458\u88ab\u52a8\u626d\u77e9\u5efa\u6a21\u4e3a\u6269\u5c55\u72b6\u6001\uff0c\u5e76\u96c6\u6210\u5230\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7ebf\u63a7\u8f6c\u5411\u7cfb\u7edf\u6a21\u578b\u4e2d\u3002\u8bc4\u4f30\u4e0d\u540c\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\uff0c\u5305\u62ec\u975e\u7ebf\u6027\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3002", "result": "\u63d0\u51fa\u7684\u6270\u52a8\u89c2\u6d4b\u5668\u80fd\u591f\u51c6\u786e\u91cd\u6784\u9a7e\u9a76\u5458\u5f15\u8d77\u7684\u6270\u52a8\uff0c\u5ef6\u8fdf\u4ec5\u4e3a14ms\u3002\u975e\u7ebf\u6027\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u5904\u7406\u6469\u64e6\u975e\u7ebf\u6027\u65b9\u9762\u4f18\u4e8e\u7ebf\u6027\u7248\u672c\uff0c\u5728\u9759\u6001\u5230\u52a8\u6001\u6469\u64e6\u8f6c\u6362\u671f\u95f4\u6539\u5584\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u63a7\u8f6c\u5411\u7cfb\u7edf\u4e2d\u9a7e\u9a76\u5458\u963b\u6297\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u4f46\u7814\u7a76\u57fa\u4e8e\u4eff\u771f\u9a8c\u8bc1\uff0c\u9700\u8981\u5728\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u8fdb\u4e00\u6b65\u7814\u7a76\u89c2\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.23616", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23616", "abs": "https://arxiv.org/abs/2512.23616", "authors": ["Christoph Willibald", "Lugh Martensen", "Thomas Eiband", "Dongheui Lee"], "title": "Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces", "comment": "Currently under review at Intelligent Service Robotics", "summary": "Lengthy setup processes that require robotics expertise remain a major barrier to deploying robots for tasks involving high product variability and small batch sizes. As a result, collaborative robots, despite their advanced sensing and control capabilities, are rarely used for surface finishing in small-scale craft and manufacturing settings. To address this gap, we propose a novel robot programming approach that enables non-experts to intuitively program robots through interactive, task-focused workflows. For that, we developed a new surface segmentation algorithm that incorporates human input to identify and refine workpiece regions for processing. Throughout the programming process, users receive continuous visual feedback on the robot's learned model, enabling them to iteratively refine the segmentation result. Based on the segmented surface model, a robot trajectory is generated to cover the desired processing area. We evaluated multiple interaction designs across two comprehensive user studies to derive an optimal interface that significantly reduces user workload, improves usability and enables effective task programming even for users with limited practical experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u975e\u4e13\u5bb6\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5bfc\u5411\u5de5\u4f5c\u6d41\u7a0b\u7b80\u5316\u8868\u9762\u5904\u7406\u4efb\u52a1\u7684\u7f16\u7a0b\uff0c\u964d\u4f4e\u673a\u5668\u4eba\u90e8\u7f72\u95e8\u69db", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7f16\u7a0b\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8bbe\u7f6e\u8fc7\u7a0b\u590d\u6742\uff0c\u963b\u788d\u4e86\u673a\u5668\u4eba\u5728\u9ad8\u4ea7\u54c1\u53d8\u5f02\u6027\u3001\u5c0f\u6279\u91cf\u751f\u4ea7\u573a\u666f\uff08\u5982\u5c0f\u578b\u624b\u5de5\u827a\u5236\u9020\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u8868\u9762\u5904\u7406\u4efb\u52a1", "method": "\u5f00\u53d1\u4e86\u7ed3\u5408\u4eba\u5de5\u8f93\u5165\u7684\u65b0\u578b\u8868\u9762\u5206\u5272\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5bfc\u5411\u5de5\u4f5c\u6d41\u7a0b\u8ba9\u975e\u4e13\u5bb6\u76f4\u89c2\u7f16\u7a0b\uff0c\u63d0\u4f9b\u8fde\u7eed\u89c6\u89c9\u53cd\u9988\u8ba9\u7528\u6237\u8fed\u4ee3\u4f18\u5316\u5206\u5272\u7ed3\u679c\uff0c\u57fa\u4e8e\u5206\u5272\u8868\u9762\u6a21\u578b\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9", "result": "\u901a\u8fc7\u4e24\u4e2a\u7efc\u5408\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u591a\u79cd\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u5f97\u51fa\u6700\u4f18\u754c\u9762\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u7528\u6237\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u53ef\u7528\u6027\uff0c\u4f7f\u7f3a\u4e4f\u5b9e\u8df5\u7ecf\u9a8c\u7684\u7528\u6237\u4e5f\u80fd\u6709\u6548\u5b8c\u6210\u4efb\u52a1\u7f16\u7a0b", "conclusion": "\u63d0\u51fa\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u65b9\u6cd5\u6210\u529f\u964d\u4f4e\u4e86\u534f\u4f5c\u673a\u5668\u4eba\u5728\u8868\u9762\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u90e8\u7f72\u95e8\u69db\uff0c\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u591f\u6709\u6548\u7f16\u7a0b\uff0c\u6709\u671b\u4fc3\u8fdb\u673a\u5668\u4eba\u5728\u5c0f\u578b\u5236\u9020\u548c\u624b\u5de5\u827a\u9886\u57df\u7684\u5e94\u7528"}}
{"id": "2512.23619", "categories": ["cs.RO", "math.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.23619", "abs": "https://arxiv.org/abs/2512.23619", "authors": ["Antonio Franchi"], "title": "The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors", "comment": null, "summary": "The geometric design of fully-actuated and omnidirectional N-rotor aerial vehicles is conventionally formulated as a parametric optimization problem, seeking a single optimal set of N orientations within a fixed architectural family. This work departs from that paradigm to investigate the intrinsic topological structure of the optimization landscape itself. We formulate the design problem on the product manifold of Projective Lines \\RP^2^N, fixing the rotor positions to the vertices of polyhedral chassis while varying their lines of action. By minimizing a coordinate-invariant Log-Volume isotropy metric, we reveal that the topology of the global optima is governed strictly by the symmetry of the chassis. For generic (irregular) vertex arrangements, the solutions appear as a discrete set of isolated points. However, as the chassis geometry approaches regularity, the solution space undergoes a critical phase transition, collapsing onto an N-dimensional Torus of the lines tangent at the vertexes to the circumscribing sphere of the chassis, and subsequently reducing to continuous 1-dimensional curves driven by Affine Phase Locking. We synthesize these observations into the N-5 Scaling Law: an empirical relationship holding for all examined regular planar polygons and Platonic solids (N <= 10), where the space of optimal configurations consists of K=N-5 disconnected 1D topological branches. We demonstrate that these locking patterns correspond to a sequence of admissible Star Polygons {N/q}, allowing for the exact prediction of optimal phases for arbitrary N. Crucially, this topology reveals a design redundancy that enables optimality-preserving morphing: the vehicle can continuously reconfigure along these branches while preserving optimal isotropic control authority.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86N\u65cb\u7ffc\u98de\u884c\u5668\u7684\u51e0\u4f55\u8bbe\u8ba1\uff0c\u53d1\u73b0\u6700\u4f18\u914d\u7f6e\u7684\u62d3\u6251\u7ed3\u6784\u7531\u673a\u67b6\u5bf9\u79f0\u6027\u51b3\u5b9a\uff0c\u63d0\u51fa\u4e86N-5\u7f29\u653e\u5b9a\u5f8b\uff0c\u63ed\u793a\u4e86\u6700\u4f18\u914d\u7f6e\u4fdd\u6301\u4e0b\u7684\u8fde\u7eed\u91cd\u6784\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5168\u9a71\u52a8\u5168\u5411N\u65cb\u7ffc\u98de\u884c\u5668\u7684\u51e0\u4f55\u8bbe\u8ba1\u89c6\u4e3a\u53c2\u6570\u4f18\u5316\u95ee\u9898\uff0c\u5bfb\u627e\u56fa\u5b9a\u67b6\u6784\u65cf\u4e2d\u7684\u5355\u4e00\u6700\u4f18\u65b9\u5411\u96c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f18\u5316\u666f\u89c2\u672c\u8eab\u7684\u5185\u5728\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u5c06\u8bbe\u8ba1\u95ee\u9898\u8868\u8ff0\u5728\u6295\u5f71\u7ebf\u4e58\u79ef\u6d41\u5f62RP^2^N\u4e0a\uff0c\u56fa\u5b9a\u8f6c\u5b50\u4f4d\u7f6e\u4e8e\u591a\u9762\u4f53\u673a\u67b6\u9876\u70b9\uff0c\u53d8\u5316\u5176\u4f5c\u7528\u7ebf\u65b9\u5411\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u5750\u6807\u4e0d\u53d8\u7684Log-Volume\u5404\u5411\u540c\u6027\u5ea6\u91cf\uff0c\u5206\u6790\u5168\u5c40\u6700\u4f18\u89e3\u7684\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u53d1\u73b0\u6700\u4f18\u89e3\u7684\u62d3\u6251\u4e25\u683c\u7531\u673a\u67b6\u5bf9\u79f0\u6027\u51b3\u5b9a\uff1a\u5bf9\u4e8e\u4e00\u822c\uff08\u4e0d\u89c4\u5219\uff09\u9876\u70b9\u6392\u5217\uff0c\u89e3\u4e3a\u79bb\u6563\u5b64\u7acb\u70b9\u96c6\uff1b\u5f53\u673a\u67b6\u51e0\u4f55\u8d8b\u4e8e\u89c4\u5219\u65f6\uff0c\u89e3\u7a7a\u95f4\u7ecf\u5386\u4e34\u754c\u76f8\u53d8\uff0c\u584c\u7f29\u5230N\u7ef4\u73af\u9762\uff0c\u968f\u540e\u7b80\u5316\u4e3a\u7531\u4eff\u5c04\u76f8\u4f4d\u9501\u5b9a\u9a71\u52a8\u7684\u8fde\u7eed1\u7ef4\u66f2\u7ebf\u3002\u63d0\u51fa\u4e86N-5\u7f29\u653e\u5b9a\u5f8b\uff1a\u5bf9\u4e8e\u6240\u6709\u68c0\u67e5\u7684\u6b63\u5219\u5e73\u9762\u591a\u8fb9\u5f62\u548c\u67cf\u62c9\u56fe\u7acb\u4f53\uff08N\u226410\uff09\uff0c\u6700\u4f18\u914d\u7f6e\u7a7a\u95f4\u7531K=N-5\u4e2a\u4e0d\u76f8\u8fde\u76841\u7ef4\u62d3\u6251\u5206\u652f\u7ec4\u6210\u3002", "conclusion": "\u8fd9\u4e9b\u9501\u5b9a\u6a21\u5f0f\u5bf9\u5e94\u4e00\u7cfb\u5217\u53ef\u5bb9\u8bb8\u7684\u661f\u5f62\u591a\u8fb9\u5f62{N/q}\uff0c\u5141\u8bb8\u7cbe\u786e\u9884\u6d4b\u4efb\u610fN\u7684\u6700\u4f18\u76f8\u4f4d\u3002\u8fd9\u79cd\u62d3\u6251\u63ed\u793a\u4e86\u8bbe\u8ba1\u5197\u4f59\uff0c\u4f7f\u8f66\u8f86\u80fd\u591f\u6cbf\u8fd9\u4e9b\u5206\u652f\u8fde\u7eed\u91cd\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f18\u7684\u5404\u5411\u540c\u6027\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2512.23649", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23649", "abs": "https://arxiv.org/abs/2512.23649", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Tao Huang", "Zhenguo Sun", "Yibo Peng", "Pengwei Wang", "Zhongyuan Wang", "Fangzhou Liu", "Chang Xu", "Shanghang Zhang"], "title": "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion", "comment": null, "summary": "Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying \"understand before you imitate\". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.", "AI": {"tldr": "RoboMirror\u662f\u9996\u4e2a\u514d\u91cd\u5b9a\u5411\u7684\u89c6\u9891\u5230\u8fd0\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u7b2c\u4e00\u4eba\u79f0/\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u4e2d\u63d0\u53d6\u89c6\u89c9\u8fd0\u52a8\u610f\u56fe\uff0c\u76f4\u63a5\u6307\u5bfc\u6269\u6563\u7b56\u7565\u751f\u6210\u7269\u7406\u5408\u7406\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\uff0c\u65e0\u9700\u663e\u5f0f\u59ff\u6001\u91cd\u5efa\u6216\u91cd\u5b9a\u5411\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u52a8\u4f5c\u6355\u6349\u8f68\u8ff9\u6216\u7a00\u758f\u6587\u672c\u6307\u4ee4\uff0c\u5728\u89c6\u89c9\u7406\u89e3\u548c\u63a7\u5236\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002\u6587\u672c\u5230\u8fd0\u52a8\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u7a00\u758f\u6027\u548c\u6d41\u6c34\u7ebf\u9519\u8bef\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u89c6\u9891\u7684\u65b9\u6cd5\u53ea\u8fdb\u884c\u673a\u68b0\u59ff\u6001\u6a21\u4eff\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u89c6\u89c9\u7406\u89e3\u3002", "method": "\u63d0\u51faRoboMirror\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u539f\u59cb\u7684\u7b2c\u4e00\u4eba\u79f0/\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u4e2d\u63d0\u53d6\u89c6\u89c9\u8fd0\u52a8\u610f\u56fe\uff0c\u8fd9\u4e9b\u610f\u56fe\u76f4\u63a5\u6761\u4ef6\u5316\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\uff0c\u751f\u6210\u7269\u7406\u5408\u7406\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\uff0c\u65e0\u9700\u663e\u5f0f\u59ff\u6001\u91cd\u5efa\u6216\u91cd\u5b9a\u5411\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RoboMirror\u7684\u6709\u6548\u6027\uff1a\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5b9e\u73b0\u8fdc\u7a0b\u5448\u73b0\uff0c\u5c06\u7b2c\u4e09\u4eba\u79f0\u63a7\u5236\u5ef6\u8fdf\u964d\u4f4e80%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad83.7%\u3002", "conclusion": "\u901a\u8fc7\u56f4\u7ed5\u89c6\u9891\u7406\u89e3\u91cd\u65b0\u6784\u5efa\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0cRoboMirror\u5f25\u5408\u4e86\u89c6\u89c9\u7406\u89e3\u548c\u884c\u52a8\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\"\u5148\u7406\u89e3\u540e\u6a21\u4eff\"\u7684\u8303\u5f0f\u3002"}}
{"id": "2512.23650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23650", "abs": "https://arxiv.org/abs/2512.23650", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Tao Huang", "Zhenguo Sun", "Yibo Peng", "Pengwei Wang", "Zhongyuan Wang", "Fangzhou Liu", "Chang Xu", "Shanghang Zhang"], "title": "Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control", "comment": null, "summary": "Humans intuitively move to sound, but current humanoid robots lack expressive improvisational capabilities, confined to predefined motions or sparse commands. Generating motion from audio and then retargeting it to robots relies on explicit motion reconstruction, leading to cascaded errors, high latency, and disjointed acoustic-actuation mapping. We propose RoboPerform, the first unified audio-to-locomotion framework that can directly generate music-driven dance and speech-driven co-speech gestures from audio. Guided by the core principle of \"motion = content + style\", the framework treats audio as implicit style signals and eliminates the need for explicit motion reconstruction. RoboPerform integrates a ResMoE teacher policy for adapting to diverse motion patterns and a diffusion-based student policy for audio style injection. This retargeting-free design ensures low latency and high fidelity. Experimental validation shows that RoboPerform achieves promising results in physical plausibility and audio alignment, successfully transforming robots into responsive performers capable of reacting to audio.", "AI": {"tldr": "RoboPerform\uff1a\u9996\u4e2a\u7edf\u4e00\u7684\u97f3\u9891\u5230\u8fd0\u52a8\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u4ece\u97f3\u9891\u751f\u6210\u97f3\u4e50\u9a71\u52a8\u7684\u821e\u8e48\u548c\u8bed\u97f3\u9a71\u52a8\u7684\u4f34\u968f\u624b\u52bf\uff0c\u65e0\u9700\u663e\u5f0f\u8fd0\u52a8\u91cd\u5efa\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9ad8\u4fdd\u771f", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u7f3a\u4e4f\u5373\u5174\u8868\u8fbe\u80fd\u529b\uff0c\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u52a8\u4f5c\u6216\u7a00\u758f\u6307\u4ee4\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u97f3\u9891\u751f\u6210\u8fd0\u52a8\u518d\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\uff0c\u5b58\u5728\u7ea7\u8054\u8bef\u5dee\u3001\u9ad8\u5ef6\u8fdf\u548c\u58f0\u5b66-\u9a71\u52a8\u6620\u5c04\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\"\u8fd0\u52a8=\u5185\u5bb9+\u98ce\u683c\"\u6838\u5fc3\u539f\u5219\uff0c\u5c06\u97f3\u9891\u4f5c\u4e3a\u9690\u5f0f\u98ce\u683c\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u8fd0\u52a8\u91cd\u5efa\u3002\u91c7\u7528ResMoE\u6559\u5e08\u7b56\u7565\u9002\u5e94\u591a\u6837\u5316\u8fd0\u52a8\u6a21\u5f0f\uff0c\u7ed3\u5408\u6269\u6563\u5f0f\u5b66\u751f\u7b56\u7565\u8fdb\u884c\u97f3\u9891\u98ce\u683c\u6ce8\u5165", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aRoboPerform\u5728\u7269\u7406\u5408\u7406\u6027\u548c\u97f3\u9891\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u6210\u529f\u5c06\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u80fd\u591f\u54cd\u5e94\u97f3\u9891\u7684\u8868\u6f14\u8005", "conclusion": "RoboPerform\u901a\u8fc7\u514d\u91cd\u5b9a\u5411\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u9ad8\u4fdd\u771f\u7684\u97f3\u9891\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\uff0c\u4e3a\u673a\u5668\u4eba\u8868\u6f14\u80fd\u529b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.23672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23672", "abs": "https://arxiv.org/abs/2512.23672", "authors": ["Mohammed Baziyad", "Manal Al Shohna", "Tamer Rabie"], "title": "The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation", "comment": null, "summary": "Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u63a8\u571f\u673a\"\u7684\u65b0\u578b\u8def\u5f84\u89c4\u5212\u6280\u672f\uff0c\u901a\u8fc7\u540e\u586b\u5145\u673a\u5236\u548c\u659c\u5761\u589e\u5f3a\u89e3\u51b3\u4eba\u5de5\u52bf\u573a\u6cd5\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u9677\u9631\u95ee\u9898\uff0c\u5728\u4fdd\u6301APF\u4f18\u70b9\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u5feb\u7684\u6267\u884c\u901f\u5ea6\u548c\u826f\u597d\u7684\u8def\u5f84\u8d28\u91cf\u3002", "motivation": "\u4eba\u5de5\u52bf\u573a\u6cd5\u5728\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u56e0\u7b80\u5355\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u8ba1\u7b97\u9700\u6c42\u4f4e\u800c\u53d7\u6b22\u8fce\uff0c\u4f46\u4f20\u7edfAPF\u65b9\u6cd5\u5b58\u5728\u5c40\u90e8\u6700\u5c0f\u503c\u9677\u9631\u95ee\u9898\uff0c\u673a\u5668\u4eba\u4f1a\u5361\u5728\u65e0\u6cd5\u660e\u786e\u671d\u5411\u76ee\u6807\u7684\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\"\u63a8\u571f\u673a\"\u6280\u672f\uff0c\u5f15\u5165\u540e\u586b\u5145\u673a\u5236\u7cfb\u7edf\u8bc6\u522b\u5e76\u6d88\u9664\u5c40\u90e8\u6700\u5c0f\u503c\u533a\u57df\uff08\u901a\u8fc7\u589e\u52a0\u5176\u52bf\u80fd\u503c\uff0c\u7c7b\u4f3c\u63a8\u571f\u673a\u586b\u5e73\u9053\u8def\u5751\u6d3c\uff09\uff0c\u5e76\u52a0\u5165\u659c\u5761\u589e\u5f3a\u5e2e\u52a9\u673a\u5668\u4eba\u4ece\u5c40\u90e8\u6700\u5c0f\u503c\u9677\u9631\u4e2d\u9003\u8131\u3002", "result": "\u901a\u8fc7\u7269\u7406\u79fb\u52a8\u673a\u5668\u4eba\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u5730\u56fe\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e0e\u6807\u51c6APF\u3001\u81ea\u9002\u5e94APF\u4ee5\u53caA*\u3001PRM\u3001RRT\u7b49\u6210\u719f\u89c4\u5212\u7b97\u6cd5\u6bd4\u8f83\uff0c\u63a8\u571f\u673a\u6280\u672f\u6709\u6548\u89e3\u51b3\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u6267\u884c\u901f\u5ea6\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u8def\u5f84\u8d28\u91cf\u3002\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u5668\u8bc4\u4f30\u786e\u8ba4\u89c4\u5212\u8def\u5f84\u7684\u5e73\u6ed1\u6027\u548c\u53ef\u8ddf\u8e2a\u6027\uff0c\u9002\u5408\u5b9e\u9645\u6267\u884c\u3002", "conclusion": "\u63a8\u571f\u673a\u6280\u672f\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u5de5\u52bf\u573a\u6cd5\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u9677\u9631\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86APF\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8def\u5f84\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23703", "abs": "https://arxiv.org/abs/2512.23703", "authors": ["Huajie Tan", "Sixiang Chen", "Yijie Xu", "Zixiao Wang", "Yuheng Ji", "Cheng Chi", "Yaoxu Lyu", "Zhongxia Zhao", "Xiansheng Chen", "Peterson Co", "Shaoxuan Xie", "Guocai Yao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation", "comment": "27 pages, 11 figures", "summary": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io", "AI": {"tldr": "Dopamine-Reward\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8f93\u5165\u5b66\u4e60\u901a\u7528\u3001\u6b65\u9aa4\u611f\u77e5\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u6838\u5fc3\u96be\u9898\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7684\u4e3b\u8981\u969c\u788d\u662f\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u5956\u52b1\u6a21\u578b\u7f3a\u4e4f\u6b65\u9aa4\u611f\u77e5\u7406\u89e3\u4e14\u4f9d\u8d56\u5355\u89c6\u89d2\u611f\u77e5\uff0c\u5bfc\u81f4\u5bf9\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u8fdb\u5c55\u7684\u8bc4\u4f30\u4e0d\u53ef\u9760\uff1b\u5956\u52b1\u5851\u9020\u8fc7\u7a0b\u7406\u8bba\u4e0d\u4e25\u8c28\uff0c\u5e38\u5bfc\u81f4\u8bed\u4e49\u9677\u9631\u8bef\u5bfc\u7b56\u7565\u4f18\u5316\u3002", "method": "\u63d0\u51faDopamine-Reward\u65b9\u6cd5\uff0c\u6838\u5fc3\u662f\u901a\u7528\u5956\u52b1\u6a21\u578b\uff08GRM\uff09\uff0c\u57283400+\u5c0f\u65f6\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u91c7\u7528\u6b65\u9aa4\u5956\u52b1\u79bb\u6563\u5316\u5b9e\u73b0\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u591a\u89c6\u89d2\u5956\u52b1\u878d\u5408\u514b\u670d\u611f\u77e5\u9650\u5236\u3002\u57fa\u4e8e\u6b64\u63d0\u51faDopamine-RL\u6846\u67b6\uff0c\u91c7\u7528\u7406\u8bba\u4e25\u8c28\u7684\u7b56\u7565\u4e0d\u53d8\u5956\u52b1\u5851\u9020\u65b9\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5229\u7528\u5bc6\u96c6\u5956\u52b1\u8fdb\u884c\u9ad8\u6548\u81ea\u6211\u6539\u8fdb\u800c\u4e0d\u6539\u53d8\u6700\u4f18\u7b56\u7565\u3002", "result": "GRM\u5728\u5956\u52b1\u8bc4\u4f30\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u5ea6\uff0c\u57fa\u4e8eGRM\u7684Dopamine-RL\u663e\u8457\u63d0\u9ad8\u7b56\u7565\u5b66\u4e60\u6548\u7387\u3002GRM\u901a\u8fc7\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\u4e00\u6b21\u6027\u9002\u5e94\u65b0\u4efb\u52a1\u540e\uff0cDopamine-RL\u4ec5\u9700150\u6b21\u5728\u7ebf\u5c1d\u8bd5\uff08\u7ea61\u5c0f\u65f6\u771f\u5b9e\u673a\u5668\u4eba\u4ea4\u4e92\uff09\u5c31\u80fd\u5c06\u7b56\u7565\u4ece\u63a5\u8fd1\u96f6\u6210\u529f\u7387\u63d0\u5347\u523095%\uff0c\u5e76\u5728\u4efb\u52a1\u95f4\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Dopamine-Reward\u901a\u8fc7\u6b65\u9aa4\u611f\u77e5\u548c\u591a\u89c6\u89d2\u878d\u5408\u7684\u5956\u52b1\u5efa\u6a21\uff0c\u7ed3\u5408\u7406\u8bba\u4e25\u8c28\u7684\u5956\u52b1\u5851\u9020\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u3002"}}
